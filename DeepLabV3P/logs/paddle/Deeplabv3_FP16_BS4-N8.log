WARNING: Logging before InitGoogleLogging() is written to STDERR
W0205 04:33:17.680743 41833 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.
The TensorRT that Paddle depends on is not configured correctly.
  Suggestions:
  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;%PATH%`
  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`
2022-02-05 04:33:18 [INFO]	
------------Environment Information-------------
platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.10 (default, Feb 20 2021, 21:17:23) [GCC 7.5.0]
Paddle compiled with cuda: True
NVCC: Build cuda_11.2.r11.2/compiler.29558016_0
cudnn: 8.1
GPUs used: 8
CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddleSeg: develop
PaddlePaddle: 0.0.0
OpenCV: 4.2.0
------------------------------------------------
2022-02-05 04:33:19 [INFO]	
---------------Config Information---------------
batch_size: 4
iters: 200
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
lr_scheduler:
  end_lr: 0
  learning_rate: 0.01
  power: 0.9
  type: PolynomialDecay
model:
  aspp_ratios:
  - 1
  - 12
  - 24
  - 36
  backbone:
    multi_grid:
    - 1
    - 2
    - 4
    output_stride: 8
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
    type: ResNet50_vd
  backbone_indices:
  - 0
  - 3
  num_classes: 19
  type: DeepLabV3P
optimizer:
  type: sgd
  weight_decay: 4.0e-05
to_static_training: false
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
W0205 04:33:19.098752 41833 device_context.cc:484] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0205 04:33:19.098785 41833 device_context.cc:503] device: 0, cuDNN Version: 8.1.
2022-02-05 04:33:22 [INFO]	Loading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
2022-02-05 04:33:23 [INFO]	There are 275/275 variables loaded into ResNet_vd.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:60710', '127.0.0.1:35590', '127.0.0.1:40171', '127.0.0.1:41645', '127.0.0.1:33554', '127.0.0.1:39540', '127.0.0.1:51444']
I0205 04:33:26.832818 41833 nccl_context.cc:82] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
I0205 04:33:27.785459 41833 nccl_context.cc:114] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 10
2022-02-05 04:33:28,393-INFO: [topology.py:169:__init__] HybridParallelInfo: rank_id: 0, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 8, mp_group: [0],  sharding_group: [0], pp_group: [0], dp_group: [0, 1, 2, 3, 4, 5, 6, 7], check/clip group: [0]
2022-02-05 04:33:28 [INFO]	use amp to train
/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/math_op_patch.py:259: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int64, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
2022-02-05 04:33:41 [INFO]	[TRAIN] epoch: 1, iter: 5/200, loss: 2.6842, lr: 0.009820, batch_cost: 2.5496, reader_cost: 0.65261, ips: 1.5689 samples/sec | ETA 00:08:17
2022-02-05 04:33:42 [INFO]	[TRAIN] epoch: 1, iter: 10/200, loss: 1.6422, lr: 0.009594, batch_cost: 0.3241, reader_cost: 0.00325, ips: 12.3435 samples/sec | ETA 00:01:01
2022-02-05 04:33:44 [INFO]	[TRAIN] epoch: 1, iter: 15/200, loss: 1.0442, lr: 0.009368, batch_cost: 0.3246, reader_cost: 0.00108, ips: 12.3235 samples/sec | ETA 00:01:00
2022-02-05 04:33:46 [INFO]	[TRAIN] epoch: 1, iter: 20/200, loss: 1.2958, lr: 0.009141, batch_cost: 0.3321, reader_cost: 0.00190, ips: 12.0455 samples/sec | ETA 00:00:59
2022-02-05 04:33:47 [INFO]	[TRAIN] epoch: 1, iter: 25/200, loss: 0.9670, lr: 0.008913, batch_cost: 0.3229, reader_cost: 0.00268, ips: 12.3864 samples/sec | ETA 00:00:56
2022-02-05 04:33:49 [INFO]	[TRAIN] epoch: 1, iter: 30/200, loss: 0.7031, lr: 0.008685, batch_cost: 0.3225, reader_cost: 0.00494, ips: 12.4043 samples/sec | ETA 00:00:54
2022-02-05 04:33:50 [INFO]	[TRAIN] epoch: 1, iter: 35/200, loss: 0.8410, lr: 0.008456, batch_cost: 0.3277, reader_cost: 0.00338, ips: 12.2062 samples/sec | ETA 00:00:54
2022-02-05 04:33:52 [INFO]	[TRAIN] epoch: 1, iter: 40/200, loss: 0.9067, lr: 0.008227, batch_cost: 0.3300, reader_cost: 0.00276, ips: 12.1194 samples/sec | ETA 00:00:52
2022-02-05 04:33:54 [INFO]	[TRAIN] epoch: 1, iter: 45/200, loss: 0.7434, lr: 0.007996, batch_cost: 0.3265, reader_cost: 0.00304, ips: 12.2512 samples/sec | ETA 00:00:50
2022-02-05 04:33:55 [INFO]	[TRAIN] epoch: 1, iter: 50/200, loss: 0.7764, lr: 0.007765, batch_cost: 0.3312, reader_cost: 0.00137, ips: 12.0775 samples/sec | ETA 00:00:49
2022-02-05 04:33:57 [INFO]	[TRAIN] epoch: 1, iter: 55/200, loss: 0.8578, lr: 0.007533, batch_cost: 0.3230, reader_cost: 0.00071, ips: 12.3827 samples/sec | ETA 00:00:46
2022-02-05 04:33:59 [INFO]	[TRAIN] epoch: 1, iter: 60/200, loss: 0.5185, lr: 0.007301, batch_cost: 0.3219, reader_cost: 0.00156, ips: 12.4281 samples/sec | ETA 00:00:45
2022-02-05 04:34:00 [INFO]	[TRAIN] epoch: 1, iter: 65/200, loss: 0.7373, lr: 0.007067, batch_cost: 0.3233, reader_cost: 0.00359, ips: 12.3737 samples/sec | ETA 00:00:43
2022-02-05 04:34:02 [INFO]	[TRAIN] epoch: 1, iter: 70/200, loss: 0.5305, lr: 0.006833, batch_cost: 0.3167, reader_cost: 0.00156, ips: 12.6321 samples/sec | ETA 00:00:41
2022-02-05 04:34:03 [INFO]	[TRAIN] epoch: 1, iter: 75/200, loss: 0.5510, lr: 0.006598, batch_cost: 0.3186, reader_cost: 0.00021, ips: 12.5544 samples/sec | ETA 00:00:39
2022-02-05 04:34:05 [INFO]	[TRAIN] epoch: 1, iter: 80/200, loss: 0.4868, lr: 0.006362, batch_cost: 0.3150, reader_cost: 0.00373, ips: 12.7001 samples/sec | ETA 00:00:37
2022-02-05 04:34:07 [INFO]	[TRAIN] epoch: 1, iter: 85/200, loss: 0.5827, lr: 0.006125, batch_cost: 0.3037, reader_cost: 0.00139, ips: 13.1710 samples/sec | ETA 00:00:34
2022-02-05 04:34:08 [INFO]	[TRAIN] epoch: 1, iter: 90/200, loss: 0.4985, lr: 0.005887, batch_cost: 0.2997, reader_cost: 0.00009, ips: 13.3469 samples/sec | ETA 00:00:32
2022-02-05 04:34:12 [INFO]	[TRAIN] epoch: 2, iter: 95/200, loss: 0.4607, lr: 0.005647, batch_cost: 0.8982, reader_cost: 0.50235, ips: 4.4533 samples/sec | ETA 00:01:34
2022-02-05 04:34:14 [INFO]	[TRAIN] epoch: 2, iter: 100/200, loss: 0.4454, lr: 0.005407, batch_cost: 0.3472, reader_cost: 0.00183, ips: 11.5224 samples/sec | ETA 00:00:34
2022-02-05 04:34:16 [INFO]	[TRAIN] epoch: 2, iter: 105/200, loss: 0.3840, lr: 0.005166, batch_cost: 0.3254, reader_cost: 0.00017, ips: 12.2933 samples/sec | ETA 00:00:30
2022-02-05 04:34:17 [INFO]	[TRAIN] epoch: 2, iter: 110/200, loss: 0.4441, lr: 0.004923, batch_cost: 0.3251, reader_cost: 0.00098, ips: 12.3027 samples/sec | ETA 00:00:29
2022-02-05 04:34:19 [INFO]	[TRAIN] epoch: 2, iter: 115/200, loss: 0.4028, lr: 0.004679, batch_cost: 0.3247, reader_cost: 0.00329, ips: 12.3184 samples/sec | ETA 00:00:27
2022-02-05 04:34:21 [INFO]	[TRAIN] epoch: 2, iter: 120/200, loss: 0.6332, lr: 0.004433, batch_cost: 0.3211, reader_cost: 0.00232, ips: 12.4576 samples/sec | ETA 00:00:25
2022-02-05 04:34:22 [INFO]	[TRAIN] epoch: 2, iter: 125/200, loss: 0.5137, lr: 0.004186, batch_cost: 0.3285, reader_cost: 0.00071, ips: 12.1752 samples/sec | ETA 00:00:24
2022-02-05 04:34:24 [INFO]	[TRAIN] epoch: 2, iter: 130/200, loss: 0.3371, lr: 0.003937, batch_cost: 0.3210, reader_cost: 0.00020, ips: 12.4605 samples/sec | ETA 00:00:22
2022-02-05 04:34:26 [INFO]	[TRAIN] epoch: 2, iter: 135/200, loss: 0.3927, lr: 0.003687, batch_cost: 0.3224, reader_cost: 0.00020, ips: 12.4052 samples/sec | ETA 00:00:20
2022-02-05 04:34:27 [INFO]	[TRAIN] epoch: 2, iter: 140/200, loss: 0.4388, lr: 0.003435, batch_cost: 0.3187, reader_cost: 0.00017, ips: 12.5520 samples/sec | ETA 00:00:19
2022-02-05 04:34:29 [INFO]	[TRAIN] epoch: 2, iter: 145/200, loss: 0.3639, lr: 0.003180, batch_cost: 0.3247, reader_cost: 0.00018, ips: 12.3204 samples/sec | ETA 00:00:17
2022-02-05 04:34:30 [INFO]	[TRAIN] epoch: 2, iter: 150/200, loss: 0.4906, lr: 0.002923, batch_cost: 0.3287, reader_cost: 0.00018, ips: 12.1703 samples/sec | ETA 00:00:16
2022-02-05 04:34:32 [INFO]	[TRAIN] epoch: 2, iter: 155/200, loss: 0.5656, lr: 0.002664, batch_cost: 0.3222, reader_cost: 0.00181, ips: 12.4142 samples/sec | ETA 00:00:14
2022-02-05 04:34:34 [INFO]	[TRAIN] epoch: 2, iter: 160/200, loss: 0.3813, lr: 0.002402, batch_cost: 0.3169, reader_cost: 0.00331, ips: 12.6231 samples/sec | ETA 00:00:12
2022-02-05 04:34:35 [INFO]	[TRAIN] epoch: 2, iter: 165/200, loss: 0.3363, lr: 0.002137, batch_cost: 0.3248, reader_cost: 0.00548, ips: 12.3136 samples/sec | ETA 00:00:11
2022-02-05 04:34:37 [INFO]	[TRAIN] epoch: 2, iter: 170/200, loss: 0.4190, lr: 0.001868, batch_cost: 0.3224, reader_cost: 0.00268, ips: 12.4065 samples/sec | ETA 00:00:09
2022-02-05 04:34:38 [INFO]	[TRAIN] epoch: 2, iter: 175/200, loss: 0.5696, lr: 0.001594, batch_cost: 0.3059, reader_cost: 0.00104, ips: 13.0772 samples/sec | ETA 00:00:07
2022-02-05 04:34:40 [INFO]	[TRAIN] epoch: 2, iter: 180/200, loss: 0.4213, lr: 0.001315, batch_cost: 0.3007, reader_cost: 0.00076, ips: 13.3026 samples/sec | ETA 00:00:06
2022-02-05 04:34:41 [INFO]	[TRAIN] epoch: 2, iter: 185/200, loss: 0.3723, lr: 0.001030, batch_cost: 0.2994, reader_cost: 0.00009, ips: 13.3618 samples/sec | ETA 00:00:04
2022-02-05 04:34:46 [INFO]	[TRAIN] epoch: 3, iter: 190/200, loss: 0.4311, lr: 0.000735, batch_cost: 0.9280, reader_cost: 0.47894, ips: 4.3104 samples/sec | ETA 00:00:09
2022-02-05 04:34:48 [INFO]	[TRAIN] epoch: 3, iter: 195/200, loss: 0.4931, lr: 0.000426, batch_cost: 0.3325, reader_cost: 0.00362, ips: 12.0306 samples/sec | ETA 00:00:01
2022-02-05 04:34:49 [INFO]	[TRAIN] epoch: 3, iter: 200/200, loss: 0.4326, lr: 0.000085, batch_cost: 0.3265, reader_cost: 0.00062, ips: 12.2524 samples/sec | ETA 00:00:00
<class 'paddle.nn.layer.pooling.AvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted
Customize Function has been applied to <class 'paddle.nn.layer.norm.SyncBatchNorm'>
<class 'paddle.nn.layer.activation.ReLU'>'s flops has been counted
Cannot find suitable count function for <class 'paddle.nn.layer.pooling.MaxPool2D'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.activation.Activation'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.wrap_functions.Add'>. Treat it as zero FLOPs.
<class 'paddle.nn.layer.pooling.AdaptiveAvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.common.Dropout'>'s flops has been counted
Total Flops: 228440246784     Total Params: 26794243
