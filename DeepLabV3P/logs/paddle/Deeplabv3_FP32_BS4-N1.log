WARNING: Logging before InitGoogleLogging() is written to STDERR
W0205 04:25:37.551187 23767 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.
The TensorRT that Paddle depends on is not configured correctly.
  Suggestions:
  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;%PATH%`
  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`
/home/crim/benchmark/PaddleSeg/paddleseg/models/losses/rmi_loss.py:78: DeprecationWarning: invalid escape sequence \i
  """
2022-02-05 04:25:38 [INFO]	
------------Environment Information-------------
platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.10 (default, Feb 20 2021, 21:17:23) [GCC 7.5.0]
Paddle compiled with cuda: True
NVCC: Build cuda_11.2.r11.2/compiler.29558016_0
cudnn: 8.1
GPUs used: 1
CUDA_VISIBLE_DEVICES: 5
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddleSeg: develop
PaddlePaddle: 0.0.0
OpenCV: 4.2.0
------------------------------------------------
2022-02-05 04:25:38 [INFO]	
---------------Config Information---------------
batch_size: 4
iters: 200
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
lr_scheduler:
  end_lr: 0
  learning_rate: 0.01
  power: 0.9
  type: PolynomialDecay
model:
  aspp_ratios:
  - 1
  - 12
  - 24
  - 36
  backbone:
    multi_grid:
    - 1
    - 2
    - 4
    output_stride: 8
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
    type: ResNet50_vd
  backbone_indices:
  - 0
  - 3
  num_classes: 19
  type: DeepLabV3P
optimizer:
  type: sgd
  weight_decay: 4.0e-05
to_static_training: false
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
W0205 04:25:38.975216 23767 device_context.cc:484] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0205 04:25:38.975245 23767 device_context.cc:503] device: 0, cuDNN Version: 8.1.
2022-02-05 04:25:41 [INFO]	Loading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
Connecting to https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
Downloading resnet50_vd_ssld_v2.tar.gz
[                                                  ] 0.00%[==                                                ] 4.65%[=======                                           ] 15.87%[==============                                    ] 28.35%[====================                              ] 40.88%[==========================                        ] 53.49%[================================                  ] 65.90%[=====================================             ] 74.95%[==========================================        ] 85.53%[================================================  ] 96.89%[==================================================] 100.00%
Uncompress resnet50_vd_ssld_v2.tar.gz
[                                                  ] 0.00%[=========================                         ] 50.00%[==================================================] 100.00%
2022-02-05 04:25:46 [INFO]	There are 275/275 variables loaded into ResNet_vd.
/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/math_op_patch.py:259: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int64, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
2022-02-05 04:25:54 [INFO]	[TRAIN] epoch: 1, iter: 5/200, loss: 2.2272, lr: 0.009820, batch_cost: 1.5326, reader_cost: 0.36675, ips: 2.6100 samples/sec | ETA 00:04:58
2022-02-05 04:25:57 [INFO]	[TRAIN] epoch: 1, iter: 10/200, loss: 1.9172, lr: 0.009594, batch_cost: 0.6474, reader_cost: 0.00152, ips: 6.1783 samples/sec | ETA 00:02:03
2022-02-05 04:26:01 [INFO]	[TRAIN] epoch: 1, iter: 15/200, loss: 1.5051, lr: 0.009368, batch_cost: 0.6493, reader_cost: 0.00150, ips: 6.1601 samples/sec | ETA 00:02:00
2022-02-05 04:26:04 [INFO]	[TRAIN] epoch: 1, iter: 20/200, loss: 1.3416, lr: 0.009141, batch_cost: 0.6490, reader_cost: 0.00181, ips: 6.1637 samples/sec | ETA 00:01:56
2022-02-05 04:26:07 [INFO]	[TRAIN] epoch: 1, iter: 25/200, loss: 1.2941, lr: 0.008913, batch_cost: 0.6500, reader_cost: 0.00141, ips: 6.1538 samples/sec | ETA 00:01:53
2022-02-05 04:26:10 [INFO]	[TRAIN] epoch: 1, iter: 30/200, loss: 1.6582, lr: 0.008685, batch_cost: 0.6495, reader_cost: 0.00221, ips: 6.1586 samples/sec | ETA 00:01:50
2022-02-05 04:26:14 [INFO]	[TRAIN] epoch: 1, iter: 35/200, loss: 1.4450, lr: 0.008456, batch_cost: 0.6509, reader_cost: 0.00186, ips: 6.1458 samples/sec | ETA 00:01:47
2022-02-05 04:26:17 [INFO]	[TRAIN] epoch: 1, iter: 40/200, loss: 1.3178, lr: 0.008227, batch_cost: 0.6491, reader_cost: 0.00161, ips: 6.1624 samples/sec | ETA 00:01:43
2022-02-05 04:26:20 [INFO]	[TRAIN] epoch: 1, iter: 45/200, loss: 1.5263, lr: 0.007996, batch_cost: 0.6493, reader_cost: 0.00064, ips: 6.1608 samples/sec | ETA 00:01:40
2022-02-05 04:26:23 [INFO]	[TRAIN] epoch: 1, iter: 50/200, loss: 0.8850, lr: 0.007765, batch_cost: 0.6524, reader_cost: 0.00208, ips: 6.1311 samples/sec | ETA 00:01:37
2022-02-05 04:26:27 [INFO]	[TRAIN] epoch: 1, iter: 55/200, loss: 0.9072, lr: 0.007533, batch_cost: 0.6519, reader_cost: 0.00263, ips: 6.1364 samples/sec | ETA 00:01:34
2022-02-05 04:26:30 [INFO]	[TRAIN] epoch: 1, iter: 60/200, loss: 1.5256, lr: 0.007301, batch_cost: 0.6527, reader_cost: 0.00235, ips: 6.1288 samples/sec | ETA 00:01:31
2022-02-05 04:26:33 [INFO]	[TRAIN] epoch: 1, iter: 65/200, loss: 0.9676, lr: 0.007067, batch_cost: 0.6518, reader_cost: 0.00269, ips: 6.1370 samples/sec | ETA 00:01:27
2022-02-05 04:26:36 [INFO]	[TRAIN] epoch: 1, iter: 70/200, loss: 1.2087, lr: 0.006833, batch_cost: 0.6532, reader_cost: 0.00256, ips: 6.1234 samples/sec | ETA 00:01:24
2022-02-05 04:26:40 [INFO]	[TRAIN] epoch: 1, iter: 75/200, loss: 0.9867, lr: 0.006598, batch_cost: 0.6520, reader_cost: 0.00237, ips: 6.1350 samples/sec | ETA 00:01:21
2022-02-05 04:26:43 [INFO]	[TRAIN] epoch: 1, iter: 80/200, loss: 0.9420, lr: 0.006362, batch_cost: 0.6526, reader_cost: 0.00251, ips: 6.1293 samples/sec | ETA 00:01:18
2022-02-05 04:26:46 [INFO]	[TRAIN] epoch: 1, iter: 85/200, loss: 1.2080, lr: 0.006125, batch_cost: 0.6535, reader_cost: 0.00250, ips: 6.1212 samples/sec | ETA 00:01:15
2022-02-05 04:26:49 [INFO]	[TRAIN] epoch: 1, iter: 90/200, loss: 0.9289, lr: 0.005887, batch_cost: 0.6521, reader_cost: 0.00249, ips: 6.1339 samples/sec | ETA 00:01:11
2022-02-05 04:26:53 [INFO]	[TRAIN] epoch: 1, iter: 95/200, loss: 0.7476, lr: 0.005647, batch_cost: 0.6521, reader_cost: 0.00149, ips: 6.1337 samples/sec | ETA 00:01:08
2022-02-05 04:26:56 [INFO]	[TRAIN] epoch: 1, iter: 100/200, loss: 1.0775, lr: 0.005407, batch_cost: 0.6521, reader_cost: 0.00246, ips: 6.1338 samples/sec | ETA 00:01:05
2022-02-05 04:26:59 [INFO]	[TRAIN] epoch: 1, iter: 105/200, loss: 0.7555, lr: 0.005166, batch_cost: 0.6515, reader_cost: 0.00198, ips: 6.1394 samples/sec | ETA 00:01:01
2022-02-05 04:27:02 [INFO]	[TRAIN] epoch: 1, iter: 110/200, loss: 1.1007, lr: 0.004923, batch_cost: 0.6518, reader_cost: 0.00200, ips: 6.1372 samples/sec | ETA 00:00:58
2022-02-05 04:27:06 [INFO]	[TRAIN] epoch: 1, iter: 115/200, loss: 1.0076, lr: 0.004679, batch_cost: 0.6508, reader_cost: 0.00178, ips: 6.1462 samples/sec | ETA 00:00:55
2022-02-05 04:27:09 [INFO]	[TRAIN] epoch: 1, iter: 120/200, loss: 0.9074, lr: 0.004433, batch_cost: 0.6530, reader_cost: 0.00136, ips: 6.1258 samples/sec | ETA 00:00:52
2022-02-05 04:27:12 [INFO]	[TRAIN] epoch: 1, iter: 125/200, loss: 0.9724, lr: 0.004186, batch_cost: 0.6534, reader_cost: 0.00186, ips: 6.1214 samples/sec | ETA 00:00:49
2022-02-05 04:27:15 [INFO]	[TRAIN] epoch: 1, iter: 130/200, loss: 0.7142, lr: 0.003937, batch_cost: 0.6517, reader_cost: 0.00234, ips: 6.1376 samples/sec | ETA 00:00:45
2022-02-05 04:27:19 [INFO]	[TRAIN] epoch: 1, iter: 135/200, loss: 0.8439, lr: 0.003687, batch_cost: 0.6519, reader_cost: 0.00111, ips: 6.1357 samples/sec | ETA 00:00:42
2022-02-05 04:27:22 [INFO]	[TRAIN] epoch: 1, iter: 140/200, loss: 0.8576, lr: 0.003435, batch_cost: 0.6534, reader_cost: 0.00090, ips: 6.1221 samples/sec | ETA 00:00:39
2022-02-05 04:27:25 [INFO]	[TRAIN] epoch: 1, iter: 145/200, loss: 0.9565, lr: 0.003180, batch_cost: 0.6521, reader_cost: 0.00179, ips: 6.1344 samples/sec | ETA 00:00:35
2022-02-05 04:27:29 [INFO]	[TRAIN] epoch: 1, iter: 150/200, loss: 0.8245, lr: 0.002923, batch_cost: 0.6524, reader_cost: 0.00246, ips: 6.1311 samples/sec | ETA 00:00:32
2022-02-05 04:27:32 [INFO]	[TRAIN] epoch: 1, iter: 155/200, loss: 0.7315, lr: 0.002664, batch_cost: 0.6511, reader_cost: 0.00088, ips: 6.1433 samples/sec | ETA 00:00:29
2022-02-05 04:27:35 [INFO]	[TRAIN] epoch: 1, iter: 160/200, loss: 0.9889, lr: 0.002402, batch_cost: 0.6515, reader_cost: 0.00116, ips: 6.1400 samples/sec | ETA 00:00:26
2022-02-05 04:27:38 [INFO]	[TRAIN] epoch: 1, iter: 165/200, loss: 0.7616, lr: 0.002137, batch_cost: 0.6510, reader_cost: 0.00052, ips: 6.1448 samples/sec | ETA 00:00:22
2022-02-05 04:27:42 [INFO]	[TRAIN] epoch: 1, iter: 170/200, loss: 0.7045, lr: 0.001868, batch_cost: 0.6519, reader_cost: 0.00184, ips: 6.1361 samples/sec | ETA 00:00:19
2022-02-05 04:27:45 [INFO]	[TRAIN] epoch: 1, iter: 175/200, loss: 0.8638, lr: 0.001594, batch_cost: 0.6513, reader_cost: 0.00061, ips: 6.1417 samples/sec | ETA 00:00:16
2022-02-05 04:27:48 [INFO]	[TRAIN] epoch: 1, iter: 180/200, loss: 0.6456, lr: 0.001315, batch_cost: 0.6535, reader_cost: 0.00187, ips: 6.1207 samples/sec | ETA 00:00:13
2022-02-05 04:27:51 [INFO]	[TRAIN] epoch: 1, iter: 185/200, loss: 0.8971, lr: 0.001030, batch_cost: 0.6522, reader_cost: 0.00177, ips: 6.1334 samples/sec | ETA 00:00:09
2022-02-05 04:27:55 [INFO]	[TRAIN] epoch: 1, iter: 190/200, loss: 0.9045, lr: 0.000735, batch_cost: 0.6531, reader_cost: 0.00153, ips: 6.1243 samples/sec | ETA 00:00:06
2022-02-05 04:27:58 [INFO]	[TRAIN] epoch: 1, iter: 195/200, loss: 0.9522, lr: 0.000426, batch_cost: 0.6512, reader_cost: 0.00108, ips: 6.1421 samples/sec | ETA 00:00:03
2022-02-05 04:28:01 [INFO]	[TRAIN] epoch: 1, iter: 200/200, loss: 0.7405, lr: 0.000085, batch_cost: 0.6531, reader_cost: 0.00156, ips: 6.1245 samples/sec | ETA 00:00:00
<class 'paddle.nn.layer.pooling.AvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted
<class 'paddle.nn.layer.norm.BatchNorm2D'>'s flops has been counted
<class 'paddle.nn.layer.activation.ReLU'>'s flops has been counted
Cannot find suitable count function for <class 'paddle.nn.layer.pooling.MaxPool2D'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.activation.Activation'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.wrap_functions.Add'>. Treat it as zero FLOPs.
<class 'paddle.nn.layer.pooling.AdaptiveAvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.common.Dropout'>'s flops has been counted
Total Flops: 228440246784     Total Params: 26794243
