WARNING: Logging before InitGoogleLogging() is written to STDERR
W0205 04:31:44.478247 40892 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.
The TensorRT that Paddle depends on is not configured correctly.
  Suggestions:
  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;%PATH%`
  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`
2022-02-05 04:31:45 [INFO]	
------------Environment Information-------------
platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.10 (default, Feb 20 2021, 21:17:23) [GCC 7.5.0]
Paddle compiled with cuda: True
NVCC: Build cuda_11.2.r11.2/compiler.29558016_0
cudnn: 8.1
GPUs used: 1
CUDA_VISIBLE_DEVICES: 5
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddleSeg: develop
PaddlePaddle: 0.0.0
OpenCV: 4.2.0
------------------------------------------------
2022-02-05 04:31:45 [INFO]	
---------------Config Information---------------
batch_size: 4
iters: 200
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
lr_scheduler:
  end_lr: 0
  learning_rate: 0.01
  power: 0.9
  type: PolynomialDecay
model:
  aspp_ratios:
  - 1
  - 12
  - 24
  - 36
  backbone:
    multi_grid:
    - 1
    - 2
    - 4
    output_stride: 8
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
    type: ResNet50_vd
  backbone_indices:
  - 0
  - 3
  num_classes: 19
  type: DeepLabV3P
optimizer:
  type: sgd
  weight_decay: 4.0e-05
to_static_training: false
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
W0205 04:31:45.757246 40892 device_context.cc:484] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W0205 04:31:45.757275 40892 device_context.cc:503] device: 0, cuDNN Version: 8.1.
2022-02-05 04:31:48 [INFO]	Loading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
2022-02-05 04:31:49 [INFO]	There are 275/275 variables loaded into ResNet_vd.
2022-02-05 04:31:49 [INFO]	use amp to train
/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/math_op_patch.py:259: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int64, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
2022-02-05 04:31:58 [INFO]	[TRAIN] epoch: 1, iter: 5/200, loss: 2.9883, lr: 0.009820, batch_cost: 1.7446, reader_cost: 0.37615, ips: 2.2928 samples/sec | ETA 00:05:40
2022-02-05 04:31:59 [INFO]	[TRAIN] epoch: 1, iter: 10/200, loss: 1.9683, lr: 0.009594, batch_cost: 0.2608, reader_cost: 0.00176, ips: 15.3378 samples/sec | ETA 00:00:49
2022-02-05 04:32:01 [INFO]	[TRAIN] epoch: 1, iter: 15/200, loss: 1.7255, lr: 0.009368, batch_cost: 0.2583, reader_cost: 0.00016, ips: 15.4875 samples/sec | ETA 00:00:47
2022-02-05 04:32:02 [INFO]	[TRAIN] epoch: 1, iter: 20/200, loss: 1.8322, lr: 0.009141, batch_cost: 0.2649, reader_cost: 0.00113, ips: 15.0994 samples/sec | ETA 00:00:47
2022-02-05 04:32:03 [INFO]	[TRAIN] epoch: 1, iter: 25/200, loss: 1.5818, lr: 0.008913, batch_cost: 0.2670, reader_cost: 0.00222, ips: 14.9826 samples/sec | ETA 00:00:46
2022-02-05 04:32:04 [INFO]	[TRAIN] epoch: 1, iter: 30/200, loss: 1.6365, lr: 0.008685, batch_cost: 0.2617, reader_cost: 0.00281, ips: 15.2856 samples/sec | ETA 00:00:44
2022-02-05 04:32:06 [INFO]	[TRAIN] epoch: 1, iter: 35/200, loss: 0.9700, lr: 0.008456, batch_cost: 0.2613, reader_cost: 0.00202, ips: 15.3086 samples/sec | ETA 00:00:43
2022-02-05 04:32:07 [INFO]	[TRAIN] epoch: 1, iter: 40/200, loss: 1.5026, lr: 0.008227, batch_cost: 0.2599, reader_cost: 0.00204, ips: 15.3906 samples/sec | ETA 00:00:41
2022-02-05 04:32:08 [INFO]	[TRAIN] epoch: 1, iter: 45/200, loss: 1.2014, lr: 0.007996, batch_cost: 0.2621, reader_cost: 0.00233, ips: 15.2597 samples/sec | ETA 00:00:40
2022-02-05 04:32:10 [INFO]	[TRAIN] epoch: 1, iter: 50/200, loss: 1.2147, lr: 0.007765, batch_cost: 0.2621, reader_cost: 0.00105, ips: 15.2619 samples/sec | ETA 00:00:39
2022-02-05 04:32:11 [INFO]	[TRAIN] epoch: 1, iter: 55/200, loss: 0.9739, lr: 0.007533, batch_cost: 0.2597, reader_cost: 0.00248, ips: 15.4022 samples/sec | ETA 00:00:37
2022-02-05 04:32:12 [INFO]	[TRAIN] epoch: 1, iter: 60/200, loss: 1.0324, lr: 0.007301, batch_cost: 0.2622, reader_cost: 0.00198, ips: 15.2558 samples/sec | ETA 00:00:36
2022-02-05 04:32:14 [INFO]	[TRAIN] epoch: 1, iter: 65/200, loss: 0.9134, lr: 0.007067, batch_cost: 0.2604, reader_cost: 0.00203, ips: 15.3607 samples/sec | ETA 00:00:35
2022-02-05 04:32:15 [INFO]	[TRAIN] epoch: 1, iter: 70/200, loss: 0.8706, lr: 0.006833, batch_cost: 0.2616, reader_cost: 0.00207, ips: 15.2907 samples/sec | ETA 00:00:34
2022-02-05 04:32:16 [INFO]	[TRAIN] epoch: 1, iter: 75/200, loss: 1.0932, lr: 0.006598, batch_cost: 0.2609, reader_cost: 0.00190, ips: 15.3302 samples/sec | ETA 00:00:32
2022-02-05 04:32:18 [INFO]	[TRAIN] epoch: 1, iter: 80/200, loss: 1.0546, lr: 0.006362, batch_cost: 0.2615, reader_cost: 0.00203, ips: 15.2989 samples/sec | ETA 00:00:31
2022-02-05 04:32:19 [INFO]	[TRAIN] epoch: 1, iter: 85/200, loss: 1.0612, lr: 0.006125, batch_cost: 0.2614, reader_cost: 0.00221, ips: 15.3049 samples/sec | ETA 00:00:30
2022-02-05 04:32:20 [INFO]	[TRAIN] epoch: 1, iter: 90/200, loss: 1.0727, lr: 0.005887, batch_cost: 0.2599, reader_cost: 0.00138, ips: 15.3917 samples/sec | ETA 00:00:28
2022-02-05 04:32:21 [INFO]	[TRAIN] epoch: 1, iter: 95/200, loss: 1.0454, lr: 0.005647, batch_cost: 0.2593, reader_cost: 0.00144, ips: 15.4279 samples/sec | ETA 00:00:27
2022-02-05 04:32:23 [INFO]	[TRAIN] epoch: 1, iter: 100/200, loss: 1.1094, lr: 0.005407, batch_cost: 0.2595, reader_cost: 0.00141, ips: 15.4143 samples/sec | ETA 00:00:25
2022-02-05 04:32:24 [INFO]	[TRAIN] epoch: 1, iter: 105/200, loss: 1.0116, lr: 0.005166, batch_cost: 0.2590, reader_cost: 0.00110, ips: 15.4460 samples/sec | ETA 00:00:24
2022-02-05 04:32:25 [INFO]	[TRAIN] epoch: 1, iter: 110/200, loss: 0.8947, lr: 0.004923, batch_cost: 0.2599, reader_cost: 0.00166, ips: 15.3900 samples/sec | ETA 00:00:23
2022-02-05 04:32:27 [INFO]	[TRAIN] epoch: 1, iter: 115/200, loss: 1.3101, lr: 0.004679, batch_cost: 0.2587, reader_cost: 0.00267, ips: 15.4599 samples/sec | ETA 00:00:21
2022-02-05 04:32:28 [INFO]	[TRAIN] epoch: 1, iter: 120/200, loss: 1.1282, lr: 0.004433, batch_cost: 0.2579, reader_cost: 0.00141, ips: 15.5103 samples/sec | ETA 00:00:20
2022-02-05 04:32:29 [INFO]	[TRAIN] epoch: 1, iter: 125/200, loss: 0.7833, lr: 0.004186, batch_cost: 0.2578, reader_cost: 0.00101, ips: 15.5159 samples/sec | ETA 00:00:19
2022-02-05 04:32:30 [INFO]	[TRAIN] epoch: 1, iter: 130/200, loss: 0.8068, lr: 0.003937, batch_cost: 0.2586, reader_cost: 0.00066, ips: 15.4654 samples/sec | ETA 00:00:18
2022-02-05 04:32:32 [INFO]	[TRAIN] epoch: 1, iter: 135/200, loss: 0.8676, lr: 0.003687, batch_cost: 0.2613, reader_cost: 0.00198, ips: 15.3055 samples/sec | ETA 00:00:16
2022-02-05 04:32:33 [INFO]	[TRAIN] epoch: 1, iter: 140/200, loss: 0.8124, lr: 0.003435, batch_cost: 0.2593, reader_cost: 0.00179, ips: 15.4234 samples/sec | ETA 00:00:15
2022-02-05 04:32:34 [INFO]	[TRAIN] epoch: 1, iter: 145/200, loss: 0.7234, lr: 0.003180, batch_cost: 0.2596, reader_cost: 0.00220, ips: 15.4063 samples/sec | ETA 00:00:14
2022-02-05 04:32:36 [INFO]	[TRAIN] epoch: 1, iter: 150/200, loss: 1.0538, lr: 0.002923, batch_cost: 0.2571, reader_cost: 0.00062, ips: 15.5557 samples/sec | ETA 00:00:12
2022-02-05 04:32:37 [INFO]	[TRAIN] epoch: 1, iter: 155/200, loss: 0.7461, lr: 0.002664, batch_cost: 0.2568, reader_cost: 0.00052, ips: 15.5757 samples/sec | ETA 00:00:11
2022-02-05 04:32:38 [INFO]	[TRAIN] epoch: 1, iter: 160/200, loss: 0.8829, lr: 0.002402, batch_cost: 0.2575, reader_cost: 0.00051, ips: 15.5312 samples/sec | ETA 00:00:10
2022-02-05 04:32:40 [INFO]	[TRAIN] epoch: 1, iter: 165/200, loss: 0.7292, lr: 0.002137, batch_cost: 0.2582, reader_cost: 0.00064, ips: 15.4944 samples/sec | ETA 00:00:09
2022-02-05 04:32:41 [INFO]	[TRAIN] epoch: 1, iter: 170/200, loss: 0.7215, lr: 0.001868, batch_cost: 0.2587, reader_cost: 0.00206, ips: 15.4616 samples/sec | ETA 00:00:07
2022-02-05 04:32:42 [INFO]	[TRAIN] epoch: 1, iter: 175/200, loss: 0.7484, lr: 0.001594, batch_cost: 0.2599, reader_cost: 0.00248, ips: 15.3893 samples/sec | ETA 00:00:06
2022-02-05 04:32:43 [INFO]	[TRAIN] epoch: 1, iter: 180/200, loss: 0.6790, lr: 0.001315, batch_cost: 0.2593, reader_cost: 0.00155, ips: 15.4255 samples/sec | ETA 00:00:05
2022-02-05 04:32:45 [INFO]	[TRAIN] epoch: 1, iter: 185/200, loss: 0.8693, lr: 0.001030, batch_cost: 0.2580, reader_cost: 0.00016, ips: 15.5033 samples/sec | ETA 00:00:03
2022-02-05 04:32:46 [INFO]	[TRAIN] epoch: 1, iter: 190/200, loss: 0.7412, lr: 0.000735, batch_cost: 0.2573, reader_cost: 0.00114, ips: 15.5453 samples/sec | ETA 00:00:02
2022-02-05 04:32:47 [INFO]	[TRAIN] epoch: 1, iter: 195/200, loss: 0.7222, lr: 0.000426, batch_cost: 0.2580, reader_cost: 0.00062, ips: 15.5057 samples/sec | ETA 00:00:01
2022-02-05 04:32:49 [INFO]	[TRAIN] epoch: 1, iter: 200/200, loss: 0.7363, lr: 0.000085, batch_cost: 0.2580, reader_cost: 0.00016, ips: 15.5023 samples/sec | ETA 00:00:00
<class 'paddle.nn.layer.pooling.AvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted
<class 'paddle.nn.layer.norm.BatchNorm2D'>'s flops has been counted
<class 'paddle.nn.layer.activation.ReLU'>'s flops has been counted
Cannot find suitable count function for <class 'paddle.nn.layer.pooling.MaxPool2D'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.activation.Activation'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.wrap_functions.Add'>. Treat it as zero FLOPs.
<class 'paddle.nn.layer.pooling.AdaptiveAvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.common.Dropout'>'s flops has been counted
Total Flops: 228440246784     Total Params: 26794243
