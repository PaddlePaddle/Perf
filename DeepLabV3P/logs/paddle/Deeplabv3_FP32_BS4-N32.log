-----------  Configuration Arguments -----------
gpus: 0,1,2,3,4,5,6,7
heter_worker_num: None
heter_workers: 
http_port: None
ips: 127.0.0.1
log_dir: log
nproc_per_node: None
run_mode: None
server_num: None
servers: 
training_script: train.py
training_script_args: ['--config', 'benchmark/deeplabv3p.yml', '--iters', '20', '--log_iters', '1', '--batch_size', '4', '--num_workers', '8', '--data_format', 'NCHW']
worker_num: None
workers: 
------------------------------------------------
launch train in GPU mode!
launch proc_id:4643 idx:0
launch proc_id:4646 idx:1
launch proc_id:4649 idx:2
launch proc_id:4652 idx:3
launch proc_id:4655 idx:4
launch proc_id:4658 idx:5
launch proc_id:4661 idx:6
launch proc_id:4664 idx:7
/usr/local/lib/python3.7/dist-packages/scipy/linalg/__init__.py:212: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.
  from numpy.dual import register_func
/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,
2021-05-31 20:11:48 [INFO]	
------------Environment Information-------------
platform: Linux-4.14.0_1-0-0-32-x86_64-with-Ubuntu-18.04-bionic
Python: 3.7.10 (default, Feb 20 2021, 21:17:23) [GCC 7.5.0]
Paddle compiled with cuda: True
NVCC: Build cuda_11.0_bu.TC445_37.28845127_0
cudnn: 8.0
GPUs used: 32
CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddlePaddle: 0.0.0
OpenCV: 4.0.0
------------------------------------------------
2021-05-31 20:11:48 [INFO]	
---------------Config Information---------------
batch_size: 4
iters: 20
learning_rate:
  decay:
    end_lr: 0.0
    power: 0.9
    type: poly
  value: 0.01
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
model:
  aspp_ratios:
  - 1
  - 12
  - 24
  - 36
  backbone:
    multi_grid:
    - 1
    - 2
    - 4
    output_stride: 8
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/resnet50_vd_ssld_v2.tar.gz
    type: ResNet50_vd
  backbone_indices:
  - 0
  - 3
  num_classes: 19
  type: DeepLabV3P
optimizer:
  type: sgd
  weight_decay: 4.0e-05
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
2021-05-31 20:12:08 [INFO]	[TRAIN] epoch: 1, iter: 1/20, loss: 2.8204, lr: 0.010000, batch_cost: 8.4547, reader_cost: 2.75894, ips: 0.4731 samples/sec | ETA 00:02:40
2021-05-31 20:12:09 [INFO]	[TRAIN] epoch: 1, iter: 2/20, loss: 2.7339, lr: 0.009549, batch_cost: 0.7719, reader_cost: 0.00317, ips: 5.1817 samples/sec | ETA 00:00:13
2021-05-31 20:12:09 [INFO]	[TRAIN] epoch: 1, iter: 3/20, loss: 2.4438, lr: 0.009095, batch_cost: 0.7599, reader_cost: 0.00015, ips: 5.2640 samples/sec | ETA 00:00:12
2021-05-31 20:12:10 [INFO]	[TRAIN] epoch: 1, iter: 4/20, loss: 2.1860, lr: 0.008639, batch_cost: 0.7808, reader_cost: 0.00242, ips: 5.1228 samples/sec | ETA 00:00:12
2021-05-31 20:12:11 [INFO]	[TRAIN] epoch: 1, iter: 5/20, loss: 2.0671, lr: 0.008181, batch_cost: 0.7761, reader_cost: 0.00220, ips: 5.1540 samples/sec | ETA 00:00:11
2021-05-31 20:12:12 [INFO]	[TRAIN] epoch: 1, iter: 6/20, loss: 1.6174, lr: 0.007719, batch_cost: 0.7483, reader_cost: 0.00337, ips: 5.3458 samples/sec | ETA 00:00:10
2021-05-31 20:12:12 [INFO]	[TRAIN] epoch: 1, iter: 7/20, loss: 1.5665, lr: 0.007254, batch_cost: 0.7582, reader_cost: 0.00387, ips: 5.2758 samples/sec | ETA 00:00:09
2021-05-31 20:12:13 [INFO]	[TRAIN] epoch: 1, iter: 8/20, loss: 1.6083, lr: 0.006786, batch_cost: 0.7324, reader_cost: 0.00346, ips: 5.4615 samples/sec | ETA 00:00:08
2021-05-31 20:12:14 [INFO]	[TRAIN] epoch: 1, iter: 9/20, loss: 1.2350, lr: 0.006314, batch_cost: 0.7266, reader_cost: 0.00010, ips: 5.5053 samples/sec | ETA 00:00:07
2021-05-31 20:12:15 [INFO]	[TRAIN] epoch: 1, iter: 10/20, loss: 0.8736, lr: 0.005839, batch_cost: 0.7223, reader_cost: 0.00221, ips: 5.5376 samples/sec | ETA 00:00:07
2021-05-31 20:12:15 [INFO]	[TRAIN] epoch: 1, iter: 11/20, loss: 1.0210, lr: 0.005359, batch_cost: 0.7266, reader_cost: 0.00225, ips: 5.5052 samples/sec | ETA 00:00:06
2021-05-31 20:12:16 [INFO]	[TRAIN] epoch: 1, iter: 12/20, loss: 0.7673, lr: 0.004874, batch_cost: 0.7341, reader_cost: 0.00334, ips: 5.4485 samples/sec | ETA 00:00:05
2021-05-31 20:12:17 [INFO]	[TRAIN] epoch: 1, iter: 13/20, loss: 1.0239, lr: 0.004384, batch_cost: 0.7221, reader_cost: 0.00267, ips: 5.5390 samples/sec | ETA 00:00:05
2021-05-31 20:12:18 [INFO]	[TRAIN] epoch: 1, iter: 14/20, loss: 1.7465, lr: 0.003887, batch_cost: 0.7474, reader_cost: 0.00008, ips: 5.3520 samples/sec | ETA 00:00:04
2021-05-31 20:12:18 [INFO]	[TRAIN] epoch: 1, iter: 15/20, loss: 1.0146, lr: 0.003384, batch_cost: 0.7307, reader_cost: 0.00391, ips: 5.4742 samples/sec | ETA 00:00:03
2021-05-31 20:12:19 [INFO]	[TRAIN] epoch: 1, iter: 16/20, loss: 0.5210, lr: 0.002872, batch_cost: 0.7285, reader_cost: 0.00008, ips: 5.4907 samples/sec | ETA 00:00:02
2021-05-31 20:12:20 [INFO]	[TRAIN] epoch: 1, iter: 17/20, loss: 0.9790, lr: 0.002349, batch_cost: 0.7288, reader_cost: 0.00108, ips: 5.4888 samples/sec | ETA 00:00:02
2021-05-31 20:12:20 [INFO]	[TRAIN] epoch: 1, iter: 18/20, loss: 0.9410, lr: 0.001813, batch_cost: 0.7352, reader_cost: 0.00112, ips: 5.4407 samples/sec | ETA 00:00:01
2021-05-31 20:12:21 [INFO]	[TRAIN] epoch: 1, iter: 19/20, loss: 0.7847, lr: 0.001259, batch_cost: 0.7264, reader_cost: 0.00116, ips: 5.5066 samples/sec | ETA 00:00:00
2021-05-31 20:12:22 [INFO]	[TRAIN] epoch: 1, iter: 20/20, loss: 1.3367, lr: 0.000675, batch_cost: 0.7225, reader_cost: 0.00009, ips: 5.5365 samples/sec | ETA 00:00:00
<class 'paddle.nn.layer.pooling.AvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted
Customize Function has been applied to <class 'paddle.nn.layer.norm.SyncBatchNorm'>
<class 'paddle.nn.layer.activation.ReLU'>'s flops has been counted
Cannot find suitable count function for <class 'paddle.nn.layer.pooling.MaxPool2D'>. Treat it as zero FLOPs.
Cannot find suitable count function for <class 'paddleseg.models.layers.activation.Activation'>. Treat it as zero FLOPs.
<class 'paddle.nn.layer.pooling.AdaptiveAvgPool2D'>'s flops has been counted
<class 'paddle.nn.layer.common.Dropout'>'s flops has been counted
/usr/local/lib/python3.7/dist-packages/paddle/tensor/creation.py:135: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
Total Flops: 645499392     Total Params: 26794243
