/bin/sh: /usr/local/cuda/bin/nvcc: No such file or directory
2022-06-10 17:42:57 [INFO]	
------------Environment Information-------------
platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-debian-stretch-sid
Python: 3.7.0 (default, Jun  7 2022, 14:17:04) [GCC 8.2.0]
Paddle compiled with cuda: True
NVCC: Not Available
cudnn: 8.1
GPUs used: 1
CUDA_VISIBLE_DEVICES: 5
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddleSeg: develop
PaddlePaddle: 2.3.0
OpenCV: 4.2.0
------------------------------------------------
2022-06-10 17:42:57 [INFO]	
---------------Config Information---------------
batch_size: 8
iters: 200
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
lr_scheduler:
  end_lr: 0
  learning_rate: 0.01
  power: 0.9
  type: PolynomialDecay
model:
  backbone:
    padding_same: false
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz
    type: HRNet_W18
  backbone_indices:
  - -1
  bias: false
  num_classes: 19
  type: FCN
optimizer:
  type: sgd
  weight_decay: 0.0005
to_static_training: false
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
W0610 17:42:58.019815 34369 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 11.2
W0610 17:42:58.019852 34369 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.
2022-06-10 17:43:01 [INFO]	Loading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz
2022-06-10 17:43:03 [INFO]	There are 1525/1525 variables loaded into HRNet.
2022-06-10 17:43:03 [INFO]	use AMP to train. AMP level = O2
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:131: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._found_inf = to_variable(np.array([0]).astype(np.bool))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:133: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.array([0]).astype(np.bool))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:135: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.array([0]).astype(np.bool))
/usr/local/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.
  "When training, we now always track global mean and variance.")
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:278: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float16, but right dtype is paddle.float32, the right dtype will convert to paddle.float16
  format(lhs_dtype, rhs_dtype, lhs_dtype))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:278: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int64, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
2022-06-10 17:43:10 [INFO]	[TRAIN] epoch: 1, iter: 5/200, loss: 2.6996, lr: 0.009820, batch_cost: 1.4746, reader_cost: 0.51823, ips: 5.4251 samples/sec | ETA 00:04:47
2022-06-10 17:43:13 [INFO]	[TRAIN] epoch: 1, iter: 10/200, loss: 1.7086, lr: 0.009594, batch_cost: 0.4972, reader_cost: 0.00315, ips: 16.0900 samples/sec | ETA 00:01:34
2022-06-10 17:43:15 [INFO]	[TRAIN] epoch: 1, iter: 15/200, loss: 1.1910, lr: 0.009368, batch_cost: 0.5009, reader_cost: 0.00201, ips: 15.9725 samples/sec | ETA 00:01:32
2022-06-10 17:43:18 [INFO]	[TRAIN] epoch: 1, iter: 20/200, loss: 1.2460, lr: 0.009141, batch_cost: 0.5198, reader_cost: 0.00317, ips: 15.3907 samples/sec | ETA 00:01:33
2022-06-10 17:43:20 [INFO]	[TRAIN] epoch: 1, iter: 25/200, loss: 1.0445, lr: 0.008913, batch_cost: 0.4976, reader_cost: 0.00314, ips: 16.0785 samples/sec | ETA 00:01:27
2022-06-10 17:43:23 [INFO]	[TRAIN] epoch: 1, iter: 30/200, loss: 0.9715, lr: 0.008685, batch_cost: 0.5040, reader_cost: 0.00381, ips: 15.8716 samples/sec | ETA 00:01:25
2022-06-10 17:43:25 [INFO]	[TRAIN] epoch: 1, iter: 35/200, loss: 0.9075, lr: 0.008456, batch_cost: 0.5007, reader_cost: 0.00317, ips: 15.9776 samples/sec | ETA 00:01:22
2022-06-10 17:43:28 [INFO]	[TRAIN] epoch: 1, iter: 40/200, loss: 0.8472, lr: 0.008227, batch_cost: 0.5018, reader_cost: 0.00275, ips: 15.9420 samples/sec | ETA 00:01:20
2022-06-10 17:43:30 [INFO]	[TRAIN] epoch: 1, iter: 45/200, loss: 0.8754, lr: 0.007996, batch_cost: 0.4979, reader_cost: 0.00305, ips: 16.0671 samples/sec | ETA 00:01:17
2022-06-10 17:43:33 [INFO]	[TRAIN] epoch: 1, iter: 50/200, loss: 0.8955, lr: 0.007765, batch_cost: 0.4942, reader_cost: 0.00168, ips: 16.1872 samples/sec | ETA 00:01:14
2022-06-10 17:43:35 [INFO]	[TRAIN] epoch: 1, iter: 55/200, loss: 0.7867, lr: 0.007533, batch_cost: 0.5036, reader_cost: 0.00386, ips: 15.8871 samples/sec | ETA 00:01:13
2022-06-10 17:43:38 [INFO]	[TRAIN] epoch: 1, iter: 60/200, loss: 0.7895, lr: 0.007301, batch_cost: 0.5008, reader_cost: 0.00155, ips: 15.9746 samples/sec | ETA 00:01:10
2022-06-10 17:43:40 [INFO]	[TRAIN] epoch: 1, iter: 65/200, loss: 0.6563, lr: 0.007067, batch_cost: 0.4949, reader_cost: 0.00087, ips: 16.1641 samples/sec | ETA 00:01:06
2022-06-10 17:43:43 [INFO]	[TRAIN] epoch: 1, iter: 70/200, loss: 0.7013, lr: 0.006833, batch_cost: 0.5051, reader_cost: 0.00239, ips: 15.8376 samples/sec | ETA 00:01:05
2022-06-10 17:43:45 [INFO]	[TRAIN] epoch: 1, iter: 75/200, loss: 0.6300, lr: 0.006598, batch_cost: 0.5019, reader_cost: 0.00413, ips: 15.9385 samples/sec | ETA 00:01:02
2022-06-10 17:43:48 [INFO]	[TRAIN] epoch: 1, iter: 80/200, loss: 0.5714, lr: 0.006362, batch_cost: 0.5037, reader_cost: 0.00165, ips: 15.8838 samples/sec | ETA 00:01:00
2022-06-10 17:43:50 [INFO]	[TRAIN] epoch: 1, iter: 85/200, loss: 0.6732, lr: 0.006125, batch_cost: 0.4993, reader_cost: 0.00018, ips: 16.0232 samples/sec | ETA 00:00:57
2022-06-10 17:43:53 [INFO]	[TRAIN] epoch: 1, iter: 90/200, loss: 0.7240, lr: 0.005887, batch_cost: 0.5000, reader_cost: 0.00195, ips: 16.0010 samples/sec | ETA 00:00:54
2022-06-10 17:43:55 [INFO]	[TRAIN] epoch: 1, iter: 95/200, loss: 0.6456, lr: 0.005647, batch_cost: 0.4980, reader_cost: 0.00087, ips: 16.0651 samples/sec | ETA 00:00:52
2022-06-10 17:43:58 [INFO]	[TRAIN] epoch: 1, iter: 100/200, loss: 0.6745, lr: 0.005407, batch_cost: 0.4979, reader_cost: 0.00151, ips: 16.0671 samples/sec | ETA 00:00:49
2022-06-10 17:44:00 [INFO]	[TRAIN] epoch: 1, iter: 105/200, loss: 0.6823, lr: 0.005166, batch_cost: 0.5004, reader_cost: 0.00481, ips: 15.9858 samples/sec | ETA 00:00:47
2022-06-10 17:44:03 [INFO]	[TRAIN] epoch: 1, iter: 110/200, loss: 0.5198, lr: 0.004923, batch_cost: 0.4958, reader_cost: 0.00018, ips: 16.1343 samples/sec | ETA 00:00:44
2022-06-10 17:44:05 [INFO]	[TRAIN] epoch: 1, iter: 115/200, loss: 0.5494, lr: 0.004679, batch_cost: 0.4935, reader_cost: 0.00017, ips: 16.2094 samples/sec | ETA 00:00:41
2022-06-10 17:44:08 [INFO]	[TRAIN] epoch: 1, iter: 120/200, loss: 0.6414, lr: 0.004433, batch_cost: 0.4893, reader_cost: 0.00016, ips: 16.3493 samples/sec | ETA 00:00:39
2022-06-10 17:44:10 [INFO]	[TRAIN] epoch: 1, iter: 125/200, loss: 0.5633, lr: 0.004186, batch_cost: 0.5023, reader_cost: 0.00092, ips: 15.9280 samples/sec | ETA 00:00:37
2022-06-10 17:44:13 [INFO]	[TRAIN] epoch: 1, iter: 130/200, loss: 0.6101, lr: 0.003937, batch_cost: 0.4984, reader_cost: 0.00181, ips: 16.0526 samples/sec | ETA 00:00:34
2022-06-10 17:44:15 [INFO]	[TRAIN] epoch: 1, iter: 135/200, loss: 0.5156, lr: 0.003687, batch_cost: 0.5044, reader_cost: 0.00367, ips: 15.8598 samples/sec | ETA 00:00:32
2022-06-10 17:44:18 [INFO]	[TRAIN] epoch: 1, iter: 140/200, loss: 0.5434, lr: 0.003435, batch_cost: 0.5033, reader_cost: 0.00380, ips: 15.8957 samples/sec | ETA 00:00:30
2022-06-10 17:44:20 [INFO]	[TRAIN] epoch: 1, iter: 145/200, loss: 0.3607, lr: 0.003180, batch_cost: 0.5023, reader_cost: 0.00406, ips: 15.9282 samples/sec | ETA 00:00:27
2022-06-10 17:44:23 [INFO]	[TRAIN] epoch: 1, iter: 150/200, loss: 0.4605, lr: 0.002923, batch_cost: 0.4993, reader_cost: 0.00323, ips: 16.0222 samples/sec | ETA 00:00:24
2022-06-10 17:44:25 [INFO]	[TRAIN] epoch: 1, iter: 155/200, loss: 0.7014, lr: 0.002664, batch_cost: 0.5002, reader_cost: 0.00359, ips: 15.9931 samples/sec | ETA 00:00:22
2022-06-10 17:44:28 [INFO]	[TRAIN] epoch: 1, iter: 160/200, loss: 0.4915, lr: 0.002402, batch_cost: 0.4985, reader_cost: 0.00242, ips: 16.0467 samples/sec | ETA 00:00:19
2022-06-10 17:44:30 [INFO]	[TRAIN] epoch: 1, iter: 165/200, loss: 0.6177, lr: 0.002137, batch_cost: 0.4997, reader_cost: 0.00308, ips: 16.0098 samples/sec | ETA 00:00:17
2022-06-10 17:44:33 [INFO]	[TRAIN] epoch: 1, iter: 170/200, loss: 0.5716, lr: 0.001868, batch_cost: 0.5024, reader_cost: 0.00389, ips: 15.9242 samples/sec | ETA 00:00:15
2022-06-10 17:44:35 [INFO]	[TRAIN] epoch: 1, iter: 175/200, loss: 0.5104, lr: 0.001594, batch_cost: 0.4979, reader_cost: 0.00161, ips: 16.0669 samples/sec | ETA 00:00:12
2022-06-10 17:44:38 [INFO]	[TRAIN] epoch: 1, iter: 180/200, loss: 0.4662, lr: 0.001315, batch_cost: 0.4967, reader_cost: 0.00171, ips: 16.1069 samples/sec | ETA 00:00:09
2022-06-10 17:44:40 [INFO]	[TRAIN] epoch: 1, iter: 185/200, loss: 0.5625, lr: 0.001030, batch_cost: 0.5001, reader_cost: 0.00285, ips: 15.9956 samples/sec | ETA 00:00:07
2022-06-10 17:44:43 [INFO]	[TRAIN] epoch: 1, iter: 190/200, loss: 0.6565, lr: 0.000735, batch_cost: 0.4978, reader_cost: 0.00363, ips: 16.0693 samples/sec | ETA 00:00:04
2022-06-10 17:44:45 [INFO]	[TRAIN] epoch: 1, iter: 195/200, loss: 0.4348, lr: 0.000426, batch_cost: 0.4967, reader_cost: 0.00105, ips: 16.1066 samples/sec | ETA 00:00:02
2022-06-10 17:44:48 [INFO]	[TRAIN] epoch: 1, iter: 200/200, loss: 0.5768, lr: 0.000085, batch_cost: 0.4931, reader_cost: 0.00016, ips: 16.2235 samples/sec | ETA 00:00:00
