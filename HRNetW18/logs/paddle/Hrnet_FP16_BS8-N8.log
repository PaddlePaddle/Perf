/bin/sh: /usr/local/cuda/bin/nvcc: No such file or directory
2022-06-10 17:49:30 [INFO]	
------------Environment Information-------------
platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-debian-stretch-sid
Python: 3.7.0 (default, Jun  7 2022, 14:17:04) [GCC 8.2.0]
Paddle compiled with cuda: True
NVCC: Not Available
cudnn: 8.1
GPUs used: 8
CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
GPU: ['GPU 0: Tesla V100-SXM2-32GB', 'GPU 1: Tesla V100-SXM2-32GB', 'GPU 2: Tesla V100-SXM2-32GB', 'GPU 3: Tesla V100-SXM2-32GB', 'GPU 4: Tesla V100-SXM2-32GB', 'GPU 5: Tesla V100-SXM2-32GB', 'GPU 6: Tesla V100-SXM2-32GB', 'GPU 7: Tesla V100-SXM2-32GB']
GCC: gcc (GCC) 8.2.0
PaddleSeg: develop
PaddlePaddle: 2.3.0
OpenCV: 4.2.0
------------------------------------------------
2022-06-10 17:49:30 [INFO]	
---------------Config Information---------------
batch_size: 8
iters: 200
loss:
  coef:
  - 1
  types:
  - ignore_index: 255
    type: CrossEntropyLoss
lr_scheduler:
  end_lr: 0
  learning_rate: 0.01
  power: 0.9
  type: PolynomialDecay
model:
  backbone:
    padding_same: false
    pretrained: https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz
    type: HRNet_W18
  backbone_indices:
  - -1
  bias: false
  num_classes: 19
  type: FCN
optimizer:
  type: sgd
  weight_decay: 0.0005
to_static_training: false
train_dataset:
  dataset_root: data/cityscapes
  mode: train
  transforms:
  - max_scale_factor: 2.0
    min_scale_factor: 0.5
    scale_step_size: 0.25
    type: ResizeStepScaling
  - crop_size:
    - 1024
    - 512
    type: RandomPaddingCrop
  - type: RandomHorizontalFlip
  - type: RandomDistort
  - type: Normalize
  type: Cityscapes
val_dataset:
  dataset_root: data/cityscapes
  mode: val
  transforms:
  - type: Normalize
  type: Cityscapes
------------------------------------------------
W0610 17:49:30.479965 35334 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 11.2
W0610 17:49:30.480010 35334 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.
2022-06-10 17:49:35 [INFO]	Loading pretrained model from https://bj.bcebos.com/paddleseg/dygraph/hrnet_w18_ssld.tar.gz
2022-06-10 17:49:36 [INFO]	There are 1525/1525 variables loaded into HRNet.
2022-06-10 17:49:36 [INFO]	use AMP to train. AMP level = O2
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:131: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self._found_inf = to_variable(np.array([0]).astype(np.bool))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:133: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.array([0]).astype(np.bool))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/amp/loss_scaler.py:135: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.array([0]).astype(np.bool))
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:41324', '127.0.0.1:52498', '127.0.0.1:39542', '127.0.0.1:46423', '127.0.0.1:48569', '127.0.0.1:45469', '127.0.0.1:59423']
I0610 17:49:39.844302 35334 nccl_context.cc:83] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
I0610 17:49:41.357512 35334 nccl_context.cc:115] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 10
/usr/local/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
2022-06-10 17:49:42,475-INFO: [topology.py:169:__init__] HybridParallelInfo: rank_id: 0, mp_degree: 1, sharding_degree: 1, pp_degree: 1, dp_degree: 8, mp_group: [0],  sharding_group: [0], pp_group: [0], dp_group: [0, 1, 2, 3, 4, 5, 6, 7], check/clip group: [0]
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:278: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float16, but right dtype is paddle.float32, the right dtype will convert to paddle.float16
  format(lhs_dtype, rhs_dtype, lhs_dtype))
/usr/local/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:278: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int64, the right dtype will convert to paddle.float32
  format(lhs_dtype, rhs_dtype, lhs_dtype))
2022-06-10 17:49:56 [INFO]	[TRAIN] epoch: 1, iter: 5/200, loss: 2.6922, lr: 0.009820, batch_cost: 2.6769, reader_cost: 0.92168, ips: 2.9885 samples/sec | ETA 00:08:42
2022-06-10 17:49:59 [INFO]	[TRAIN] epoch: 1, iter: 10/200, loss: 1.5141, lr: 0.009594, batch_cost: 0.7214, reader_cost: 0.00561, ips: 11.0890 samples/sec | ETA 00:02:17
2022-06-10 17:50:03 [INFO]	[TRAIN] epoch: 1, iter: 15/200, loss: 0.8794, lr: 0.009368, batch_cost: 0.7356, reader_cost: 0.00183, ips: 10.8756 samples/sec | ETA 00:02:16
2022-06-10 17:50:07 [INFO]	[TRAIN] epoch: 1, iter: 20/200, loss: 0.8729, lr: 0.009141, batch_cost: 0.7202, reader_cost: 0.00471, ips: 11.1077 samples/sec | ETA 00:02:09
2022-06-10 17:50:10 [INFO]	[TRAIN] epoch: 1, iter: 25/200, loss: 0.8738, lr: 0.008913, batch_cost: 0.7232, reader_cost: 0.00374, ips: 11.0623 samples/sec | ETA 00:02:06
2022-06-10 17:50:14 [INFO]	[TRAIN] epoch: 1, iter: 30/200, loss: 0.7619, lr: 0.008685, batch_cost: 0.7096, reader_cost: 0.00425, ips: 11.2735 samples/sec | ETA 00:02:00
2022-06-10 17:50:17 [INFO]	[TRAIN] epoch: 1, iter: 35/200, loss: 0.7158, lr: 0.008456, batch_cost: 0.6108, reader_cost: 0.00099, ips: 13.0972 samples/sec | ETA 00:01:40
2022-06-10 17:50:20 [INFO]	[TRAIN] epoch: 1, iter: 40/200, loss: 0.7767, lr: 0.008227, batch_cost: 0.6055, reader_cost: 0.00161, ips: 13.2130 samples/sec | ETA 00:01:36
2022-06-10 17:50:23 [INFO]	[TRAIN] epoch: 1, iter: 45/200, loss: 0.8795, lr: 0.007996, batch_cost: 0.5862, reader_cost: 0.00057, ips: 13.6473 samples/sec | ETA 00:01:30
2022-06-10 17:50:32 [INFO]	[TRAIN] epoch: 2, iter: 50/200, loss: 0.5736, lr: 0.007765, batch_cost: 1.8715, reader_cost: 0.85702, ips: 4.2748 samples/sec | ETA 00:04:40
2022-06-10 17:50:36 [INFO]	[TRAIN] epoch: 2, iter: 55/200, loss: 0.5879, lr: 0.007533, batch_cost: 0.7015, reader_cost: 0.00590, ips: 11.4047 samples/sec | ETA 00:01:41
2022-06-10 17:50:39 [INFO]	[TRAIN] epoch: 2, iter: 60/200, loss: 0.5547, lr: 0.007301, batch_cost: 0.7224, reader_cost: 0.00434, ips: 11.0742 samples/sec | ETA 00:01:41
2022-06-10 17:50:43 [INFO]	[TRAIN] epoch: 2, iter: 65/200, loss: 0.6095, lr: 0.007067, batch_cost: 0.6906, reader_cost: 0.00104, ips: 11.5839 samples/sec | ETA 00:01:33
2022-06-10 17:50:46 [INFO]	[TRAIN] epoch: 2, iter: 70/200, loss: 0.6186, lr: 0.006833, batch_cost: 0.7230, reader_cost: 0.00323, ips: 11.0652 samples/sec | ETA 00:01:33
2022-06-10 17:50:50 [INFO]	[TRAIN] epoch: 2, iter: 75/200, loss: 0.5248, lr: 0.006598, batch_cost: 0.6900, reader_cost: 0.00385, ips: 11.5940 samples/sec | ETA 00:01:26
2022-06-10 17:50:53 [INFO]	[TRAIN] epoch: 2, iter: 80/200, loss: 0.5087, lr: 0.006362, batch_cost: 0.6250, reader_cost: 0.00344, ips: 12.7995 samples/sec | ETA 00:01:15
2022-06-10 17:50:56 [INFO]	[TRAIN] epoch: 2, iter: 85/200, loss: 0.5403, lr: 0.006125, batch_cost: 0.6124, reader_cost: 0.00099, ips: 13.0642 samples/sec | ETA 00:01:10
2022-06-10 17:50:59 [INFO]	[TRAIN] epoch: 2, iter: 90/200, loss: 0.5082, lr: 0.005887, batch_cost: 0.6120, reader_cost: 0.00014, ips: 13.0728 samples/sec | ETA 00:01:07
2022-06-10 17:51:09 [INFO]	[TRAIN] epoch: 3, iter: 95/200, loss: 0.5643, lr: 0.005647, batch_cost: 1.8789, reader_cost: 0.89707, ips: 4.2577 samples/sec | ETA 00:03:17
2022-06-10 17:51:12 [INFO]	[TRAIN] epoch: 3, iter: 100/200, loss: 0.3576, lr: 0.005407, batch_cost: 0.7270, reader_cost: 0.00388, ips: 11.0041 samples/sec | ETA 00:01:12
2022-06-10 17:51:16 [INFO]	[TRAIN] epoch: 3, iter: 105/200, loss: 0.6364, lr: 0.005166, batch_cost: 0.7138, reader_cost: 0.00136, ips: 11.2076 samples/sec | ETA 00:01:07
2022-06-10 17:51:19 [INFO]	[TRAIN] epoch: 3, iter: 110/200, loss: 0.4099, lr: 0.004923, batch_cost: 0.7090, reader_cost: 0.00022, ips: 11.2834 samples/sec | ETA 00:01:03
2022-06-10 17:51:23 [INFO]	[TRAIN] epoch: 3, iter: 115/200, loss: 0.3874, lr: 0.004679, batch_cost: 0.7012, reader_cost: 0.00155, ips: 11.4096 samples/sec | ETA 00:00:59
2022-06-10 17:51:26 [INFO]	[TRAIN] epoch: 3, iter: 120/200, loss: 0.5972, lr: 0.004433, batch_cost: 0.6941, reader_cost: 0.00370, ips: 11.5260 samples/sec | ETA 00:00:55
2022-06-10 17:51:29 [INFO]	[TRAIN] epoch: 3, iter: 125/200, loss: 0.5485, lr: 0.004186, batch_cost: 0.6430, reader_cost: 0.00137, ips: 12.4413 samples/sec | ETA 00:00:48
2022-06-10 17:51:32 [INFO]	[TRAIN] epoch: 3, iter: 130/200, loss: 0.5061, lr: 0.003937, batch_cost: 0.5907, reader_cost: 0.00013, ips: 13.5429 samples/sec | ETA 00:00:41
2022-06-10 17:51:35 [INFO]	[TRAIN] epoch: 3, iter: 135/200, loss: 0.5085, lr: 0.003687, batch_cost: 0.5803, reader_cost: 0.00014, ips: 13.7870 samples/sec | ETA 00:00:37
2022-06-10 17:51:44 [INFO]	[TRAIN] epoch: 4, iter: 140/200, loss: 0.4048, lr: 0.003435, batch_cost: 1.7997, reader_cost: 0.68847, ips: 4.4451 samples/sec | ETA 00:01:47
2022-06-10 17:51:48 [INFO]	[TRAIN] epoch: 4, iter: 145/200, loss: 0.3754, lr: 0.003180, batch_cost: 0.7576, reader_cost: 0.00678, ips: 10.5602 samples/sec | ETA 00:00:41
2022-06-10 17:51:52 [INFO]	[TRAIN] epoch: 4, iter: 150/200, loss: 0.4107, lr: 0.002923, batch_cost: 0.6923, reader_cost: 0.00022, ips: 11.5556 samples/sec | ETA 00:00:34
2022-06-10 17:51:55 [INFO]	[TRAIN] epoch: 4, iter: 155/200, loss: 0.3067, lr: 0.002664, batch_cost: 0.7101, reader_cost: 0.00280, ips: 11.2656 samples/sec | ETA 00:00:31
2022-06-10 17:51:59 [INFO]	[TRAIN] epoch: 4, iter: 160/200, loss: 0.2871, lr: 0.002402, batch_cost: 0.7385, reader_cost: 0.00705, ips: 10.8332 samples/sec | ETA 00:00:29
2022-06-10 17:52:02 [INFO]	[TRAIN] epoch: 4, iter: 165/200, loss: 0.3351, lr: 0.002137, batch_cost: 0.7040, reader_cost: 0.00546, ips: 11.3632 samples/sec | ETA 00:00:24
2022-06-10 17:52:06 [INFO]	[TRAIN] epoch: 4, iter: 170/200, loss: 0.3624, lr: 0.001868, batch_cost: 0.6923, reader_cost: 0.00559, ips: 11.5562 samples/sec | ETA 00:00:20
2022-06-10 17:52:09 [INFO]	[TRAIN] epoch: 4, iter: 175/200, loss: 0.4208, lr: 0.001594, batch_cost: 0.6021, reader_cost: 0.00014, ips: 13.2862 samples/sec | ETA 00:00:15
2022-06-10 17:52:12 [INFO]	[TRAIN] epoch: 4, iter: 180/200, loss: 0.3748, lr: 0.001315, batch_cost: 0.6094, reader_cost: 0.00062, ips: 13.1266 samples/sec | ETA 00:00:12
2022-06-10 17:52:20 [INFO]	[TRAIN] epoch: 5, iter: 185/200, loss: 0.3468, lr: 0.001030, batch_cost: 1.6110, reader_cost: 0.79955, ips: 4.9658 samples/sec | ETA 00:00:24
2022-06-10 17:52:24 [INFO]	[TRAIN] epoch: 5, iter: 190/200, loss: 0.3268, lr: 0.000735, batch_cost: 0.8617, reader_cost: 0.00294, ips: 9.2841 samples/sec | ETA 00:00:08
2022-06-10 17:52:28 [INFO]	[TRAIN] epoch: 5, iter: 195/200, loss: 0.3464, lr: 0.000426, batch_cost: 0.7146, reader_cost: 0.00369, ips: 11.1955 samples/sec | ETA 00:00:03
2022-06-10 17:52:31 [INFO]	[TRAIN] epoch: 5, iter: 200/200, loss: 0.4710, lr: 0.000085, batch_cost: 0.7193, reader_cost: 0.00433, ips: 11.1218 samples/sec | ETA 00:00:00
