| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 8421, WORLD_SIZE: 1, RANK: 0
| distributed init done!
| initialized host 70bfc6a3004d as rank 0 and device id 0
Namespace(adam_betas=[0.9, 0.997], adam_eps=1e-09, amp=True, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=1, do_sanity_check=False, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, file=None, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=10, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=1, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_lr=0.0, momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, path=None, prefix_size=0, print_alignment=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='/workspace/checkpoint', save_interval=1, save_predictions=False, seed=1, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| /data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 1 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
Transformer | epoch 0 | step 10 |avg loss 16.121 |avg tokens 4746.000 |tokens/s 33002.987 |walltime 10.696 |
Transformer | epoch 0 | step 20 |avg loss 15.626 |avg tokens 4684.900 |tokens/s 31033.193 |walltime 12.206 |
Transformer | epoch 0 | step 30 |avg loss 14.769 |avg tokens 4825.900 |tokens/s 31889.433 |walltime 13.719 |
Transformer | epoch 0 | step 40 |avg loss 14.189 |avg tokens 4474.100 |tokens/s 30533.061 |walltime 15.185 |
Transformer | epoch 0 | step 50 |avg loss 13.675 |avg tokens 4491.300 |tokens/s 29785.800 |walltime 16.693 |
Transformer | epoch 0 | step 60 |avg loss 13.386 |avg tokens 4605.600 |tokens/s 30957.626 |walltime 18.180 |
Transformer | epoch 0 | step 70 |avg loss 13.098 |avg tokens 4797.300 |tokens/s 31777.083 |walltime 19.690 |
Transformer | epoch 0 | step 80 |avg loss 13.046 |avg tokens 4635.400 |tokens/s 31396.891 |walltime 21.166 |
Transformer | epoch 0 | step 90 |avg loss 12.766 |avg tokens 4480.700 |tokens/s 30835.780 |walltime 22.620 |
Transformer | epoch 0 | step 100 |avg loss 12.568 |avg tokens 4863.700 |tokens/s 31994.509 |walltime 24.140 |
Transformer | epoch 0 | step 110 |avg loss 12.480 |avg tokens 4102.000 |tokens/s 28357.679 |walltime 25.586 |
Transformer | epoch 0 | step 120 |avg loss 12.197 |avg tokens 4813.100 |tokens/s 31574.732 |walltime 27.111 |
Transformer | epoch 0 | step 130 |avg loss 12.283 |avg tokens 4012.100 |tokens/s 29097.542 |walltime 28.489 |
Transformer | epoch 0 | step 140 |avg loss 12.047 |avg tokens 4501.700 |tokens/s 30520.811 |walltime 29.964 |
Transformer | epoch 0 | step 150 |avg loss 11.980 |avg tokens 4478.300 |tokens/s 30454.253 |walltime 31.435 |
Transformer | epoch 0 | step 160 |avg loss 11.873 |avg tokens 4764.200 |tokens/s 31626.612 |walltime 32.941 |
Transformer | epoch 0 | step 170 |avg loss 11.842 |avg tokens 4419.500 |tokens/s 30141.950 |walltime 34.408 |
Transformer | epoch 0 | step 180 |avg loss 11.587 |avg tokens 4704.800 |tokens/s 30925.802 |walltime 35.929 |
Transformer | epoch 0 | step 190 |avg loss 11.600 |avg tokens 4298.700 |tokens/s 29476.994 |walltime 37.387 |
Transformer | epoch 0 | step 200 |avg loss 11.645 |avg tokens 4392.700 |tokens/s 30529.704 |walltime 38.826 |
Transformer | epoch 0 | step 210 |avg loss 11.697 |avg tokens 4111.300 |tokens/s 28155.804 |walltime 40.286 |
Transformer | epoch 0 | step 220 |avg loss 11.508 |avg tokens 4313.800 |tokens/s 29768.538 |walltime 41.735 |
Transformer | epoch 0 | step 230 |avg loss 11.395 |avg tokens 4865.800 |tokens/s 30665.983 |walltime 43.322 |
Transformer | epoch 0 | step 240 |avg loss 11.408 |avg tokens 4560.800 |tokens/s 30693.089 |walltime 44.808 |
Transformer | epoch 0 | step 250 |avg loss 11.624 |avg tokens 4366.200 |tokens/s 30730.601 |walltime 46.229 |
Transformer | epoch 0 | step 260 |avg loss 11.382 |avg tokens 4891.600 |tokens/s 32237.946 |walltime 47.746 |
Transformer | epoch 0 | step 270 |avg loss 11.187 |avg tokens 4857.800 |tokens/s 31165.027 |walltime 49.305 |
Transformer | epoch 0 | step 280 |avg loss 11.391 |avg tokens 4607.100 |tokens/s 30926.518 |walltime 50.795 |
Transformer | epoch 0 | step 290 |avg loss 11.616 |avg tokens 4808.600 |tokens/s 35030.397 |walltime 52.167 |
Transformer | epoch 0 | step 300 |avg loss 11.370 |avg tokens 3985.000 |tokens/s 27600.612 |walltime 53.611 |
Transformer | epoch 0 | step 310 |avg loss 11.035 |avg tokens 4752.000 |tokens/s 31123.906 |walltime 55.138 |
Transformer | epoch 0 | step 320 |avg loss 11.405 |avg tokens 4491.200 |tokens/s 30621.887 |walltime 56.605 |
Transformer | epoch 0 | step 330 |avg loss 11.195 |avg tokens 4595.600 |tokens/s 30718.771 |walltime 58.101 |
Transformer | epoch 0 | step 340 |avg loss 11.201 |avg tokens 4527.900 |tokens/s 29248.705 |walltime 59.649 |
Transformer | epoch 0 | step 350 |avg loss 10.928 |avg tokens 4840.000 |tokens/s 31522.889 |walltime 61.184 |
Transformer | epoch 0 | step 360 |avg loss 11.423 |avg tokens 4818.300 |tokens/s 32302.204 |walltime 62.676 |
Transformer | epoch 0 | step 370 |avg loss 11.201 |avg tokens 4466.600 |tokens/s 30707.225 |walltime 64.130 |
Transformer | epoch 0 | step 380 |avg loss 11.263 |avg tokens 4497.300 |tokens/s 30538.908 |walltime 65.603 |
Transformer | epoch 0 | step 390 |avg loss 11.170 |avg tokens 4554.000 |tokens/s 29739.903 |walltime 67.134 |
Transformer | epoch 0 | step 400 |avg loss 11.569 |avg tokens 4280.600 |tokens/s 29631.079 |walltime 68.579 |
Transformer | epoch 0 | step 410 |avg loss 11.231 |avg tokens 4490.200 |tokens/s 31327.017 |walltime 70.012 |
Transformer | epoch 0 | step 420 |avg loss 10.969 |avg tokens 4122.700 |tokens/s 28160.038 |walltime 71.476 |
Transformer | epoch 0 | step 430 |avg loss 11.006 |avg tokens 4399.400 |tokens/s 29414.527 |walltime 72.972 |
Transformer | epoch 0 | step 440 |avg loss 11.296 |avg tokens 4768.500 |tokens/s 32464.565 |walltime 74.441 |
Transformer | epoch 0 | step 450 |avg loss 11.252 |avg tokens 3972.100 |tokens/s 28590.802 |walltime 75.830 |
Transformer | epoch 0 | step 460 |avg loss 10.783 |avg tokens 5023.200 |tokens/s 32151.374 |walltime 77.392 |
Transformer | epoch 0 | step 470 |avg loss 10.960 |avg tokens 4543.900 |tokens/s 31024.417 |walltime 78.857 |
Transformer | epoch 0 | step 480 |avg loss 11.060 |avg tokens 4486.900 |tokens/s 30055.318 |walltime 80.350 |
Transformer | epoch 0 | step 490 |avg loss 11.071 |avg tokens 4767.300 |tokens/s 32084.164 |walltime 81.836 |
Transformer | epoch 0 | step 500 |avg loss 10.834 |avg tokens 4836.000 |tokens/s 32023.087 |walltime 83.346 |
Transformer | epoch 0 | step 510 |avg loss 11.098 |avg tokens 4769.600 |tokens/s 32137.813 |walltime 84.830 |
Transformer | epoch 0 | step 520 |avg loss 10.846 |avg tokens 4485.800 |tokens/s 31153.575 |walltime 86.270 |
Transformer | epoch 0 | step 530 |avg loss 10.744 |avg tokens 4718.800 |tokens/s 31224.291 |walltime 87.781 |
Transformer | epoch 0 | step 540 |avg loss 10.835 |avg tokens 4225.400 |tokens/s 29534.394 |walltime 89.212 |
Transformer | epoch 0 | step 550 |avg loss 10.779 |avg tokens 4537.400 |tokens/s 31224.678 |walltime 90.665 |
Transformer | epoch 0 | step 560 |avg loss 10.536 |avg tokens 4657.600 |tokens/s 30938.150 |walltime 92.170 |
Transformer | epoch 0 | step 570 |avg loss 10.628 |avg tokens 4406.300 |tokens/s 29377.729 |walltime 93.670 |
Transformer | epoch 0 | step 580 |avg loss 10.469 |avg tokens 4371.300 |tokens/s 29569.064 |walltime 95.149 |
Transformer | epoch 0 | step 590 |avg loss 10.519 |avg tokens 4391.900 |tokens/s 28964.959 |walltime 96.665 |
Transformer | epoch 0 | step 600 |avg loss 10.449 |avg tokens 4589.800 |tokens/s 30630.887 |walltime 98.163 |
Transformer | epoch 0 | step 610 |avg loss 10.569 |avg tokens 4798.900 |tokens/s 32458.548 |walltime 99.642 |
Transformer | epoch 0 | step 620 |avg loss 10.401 |avg tokens 4643.200 |tokens/s 31020.198 |walltime 101.139 |
Transformer | epoch 0 | step 630 |avg loss 10.607 |avg tokens 4290.200 |tokens/s 29580.469 |walltime 102.589 |
Transformer | epoch 0 | step 640 |avg loss 10.227 |avg tokens 4483.200 |tokens/s 29920.315 |walltime 104.087 |
Transformer | epoch 0 | step 650 |avg loss 10.611 |avg tokens 4729.900 |tokens/s 32559.182 |walltime 105.540 |
Transformer | epoch 0 | step 660 |avg loss 10.046 |avg tokens 4681.700 |tokens/s 32327.446 |walltime 106.988 |
Transformer | epoch 0 | step 670 |avg loss 10.483 |avg tokens 4824.000 |tokens/s 32095.538 |walltime 108.491 |
Transformer | epoch 0 | step 680 |avg loss 10.341 |avg tokens 4452.400 |tokens/s 30646.734 |walltime 109.944 |
Transformer | epoch 0 | step 690 |avg loss 10.415 |avg tokens 4292.800 |tokens/s 29207.898 |walltime 111.414 |
Transformer | epoch 0 | step 700 |avg loss 10.353 |avg tokens 4044.200 |tokens/s 28507.390 |walltime 112.833 |
Transformer | epoch 0 | step 710 |avg loss 10.181 |avg tokens 4747.600 |tokens/s 31303.566 |walltime 114.349 |
Transformer | epoch 0 | step 720 |avg loss 10.182 |avg tokens 4380.400 |tokens/s 29504.812 |walltime 115.834 |
Transformer | epoch 0 | step 730 |avg loss 10.372 |avg tokens 3953.800 |tokens/s 27464.928 |walltime 117.273 |
Transformer | epoch 0 | step 740 |avg loss 10.359 |avg tokens 4533.800 |tokens/s 30971.495 |walltime 118.737 |
Transformer | epoch 0 | step 750 |avg loss 10.267 |avg tokens 4559.400 |tokens/s 31010.386 |walltime 120.208 |
Transformer | epoch 0 | step 760 |avg loss 10.054 |avg tokens 4836.800 |tokens/s 31680.706 |walltime 121.734 |
Transformer | epoch 0 | step 770 |avg loss 10.152 |avg tokens 4763.100 |tokens/s 32264.518 |walltime 123.211 |
Transformer | epoch 0 | step 780 |avg loss 10.184 |avg tokens 4412.300 |tokens/s 30655.589 |walltime 124.650 |
Transformer | epoch 0 | step 790 |avg loss 10.319 |avg tokens 4735.200 |tokens/s 32033.023 |walltime 126.128 |
Transformer | epoch 0 | step 800 |avg loss 9.907 |avg tokens 4525.600 |tokens/s 30597.370 |walltime 127.607 |
Transformer | epoch 0 | step 810 |avg loss 10.036 |avg tokens 4537.300 |tokens/s 31055.534 |walltime 129.068 |
Transformer | epoch 0 | step 820 |avg loss 10.089 |avg tokens 4601.600 |tokens/s 31494.663 |walltime 130.529 |
Transformer | epoch 0 | step 830 |avg loss 10.027 |avg tokens 4701.700 |tokens/s 31426.927 |walltime 132.025 |
Transformer | epoch 0 | step 840 |avg loss 10.152 |avg tokens 4362.600 |tokens/s 30388.475 |walltime 133.461 |
Transformer | epoch 0 | step 850 |avg loss 9.941 |avg tokens 4365.300 |tokens/s 30014.954 |walltime 134.915 |
Transformer | epoch 0 | step 860 |avg loss 9.799 |avg tokens 4655.900 |tokens/s 30969.469 |walltime 136.419 |
Transformer | epoch 0 | step 870 |avg loss 9.949 |avg tokens 4686.300 |tokens/s 31764.596 |walltime 137.894 |
Transformer | epoch 0 | step 880 |avg loss 9.832 |avg tokens 4841.900 |tokens/s 33213.819 |walltime 139.352 |
Transformer | epoch 0 | step 890 |avg loss 10.162 |avg tokens 4208.500 |tokens/s 30159.794 |walltime 140.747 |
Transformer | epoch 0 | step 900 |avg loss 10.097 |avg tokens 4439.500 |tokens/s 30320.010 |walltime 142.211 |
Transformer | epoch 0 | step 910 |avg loss 9.805 |avg tokens 4735.800 |tokens/s 31296.486 |walltime 143.725 |
Transformer | epoch 0 | step 920 |avg loss 9.816 |avg tokens 4521.000 |tokens/s 30349.241 |walltime 145.214 |
Transformer | epoch 0 | step 930 |avg loss 9.936 |avg tokens 4485.700 |tokens/s 31328.040 |walltime 146.646 |
Transformer | epoch 0 | step 940 |avg loss 9.752 |avg tokens 4760.800 |tokens/s 32239.571 |walltime 148.123 |
Transformer | epoch 0 | step 950 |avg loss 10.104 |avg tokens 3946.700 |tokens/s 28775.776 |walltime 149.494 |
Transformer | epoch 0 | step 960 |avg loss 9.965 |avg tokens 4745.200 |tokens/s 32425.680 |walltime 150.958 |
Transformer | epoch 0 | step 970 |avg loss 9.800 |avg tokens 4967.000 |tokens/s 33124.300 |walltime 152.457 |
Transformer | epoch 0 | step 980 |avg loss 9.749 |avg tokens 4290.400 |tokens/s 29448.528 |walltime 153.914 |
Transformer | epoch 0 | step 990 |avg loss 9.845 |avg tokens 4642.100 |tokens/s 32067.504 |walltime 155.362 |
Transformer | epoch 0 | step 1000 |avg loss 9.589 |avg tokens 4810.900 |tokens/s 32382.738 |walltime 156.848 |
Transformer | epoch 0 | step 1010 |avg loss 9.847 |avg tokens 3619.400 |tokens/s 26646.853 |walltime 158.206 |
Transformer | epoch 0 | step 1020 |avg loss 9.493 |avg tokens 4821.800 |tokens/s 31807.208 |walltime 159.722 |
Transformer | epoch 0 | step 1030 |avg loss 9.802 |avg tokens 4113.200 |tokens/s 29081.429 |walltime 161.136 |
Transformer | epoch 0 | step 1040 |avg loss 9.434 |avg tokens 4625.200 |tokens/s 30432.681 |walltime 162.656 |
Transformer | epoch 0 | step 1050 |avg loss 9.735 |avg tokens 4734.700 |tokens/s 32644.273 |walltime 164.106 |
Transformer | epoch 0 | step 1060 |avg loss 9.477 |avg tokens 4226.700 |tokens/s 29227.060 |walltime 165.553 |
Transformer | epoch 0 | step 1070 |avg loss 9.761 |avg tokens 4131.800 |tokens/s 27841.098 |walltime 167.037 |
Transformer | epoch 0 | step 1080 |avg loss 9.649 |avg tokens 4392.200 |tokens/s 30893.389 |walltime 168.458 |
Transformer | epoch 0 | step 1090 |avg loss 9.772 |avg tokens 4145.900 |tokens/s 28590.847 |walltime 169.908 |
Transformer | epoch 0 | step 1100 |avg loss 9.285 |avg tokens 4239.500 |tokens/s 29063.252 |walltime 171.367 |
Transformer | epoch 0 | step 1110 |avg loss 9.431 |avg tokens 4679.700 |tokens/s 31494.622 |walltime 172.853 |
Transformer | epoch 0 | step 1120 |avg loss 9.461 |avg tokens 4609.100 |tokens/s 31081.127 |walltime 174.336 |
Transformer | epoch 0 | step 1130 |avg loss 9.341 |avg tokens 4714.600 |tokens/s 31183.521 |walltime 175.848 |
Transformer | epoch 0 | step 1140 |avg loss 9.378 |avg tokens 4256.700 |tokens/s 28845.818 |walltime 177.323 |
Transformer | epoch 0 | step 1150 |avg loss 9.774 |avg tokens 4098.100 |tokens/s 29372.152 |walltime 178.719 |
Transformer | epoch 0 | step 1160 |avg loss 9.642 |avg tokens 3743.600 |tokens/s 26779.081 |walltime 180.117 |
Transformer | epoch 0 | step 1170 |avg loss 9.684 |avg tokens 4452.500 |tokens/s 30981.287 |walltime 181.554 |
Transformer | epoch 0 | step 1180 |avg loss 9.424 |avg tokens 4444.200 |tokens/s 30316.281 |walltime 183.020 |
Transformer | epoch 0 | step 1190 |avg loss 9.204 |avg tokens 4816.700 |tokens/s 31858.705 |walltime 184.532 |
Transformer | epoch 0 | step 1200 |avg loss 9.401 |avg tokens 4153.500 |tokens/s 28677.948 |walltime 185.980 |
Transformer | epoch 0 | step 1210 |avg loss 9.173 |avg tokens 4412.700 |tokens/s 30776.362 |walltime 187.414 |
Transformer | epoch 0 | step 1220 |avg loss 9.456 |avg tokens 4489.100 |tokens/s 30319.765 |walltime 188.894 |
Transformer | epoch 0 | step 1230 |avg loss 9.445 |avg tokens 4406.300 |tokens/s 29440.649 |walltime 190.391 |
Transformer | epoch 0 | step 1240 |avg loss 9.387 |avg tokens 4218.800 |tokens/s 29844.133 |walltime 191.805 |
Transformer | epoch 0 | step 1250 |avg loss 9.421 |avg tokens 4411.800 |tokens/s 30601.434 |walltime 193.246 |
Transformer | epoch 0 | step 1260 |avg loss 9.162 |avg tokens 4450.000 |tokens/s 29757.669 |walltime 194.742 |
Transformer | epoch 0 | step 1270 |avg loss 9.462 |avg tokens 4509.000 |tokens/s 30690.492 |walltime 196.211 |
Transformer | epoch 0 | step 1280 |avg loss 9.226 |avg tokens 4760.800 |tokens/s 31959.108 |walltime 197.701 |
Transformer | epoch 0 | step 1290 |avg loss 9.324 |avg tokens 4139.700 |tokens/s 29403.708 |walltime 199.109 |
Transformer | epoch 0 | step 1300 |avg loss 9.265 |avg tokens 4875.700 |tokens/s 32328.330 |walltime 200.617 |
Transformer | epoch 0 | step 1310 |avg loss 9.266 |avg tokens 4744.100 |tokens/s 32291.454 |walltime 202.086 |
Transformer | epoch 0 | step 1320 |avg loss 9.128 |avg tokens 4128.400 |tokens/s 29311.260 |walltime 203.494 |
Transformer | epoch 0 | step 1330 |avg loss 9.380 |avg tokens 4701.300 |tokens/s 31790.045 |walltime 204.973 |
Transformer | epoch 0 | step 1340 |avg loss 9.449 |avg tokens 4167.600 |tokens/s 29274.867 |walltime 206.397 |
Transformer | epoch 0 | step 1350 |avg loss 8.848 |avg tokens 4708.800 |tokens/s 30816.415 |walltime 207.925 |
Transformer | epoch 0 | step 1360 |avg loss 9.302 |avg tokens 4692.900 |tokens/s 31796.306 |walltime 209.401 |
Transformer | epoch 0 | step 1370 |avg loss 9.314 |avg tokens 4811.200 |tokens/s 32590.698 |walltime 210.877 |
Transformer | epoch 0 | step 1380 |avg loss 9.150 |avg tokens 4095.900 |tokens/s 29146.601 |walltime 212.282 |
Transformer | epoch 0 | step 1390 |avg loss 9.188 |avg tokens 4153.500 |tokens/s 28610.908 |walltime 213.734 |
Transformer | epoch 0 | step 1400 |avg loss 9.213 |avg tokens 4743.700 |tokens/s 32372.689 |walltime 215.199 |
Transformer | epoch 0 | step 1410 |avg loss 9.513 |avg tokens 4357.600 |tokens/s 31660.592 |walltime 216.576 |
Transformer | epoch 0 | step 1420 |avg loss 8.682 |avg tokens 4777.600 |tokens/s 32006.046 |walltime 218.068 |
Transformer | epoch 0 | step 1430 |avg loss 9.068 |avg tokens 4613.700 |tokens/s 30918.039 |walltime 219.561 |
Transformer | epoch 0 | step 1440 |avg loss 9.277 |avg tokens 4474.100 |tokens/s 31250.543 |walltime 220.992 |
Transformer | epoch 0 | step 1450 |avg loss 9.139 |avg tokens 4552.100 |tokens/s 32092.237 |walltime 222.411 |
Transformer | epoch 0 | step 1460 |avg loss 9.137 |avg tokens 4805.200 |tokens/s 33038.668 |walltime 223.865 |
Transformer | epoch 0 | step 1470 |avg loss 8.986 |avg tokens 4808.300 |tokens/s 32089.568 |walltime 225.364 |
Transformer | epoch 0 | step 1480 |avg loss 8.991 |avg tokens 4686.200 |tokens/s 29685.516 |walltime 226.942 |
Transformer | epoch 0 | step 1490 |avg loss 9.120 |avg tokens 4018.100 |tokens/s 27599.799 |walltime 228.398 |
Transformer | epoch 0 | step 1500 |avg loss 9.319 |avg tokens 4492.800 |tokens/s 30806.398 |walltime 229.856 |
Transformer | epoch 0 | step 1510 |avg loss 9.302 |avg tokens 4327.400 |tokens/s 30190.398 |walltime 231.290 |
Transformer | epoch 0 | step 1520 |avg loss 8.877 |avg tokens 4669.500 |tokens/s 31341.754 |walltime 232.780 |
Transformer | epoch 0 | step 1530 |avg loss 8.687 |avg tokens 4531.100 |tokens/s 30363.546 |walltime 234.272 |
Transformer | epoch 0 | step 1540 |avg loss 9.382 |avg tokens 4215.400 |tokens/s 29489.112 |walltime 235.701 |
Transformer | epoch 0 | step 1550 |avg loss 9.199 |avg tokens 4473.100 |tokens/s 30327.241 |walltime 237.176 |
Transformer | epoch 0 | step 1560 |avg loss 8.776 |avg tokens 4819.700 |tokens/s 32681.608 |walltime 238.651 |
Transformer | epoch 0 | step 1570 |avg loss 8.673 |avg tokens 4379.700 |tokens/s 29928.988 |walltime 240.115 |
Transformer | epoch 0 | step 1580 |avg loss 8.661 |avg tokens 4669.200 |tokens/s 31550.076 |walltime 241.594 |
Transformer | epoch 0 | step 1590 |avg loss 9.032 |avg tokens 4354.100 |tokens/s 29785.087 |walltime 243.056 |
Transformer | epoch 0 | step 1600 |avg loss 9.265 |avg tokens 3894.400 |tokens/s 28235.136 |walltime 244.436 |
Transformer | epoch 0 | step 1610 |avg loss 9.251 |avg tokens 4300.400 |tokens/s 29805.902 |walltime 245.878 |
Transformer | epoch 0 | step 1620 |avg loss 8.929 |avg tokens 4232.900 |tokens/s 29258.100 |walltime 247.325 |
Transformer | epoch 0 | step 1630 |avg loss 8.737 |avg tokens 4570.000 |tokens/s 31048.537 |walltime 248.797 |
Transformer | epoch 0 | step 1640 |avg loss 9.121 |avg tokens 4848.400 |tokens/s 33307.357 |walltime 250.253 |
Transformer | epoch 0 | step 1650 |avg loss 8.922 |avg tokens 4497.100 |tokens/s 30293.328 |walltime 251.737 |
Transformer | epoch 0 | step 1660 |avg loss 8.977 |avg tokens 4492.200 |tokens/s 31118.739 |walltime 253.181 |
Transformer | epoch 0 | step 1670 |avg loss 8.809 |avg tokens 4453.700 |tokens/s 30692.966 |walltime 254.632 |
Transformer | epoch 0 | step 1680 |avg loss 8.801 |avg tokens 4243.300 |tokens/s 29106.795 |walltime 256.090 |
Transformer | epoch 0 | step 1690 |avg loss 8.903 |avg tokens 4299.500 |tokens/s 29740.572 |walltime 257.535 |
Transformer | epoch 0 | step 1700 |avg loss 8.297 |avg tokens 4318.700 |tokens/s 29510.236 |walltime 258.999 |
Transformer | epoch 0 | step 1710 |avg loss 9.258 |avg tokens 4495.800 |tokens/s 30997.091 |walltime 260.449 |
Transformer | epoch 0 | step 1720 |avg loss 8.957 |avg tokens 4581.900 |tokens/s 31631.139 |walltime 261.898 |
Transformer | epoch 0 | step 1730 |avg loss 9.069 |avg tokens 4460.300 |tokens/s 31542.416 |walltime 263.312 |
Transformer | epoch 0 | step 1740 |avg loss 8.710 |avg tokens 4639.400 |tokens/s 31516.027 |walltime 264.784 |
Transformer | epoch 0 | step 1750 |avg loss 8.299 |avg tokens 4440.200 |tokens/s 29206.061 |walltime 266.304 |
Transformer | epoch 0 | step 1760 |avg loss 8.717 |avg tokens 4516.800 |tokens/s 30524.446 |walltime 267.784 |
Transformer | epoch 0 | step 1770 |avg loss 8.918 |avg tokens 4522.000 |tokens/s 31542.115 |walltime 269.218 |
Transformer | epoch 0 | step 1780 |avg loss 8.873 |avg tokens 4501.000 |tokens/s 30658.240 |walltime 270.686 |
Transformer | epoch 0 | step 1790 |avg loss 9.033 |avg tokens 4117.000 |tokens/s 29290.920 |walltime 272.091 |
Transformer | epoch 0 | step 1800 |avg loss 8.442 |avg tokens 4558.700 |tokens/s 30523.870 |walltime 273.585 |
Transformer | epoch 0 | step 1810 |avg loss 8.911 |avg tokens 4614.500 |tokens/s 31821.594 |walltime 275.035 |
Transformer | epoch 0 | step 1820 |avg loss 8.539 |avg tokens 4472.500 |tokens/s 30560.992 |walltime 276.498 |
Transformer | epoch 0 | step 1830 |avg loss 8.837 |avg tokens 4461.200 |tokens/s 30558.274 |walltime 277.958 |
Transformer | epoch 0 | step 1840 |avg loss 8.599 |avg tokens 4369.700 |tokens/s 29222.226 |walltime 279.454 |
Transformer | epoch 0 | step 1850 |avg loss 8.643 |avg tokens 4823.600 |tokens/s 32447.098 |walltime 280.940 |
Transformer | epoch 0 | step 1860 |avg loss 8.382 |avg tokens 4683.200 |tokens/s 31277.685 |walltime 282.437 |
Transformer | epoch 0 | step 1870 |avg loss 8.951 |avg tokens 4443.800 |tokens/s 32151.127 |walltime 283.820 |
Transformer | epoch 0 | step 1880 |avg loss 8.694 |avg tokens 4682.600 |tokens/s 31691.723 |walltime 285.297 |
Transformer | epoch 0 | step 1890 |avg loss 8.542 |avg tokens 4498.200 |tokens/s 30746.886 |walltime 286.760 |
Transformer | epoch 0 | step 1900 |avg loss 8.210 |avg tokens 4576.800 |tokens/s 30734.217 |walltime 288.249 |
Transformer | epoch 0 | step 1910 |avg loss 8.009 |avg tokens 4909.700 |tokens/s 32096.100 |walltime 289.779 |
Transformer | epoch 0 | step 1920 |avg loss 8.562 |avg tokens 4542.900 |tokens/s 31274.000 |walltime 291.232 |
Transformer | epoch 0 | step 1930 |avg loss 8.355 |avg tokens 4310.000 |tokens/s 29478.267 |walltime 292.694 |
Transformer | epoch 0 | step 1940 |avg loss 8.459 |avg tokens 4631.200 |tokens/s 30845.681 |walltime 294.195 |
Transformer | epoch 0 | step 1950 |avg loss 8.566 |avg tokens 4509.200 |tokens/s 31290.114 |walltime 295.636 |
Transformer | epoch 0 | step 1960 |avg loss 8.295 |avg tokens 4483.900 |tokens/s 30423.391 |walltime 297.110 |
Transformer | epoch 0 | step 1970 |avg loss 7.984 |avg tokens 4791.100 |tokens/s 31022.107 |walltime 298.655 |
Transformer | epoch 0 | step 1980 |avg loss 8.954 |avg tokens 4380.000 |tokens/s 30721.834 |walltime 300.080 |
Transformer | epoch 0 | step 1990 |avg loss 8.271 |avg tokens 4672.000 |tokens/s 28410.860 |walltime 301.725 |
Transformer | epoch 0 | step 2000 |avg loss 8.354 |avg tokens 4797.000 |tokens/s 32614.567 |walltime 303.195 |
Transformer | epoch 0 | step 2010 |avg loss 8.051 |avg tokens 4725.900 |tokens/s 32056.246 |walltime 304.670 |
Transformer | epoch 0 | step 2020 |avg loss 7.982 |avg tokens 4814.000 |tokens/s 31726.670 |walltime 306.187 |
Transformer | epoch 0 | step 2030 |avg loss 7.941 |avg tokens 4819.800 |tokens/s 31726.200 |walltime 307.706 |
Transformer | epoch 0 | step 2040 |avg loss 8.557 |avg tokens 4704.800 |tokens/s 32337.384 |walltime 309.161 |
Transformer | epoch 0 | step 2050 |avg loss 8.399 |avg tokens 4561.100 |tokens/s 31444.207 |walltime 310.612 |
Transformer | epoch 0 | step 2060 |avg loss 7.973 |avg tokens 4978.400 |tokens/s 32949.616 |walltime 312.123 |
Transformer | epoch 0 | step 2070 |avg loss 8.453 |avg tokens 4304.400 |tokens/s 29588.457 |walltime 313.577 |
Transformer | epoch 0 | step 2080 |avg loss 8.075 |avg tokens 4396.100 |tokens/s 29921.233 |walltime 315.047 |
Transformer | epoch 0 | step 2090 |avg loss 8.050 |avg tokens 4608.100 |tokens/s 31191.666 |walltime 316.524 |
Transformer | epoch 0 | step 2100 |avg loss 8.418 |avg tokens 4488.200 |tokens/s 30560.964 |walltime 317.993 |
Transformer | epoch 0 | step 2110 |avg loss 8.643 |avg tokens 4167.900 |tokens/s 29456.043 |walltime 319.408 |
Transformer | epoch 0 | step 2120 |avg loss 8.049 |avg tokens 4437.600 |tokens/s 29949.644 |walltime 320.889 |
Transformer | epoch 0 | step 2130 |avg loss 8.278 |avg tokens 4524.900 |tokens/s 30937.382 |walltime 322.352 |
Transformer | epoch 0 | step 2140 |avg loss 8.221 |avg tokens 4858.100 |tokens/s 32006.619 |walltime 323.870 |
Transformer | epoch 0 | step 2150 |avg loss 8.562 |avg tokens 4491.400 |tokens/s 31048.625 |walltime 325.316 |
Transformer | epoch 0 | step 2160 |avg loss 8.217 |avg tokens 4675.100 |tokens/s 31753.625 |walltime 326.789 |
Transformer | epoch 0 | step 2170 |avg loss 8.255 |avg tokens 4459.400 |tokens/s 30522.203 |walltime 328.250 |
Transformer | epoch 0 | step 2180 |avg loss 8.253 |avg tokens 4088.200 |tokens/s 29179.756 |walltime 329.651 |
Transformer | epoch 0 | step 2190 |avg loss 7.994 |avg tokens 4880.600 |tokens/s 31927.278 |walltime 331.179 |
Transformer | epoch 0 | step 2200 |avg loss 8.330 |avg tokens 4615.800 |tokens/s 31503.237 |walltime 332.644 |
Transformer | epoch 0 | step 2210 |avg loss 7.740 |avg tokens 4213.400 |tokens/s 28701.883 |walltime 334.112 |
Transformer | epoch 0 | step 2220 |avg loss 8.212 |avg tokens 4809.100 |tokens/s 32232.592 |walltime 335.604 |
Transformer | epoch 0 | step 2230 |avg loss 7.662 |avg tokens 4847.200 |tokens/s 31980.650 |walltime 337.120 |
Transformer | epoch 0 | step 2240 |avg loss 7.800 |avg tokens 4871.200 |tokens/s 32591.113 |walltime 338.615 |
Transformer | epoch 0 | step 2250 |avg loss 7.692 |avg tokens 4487.800 |tokens/s 29928.242 |walltime 340.114 |
Transformer | epoch 0 | step 2260 |avg loss 7.565 |avg tokens 4374.500 |tokens/s 29089.674 |walltime 341.618 |
Transformer | epoch 0 | step 2270 |avg loss 8.180 |avg tokens 4482.900 |tokens/s 29576.213 |walltime 343.134 |
Transformer | epoch 0 | step 2280 |avg loss 7.906 |avg tokens 4619.200 |tokens/s 30579.551 |walltime 344.644 |
Transformer | epoch 0 | step 2290 |avg loss 7.876 |avg tokens 4869.600 |tokens/s 32108.414 |walltime 346.161 |
Transformer | epoch 0 | step 2300 |avg loss 8.043 |avg tokens 4270.100 |tokens/s 29269.887 |walltime 347.620 |
Transformer | epoch 0 | step 2310 |avg loss 8.113 |avg tokens 4845.700 |tokens/s 32664.931 |walltime 349.103 |
Transformer | epoch 0 | step 2320 |avg loss 8.343 |avg tokens 3956.200 |tokens/s 27491.653 |walltime 350.542 |
Transformer | epoch 0 | step 2330 |avg loss 8.409 |avg tokens 4580.200 |tokens/s 31783.573 |walltime 351.983 |
Transformer | epoch 0 | step 2340 |avg loss 8.401 |avg tokens 3958.800 |tokens/s 27427.808 |walltime 353.427 |
Transformer | epoch 0 | step 2350 |avg loss 8.242 |avg tokens 4497.200 |tokens/s 30699.451 |walltime 354.892 |
Transformer | epoch 0 | step 2360 |avg loss 8.190 |avg tokens 4488.300 |tokens/s 31157.771 |walltime 356.332 |
Transformer | epoch 0 | step 2370 |avg loss 7.643 |avg tokens 4794.400 |tokens/s 31789.994 |walltime 357.840 |
Transformer | epoch 0 | step 2380 |avg loss 7.996 |avg tokens 4752.000 |tokens/s 32157.181 |walltime 359.318 |
Transformer | epoch 0 | step 2390 |avg loss 8.036 |avg tokens 4304.100 |tokens/s 28105.117 |walltime 360.850 |
Transformer | epoch 0 | step 2400 |avg loss 8.043 |avg tokens 4257.000 |tokens/s 29494.181 |walltime 362.293 |
Transformer | epoch 0 | step 2410 |avg loss 7.911 |avg tokens 4707.900 |tokens/s 31707.762 |walltime 363.778 |
Transformer | epoch 0 | step 2420 |avg loss 7.938 |avg tokens 4799.400 |tokens/s 32578.508 |walltime 365.251 |
Transformer | epoch 0 | step 2430 |avg loss 8.448 |avg tokens 4149.300 |tokens/s 29633.535 |walltime 366.651 |
Transformer | epoch 0 | step 2440 |avg loss 7.630 |avg tokens 4544.300 |tokens/s 30074.440 |walltime 368.162 |
Transformer | epoch 0 | step 2450 |avg loss 7.823 |avg tokens 4848.400 |tokens/s 32216.372 |walltime 369.667 |
Transformer | epoch 0 | step 2460 |avg loss 8.248 |avg tokens 4652.000 |tokens/s 32302.003 |walltime 371.107 |
Transformer | epoch 0 | step 2470 |avg loss 7.259 |avg tokens 4695.100 |tokens/s 31089.313 |walltime 372.617 |
Transformer | epoch 0 | step 2480 |avg loss 8.193 |avg tokens 4430.800 |tokens/s 30490.631 |walltime 374.071 |
Transformer | epoch 0 | step 2490 |avg loss 7.867 |avg tokens 4577.000 |tokens/s 30913.126 |walltime 375.551 |
Transformer | epoch 0 | step 2500 |avg loss 8.115 |avg tokens 4749.400 |tokens/s 32841.324 |walltime 376.997 |
Transformer | epoch 0 | step 2510 |avg loss 7.637 |avg tokens 4614.400 |tokens/s 30686.310 |walltime 378.501 |
Transformer | epoch 0 | step 2520 |avg loss 7.590 |avg tokens 4528.500 |tokens/s 29905.422 |walltime 380.015 |
Transformer | epoch 0 | step 2530 |avg loss 7.519 |avg tokens 4942.600 |tokens/s 32918.802 |walltime 381.517 |
Transformer | epoch 0 | step 2540 |avg loss 8.306 |avg tokens 4209.700 |tokens/s 29739.367 |walltime 382.932 |
Transformer | epoch 0 | step 2550 |avg loss 7.802 |avg tokens 4218.800 |tokens/s 28311.054 |walltime 384.422 |
Transformer | epoch 0 | step 2560 |avg loss 7.992 |avg tokens 4196.600 |tokens/s 29211.578 |walltime 385.859 |
Transformer | epoch 0 | step 2570 |avg loss 8.443 |avg tokens 4717.000 |tokens/s 33094.258 |walltime 387.284 |
Transformer | epoch 0 | step 2580 |avg loss 8.093 |avg tokens 4259.000 |tokens/s 29087.968 |walltime 388.749 |
Transformer | epoch 0 | step 2590 |avg loss 7.732 |avg tokens 4557.600 |tokens/s 30660.578 |walltime 390.235 |
Transformer | epoch 0 | step 2600 |avg loss 8.007 |avg tokens 4531.900 |tokens/s 30985.530 |walltime 391.698 |
Transformer | epoch 0 | step 2610 |avg loss 8.202 |avg tokens 4809.800 |tokens/s 32506.026 |walltime 393.177 |
Transformer | epoch 0 | step 2620 |avg loss 7.507 |avg tokens 4689.500 |tokens/s 30736.238 |walltime 394.703 |
Transformer | epoch 0 | step 2630 |avg loss 7.607 |avg tokens 4837.000 |tokens/s 31947.687 |walltime 396.217 |
Transformer | epoch 0 | step 2640 |avg loss 7.479 |avg tokens 4858.600 |tokens/s 32894.813 |walltime 397.694 |
Transformer | epoch 0 | step 2650 |avg loss 7.487 |avg tokens 4916.600 |tokens/s 33092.311 |walltime 399.180 |
Transformer | epoch 0 | step 2660 |avg loss 7.817 |avg tokens 4636.300 |tokens/s 32038.562 |walltime 400.627 |
Transformer | epoch 0 | step 2670 |avg loss 7.565 |avg tokens 4866.200 |tokens/s 32054.456 |walltime 402.145 |
Transformer | epoch 0 | step 2680 |avg loss 7.836 |avg tokens 4668.700 |tokens/s 31975.511 |walltime 403.605 |
Transformer | epoch 0 | step 2690 |avg loss 7.512 |avg tokens 4719.500 |tokens/s 30683.980 |walltime 405.143 |
Transformer | epoch 0 | step 2700 |avg loss 7.003 |avg tokens 4715.200 |tokens/s 31101.043 |walltime 406.659 |
Transformer | epoch 0 | step 2710 |avg loss 8.280 |avg tokens 4519.100 |tokens/s 31730.976 |walltime 408.084 |
Transformer | epoch 0 | step 2720 |avg loss 7.972 |avg tokens 4620.000 |tokens/s 31625.186 |walltime 409.544 |
Transformer | epoch 0 | step 2730 |avg loss 7.746 |avg tokens 4460.700 |tokens/s 30311.001 |walltime 411.016 |
Transformer | epoch 0 | step 2740 |avg loss 7.398 |avg tokens 4468.000 |tokens/s 30518.967 |walltime 412.480 |
Transformer | epoch 0 | step 2750 |avg loss 7.666 |avg tokens 4680.800 |tokens/s 31439.924 |walltime 413.969 |
Transformer | epoch 0 | step 2760 |avg loss 7.112 |avg tokens 4638.400 |tokens/s 30455.074 |walltime 415.492 |
Transformer | epoch 0 | step 2770 |avg loss 8.048 |avg tokens 4048.100 |tokens/s 29299.730 |walltime 416.874 |
Transformer | epoch 0 | step 2780 |avg loss 7.406 |avg tokens 4778.100 |tokens/s 31964.988 |walltime 418.368 |
Transformer | epoch 0 | step 2790 |avg loss 8.390 |avg tokens 3867.800 |tokens/s 27105.546 |walltime 419.795 |
Transformer | epoch 0 | step 2800 |avg loss 8.571 |avg tokens 4538.000 |tokens/s 31481.878 |walltime 421.237 |
Transformer | epoch 0 | step 2810 |avg loss 8.406 |avg tokens 4381.000 |tokens/s 30679.390 |walltime 422.665 |
Transformer | epoch 0 | step 2820 |avg loss 8.175 |avg tokens 4419.200 |tokens/s 28229.411 |walltime 424.230 |
Transformer | epoch 0 | step 2830 |avg loss 7.805 |avg tokens 4910.600 |tokens/s 33155.598 |walltime 425.711 |
Transformer | epoch 0 | step 2840 |avg loss 7.291 |avg tokens 4520.400 |tokens/s 30137.777 |walltime 427.211 |
Transformer | epoch 0 | step 2850 |avg loss 7.324 |avg tokens 4584.200 |tokens/s 30485.515 |walltime 428.715 |
Transformer | epoch 0 | step 2860 |avg loss 7.575 |avg tokens 4672.600 |tokens/s 32170.739 |walltime 430.167 |
Transformer | epoch 0 | step 2870 |avg loss 8.256 |avg tokens 4281.600 |tokens/s 30324.381 |walltime 431.579 |
Transformer | epoch 0 | step 2880 |avg loss 7.810 |avg tokens 4483.200 |tokens/s 30310.637 |walltime 433.058 |
Transformer | epoch 0 | step 2890 |avg loss 7.623 |avg tokens 4717.300 |tokens/s 32165.845 |walltime 434.525 |
Transformer | epoch 0 | step 2900 |avg loss 7.238 |avg tokens 4648.500 |tokens/s 31135.198 |walltime 436.018 |
Transformer | epoch 0 | step 2910 |avg loss 7.967 |avg tokens 4441.700 |tokens/s 30970.974 |walltime 437.452 |
Transformer | epoch 0 | step 2920 |avg loss 7.581 |avg tokens 4958.700 |tokens/s 33683.212 |walltime 438.924 |
Transformer | epoch 0 | step 2930 |avg loss 7.560 |avg tokens 4812.300 |tokens/s 31294.360 |walltime 440.462 |
Transformer | epoch 0 | step 2940 |avg loss 7.223 |avg tokens 4592.800 |tokens/s 30429.410 |walltime 441.971 |
Transformer | epoch 0 | step 2950 |avg loss 7.929 |avg tokens 4451.900 |tokens/s 30685.315 |walltime 443.422 |
Transformer | epoch 0 | step 2960 |avg loss 7.525 |avg tokens 4556.400 |tokens/s 30881.944 |walltime 444.898 |
Transformer | epoch 0 | step 2970 |avg loss 7.449 |avg tokens 4738.000 |tokens/s 30745.491 |walltime 446.439 |
Transformer | epoch 0 | step 2980 |avg loss 7.933 |avg tokens 4520.100 |tokens/s 30980.558 |walltime 447.898 |
Transformer | epoch 0 | step 2990 |avg loss 7.471 |avg tokens 4483.200 |tokens/s 30333.339 |walltime 449.376 |
Transformer | epoch 0 | step 3000 |avg loss 7.316 |avg tokens 4767.100 |tokens/s 31739.753 |walltime 450.878 |
Transformer | epoch 0 | step 3010 |avg loss 8.106 |avg tokens 4051.600 |tokens/s 28942.221 |walltime 452.277 |
Transformer | epoch 0 | step 3020 |avg loss 7.583 |avg tokens 4708.000 |tokens/s 31521.944 |walltime 453.771 |
Transformer | epoch 0 | step 3030 |avg loss 7.857 |avg tokens 4462.500 |tokens/s 30780.629 |walltime 455.221 |
Transformer | epoch 0 | step 3040 |avg loss 8.154 |avg tokens 4632.900 |tokens/s 31048.396 |walltime 456.713 |
Transformer | epoch 0 | step 3050 |avg loss 7.323 |avg tokens 4756.600 |tokens/s 30838.526 |walltime 458.255 |
Transformer | epoch 0 | step 3060 |avg loss 7.244 |avg tokens 4733.800 |tokens/s 31750.899 |walltime 459.746 |
Transformer | epoch 0 | step 3070 |avg loss 7.748 |avg tokens 3856.500 |tokens/s 28007.635 |walltime 461.123 |
Transformer | epoch 0 | step 3080 |avg loss 7.566 |avg tokens 4400.200 |tokens/s 29236.508 |walltime 462.628 |
Transformer | epoch 0 | step 3090 |avg loss 7.046 |avg tokens 4792.600 |tokens/s 31273.637 |walltime 464.161 |
Transformer | epoch 0 | step 3100 |avg loss 7.023 |avg tokens 4781.700 |tokens/s 31595.061 |walltime 465.674 |
Transformer | epoch 0 | step 3110 |avg loss 6.801 |avg tokens 4752.800 |tokens/s 30562.797 |walltime 467.229 |
Transformer | epoch 0 | step 3120 |avg loss 7.806 |avg tokens 4234.800 |tokens/s 29418.156 |walltime 468.669 |
Transformer | epoch 0 | step 3130 |avg loss 7.747 |avg tokens 4025.100 |tokens/s 27998.866 |walltime 470.106 |
Transformer | epoch 0 | step 3140 |avg loss 8.013 |avg tokens 4137.600 |tokens/s 29277.757 |walltime 471.520 |
Transformer | epoch 0 | step 3150 |avg loss 7.139 |avg tokens 4786.500 |tokens/s 31479.003 |walltime 473.040 |
Transformer | epoch 0 | step 3160 |avg loss 7.276 |avg tokens 4760.600 |tokens/s 31434.730 |walltime 474.555 |
Transformer | epoch 0 | step 3170 |avg loss 7.789 |avg tokens 4462.700 |tokens/s 30627.380 |walltime 476.012 |
Transformer | epoch 0 | step 3180 |avg loss 8.057 |avg tokens 4450.100 |tokens/s 31276.612 |walltime 477.435 |
Transformer | epoch 0 | step 3190 |avg loss 8.157 |avg tokens 3842.800 |tokens/s 27823.589 |walltime 478.816 |
Transformer | epoch 0 | step 3200 |avg loss 7.524 |avg tokens 4288.300 |tokens/s 29337.691 |walltime 480.277 |
Transformer | epoch 0 | step 3210 |avg loss 7.478 |avg tokens 4634.700 |tokens/s 31707.033 |walltime 481.739 |
Transformer | epoch 0 | step 3220 |avg loss 7.203 |avg tokens 4686.100 |tokens/s 30660.680 |walltime 483.268 |
Transformer | epoch 0 | step 3230 |avg loss 7.737 |avg tokens 4234.700 |tokens/s 29828.125 |walltime 484.687 |
Transformer | epoch 0 | step 3240 |avg loss 8.051 |avg tokens 4545.400 |tokens/s 32210.756 |walltime 486.098 |
Transformer | epoch 0 | step 3250 |avg loss 7.395 |avg tokens 4806.800 |tokens/s 31677.178 |walltime 487.616 |
Transformer | epoch 0 | step 3260 |avg loss 7.532 |avg tokens 4752.200 |tokens/s 32257.282 |walltime 489.089 |
Transformer | epoch 0 | step 3270 |avg loss 7.741 |avg tokens 4409.400 |tokens/s 30741.573 |walltime 490.523 |
Transformer | epoch 0 | step 3280 |avg loss 7.044 |avg tokens 4706.300 |tokens/s 31219.263 |walltime 492.031 |
Transformer | epoch 0 | step 3290 |avg loss 6.903 |avg tokens 4719.900 |tokens/s 30750.090 |walltime 493.566 |
Transformer | epoch 0 | step 3300 |avg loss 7.739 |avg tokens 4489.000 |tokens/s 31160.603 |walltime 495.006 |
Transformer | epoch 0 | step 3310 |avg loss 7.843 |avg tokens 4264.300 |tokens/s 29598.506 |walltime 496.447 |
Transformer | epoch 0 | step 3320 |avg loss 8.020 |avg tokens 4098.600 |tokens/s 28927.232 |walltime 497.864 |
Transformer | epoch 0 | step 3330 |avg loss 7.293 |avg tokens 4858.800 |tokens/s 32268.945 |walltime 499.370 |
Transformer | epoch 0 | step 3340 |avg loss 8.012 |avg tokens 4445.100 |tokens/s 32139.294 |walltime 500.753 |
Transformer | epoch 0 | step 3350 |avg loss 7.229 |avg tokens 4368.500 |tokens/s 29596.313 |walltime 502.229 |
Transformer | epoch 0 | step 3360 |avg loss 7.356 |avg tokens 4671.500 |tokens/s 31148.448 |walltime 503.729 |
Transformer | epoch 0 | step 3370 |avg loss 7.635 |avg tokens 4650.600 |tokens/s 31594.786 |walltime 505.201 |
Transformer | epoch 0 | step 3380 |avg loss 7.803 |avg tokens 4269.000 |tokens/s 28905.049 |walltime 506.677 |
Transformer | epoch 0 | step 3390 |avg loss 7.594 |avg tokens 4604.400 |tokens/s 31238.597 |walltime 508.151 |
Transformer | epoch 0 | step 3400 |avg loss 7.546 |avg tokens 4721.500 |tokens/s 31575.064 |walltime 509.647 |
Transformer | epoch 0 | step 3410 |avg loss 7.677 |avg tokens 4245.100 |tokens/s 28960.222 |walltime 511.113 |
Transformer | epoch 0 | step 3420 |avg loss 7.758 |avg tokens 4493.800 |tokens/s 30963.548 |walltime 512.564 |
Transformer | epoch 0 | step 3430 |avg loss 7.520 |avg tokens 4389.600 |tokens/s 30205.641 |walltime 514.017 |
Transformer | epoch 0 | step 3440 |avg loss 7.402 |avg tokens 4570.700 |tokens/s 30774.616 |walltime 515.502 |
Transformer | epoch 0 | step 3450 |avg loss 7.023 |avg tokens 4399.500 |tokens/s 28211.431 |walltime 517.062 |
Transformer | epoch 0 | step 3460 |avg loss 7.341 |avg tokens 4854.500 |tokens/s 31995.305 |walltime 518.579 |
Transformer | epoch 0 | step 3470 |avg loss 7.542 |avg tokens 4634.500 |tokens/s 32112.133 |walltime 520.022 |
Transformer | epoch 0 | step 3480 |avg loss 8.132 |avg tokens 4730.900 |tokens/s 33286.374 |walltime 521.444 |
Transformer | epoch 0 | step 3490 |avg loss 7.422 |avg tokens 4415.000 |tokens/s 30124.387 |walltime 522.909 |
Transformer | epoch 0 | step 3500 |avg loss 7.458 |avg tokens 4884.900 |tokens/s 33356.357 |walltime 524.374 |
Transformer | epoch 0 | step 3510 |avg loss 7.072 |avg tokens 4468.800 |tokens/s 30191.243 |walltime 525.854 |
Transformer | epoch 0 | step 3520 |avg loss 7.793 |avg tokens 4194.200 |tokens/s 30618.470 |walltime 527.224 |
Transformer | epoch 0 | step 3530 |avg loss 7.150 |avg tokens 4820.000 |tokens/s 31434.329 |walltime 528.757 |
Transformer | epoch 0 | step 3540 |avg loss 7.464 |avg tokens 4648.200 |tokens/s 31696.621 |walltime 530.223 |
Transformer | epoch 0 | step 3550 |avg loss 7.515 |avg tokens 4660.900 |tokens/s 31356.399 |walltime 531.710 |
Transformer | epoch 0 | step 3560 |avg loss 7.148 |avg tokens 4922.800 |tokens/s 31828.106 |walltime 533.257 |
Transformer | epoch 0 | step 3570 |avg loss 7.592 |avg tokens 3913.300 |tokens/s 28148.055 |walltime 534.647 |
Transformer | epoch 0 | step 3580 |avg loss 7.548 |avg tokens 4667.700 |tokens/s 31673.559 |walltime 536.121 |
Transformer | epoch 0 | step 3590 |avg loss 7.388 |avg tokens 4084.400 |tokens/s 28265.787 |walltime 537.566 |
Transformer | epoch 0 | step 3600 |avg loss 7.392 |avg tokens 4491.500 |tokens/s 30584.387 |walltime 539.034 |
Transformer | epoch 0 | step 3610 |avg loss 6.991 |avg tokens 4800.000 |tokens/s 31187.735 |walltime 540.573 |
Transformer | epoch 0 | step 3620 |avg loss 6.978 |avg tokens 4767.200 |tokens/s 31259.500 |walltime 542.098 |
Transformer | epoch 0 | step 3630 |avg loss 7.250 |avg tokens 4487.500 |tokens/s 30325.235 |walltime 543.578 |
Transformer | epoch 0 | step 3640 |avg loss 7.309 |avg tokens 4341.900 |tokens/s 28473.877 |walltime 545.103 |
Transformer | epoch 0 | step 3650 |avg loss 6.981 |avg tokens 4433.600 |tokens/s 30257.367 |walltime 546.568 |
Transformer | epoch 0 | step 3660 |avg loss 7.954 |avg tokens 4456.700 |tokens/s 30778.275 |walltime 548.016 |
Transformer | epoch 0 | step 3670 |avg loss 7.677 |avg tokens 4502.700 |tokens/s 30875.977 |walltime 549.474 |
Transformer | epoch 0 | step 3680 |avg loss 7.925 |avg tokens 4347.000 |tokens/s 30001.325 |walltime 550.923 |
Transformer | epoch 0 | step 3690 |avg loss 7.945 |avg tokens 3903.700 |tokens/s 28028.235 |walltime 552.316 |
Transformer | epoch 0 | step 3700 |avg loss 8.144 |avg tokens 4325.200 |tokens/s 31645.975 |walltime 553.683 |
Transformer | epoch 0 | step 3710 |avg loss 8.040 |avg tokens 4150.000 |tokens/s 29823.333 |walltime 555.074 |
Transformer | epoch 0 | step 3720 |avg loss 8.112 |avg tokens 3794.700 |tokens/s 28150.946 |walltime 556.422 |
Transformer | epoch 0 | step 3730 |avg loss 7.384 |avg tokens 4585.400 |tokens/s 30649.764 |walltime 557.919 |
Transformer | epoch 0 | step 3740 |avg loss 7.615 |avg tokens 4395.900 |tokens/s 31133.888 |walltime 559.330 |
Transformer | epoch 0 | step 3750 |avg loss 7.346 |avg tokens 4758.000 |tokens/s 32118.995 |walltime 560.812 |
Transformer | epoch 0 | step 3760 |avg loss 7.184 |avg tokens 4435.200 |tokens/s 30673.370 |walltime 562.258 |
Transformer | epoch 0 | step 3770 |avg loss 7.577 |avg tokens 4706.800 |tokens/s 32624.866 |walltime 563.701 |
Transformer | epoch 0 | step 3780 |avg loss 7.772 |avg tokens 3980.400 |tokens/s 27825.249 |walltime 565.131 |
Transformer | epoch 0 | step 3790 |avg loss 7.104 |avg tokens 4795.000 |tokens/s 31954.934 |walltime 566.632 |
Transformer | epoch 0 | step 3800 |avg loss 7.361 |avg tokens 4566.500 |tokens/s 31040.752 |walltime 568.103 |
Transformer | epoch 0 | step 3810 |avg loss 7.651 |avg tokens 4135.300 |tokens/s 29536.628 |walltime 569.503 |
Transformer | epoch 0 | step 3820 |avg loss 7.778 |avg tokens 4482.500 |tokens/s 31529.235 |walltime 570.924 |
Transformer | epoch 0 | step 3830 |avg loss 7.621 |avg tokens 4706.400 |tokens/s 32280.882 |walltime 572.382 |
Transformer | epoch 0 | step 3840 |avg loss 7.122 |avg tokens 4627.200 |tokens/s 30831.593 |walltime 573.883 |
Transformer | epoch 0 | step 3850 |avg loss 7.133 |avg tokens 4498.700 |tokens/s 30567.591 |walltime 575.355 |
Transformer | epoch 0 | step 3860 |avg loss 7.588 |avg tokens 4257.300 |tokens/s 29659.269 |walltime 576.790 |
Transformer | epoch 0 | step 3870 |avg loss 7.877 |avg tokens 4747.900 |tokens/s 33321.837 |walltime 578.215 |
Transformer | epoch 0 | step 3880 |avg loss 6.951 |avg tokens 4851.800 |tokens/s 32384.366 |walltime 579.713 |
Transformer | epoch 0 | step 3890 |avg loss 7.863 |avg tokens 4493.200 |tokens/s 31711.330 |walltime 581.130 |
Transformer | epoch 0 | step 3900 |avg loss 7.809 |avg tokens 4631.700 |tokens/s 31179.799 |walltime 582.616 |
Transformer | epoch 0 | step 3910 |avg loss 7.552 |avg tokens 4482.600 |tokens/s 30543.093 |walltime 584.083 |
Transformer | epoch 0 | step 3920 |avg loss 6.987 |avg tokens 4790.700 |tokens/s 31316.113 |walltime 585.613 |
Transformer | epoch 0 | step 3930 |avg loss 7.651 |avg tokens 3834.100 |tokens/s 28500.565 |walltime 586.959 |
Transformer | epoch 0 | step 3940 |avg loss 7.313 |avg tokens 4577.900 |tokens/s 30526.136 |walltime 588.458 |
Transformer | epoch 0 | step 3950 |avg loss 7.341 |avg tokens 4858.100 |tokens/s 32533.384 |walltime 589.951 |
Transformer | epoch 0 | step 3960 |avg loss 7.900 |avg tokens 4213.900 |tokens/s 30389.713 |walltime 591.338 |
Transformer | epoch 0 | step 3970 |avg loss 7.372 |avg tokens 4324.600 |tokens/s 30272.710 |walltime 592.767 |
Transformer | epoch 0 | step 3980 |avg loss 7.868 |avg tokens 4116.600 |tokens/s 29065.107 |walltime 594.183 |
Transformer | epoch 0 | step 3990 |avg loss 7.488 |avg tokens 4243.300 |tokens/s 30344.211 |walltime 595.581 |
Transformer | epoch 0 | step 4000 |avg loss 7.267 |avg tokens 4820.800 |tokens/s 32409.712 |walltime 597.069 |
Transformer | epoch 0 | step 4010 |avg loss 7.233 |avg tokens 4525.100 |tokens/s 30864.027 |walltime 598.535 |
Transformer | epoch 0 | step 4020 |avg loss 7.029 |avg tokens 4737.600 |tokens/s 31820.877 |walltime 600.024 |
Transformer | epoch 0 | step 4030 |avg loss 7.839 |avg tokens 4585.600 |tokens/s 32876.287 |walltime 601.419 |
Transformer | epoch 0 | step 4040 |avg loss 7.718 |avg tokens 4099.900 |tokens/s 28260.511 |walltime 602.869 |
Transformer | epoch 0 | step 4050 |avg loss 7.651 |avg tokens 4795.300 |tokens/s 32635.708 |walltime 604.339 |
Transformer | epoch 0 | step 4060 |avg loss 7.222 |avg tokens 4415.300 |tokens/s 29932.391 |walltime 605.814 |
Transformer | epoch 0 | step 4070 |avg loss 7.645 |avg tokens 4089.100 |tokens/s 29376.152 |walltime 607.206 |
Transformer | epoch 0 | step 4080 |avg loss 7.547 |avg tokens 4679.100 |tokens/s 31522.039 |walltime 608.690 |
Transformer | epoch 0 | step 4090 |avg loss 7.040 |avg tokens 4786.400 |tokens/s 31076.994 |walltime 610.230 |
Transformer | epoch 0 | step 4100 |avg loss 7.673 |avg tokens 4735.300 |tokens/s 31679.580 |walltime 611.725 |
Transformer | epoch 0 | step 4110 |avg loss 7.597 |avg tokens 4287.200 |tokens/s 29343.672 |walltime 613.186 |
Transformer | epoch 0 | step 4120 |avg loss 7.595 |avg tokens 4061.600 |tokens/s 27554.565 |walltime 614.660 |
Transformer | epoch 0 | step 4130 |avg loss 7.558 |avg tokens 4720.200 |tokens/s 31477.826 |walltime 616.160 |
Transformer | epoch 0 | step 4140 |avg loss 7.674 |avg tokens 4554.500 |tokens/s 30947.310 |walltime 617.631 |
Transformer | epoch 0 | step 4150 |avg loss 7.378 |avg tokens 4460.500 |tokens/s 30139.873 |walltime 619.111 |
Transformer | epoch 0 | step 4160 |avg loss 7.542 |avg tokens 4707.200 |tokens/s 31618.148 |walltime 620.600 |
Transformer | epoch 0 | step 4170 |avg loss 7.340 |avg tokens 4741.800 |tokens/s 31837.631 |walltime 622.089 |
Transformer | epoch 0 | step 4180 |avg loss 6.907 |avg tokens 4937.400 |tokens/s 31522.075 |walltime 623.656 |
Transformer | epoch 0 | step 4190 |avg loss 7.914 |avg tokens 4397.300 |tokens/s 30157.353 |walltime 625.114 |
Transformer | epoch 0 | step 4200 |avg loss 7.668 |avg tokens 4302.000 |tokens/s 29175.267 |walltime 626.588 |
Transformer | epoch 0 | step 4210 |avg loss 7.729 |avg tokens 4660.800 |tokens/s 31274.917 |walltime 628.079 |
Transformer | epoch 0 | step 4220 |avg loss 7.482 |avg tokens 4817.500 |tokens/s 32106.011 |walltime 629.579 |
Transformer | epoch 0 | step 4230 |avg loss 7.232 |avg tokens 4532.400 |tokens/s 30354.508 |walltime 631.072 |
Transformer | epoch 0 | step 4240 |avg loss 7.506 |avg tokens 4726.500 |tokens/s 31300.050 |walltime 632.582 |
Transformer | epoch 0 | step 4250 |avg loss 7.495 |avg tokens 4230.000 |tokens/s 28708.980 |walltime 634.056 |
Transformer | epoch 0 | step 4260 |avg loss 7.601 |avg tokens 4632.500 |tokens/s 31409.325 |walltime 635.531 |
Transformer | epoch 0 | step 4270 |avg loss 7.152 |avg tokens 4711.200 |tokens/s 31541.391 |walltime 637.024 |
Transformer | epoch 0 | step 4280 |avg loss 7.433 |avg tokens 4783.600 |tokens/s 32694.090 |walltime 638.488 |
Transformer | epoch 0 | step 4290 |avg loss 7.536 |avg tokens 4682.600 |tokens/s 32213.752 |walltime 639.941 |
Transformer | epoch 0 | step 4300 |avg loss 7.377 |avg tokens 4710.200 |tokens/s 31551.320 |walltime 641.434 |
Transformer | epoch 0 | step 4310 |avg loss 7.349 |avg tokens 4259.900 |tokens/s 29194.645 |walltime 642.893 |
Transformer | epoch 0 | step 4320 |avg loss 7.096 |avg tokens 4786.200 |tokens/s 31720.374 |walltime 644.402 |
Transformer | epoch 0 | step 4330 |avg loss 7.001 |avg tokens 4976.100 |tokens/s 32520.758 |walltime 645.932 |
Transformer | epoch 0 | step 4340 |avg loss 7.206 |avg tokens 4897.300 |tokens/s 32626.109 |walltime 647.433 |
Transformer | epoch 0 | step 4350 |avg loss 7.425 |avg tokens 4771.600 |tokens/s 31747.635 |walltime 648.936 |
Transformer | epoch 0 | step 4360 |avg loss 7.379 |avg tokens 4546.000 |tokens/s 30836.745 |walltime 650.410 |
Transformer | epoch 0 | step 4370 |avg loss 6.990 |avg tokens 4741.500 |tokens/s 31061.829 |walltime 651.937 |
Transformer | epoch 0 | step 4380 |avg loss 6.820 |avg tokens 4860.000 |tokens/s 31485.311 |walltime 653.480 |
Transformer | epoch 0 | step 4390 |avg loss 7.246 |avg tokens 4632.600 |tokens/s 31428.039 |walltime 654.955 |
Transformer | epoch 0 | step 4400 |avg loss 7.277 |avg tokens 4809.600 |tokens/s 32478.268 |walltime 656.435 |
Transformer | epoch 0 | step 4410 |avg loss 7.154 |avg tokens 4511.400 |tokens/s 31061.217 |walltime 657.888 |
Transformer | epoch 0 | step 4420 |avg loss 7.416 |avg tokens 4736.800 |tokens/s 31766.175 |walltime 659.379 |
Transformer | epoch 0 | step 4430 |avg loss 7.495 |avg tokens 4425.300 |tokens/s 30421.673 |walltime 660.834 |
Transformer | epoch 0 | step 4440 |avg loss 7.538 |avg tokens 4341.600 |tokens/s 30146.290 |walltime 662.274 |
Transformer | epoch 0 | step 4450 |avg loss 7.153 |avg tokens 4696.700 |tokens/s 31740.641 |walltime 663.754 |
Transformer | epoch 0 | step 4460 |avg loss 7.627 |avg tokens 4009.700 |tokens/s 28536.597 |walltime 665.159 |
Transformer | epoch 0 | step 4470 |avg loss 7.450 |avg tokens 4660.400 |tokens/s 31072.218 |walltime 666.658 |
Transformer | epoch 0 | step 4480 |avg loss 7.609 |avg tokens 4635.900 |tokens/s 31397.570 |walltime 668.135 |
Transformer | epoch 0 | step 4490 |avg loss 7.785 |avg tokens 4497.800 |tokens/s 31107.870 |walltime 669.581 |
Transformer | epoch 0 | step 4500 |avg loss 7.202 |avg tokens 4759.200 |tokens/s 31511.895 |walltime 671.091 |
Transformer | epoch 0 | step 4510 |avg loss 7.003 |avg tokens 4565.000 |tokens/s 29955.930 |walltime 672.615 |
Transformer | epoch 0 | step 4520 |avg loss 7.550 |avg tokens 4777.600 |tokens/s 32178.691 |walltime 674.100 |
Transformer | epoch 0 | step 4530 |avg loss 7.098 |avg tokens 4808.800 |tokens/s 32038.475 |walltime 675.601 |
Transformer | epoch 0 | step 4540 |avg loss 7.048 |avg tokens 4684.800 |tokens/s 30934.361 |walltime 677.115 |
Transformer | epoch 0 | step 4550 |avg loss 7.954 |avg tokens 4766.500 |tokens/s 33063.769 |walltime 678.557 |
Transformer | epoch 0 | step 4560 |avg loss 7.285 |avg tokens 4534.300 |tokens/s 31182.583 |walltime 680.011 |
Transformer | epoch 0 | step 4570 |avg loss 7.219 |avg tokens 4792.000 |tokens/s 32585.627 |walltime 681.481 |
Transformer | epoch 0 | step 4580 |avg loss 7.271 |avg tokens 4493.700 |tokens/s 30295.556 |walltime 682.965 |
Transformer | epoch 0 | step 4590 |avg loss 7.373 |avg tokens 4432.400 |tokens/s 29811.978 |walltime 684.452 |
Transformer | epoch 0 | step 4600 |avg loss 7.598 |avg tokens 4740.000 |tokens/s 31839.011 |walltime 685.940 |
Transformer | epoch 0 | step 4610 |avg loss 7.188 |avg tokens 4532.700 |tokens/s 29995.825 |walltime 687.451 |
Transformer | epoch 0 | step 4620 |avg loss 7.484 |avg tokens 4344.100 |tokens/s 29872.351 |walltime 688.906 |
Transformer | epoch 0 | step 4630 |avg loss 8.093 |avg tokens 4440.600 |tokens/s 31769.003 |walltime 690.303 |
Transformer | epoch 0 | step 4640 |avg loss 8.030 |avg tokens 4544.200 |tokens/s 32230.541 |walltime 691.713 |
Transformer | epoch 0 | step 4650 |avg loss 7.370 |avg tokens 4390.800 |tokens/s 29978.075 |walltime 693.178 |
Transformer | epoch 0 | step 4660 |avg loss 7.246 |avg tokens 4532.100 |tokens/s 31044.675 |walltime 694.638 |
Transformer | epoch 0 | step 4670 |avg loss 8.036 |avg tokens 4505.100 |tokens/s 32161.330 |walltime 696.039 |
Transformer | epoch 0 | step 4680 |avg loss 7.581 |avg tokens 4507.200 |tokens/s 31460.222 |walltime 697.471 |
Transformer | epoch 0 | step 4690 |avg loss 7.583 |avg tokens 4162.100 |tokens/s 28941.326 |walltime 698.909 |
Transformer | epoch 0 | step 4700 |avg loss 8.109 |avg tokens 3964.900 |tokens/s 28097.807 |walltime 700.321 |
Transformer | epoch 0 | step 4710 |avg loss 7.375 |avg tokens 4451.900 |tokens/s 29711.716 |walltime 701.819 |
Transformer | epoch 0 | step 4720 |avg loss 7.260 |avg tokens 4472.900 |tokens/s 30243.139 |walltime 703.298 |
Transformer | epoch 0 | step 4730 |avg loss 7.387 |avg tokens 4342.900 |tokens/s 28894.794 |walltime 704.801 |
Transformer | epoch 0 | step 4740 |avg loss 7.316 |avg tokens 4516.100 |tokens/s 30402.210 |walltime 706.286 |
Transformer | epoch 0 | step 4750 |avg loss 7.428 |avg tokens 4667.000 |tokens/s 30884.517 |walltime 707.797 |
Transformer | epoch 0 | step 4760 |avg loss 7.460 |avg tokens 4367.900 |tokens/s 28653.747 |walltime 709.322 |
Transformer | epoch 0 | step 4770 |avg loss 7.039 |avg tokens 4503.300 |tokens/s 30305.816 |walltime 710.808 |
Transformer | epoch 0 | step 4780 |avg loss 7.895 |avg tokens 4380.900 |tokens/s 30533.035 |walltime 712.243 |
Transformer | epoch 0 | step 4790 |avg loss 7.202 |avg tokens 4590.400 |tokens/s 30751.732 |walltime 713.735 |
Transformer | epoch 0 | step 4800 |avg loss 8.021 |avg tokens 4484.400 |tokens/s 30190.716 |walltime 715.221 |
Transformer | epoch 0 | step 4810 |avg loss 7.924 |avg tokens 4613.600 |tokens/s 31713.691 |walltime 716.675 |
Transformer | epoch 0 | step 4820 |avg loss 6.857 |avg tokens 4752.800 |tokens/s 31357.654 |walltime 718.191 |
Transformer | epoch 0 | step 4830 |avg loss 7.601 |avg tokens 4666.100 |tokens/s 31269.120 |walltime 719.683 |
Transformer | epoch 0 | step 4840 |avg loss 7.004 |avg tokens 4764.800 |tokens/s 31776.334 |walltime 721.183 |
Transformer | epoch 0 | step 4850 |avg loss 7.648 |avg tokens 4130.200 |tokens/s 28158.980 |walltime 722.650 |
Transformer | epoch 0 | step 4860 |avg loss 7.972 |avg tokens 4481.900 |tokens/s 30940.581 |walltime 724.098 |
Transformer | epoch 0 | step 4870 |avg loss 7.244 |avg tokens 4849.400 |tokens/s 32844.348 |walltime 725.575 |
Transformer | epoch 0 | step 4880 |avg loss 7.617 |avg tokens 4705.900 |tokens/s 32453.126 |walltime 727.025 |
Transformer | epoch 0 | step 4890 |avg loss 7.560 |avg tokens 3942.700 |tokens/s 27190.850 |walltime 728.475 |
Transformer | epoch 0 | step 4900 |avg loss 7.938 |avg tokens 3877.800 |tokens/s 27221.149 |walltime 729.899 |
Transformer | epoch 0 | step 4910 |avg loss 7.774 |avg tokens 4907.200 |tokens/s 34124.728 |walltime 731.337 |
Transformer | epoch 0 | step 4920 |avg loss 7.952 |avg tokens 4641.700 |tokens/s 32488.136 |walltime 732.766 |
Transformer | epoch 0 | step 4930 |avg loss 7.599 |avg tokens 4803.900 |tokens/s 32142.309 |walltime 734.261 |
Transformer | epoch 0 | step 4940 |avg loss 7.820 |avg tokens 4372.600 |tokens/s 30838.147 |walltime 735.679 |
Transformer | epoch 0 | step 4950 |avg loss 7.530 |avg tokens 4342.200 |tokens/s 29928.687 |walltime 737.129 |
Transformer | epoch 0 | step 4960 |avg loss 7.778 |avg tokens 4705.800 |tokens/s 31191.523 |walltime 738.638 |
Transformer | epoch 0 | step 4970 |avg loss 7.722 |avg tokens 4300.200 |tokens/s 29706.153 |walltime 740.086 |
Transformer | epoch 0 | step 4980 |avg loss 8.196 |avg tokens 3810.800 |tokens/s 27838.075 |walltime 741.455 |
Transformer | epoch 0 | step 4990 |avg loss 7.911 |avg tokens 4337.000 |tokens/s 30628.461 |walltime 742.871 |
Transformer | epoch 0 | step 5000 |avg loss 7.369 |avg tokens 4908.200 |tokens/s 32136.006 |walltime 744.398 |
Transformer | epoch 0 | step 5010 |avg loss 6.948 |avg tokens 4392.100 |tokens/s 29701.610 |walltime 745.877 |
Transformer | epoch 0 | step 5020 |avg loss 6.933 |avg tokens 4881.600 |tokens/s 31425.337 |walltime 747.430 |
Transformer | epoch 0 | step 5030 |avg loss 7.296 |avg tokens 4556.000 |tokens/s 31271.569 |walltime 748.887 |
Transformer | epoch 0 | step 5040 |avg loss 7.618 |avg tokens 4568.500 |tokens/s 31404.715 |walltime 750.342 |
Transformer | epoch 0 | step 5050 |avg loss 7.771 |avg tokens 4718.600 |tokens/s 32192.854 |walltime 751.807 |
Transformer | epoch 0 | step 5060 |avg loss 7.748 |avg tokens 4252.500 |tokens/s 29314.961 |walltime 753.258 |
Transformer | epoch 0 | step 5070 |avg loss 8.188 |avg tokens 4179.300 |tokens/s 29875.952 |walltime 754.657 |
Transformer | epoch 0 | step 5080 |avg loss 7.320 |avg tokens 4591.000 |tokens/s 31028.642 |walltime 756.137 |
Transformer | epoch 0 | step 5090 |avg loss 7.929 |avg tokens 4182.600 |tokens/s 27892.406 |walltime 757.636 |
Transformer | epoch 0 | step 5100 |avg loss 7.512 |avg tokens 4366.500 |tokens/s 30250.417 |walltime 759.080 |
Transformer | epoch 0 | step 5110 |avg loss 6.896 |avg tokens 4669.500 |tokens/s 31087.135 |walltime 760.582 |
Transformer | epoch 0 | step 5120 |avg loss 7.579 |avg tokens 4357.700 |tokens/s 29908.878 |walltime 762.039 |
Transformer | epoch 0 | step 5130 |avg loss 7.559 |avg tokens 4765.400 |tokens/s 31431.476 |walltime 763.555 |
Transformer | epoch 0 | step 5140 |avg loss 7.363 |avg tokens 4533.800 |tokens/s 30334.053 |walltime 765.049 |
Transformer | epoch 0 | step 5150 |avg loss 7.401 |avg tokens 4537.100 |tokens/s 30817.896 |walltime 766.522 |
Transformer | epoch 0 | step 5160 |avg loss 7.327 |avg tokens 4629.900 |tokens/s 30343.347 |walltime 768.047 |
Transformer | epoch 0 | step 5170 |avg loss 7.302 |avg tokens 4836.000 |tokens/s 32743.979 |walltime 769.524 |
Transformer | epoch 0 | step 5180 |avg loss 7.772 |avg tokens 4406.600 |tokens/s 30429.586 |walltime 770.973 |
Transformer | epoch 0 | step 5190 |avg loss 7.425 |avg tokens 4882.500 |tokens/s 32139.007 |walltime 772.492 |
Transformer | epoch 0 | step 5200 |avg loss 7.867 |avg tokens 4506.200 |tokens/s 30010.714 |walltime 773.993 |
Transformer | epoch 0 | step 5210 |avg loss 7.784 |avg tokens 4086.800 |tokens/s 28903.705 |walltime 775.407 |
Transformer | epoch 0 | step 5220 |avg loss 7.330 |avg tokens 4533.300 |tokens/s 30929.336 |walltime 776.873 |
Transformer | epoch 0 | step 5230 |avg loss 7.755 |avg tokens 4540.300 |tokens/s 30829.566 |walltime 778.346 |
Transformer | epoch 0 | step 5240 |avg loss 7.338 |avg tokens 4854.900 |tokens/s 32475.529 |walltime 779.841 |
Transformer | epoch 0 | step 5250 |avg loss 7.848 |avg tokens 4628.300 |tokens/s 31688.715 |walltime 781.301 |
Transformer | epoch 0 | step 5260 |avg loss 7.773 |avg tokens 4877.400 |tokens/s 33598.028 |walltime 782.753 |
Transformer | epoch 0 | step 5270 |avg loss 7.248 |avg tokens 4707.200 |tokens/s 30951.890 |walltime 784.274 |
Transformer | epoch 0 | step 5280 |avg loss 7.499 |avg tokens 4299.100 |tokens/s 29900.594 |walltime 785.711 |
Transformer | epoch 0 | step 5290 |avg loss 8.262 |avg tokens 4819.600 |tokens/s 34877.676 |walltime 787.093 |
Transformer | epoch 0 | step 5300 |avg loss 7.763 |avg tokens 4576.400 |tokens/s 30265.113 |walltime 788.605 |
Transformer | epoch 0 | step 5310 |avg loss 7.517 |avg tokens 4565.000 |tokens/s 30589.356 |walltime 790.098 |
Transformer | epoch 0 | step 5320 |avg loss 7.182 |avg tokens 4465.000 |tokens/s 30059.577 |walltime 791.583 |
Transformer | epoch 0 | step 5330 |avg loss 7.820 |avg tokens 4213.900 |tokens/s 29930.325 |walltime 792.991 |
Transformer | epoch 0 | step 5340 |avg loss 7.563 |avg tokens 4311.200 |tokens/s 29527.754 |walltime 794.451 |
Transformer | epoch 0 | step 5350 |avg loss 7.174 |avg tokens 4700.800 |tokens/s 31037.212 |walltime 795.966 |
Transformer | epoch 0 | step 5360 |avg loss 7.420 |avg tokens 4734.300 |tokens/s 31106.974 |walltime 797.488 |
Transformer | epoch 0 | step 5370 |avg loss 7.219 |avg tokens 4915.100 |tokens/s 32662.655 |walltime 798.992 |
Transformer | epoch 0 | step 5380 |avg loss 7.467 |avg tokens 4554.600 |tokens/s 30956.224 |walltime 800.464 |
Transformer | epoch 0 | step 5390 |avg loss 7.897 |avg tokens 4544.200 |tokens/s 31761.835 |walltime 801.894 |
Transformer | epoch 0 | step 5400 |avg loss 8.082 |avg tokens 4242.700 |tokens/s 30248.524 |walltime 803.297 |
Transformer | epoch 0 | step 5410 |avg loss 7.606 |avg tokens 4043.400 |tokens/s 28772.550 |walltime 804.702 |
Transformer | epoch 0 | step 5420 |avg loss 7.574 |avg tokens 4282.200 |tokens/s 29408.868 |walltime 806.158 |
Transformer | epoch 0 | step 5430 |avg loss 7.563 |avg tokens 4971.500 |tokens/s 33119.281 |walltime 807.660 |
Transformer | epoch 0 | step 5440 |avg loss 6.778 |avg tokens 4505.600 |tokens/s 29893.375 |walltime 809.167 |
Transformer | epoch 0 | step 5450 |avg loss 7.189 |avg tokens 4814.400 |tokens/s 32203.398 |walltime 810.662 |
Transformer | epoch 0 | step 5460 |avg loss 7.878 |avg tokens 4751.300 |tokens/s 32589.930 |walltime 812.120 |
Transformer | epoch 0 | step 5470 |avg loss 7.536 |avg tokens 4740.500 |tokens/s 30841.604 |walltime 813.657 |
Transformer | epoch 0 | step 5480 |avg loss 7.657 |avg tokens 4551.500 |tokens/s 31854.290 |walltime 815.086 |
Transformer | epoch 0 | step 5490 |avg loss 7.290 |avg tokens 4830.100 |tokens/s 32261.144 |walltime 816.583 |
Transformer | epoch 0 | step 5500 |avg loss 7.756 |avg tokens 3980.900 |tokens/s 27540.008 |walltime 818.028 |
Transformer | epoch 0 | step 5510 |avg loss 7.413 |avg tokens 4471.000 |tokens/s 30172.970 |walltime 819.510 |
Transformer | epoch 0 | step 5520 |avg loss 7.129 |avg tokens 4612.600 |tokens/s 30316.076 |walltime 821.032 |
Transformer | epoch 0 | step 5530 |avg loss 7.275 |avg tokens 4534.000 |tokens/s 30478.719 |walltime 822.519 |
Transformer | epoch 0 | step 5540 |avg loss 7.745 |avg tokens 4762.200 |tokens/s 32540.660 |walltime 823.983 |
Transformer | epoch 0 | step 5550 |avg loss 7.428 |avg tokens 4795.800 |tokens/s 32535.641 |walltime 825.457 |
Transformer | epoch 0 | step 5560 |avg loss 7.153 |avg tokens 4810.800 |tokens/s 32351.314 |walltime 826.944 |
Transformer | epoch 0 | step 5570 |avg loss 7.999 |avg tokens 4233.300 |tokens/s 29929.973 |walltime 828.358 |
Transformer | epoch 0 | step 5580 |avg loss 7.797 |avg tokens 3960.100 |tokens/s 27909.939 |walltime 829.777 |
Transformer | epoch 0 | step 5590 |avg loss 7.059 |avg tokens 4765.600 |tokens/s 31692.818 |walltime 831.281 |
Transformer | epoch 0 | step 5600 |avg loss 7.366 |avg tokens 4353.600 |tokens/s 28993.106 |walltime 832.782 |
Transformer | epoch 0 | step 5610 |avg loss 7.673 |avg tokens 4202.100 |tokens/s 30171.088 |walltime 834.175 |
Transformer | epoch 0 | step 5620 |avg loss 7.458 |avg tokens 4680.900 |tokens/s 31215.715 |walltime 835.675 |
Transformer | epoch 0 | step 5630 |avg loss 7.634 |avg tokens 4967.100 |tokens/s 33241.615 |walltime 837.169 |
Transformer | epoch 0 | step 5640 |avg loss 7.423 |avg tokens 4590.700 |tokens/s 31082.345 |walltime 838.646 |
Transformer | epoch 0 | step 5650 |avg loss 7.179 |avg tokens 4399.300 |tokens/s 30146.251 |walltime 840.105 |
Transformer | epoch 0 | step 5660 |avg loss 7.860 |avg tokens 4178.000 |tokens/s 30135.318 |walltime 841.491 |
Transformer | epoch 0 | step 5670 |avg loss 7.276 |avg tokens 4821.600 |tokens/s 30710.904 |walltime 843.061 |
Transformer | epoch 0 | step 5680 |avg loss 6.978 |avg tokens 4656.000 |tokens/s 31199.941 |walltime 844.554 |
Transformer | epoch 0 | step 5690 |avg loss 7.335 |avg tokens 4441.600 |tokens/s 30437.345 |walltime 846.013 |
Transformer | epoch 0 | step 5700 |avg loss 7.817 |avg tokens 4514.700 |tokens/s 31464.483 |walltime 847.448 |
Transformer | epoch 0 | step 5710 |avg loss 7.586 |avg tokens 4524.300 |tokens/s 30792.943 |walltime 848.917 |
Transformer | epoch 0 | step 5720 |avg loss 7.396 |avg tokens 4968.000 |tokens/s 33428.293 |walltime 850.403 |
Transformer | epoch 0 | step 5730 |avg loss 6.892 |avg tokens 4988.700 |tokens/s 32561.886 |walltime 851.935 |
Transformer | epoch 0 | step 5740 |avg loss 7.625 |avg tokens 4547.000 |tokens/s 30941.949 |walltime 853.405 |
Transformer | epoch 0 | step 5750 |avg loss 7.539 |avg tokens 4774.100 |tokens/s 32474.721 |walltime 854.875 |
Transformer | epoch 0 | step 5760 |avg loss 7.675 |avg tokens 4508.100 |tokens/s 30111.515 |walltime 856.372 |
Transformer | epoch 0 | step 5770 |avg loss 7.822 |avg tokens 4745.100 |tokens/s 32425.985 |walltime 857.836 |
Transformer | epoch 0 | step 5780 |avg loss 7.076 |avg tokens 4813.100 |tokens/s 31704.379 |walltime 859.354 |
Transformer | epoch 0 | step 5790 |avg loss 7.618 |avg tokens 4738.500 |tokens/s 32788.937 |walltime 860.799 |
Transformer | epoch 0 | step 5800 |avg loss 7.426 |avg tokens 4603.000 |tokens/s 30695.156 |walltime 862.298 |
Transformer | epoch 0 | step 5810 |avg loss 7.547 |avg tokens 3978.900 |tokens/s 28351.297 |walltime 863.702 |
Transformer | epoch 0 | step 5820 |avg loss 7.533 |avg tokens 4246.100 |tokens/s 28445.631 |walltime 865.195 |
Transformer | epoch 0 | step 5830 |avg loss 7.618 |avg tokens 4585.900 |tokens/s 31267.759 |walltime 866.661 |
Transformer | epoch 0 | step 5840 |avg loss 7.827 |avg tokens 4145.200 |tokens/s 28776.614 |walltime 868.102 |
Transformer | epoch 0 | step 5850 |avg loss 7.392 |avg tokens 4870.400 |tokens/s 32615.567 |walltime 869.595 |
Transformer | epoch 0 | step 5860 |avg loss 7.705 |avg tokens 4675.600 |tokens/s 32085.667 |walltime 871.052 |
Transformer | epoch 0 | step 5870 |avg loss 7.813 |avg tokens 4060.900 |tokens/s 27657.157 |walltime 872.521 |
Transformer | epoch 0 | step 5880 |avg loss 7.735 |avg tokens 4357.100 |tokens/s 30196.802 |walltime 873.963 |
Transformer | epoch 0 | step 5890 |avg loss 7.392 |avg tokens 4756.300 |tokens/s 31813.232 |walltime 875.458 |
Transformer | epoch 0 | step 5900 |avg loss 7.524 |avg tokens 4540.600 |tokens/s 32156.867 |walltime 876.871 |
Transformer | epoch 0 | step 5910 |avg loss 7.522 |avg tokens 4254.600 |tokens/s 29551.855 |walltime 878.310 |
Transformer | epoch 0 | step 5920 |avg loss 7.415 |avg tokens 4638.100 |tokens/s 31598.763 |walltime 879.778 |
Transformer | epoch 0 | step 5930 |avg loss 7.648 |avg tokens 4795.700 |tokens/s 32277.856 |walltime 881.264 |
Transformer | epoch 0 | step 5940 |avg loss 7.996 |avg tokens 3767.200 |tokens/s 27315.736 |walltime 882.643 |
Transformer | epoch 0 | step 5950 |avg loss 7.475 |avg tokens 4590.200 |tokens/s 31646.102 |walltime 884.093 |
Transformer | epoch 0 | step 5960 |avg loss 7.884 |avg tokens 3981.200 |tokens/s 27698.189 |walltime 885.531 |
Transformer | epoch 0 | step 5970 |avg loss 7.653 |avg tokens 4529.100 |tokens/s 30202.588 |walltime 887.030 |
Transformer | epoch 0 | step 5980 |avg loss 7.552 |avg tokens 4394.700 |tokens/s 29185.188 |walltime 888.536 |
Transformer | epoch 0 | step 5990 |avg loss 7.618 |avg tokens 4481.600 |tokens/s 30278.866 |walltime 890.016 |
Transformer | epoch 0 | step 6000 |avg loss 7.593 |avg tokens 4864.300 |tokens/s 31681.548 |walltime 891.552 |
Transformer | epoch 0 | step 6010 |avg loss 6.950 |avg tokens 4601.600 |tokens/s 30749.728 |walltime 893.048 |
Transformer | epoch 0 | step 6020 |avg loss 7.306 |avg tokens 4697.600 |tokens/s 30821.665 |walltime 894.572 |
Transformer | epoch 0 | step 6030 |avg loss 7.271 |avg tokens 4615.100 |tokens/s 29529.129 |walltime 896.135 |
Transformer | epoch 0 | step 6040 |avg loss 7.761 |avg tokens 4142.700 |tokens/s 27602.653 |walltime 897.636 |
Transformer | epoch 0 | step 6050 |avg loss 7.226 |avg tokens 4808.200 |tokens/s 31417.572 |walltime 899.166 |
Transformer | epoch 0 | step 6060 |avg loss 7.582 |avg tokens 4390.100 |tokens/s 29876.982 |walltime 900.636 |
Transformer | epoch 0 | step 6070 |avg loss 7.574 |avg tokens 4571.900 |tokens/s 30180.728 |walltime 902.151 |
Transformer | epoch 0 | step 6080 |avg loss 7.995 |avg tokens 3983.800 |tokens/s 27951.876 |walltime 903.576 |
Transformer | epoch 0 | step 6090 |avg loss 7.822 |avg tokens 4429.200 |tokens/s 29948.921 |walltime 905.055 |
Transformer | epoch 0 | step 6100 |avg loss 7.906 |avg tokens 4248.800 |tokens/s 29861.962 |walltime 906.478 |
Transformer | epoch 0 | step 6110 |avg loss 7.873 |avg tokens 4191.800 |tokens/s 29535.601 |walltime 907.897 |
Transformer | epoch 0 | step 6120 |avg loss 7.478 |avg tokens 4347.300 |tokens/s 29625.656 |walltime 909.364 |
Transformer | epoch 0 | step 6130 |avg loss 7.656 |avg tokens 4275.200 |tokens/s 30563.545 |walltime 910.763 |
Transformer | epoch 0 | step 6140 |avg loss 7.739 |avg tokens 4477.800 |tokens/s 30311.163 |walltime 912.240 |
Transformer | epoch 0 | step 6150 |avg loss 7.934 |avg tokens 4063.100 |tokens/s 28395.134 |walltime 913.671 |
Transformer | epoch 0 | step 6160 |avg loss 7.511 |avg tokens 4832.200 |tokens/s 31919.166 |walltime 915.185 |
Transformer | epoch 0 | step 6170 |avg loss 7.536 |avg tokens 4673.300 |tokens/s 30650.171 |walltime 916.710 |
Transformer | epoch 0 | step 6180 |avg loss 8.385 |avg tokens 4442.300 |tokens/s 27317.143 |walltime 918.336 |
Transformer | epoch 0 | step 6190 |avg loss 7.664 |avg tokens 4277.100 |tokens/s 29255.738 |walltime 919.798 |
Transformer | epoch 0 | step 6200 |avg loss 8.185 |avg tokens 3508.700 |tokens/s 24658.226 |walltime 921.221 |
Transformer | epoch 0 | step 6210 |avg loss 8.053 |avg tokens 4281.600 |tokens/s 31240.895 |walltime 922.591 |
Transformer | epoch 0 | step 6220 |avg loss 7.400 |avg tokens 4976.400 |tokens/s 32496.429 |walltime 924.123 |
Transformer | epoch 0 | step 6230 |avg loss 7.650 |avg tokens 4799.200 |tokens/s 32066.859 |walltime 925.619 |
Transformer | epoch 0 | step 6240 |avg loss 7.491 |avg tokens 4651.200 |tokens/s 30765.765 |walltime 927.131 |
Transformer | epoch 0 | step 6250 |avg loss 7.470 |avg tokens 4634.700 |tokens/s 30642.252 |walltime 928.644 |
Transformer | epoch 0 | step 6260 |avg loss 7.385 |avg tokens 4682.400 |tokens/s 30669.114 |walltime 930.171 |
Transformer | epoch 0 | step 6270 |avg loss 7.729 |avg tokens 4843.400 |tokens/s 32343.776 |walltime 931.668 |
Transformer | epoch 0 | step 6280 |avg loss 7.432 |avg tokens 4779.200 |tokens/s 32501.954 |walltime 933.138 |
Transformer | epoch 0 | step 6290 |avg loss 7.549 |avg tokens 4834.100 |tokens/s 32798.670 |walltime 934.612 |
Transformer | epoch 0 | step 6300 |avg loss 7.771 |avg tokens 4579.900 |tokens/s 30340.188 |walltime 936.122 |
Transformer | epoch 0 | step 6310 |avg loss 7.905 |avg tokens 4461.900 |tokens/s 31304.719 |walltime 937.547 |
Transformer | epoch 0 | step 6320 |avg loss 7.957 |avg tokens 4766.100 |tokens/s 32146.977 |walltime 939.030 |
Transformer | epoch 0 | step 6330 |avg loss 7.567 |avg tokens 4811.500 |tokens/s 31931.566 |walltime 940.537 |
Transformer | epoch 0 | step 6340 |avg loss 7.565 |avg tokens 4386.400 |tokens/s 29449.095 |walltime 942.026 |
Transformer | epoch 0 | step 6350 |avg loss 7.739 |avg tokens 4858.800 |tokens/s 32776.121 |walltime 943.509 |
Transformer | epoch 0 | step 6360 |avg loss 7.730 |avg tokens 4440.800 |tokens/s 30355.708 |walltime 944.971 |
Transformer | epoch 0 | step 6370 |avg loss 7.627 |avg tokens 4299.800 |tokens/s 29706.594 |walltime 946.419 |
Transformer | epoch 0 | step 6380 |avg loss 7.644 |avg tokens 4332.900 |tokens/s 29896.825 |walltime 947.868 |
Transformer | epoch 0 | step 6390 |avg loss 7.845 |avg tokens 4825.100 |tokens/s 32580.015 |walltime 949.349 |
Transformer | epoch 0 | step 6400 |avg loss 7.243 |avg tokens 4639.200 |tokens/s 30884.747 |walltime 950.851 |
Transformer | epoch 0 | step 6410 |avg loss 7.519 |avg tokens 4543.500 |tokens/s 29958.631 |walltime 952.368 |
Transformer | epoch 0 | step 6420 |avg loss 8.030 |avg tokens 4013.500 |tokens/s 28799.557 |walltime 953.761 |
Transformer | epoch 0 | step 6430 |avg loss 7.451 |avg tokens 4544.200 |tokens/s 30849.261 |walltime 955.234 |
Transformer | epoch 0 | step 6440 |avg loss 7.653 |avg tokens 4657.400 |tokens/s 31667.949 |walltime 956.705 |
Transformer | epoch 0 | step 6450 |avg loss 7.763 |avg tokens 4717.100 |tokens/s 32048.997 |walltime 958.177 |
Transformer | epoch 0 | step 6460 |avg loss 7.791 |avg tokens 4130.900 |tokens/s 28577.124 |walltime 959.623 |
Transformer | epoch 0 | step 6470 |avg loss 7.604 |avg tokens 4565.600 |tokens/s 30323.322 |walltime 961.128 |
Transformer | epoch 0 | step 6480 |avg loss 7.224 |avg tokens 4682.600 |tokens/s 30661.997 |walltime 962.655 |
Transformer | epoch 0 | step 6490 |avg loss 7.780 |avg tokens 4769.600 |tokens/s 31755.707 |walltime 964.157 |
Transformer | epoch 0 | step 6500 |avg loss 7.681 |avg tokens 4646.300 |tokens/s 31521.414 |walltime 965.631 |
Transformer | epoch 0 | step 6510 |avg loss 7.337 |avg tokens 4820.700 |tokens/s 31475.693 |walltime 967.163 |
Transformer | epoch 0 | step 6520 |avg loss 7.962 |avg tokens 4183.700 |tokens/s 29376.158 |walltime 968.587 |
Transformer | epoch 0 | step 6530 |avg loss 7.763 |avg tokens 4669.000 |tokens/s 31281.142 |walltime 970.080 |
Transformer | epoch 0 | step 6540 |avg loss 7.830 |avg tokens 4303.600 |tokens/s 30529.232 |walltime 971.489 |
Transformer | epoch 0 | step 6550 |avg loss 8.176 |avg tokens 4258.900 |tokens/s 29327.210 |walltime 972.942 |
Transformer | epoch 0 | step 6560 |avg loss 7.803 |avg tokens 4704.900 |tokens/s 31094.356 |walltime 974.455 |
Transformer | epoch 0 | step 6570 |avg loss 8.022 |avg tokens 4773.300 |tokens/s 32625.825 |walltime 975.918 |
Transformer | epoch 0 | step 6580 |avg loss 7.756 |avg tokens 4481.600 |tokens/s 31771.083 |walltime 977.328 |
Transformer | epoch 0 | step 6590 |avg loss 7.942 |avg tokens 4183.000 |tokens/s 29587.009 |walltime 978.742 |
Transformer | epoch 0 | step 6600 |avg loss 7.936 |avg tokens 4447.100 |tokens/s 31402.502 |walltime 980.158 |
Transformer | epoch 0 | step 6610 |avg loss 7.668 |avg tokens 4696.700 |tokens/s 31710.894 |walltime 981.639 |
Transformer | epoch 0 | step 6620 |avg loss 7.500 |avg tokens 4371.500 |tokens/s 29383.098 |walltime 983.127 |
Transformer | epoch 0 | step 6630 |avg loss 7.266 |avg tokens 4422.400 |tokens/s 29900.892 |walltime 984.606 |
Transformer | epoch 0 | step 6640 |avg loss 7.731 |avg tokens 4593.800 |tokens/s 30869.653 |walltime 986.094 |
Transformer | epoch 0 | step 6650 |avg loss 7.669 |avg tokens 4787.400 |tokens/s 32451.129 |walltime 987.570 |
Transformer | epoch 0 | step 6660 |avg loss 7.675 |avg tokens 4567.900 |tokens/s 31026.650 |walltime 989.042 |
Transformer | epoch 0 | step 6670 |avg loss 7.499 |avg tokens 4498.900 |tokens/s 29812.035 |walltime 990.551 |
Transformer | epoch 0 | step 6680 |avg loss 7.714 |avg tokens 4368.000 |tokens/s 29758.042 |walltime 992.019 |
Transformer | epoch 0 | step 6690 |avg loss 8.027 |avg tokens 4604.300 |tokens/s 32418.022 |walltime 993.439 |
Transformer | epoch 0 | step 6700 |avg loss 7.896 |avg tokens 4301.800 |tokens/s 29437.957 |walltime 994.900 |
Transformer | epoch 0 | step 6710 |avg loss 7.424 |avg tokens 4702.600 |tokens/s 31404.016 |walltime 996.398 |
Transformer | epoch 0 | step 6720 |avg loss 7.776 |avg tokens 4368.700 |tokens/s 29758.784 |walltime 997.866 |
Transformer | epoch 0 | step 6730 |avg loss 8.056 |avg tokens 4250.700 |tokens/s 29073.524 |walltime 999.328 |
Transformer | epoch 0 | step 6740 |avg loss 7.759 |avg tokens 4324.000 |tokens/s 30027.712 |walltime 1000.768 |
Transformer | epoch 0 | step 6750 |avg loss 8.067 |avg tokens 4482.000 |tokens/s 30889.114 |walltime 1002.219 |
Transformer | epoch 0 | step 6760 |avg loss 7.672 |avg tokens 4498.600 |tokens/s 29851.436 |walltime 1003.726 |
Transformer | epoch 0 | step 6770 |avg loss 7.672 |avg tokens 4635.300 |tokens/s 31248.806 |walltime 1005.209 |
Transformer | epoch 0 | step 6780 |avg loss 7.996 |avg tokens 4844.200 |tokens/s 32347.382 |walltime 1006.707 |
Transformer | epoch 0 | step 6790 |avg loss 7.826 |avg tokens 4699.300 |tokens/s 31236.157 |walltime 1008.211 |
Transformer | epoch 0 | step 6800 |avg loss 7.980 |avg tokens 4374.000 |tokens/s 30486.617 |walltime 1009.646 |
Transformer | epoch 0 | step 6810 |avg loss 7.754 |avg tokens 4399.600 |tokens/s 29960.355 |walltime 1011.114 |
Transformer | epoch 0 | step 6820 |avg loss 7.865 |avg tokens 4745.700 |tokens/s 31826.970 |walltime 1012.606 |
Transformer | epoch 0 | step 6830 |avg loss 7.888 |avg tokens 4489.900 |tokens/s 30618.588 |walltime 1014.072 |
Transformer | epoch 0 | step 6840 |avg loss 7.808 |avg tokens 4652.100 |tokens/s 31113.372 |walltime 1015.567 |
Transformer | epoch 0 | step 6850 |avg loss 7.582 |avg tokens 4760.700 |tokens/s 31897.063 |walltime 1017.060 |
Transformer | epoch 0 | step 6860 |avg loss 7.611 |avg tokens 4758.200 |tokens/s 31450.120 |walltime 1018.573 |
Transformer | epoch 0 | step 6870 |avg loss 7.588 |avg tokens 4793.500 |tokens/s 32873.731 |walltime 1020.031 |
Transformer | epoch 0 | step 6880 |avg loss 7.602 |avg tokens 4641.100 |tokens/s 31356.086 |walltime 1021.511 |
Transformer | epoch 0 | step 6890 |avg loss 7.624 |avg tokens 4334.000 |tokens/s 29552.152 |walltime 1022.978 |
Transformer | epoch 0 | step 6900 |avg loss 7.892 |avg tokens 4516.400 |tokens/s 30169.539 |walltime 1024.475 |
Transformer | epoch 0 | step 6910 |avg loss 7.837 |avg tokens 4318.300 |tokens/s 30381.526 |walltime 1025.896 |
Transformer | epoch 0 | step 6920 |avg loss 7.863 |avg tokens 4509.500 |tokens/s 31295.147 |walltime 1027.337 |
Transformer | epoch 0 | step 6930 |avg loss 7.913 |avg tokens 4521.000 |tokens/s 31074.631 |walltime 1028.792 |
Transformer | epoch 0 | step 6940 |avg loss 7.767 |avg tokens 4183.500 |tokens/s 27912.573 |walltime 1030.291 |
Transformer | epoch 0 | step 6950 |avg loss 7.593 |avg tokens 4538.400 |tokens/s 30339.847 |walltime 1031.786 |
Transformer | epoch 0 | step 6960 |avg loss 8.002 |avg tokens 4481.400 |tokens/s 31031.844 |walltime 1033.231 |
Transformer | epoch 0 | step 6970 |avg loss 7.917 |avg tokens 4647.500 |tokens/s 32615.068 |walltime 1034.655 |
Transformer | epoch 0 | step 6980 |avg loss 7.854 |avg tokens 4731.300 |tokens/s 31490.011 |walltime 1036.158 |
Transformer | epoch 0 | step 6990 |avg loss 7.767 |avg tokens 4698.600 |tokens/s 31824.511 |walltime 1037.634 |
Transformer | epoch 0 | step 7000 |avg loss 7.558 |avg tokens 4260.100 |tokens/s 29182.574 |walltime 1039.094 |
Transformer | epoch 0 | step 7010 |avg loss 7.360 |avg tokens 4759.000 |tokens/s 31546.916 |walltime 1040.603 |
Transformer | epoch 0 | step 7020 |avg loss 7.414 |avg tokens 4563.900 |tokens/s 30205.336 |walltime 1042.114 |
Transformer | epoch 0 | step 7030 |avg loss 7.696 |avg tokens 4581.400 |tokens/s 30450.070 |walltime 1043.618 |
Transformer | epoch 0 | step 7040 |avg loss 8.400 |avg tokens 4292.000 |tokens/s 30742.236 |walltime 1045.014 |
Transformer | epoch 0 | step 7050 |avg loss 7.927 |avg tokens 4325.900 |tokens/s 29754.554 |walltime 1046.468 |
Transformer | epoch 0 | step 7060 |avg loss 7.944 |avg tokens 4818.200 |tokens/s 32078.122 |walltime 1047.970 |
Transformer | epoch 0 | step 7070 |avg loss 8.039 |avg tokens 4407.900 |tokens/s 30826.658 |walltime 1049.400 |
Transformer | epoch 0 | step 7080 |avg loss 8.487 |avg tokens 4781.900 |tokens/s 32928.510 |walltime 1050.852 |
Transformer | epoch 0 | step 7090 |avg loss 7.771 |avg tokens 4548.500 |tokens/s 30389.014 |walltime 1052.349 |
Transformer | epoch 0 | step 7100 |avg loss 7.900 |avg tokens 4652.300 |tokens/s 31607.713 |walltime 1053.821 |
Transformer | epoch 0 | step 7110 |avg loss 8.307 |avg tokens 3993.300 |tokens/s 29605.795 |walltime 1055.170 |
Transformer | epoch 0 | step 7120 |avg loss 7.536 |avg tokens 4723.800 |tokens/s 31295.862 |walltime 1056.679 |
Transformer | epoch 0 | step 7130 |avg loss 8.181 |avg tokens 4403.300 |tokens/s 29655.853 |walltime 1058.164 |
Transformer | epoch 0 | step 7140 |avg loss 7.816 |avg tokens 4635.200 |tokens/s 30870.666 |walltime 1059.666 |
Transformer | epoch 0 | step 7150 |avg loss 7.858 |avg tokens 4669.100 |tokens/s 30628.798 |walltime 1061.190 |
Transformer | epoch 0 | step 7160 |avg loss 7.749 |avg tokens 4276.500 |tokens/s 29191.408 |walltime 1062.655 |
Transformer | epoch 0 | step 7170 |avg loss 7.868 |avg tokens 4189.200 |tokens/s 29311.930 |walltime 1064.084 |
Transformer | epoch 0 | step 7180 |avg loss 8.312 |avg tokens 3872.000 |tokens/s 27900.044 |walltime 1065.472 |
Transformer | epoch 0 | step 7190 |avg loss 7.679 |avg tokens 4417.200 |tokens/s 29794.751 |walltime 1066.955 |
Transformer | epoch 0 | step 7200 |avg loss 7.879 |avg tokens 4595.700 |tokens/s 30864.772 |walltime 1068.443 |
Transformer | epoch 0 | step 7210 |avg loss 8.010 |avg tokens 4239.300 |tokens/s 29587.106 |walltime 1069.876 |
Transformer | epoch 0 | step 7220 |avg loss 8.064 |avg tokens 4537.500 |tokens/s 30429.381 |walltime 1071.367 |
Transformer | epoch 0 | step 7230 |avg loss 7.802 |avg tokens 4641.100 |tokens/s 32085.000 |walltime 1072.814 |
Transformer | epoch 0 | step 7240 |avg loss 8.035 |avg tokens 4740.400 |tokens/s 31127.155 |walltime 1074.337 |
Transformer | epoch 0 | step 7250 |avg loss 7.766 |avg tokens 4377.700 |tokens/s 30345.472 |walltime 1075.780 |
Transformer | epoch 0 | step 7260 |avg loss 7.790 |avg tokens 4420.000 |tokens/s 30298.830 |walltime 1077.238 |
Transformer | epoch 0 | step 7270 |avg loss 7.625 |avg tokens 4913.500 |tokens/s 32426.559 |walltime 1078.754 |
Transformer | epoch 0 | step 7280 |avg loss 7.910 |avg tokens 4689.600 |tokens/s 31437.456 |walltime 1080.245 |
Transformer | epoch 0 | step 7290 |avg loss 7.723 |avg tokens 4702.200 |tokens/s 31703.710 |walltime 1081.728 |
Transformer | epoch 0 | step 7300 |avg loss 7.708 |avg tokens 4470.300 |tokens/s 30860.572 |walltime 1083.177 |
Transformer | epoch 0 | step 7310 |avg loss 7.857 |avg tokens 4326.200 |tokens/s 29904.159 |walltime 1084.624 |
Transformer | epoch 0 | step 7320 |avg loss 7.784 |avg tokens 4582.800 |tokens/s 31067.974 |walltime 1086.099 |
Transformer | epoch 0 | step 7330 |avg loss 8.037 |avg tokens 4382.600 |tokens/s 30143.403 |walltime 1087.553 |
Transformer | epoch 0 | step 7340 |avg loss 7.755 |avg tokens 4686.300 |tokens/s 31179.749 |walltime 1089.056 |
Transformer | epoch 0 | step 7350 |avg loss 8.012 |avg tokens 4648.500 |tokens/s 31948.387 |walltime 1090.511 |
Transformer | epoch 0 | step 7360 |avg loss 7.480 |avg tokens 4927.200 |tokens/s 32384.271 |walltime 1092.032 |
Transformer | epoch 0 | step 7370 |avg loss 8.158 |avg tokens 4401.400 |tokens/s 31100.035 |walltime 1093.447 |
Transformer | epoch 0 | step 7380 |avg loss 7.556 |avg tokens 4727.200 |tokens/s 30897.373 |walltime 1094.977 |
Transformer | epoch 0 | step 7390 |avg loss 7.823 |avg tokens 4283.700 |tokens/s 28655.196 |walltime 1096.472 |
Transformer | epoch 0 | step 7400 |avg loss 7.454 |avg tokens 4539.300 |tokens/s 30273.813 |walltime 1097.972 |
Transformer | epoch 0 | step 7410 |avg loss 7.767 |avg tokens 4495.000 |tokens/s 30325.324 |walltime 1099.454 |
Transformer | epoch 0 | step 7420 |avg loss 7.648 |avg tokens 4688.500 |tokens/s 31955.544 |walltime 1100.921 |
Transformer | epoch 0 | step 7430 |avg loss 7.423 |avg tokens 4866.200 |tokens/s 32309.946 |walltime 1102.427 |
Transformer | epoch 0 | step 7440 |avg loss 7.751 |avg tokens 4888.200 |tokens/s 32976.790 |walltime 1103.910 |
Transformer | epoch 0 | step 7450 |avg loss 7.803 |avg tokens 4605.200 |tokens/s 31058.268 |walltime 1105.392 |
Transformer | epoch 0 | step 7460 |avg loss 7.687 |avg tokens 4847.200 |tokens/s 32037.724 |walltime 1106.905 |
Transformer | epoch 0 | step 7470 |avg loss 7.822 |avg tokens 4662.600 |tokens/s 31359.938 |walltime 1108.392 |
Transformer | epoch 0 | step 7480 |avg loss 7.805 |avg tokens 4209.700 |tokens/s 28821.286 |walltime 1109.853 |
Transformer | epoch 0 | step 7490 |avg loss 7.760 |avg tokens 4464.500 |tokens/s 30541.271 |walltime 1111.315 |
Transformer | epoch 0 | step 7500 |avg loss 7.714 |avg tokens 4699.700 |tokens/s 30563.028 |walltime 1112.852 |
Transformer | epoch 0 | step 7510 |avg loss 7.820 |avg tokens 4515.600 |tokens/s 30500.576 |walltime 1114.333 |
Transformer | epoch 0 | step 7520 |avg loss 7.769 |avg tokens 4696.500 |tokens/s 31880.994 |walltime 1115.806 |
Transformer | epoch 0 | step 7530 |avg loss 7.909 |avg tokens 4454.000 |tokens/s 30608.470 |walltime 1117.261 |
Transformer | epoch 0 | step 7540 |avg loss 7.719 |avg tokens 4552.300 |tokens/s 30425.055 |walltime 1118.757 |
Transformer | epoch 0 | step 7550 |avg loss 7.846 |avg tokens 4198.500 |tokens/s 29241.250 |walltime 1120.193 |
Transformer | epoch 0 | step 7560 |avg loss 7.850 |avg tokens 4670.900 |tokens/s 31497.496 |walltime 1121.676 |
Transformer | epoch 0 | step 7570 |avg loss 7.215 |avg tokens 4459.900 |tokens/s 29770.380 |walltime 1123.174 |
Transformer | epoch 0 | step 7580 |avg loss 7.738 |avg tokens 4442.400 |tokens/s 29854.687 |walltime 1124.662 |
Transformer | epoch 0 | step 7590 |avg loss 7.786 |avg tokens 4687.200 |tokens/s 31277.296 |walltime 1126.161 |
Transformer | epoch 0 | step 7600 |avg loss 7.725 |avg tokens 4768.900 |tokens/s 31714.875 |walltime 1127.665 |
Transformer | epoch 0 | step 7610 |avg loss 8.046 |avg tokens 4828.000 |tokens/s 32369.028 |walltime 1129.156 |
Transformer | epoch 0 | step 7620 |avg loss 7.677 |avg tokens 4369.000 |tokens/s 29841.001 |walltime 1130.620 |
Transformer | epoch 0 | step 7630 |avg loss 7.638 |avg tokens 4227.700 |tokens/s 29172.361 |walltime 1132.069 |
Transformer | epoch 0 | step 7640 |avg loss 7.901 |avg tokens 4225.600 |tokens/s 29776.218 |walltime 1133.489 |
Transformer | epoch 0 | step 7650 |avg loss 7.723 |avg tokens 4722.400 |tokens/s 31646.901 |walltime 1134.981 |
Transformer | epoch 0 | step 7660 |avg loss 7.798 |avg tokens 4489.500 |tokens/s 30962.827 |walltime 1136.431 |
Transformer | epoch 0 | step 7670 |avg loss 7.913 |avg tokens 4387.600 |tokens/s 29965.885 |walltime 1137.895 |
Transformer | epoch 0 | step 7680 |avg loss 8.006 |avg tokens 4654.100 |tokens/s 32034.333 |walltime 1139.348 |
Transformer | epoch 0 | step 7690 |avg loss 7.413 |avg tokens 4739.400 |tokens/s 30805.314 |walltime 1140.886 |
Transformer | epoch 0 | step 7700 |avg loss 7.710 |avg tokens 4348.300 |tokens/s 29703.235 |walltime 1142.350 |
Transformer | epoch 0 | step 7710 |avg loss 7.860 |avg tokens 4419.700 |tokens/s 31435.288 |walltime 1143.756 |
Transformer | epoch 0 | step 7720 |avg loss 8.450 |avg tokens 3701.600 |tokens/s 25649.311 |walltime 1145.199 |
Transformer | epoch 0 | step 7730 |avg loss 7.570 |avg tokens 4857.200 |tokens/s 31912.256 |walltime 1146.721 |
Transformer | epoch 0 | step 7740 |avg loss 7.717 |avg tokens 4926.300 |tokens/s 33212.479 |walltime 1148.205 |
Transformer | epoch 0 | step 7750 |avg loss 7.789 |avg tokens 4336.200 |tokens/s 29393.136 |walltime 1149.680 |
Transformer | epoch 0 | step 7760 |avg loss 8.230 |avg tokens 4675.800 |tokens/s 33718.787 |walltime 1151.067 |
Transformer | epoch 0 | step 7770 |avg loss 7.436 |avg tokens 4824.400 |tokens/s 31676.041 |walltime 1152.590 |
Transformer | epoch 0 | step 7780 |avg loss 7.615 |avg tokens 4780.400 |tokens/s 32414.670 |walltime 1154.064 |
Transformer | epoch 0 | step 7790 |avg loss 7.921 |avg tokens 4546.100 |tokens/s 30109.817 |walltime 1155.574 |
Transformer | epoch 0 | step 7800 |avg loss 7.955 |avg tokens 4678.500 |tokens/s 31963.438 |walltime 1157.038 |
Transformer | epoch 0 | step 7810 |avg loss 7.762 |avg tokens 4768.400 |tokens/s 32609.179 |walltime 1158.500 |
Transformer | epoch 0 | step 7820 |avg loss 7.992 |avg tokens 4447.400 |tokens/s 29990.758 |walltime 1159.983 |
Transformer | epoch 0 | step 7830 |avg loss 7.358 |avg tokens 4864.000 |tokens/s 32525.736 |walltime 1161.479 |
Transformer | epoch 0 | step 7840 |avg loss 7.822 |avg tokens 4629.300 |tokens/s 31698.594 |walltime 1162.939 |
Transformer | epoch 0 | step 7850 |avg loss 7.714 |avg tokens 4351.200 |tokens/s 29866.929 |walltime 1164.396 |
Transformer | epoch 0 | step 7860 |avg loss 7.946 |avg tokens 4784.400 |tokens/s 32648.124 |walltime 1165.861 |
Transformer | epoch 0 | step 7870 |avg loss 7.798 |avg tokens 4261.800 |tokens/s 29061.558 |walltime 1167.328 |
Transformer | epoch 0 | step 7880 |avg loss 8.212 |avg tokens 3737.100 |tokens/s 27356.209 |walltime 1168.694 |
Transformer | epoch 0 | step 7890 |avg loss 7.672 |avg tokens 4448.800 |tokens/s 30339.030 |walltime 1170.160 |
Transformer | epoch 0 | step 7900 |avg loss 8.189 |avg tokens 4413.300 |tokens/s 30974.829 |walltime 1171.585 |
Transformer | epoch 0 | step 7910 |avg loss 7.868 |avg tokens 4544.700 |tokens/s 31040.778 |walltime 1173.049 |
Transformer | epoch 0 | step 7920 |avg loss 7.679 |avg tokens 4587.500 |tokens/s 31270.529 |walltime 1174.516 |
Transformer | epoch 0 | step 7930 |avg loss 7.834 |avg tokens 4697.700 |tokens/s 31769.892 |walltime 1175.995 |
Transformer | epoch 0 | step 7940 |avg loss 7.842 |avg tokens 4615.300 |tokens/s 31265.787 |walltime 1177.471 |
Transformer | epoch 0 | step 7950 |avg loss 7.494 |avg tokens 4754.700 |tokens/s 31250.565 |walltime 1178.992 |
Transformer | epoch 0 | step 7960 |avg loss 7.756 |avg tokens 4309.300 |tokens/s 29269.824 |walltime 1180.465 |
Transformer | epoch 0 | step 7970 |avg loss 7.903 |avg tokens 4799.800 |tokens/s 32504.488 |walltime 1181.941 |
Transformer | epoch 0 | step 7980 |avg loss 7.673 |avg tokens 4689.600 |tokens/s 31108.617 |walltime 1183.449 |
Transformer | epoch 0 | step 7990 |avg loss 7.937 |avg tokens 4243.200 |tokens/s 29382.553 |walltime 1184.893 |
Transformer | epoch 0 | step 8000 |avg loss 7.896 |avg tokens 4434.600 |tokens/s 30521.909 |walltime 1186.346 |
Transformer | epoch 0 | step 8010 |avg loss 7.693 |avg tokens 4590.400 |tokens/s 30508.075 |walltime 1187.851 |
Transformer | epoch 0 | step 8020 |avg loss 7.821 |avg tokens 4910.500 |tokens/s 32225.325 |walltime 1189.374 |
Transformer | epoch 0 | step 8030 |avg loss 8.071 |avg tokens 4260.400 |tokens/s 29003.241 |walltime 1190.843 |
Transformer | epoch 0 | step 8040 |avg loss 7.741 |avg tokens 4849.900 |tokens/s 31904.865 |walltime 1192.363 |
Transformer | epoch 0 | step 8050 |avg loss 7.895 |avg tokens 4614.700 |tokens/s 29311.786 |walltime 1193.938 |
Transformer | epoch 0 | step 8060 |avg loss 7.937 |avg tokens 4746.900 |tokens/s 31452.430 |walltime 1195.447 |
Transformer | epoch 0 | step 8070 |avg loss 7.690 |avg tokens 4722.300 |tokens/s 31078.864 |walltime 1196.967 |
Transformer | epoch 0 | step 8080 |avg loss 7.594 |avg tokens 4718.100 |tokens/s 31323.021 |walltime 1198.473 |
Transformer | epoch 0 | step 8090 |avg loss 7.670 |avg tokens 4712.600 |tokens/s 31396.773 |walltime 1199.974 |
Transformer | epoch 0 | step 8100 |avg loss 8.067 |avg tokens 4954.200 |tokens/s 33176.559 |walltime 1201.467 |
Transformer | epoch 0 | step 8110 |avg loss 8.329 |avg tokens 3927.200 |tokens/s 28430.080 |walltime 1202.848 |
Transformer | epoch 0 | step 8120 |avg loss 8.284 |avg tokens 4561.900 |tokens/s 31023.707 |walltime 1204.319 |
Transformer | epoch 0 | step 8130 |avg loss 7.652 |avg tokens 4786.800 |tokens/s 31680.974 |walltime 1205.830 |
Transformer | epoch 0 | step 8140 |avg loss 7.467 |avg tokens 4452.000 |tokens/s 30013.912 |walltime 1207.313 |
Transformer | epoch 0 | step 8150 |avg loss 7.956 |avg tokens 4906.400 |tokens/s 33099.656 |walltime 1208.795 |
Transformer | epoch 0 | step 8160 |avg loss 7.686 |avg tokens 4530.900 |tokens/s 30814.952 |walltime 1210.266 |
Transformer | epoch 0 | step 8170 |avg loss 7.370 |avg tokens 4694.500 |tokens/s 31357.818 |walltime 1211.763 |
Transformer | epoch 0 | step 8180 |avg loss 7.748 |avg tokens 4698.200 |tokens/s 31538.058 |walltime 1213.253 |
Transformer | epoch 0 | step 8190 |avg loss 8.106 |avg tokens 4699.900 |tokens/s 32203.758 |walltime 1214.712 |
Transformer | epoch 0 | step 8200 |avg loss 8.063 |avg tokens 4298.400 |tokens/s 29984.686 |walltime 1216.146 |
Transformer | epoch 0 | step 8210 |avg loss 8.198 |avg tokens 4809.600 |tokens/s 33821.267 |walltime 1217.568 |
Transformer | epoch 0 | step 8220 |avg loss 8.022 |avg tokens 4400.100 |tokens/s 30435.621 |walltime 1219.013 |
Transformer | epoch 0 | step 8230 |avg loss 8.031 |avg tokens 4618.500 |tokens/s 32229.661 |walltime 1220.446 |
Transformer | epoch 0 | step 8240 |avg loss 7.862 |avg tokens 4556.000 |tokens/s 29869.090 |walltime 1221.972 |
Transformer | epoch 0 | step 8250 |avg loss 8.449 |avg tokens 4226.500 |tokens/s 29949.389 |walltime 1223.383 |
Transformer | epoch 0 | step 8260 |avg loss 7.914 |avg tokens 4690.200 |tokens/s 31962.280 |walltime 1224.850 |
Transformer | epoch 0 | step 8270 |avg loss 8.422 |avg tokens 4401.600 |tokens/s 30353.411 |walltime 1226.300 |
Transformer | epoch 0 | step 8280 |avg loss 7.935 |avg tokens 4590.900 |tokens/s 30135.620 |walltime 1227.824 |
Transformer | epoch 0 | step 8290 |avg loss 8.358 |avg tokens 4393.700 |tokens/s 31662.562 |walltime 1229.212 |
Transformer | epoch 0 | step 8300 |avg loss 8.294 |avg tokens 4829.600 |tokens/s 32861.520 |walltime 1230.681 |
Transformer | epoch 0 | step 8310 |avg loss 7.827 |avg tokens 4804.800 |tokens/s 31651.749 |walltime 1232.199 |
Transformer | epoch 0 | step 8320 |avg loss 8.022 |avg tokens 4570.700 |tokens/s 30625.579 |walltime 1233.692 |
Transformer | epoch 0 | step 8330 |avg loss 7.585 |avg tokens 4570.100 |tokens/s 30124.345 |walltime 1235.209 |
Transformer | epoch 0 | step 8340 |avg loss 8.385 |avg tokens 4416.000 |tokens/s 30354.337 |walltime 1236.664 |
Transformer | epoch 0 | step 8350 |avg loss 7.817 |avg tokens 4934.100 |tokens/s 33043.818 |walltime 1238.157 |
Transformer | epoch 0 | step 8360 |avg loss 7.961 |avg tokens 4322.300 |tokens/s 29456.388 |walltime 1239.624 |
Transformer | epoch 0 | step 8370 |avg loss 7.942 |avg tokens 4354.700 |tokens/s 30364.903 |walltime 1241.058 |
Transformer | epoch 0 | step 8380 |avg loss 7.970 |avg tokens 4810.200 |tokens/s 33382.444 |walltime 1242.499 |
Transformer | epoch 0 | step 8390 |avg loss 8.138 |avg tokens 4110.500 |tokens/s 28997.968 |walltime 1243.917 |
Transformer | epoch 0 | step 8400 |avg loss 8.043 |avg tokens 4469.300 |tokens/s 26928.768 |walltime 1245.576 |
Transformer | epoch 0 | step 8410 |avg loss 8.313 |avg tokens 4157.700 |tokens/s 29283.042 |walltime 1246.996 |
Transformer | epoch 0 | step 8420 |avg loss 7.721 |avg tokens 4892.400 |tokens/s 32871.460 |walltime 1248.485 |
Transformer | epoch 0 | step 8430 |avg loss 7.986 |avg tokens 4794.800 |tokens/s 31852.407 |walltime 1249.990 |
Transformer | epoch 0 | step 8440 |avg loss 8.340 |avg tokens 4210.800 |tokens/s 30352.712 |walltime 1251.377 |
Transformer | epoch 0 | step 8450 |avg loss 7.733 |avg tokens 4747.300 |tokens/s 31645.251 |walltime 1252.877 |
Transformer | epoch 0 | step 8460 |avg loss 8.041 |avg tokens 4334.500 |tokens/s 29273.453 |walltime 1254.358 |
Transformer | epoch 0 | step 8470 |avg loss 7.912 |avg tokens 4366.800 |tokens/s 29537.552 |walltime 1255.836 |
Transformer | epoch 0 | step 8480 |avg loss 8.049 |avg tokens 4595.700 |tokens/s 31842.239 |walltime 1257.280 |
Transformer | epoch 0 | step 8490 |avg loss 8.039 |avg tokens 4456.500 |tokens/s 30739.010 |walltime 1258.729 |
Transformer | epoch 0 | step 8500 |avg loss 8.090 |avg tokens 4643.200 |tokens/s 32442.618 |walltime 1260.161 |
Transformer | epoch 0 | step 8510 |avg loss 8.350 |avg tokens 4325.900 |tokens/s 30831.090 |walltime 1261.564 |
Transformer | epoch 0 | step 8520 |avg loss 7.823 |avg tokens 4443.200 |tokens/s 29583.401 |walltime 1263.066 |
Transformer | epoch 0 | step 8530 |avg loss 7.907 |avg tokens 4306.300 |tokens/s 29959.911 |walltime 1264.503 |
Transformer | epoch 0 | step 8540 |avg loss 8.116 |avg tokens 4428.700 |tokens/s 30627.543 |walltime 1265.949 |
Transformer | epoch 0 | step 8550 |avg loss 7.733 |avg tokens 4715.800 |tokens/s 31761.744 |walltime 1267.434 |
Transformer | epoch 0 | step 8560 |avg loss 8.033 |avg tokens 4739.000 |tokens/s 31781.838 |walltime 1268.925 |
Transformer | epoch 0 | step 8570 |avg loss 7.842 |avg tokens 4096.700 |tokens/s 29116.810 |walltime 1270.332 |
Transformer | epoch 0 | step 8580 |avg loss 8.024 |avg tokens 4534.700 |tokens/s 31087.769 |walltime 1271.791 |
Transformer | epoch 0 | step 8590 |avg loss 8.107 |avg tokens 4554.300 |tokens/s 31234.502 |walltime 1273.249 |
Transformer | epoch 0 | step 8600 |avg loss 8.244 |avg tokens 4676.400 |tokens/s 32887.555 |walltime 1274.671 |
Transformer | epoch 0 | step 8610 |avg loss 8.234 |avg tokens 4130.700 |tokens/s 29324.875 |walltime 1276.079 |
Transformer | epoch 0 | step 8620 |avg loss 7.938 |avg tokens 4657.100 |tokens/s 31740.787 |walltime 1277.546 |
Transformer | epoch 0 | step 8630 |avg loss 7.800 |avg tokens 4786.000 |tokens/s 31828.036 |walltime 1279.050 |
Transformer | epoch 0 | step 8640 |avg loss 7.927 |avg tokens 4842.300 |tokens/s 33509.014 |walltime 1280.495 |
Transformer | epoch 0 | step 8650 |avg loss 8.154 |avg tokens 4255.400 |tokens/s 31024.823 |walltime 1281.867 |
Transformer | epoch 0 | step 8660 |avg loss 7.952 |avg tokens 4533.900 |tokens/s 31260.241 |walltime 1283.317 |
Transformer | epoch 0 | step 8670 |avg loss 8.425 |avg tokens 4484.300 |tokens/s 31462.951 |walltime 1284.743 |
Transformer | epoch 0 | step 8680 |avg loss 7.719 |avg tokens 4660.400 |tokens/s 31286.553 |walltime 1286.232 |
Transformer | epoch 0 | step 8690 |avg loss 8.281 |avg tokens 4186.600 |tokens/s 28331.259 |walltime 1287.710 |
Transformer | epoch 0 | step 8700 |avg loss 8.072 |avg tokens 4248.600 |tokens/s 29181.272 |walltime 1289.166 |
Transformer | epoch 0 | step 8710 |avg loss 8.019 |avg tokens 4432.800 |tokens/s 29997.099 |walltime 1290.644 |
Transformer | epoch 0 | step 8720 |avg loss 8.363 |avg tokens 4410.000 |tokens/s 31248.267 |walltime 1292.055 |
Transformer | epoch 0 | step 8730 |avg loss 7.938 |avg tokens 4730.600 |tokens/s 32219.889 |walltime 1293.523 |
Transformer | epoch 0 | step 8740 |avg loss 8.016 |avg tokens 4615.300 |tokens/s 31078.974 |walltime 1295.008 |
Transformer | epoch 0 | step 8750 |avg loss 7.739 |avg tokens 4415.800 |tokens/s 29702.490 |walltime 1296.495 |
Transformer | epoch 0 | step 8760 |avg loss 8.036 |avg tokens 4257.000 |tokens/s 29561.016 |walltime 1297.935 |
Transformer | epoch 0 | step 8770 |avg loss 8.155 |avg tokens 4271.900 |tokens/s 30295.481 |walltime 1299.345 |
Transformer | epoch 0 | step 8780 |avg loss 7.993 |avg tokens 4470.200 |tokens/s 31346.047 |walltime 1300.771 |
Transformer | epoch 0 | step 8790 |avg loss 7.870 |avg tokens 4782.300 |tokens/s 32189.365 |walltime 1302.257 |
Transformer | epoch 0 | step 8800 |avg loss 8.212 |avg tokens 4785.900 |tokens/s 32267.736 |walltime 1303.740 |
Transformer | epoch 0 | step 8810 |avg loss 7.467 |avg tokens 4657.600 |tokens/s 30744.519 |walltime 1305.255 |
Transformer | epoch 0 | step 8820 |avg loss 7.893 |avg tokens 4842.400 |tokens/s 31949.444 |walltime 1306.770 |
Transformer | epoch 0 | step 8830 |avg loss 7.618 |avg tokens 4771.800 |tokens/s 32556.454 |walltime 1308.236 |
Transformer | epoch 0 | step 8840 |avg loss 7.897 |avg tokens 4463.800 |tokens/s 32113.303 |walltime 1309.626 |
Transformer | epoch 0 | step 8850 |avg loss 8.031 |avg tokens 4784.900 |tokens/s 32360.766 |walltime 1311.105 |
Transformer | epoch 0 | step 8860 |avg loss 7.736 |avg tokens 4425.600 |tokens/s 30617.499 |walltime 1312.550 |
Transformer | epoch 0 | step 8870 |avg loss 7.698 |avg tokens 4124.200 |tokens/s 27855.913 |walltime 1314.031 |
Transformer | epoch 0 | step 8880 |avg loss 7.483 |avg tokens 4390.900 |tokens/s 29900.008 |walltime 1315.499 |
Transformer | epoch 0 | step 8890 |avg loss 7.629 |avg tokens 4633.200 |tokens/s 30570.798 |walltime 1317.015 |
Transformer | epoch 0 | step 8900 |avg loss 8.336 |avg tokens 4403.900 |tokens/s 30743.921 |walltime 1318.447 |
Transformer | epoch 0 | step 8910 |avg loss 7.757 |avg tokens 4525.400 |tokens/s 30399.967 |walltime 1319.936 |
Transformer | epoch 0 | step 8920 |avg loss 8.276 |avg tokens 4452.700 |tokens/s 30221.011 |walltime 1321.409 |
Transformer | epoch 0 | step 8930 |avg loss 7.996 |avg tokens 4467.700 |tokens/s 30564.338 |walltime 1322.871 |
Transformer | epoch 0 | step 8940 |avg loss 8.137 |avg tokens 4813.500 |tokens/s 32083.773 |walltime 1324.371 |
Transformer | epoch 0 | step 8950 |avg loss 7.995 |avg tokens 4714.300 |tokens/s 31742.308 |walltime 1325.857 |
Transformer | epoch 0 | step 8960 |avg loss 8.330 |avg tokens 3953.700 |tokens/s 28023.583 |walltime 1327.267 |
Transformer | epoch 0 | step 8970 |avg loss 7.873 |avg tokens 4663.900 |tokens/s 31398.202 |walltime 1328.753 |
Transformer | epoch 0 | step 8980 |avg loss 8.340 |avg tokens 4226.900 |tokens/s 30022.137 |walltime 1330.161 |
Transformer | epoch 0 | step 8990 |avg loss 8.039 |avg tokens 4653.400 |tokens/s 31362.530 |walltime 1331.644 |
Transformer | epoch 0 | step 9000 |avg loss 8.086 |avg tokens 4026.000 |tokens/s 28654.236 |walltime 1333.049 |
Transformer | epoch 0 | step 9010 |avg loss 8.071 |avg tokens 4249.100 |tokens/s 29293.454 |walltime 1334.500 |
Transformer | epoch 0 | step 9020 |avg loss 8.145 |avg tokens 4639.800 |tokens/s 31116.669 |walltime 1335.991 |
Transformer | epoch 0 | step 9030 |avg loss 8.253 |avg tokens 4369.400 |tokens/s 30799.613 |walltime 1337.410 |
Transformer | epoch 0 | step 9040 |avg loss 8.051 |avg tokens 4680.900 |tokens/s 32065.409 |walltime 1338.870 |
Transformer | epoch 0 | step 9050 |avg loss 8.015 |avg tokens 4273.300 |tokens/s 29801.322 |walltime 1340.304 |
Transformer | epoch 0 | step 9060 |avg loss 7.720 |avg tokens 4755.400 |tokens/s 31074.524 |walltime 1341.834 |
Transformer | epoch 0 | step 9070 |avg loss 8.333 |avg tokens 4526.500 |tokens/s 31693.608 |walltime 1343.262 |
Transformer | epoch 0 | step 9080 |avg loss 7.906 |avg tokens 4270.400 |tokens/s 29802.995 |walltime 1344.695 |
Transformer | epoch 0 | step 9090 |avg loss 7.948 |avg tokens 4503.400 |tokens/s 30135.892 |walltime 1346.189 |
Transformer | epoch 0 | step 9100 |avg loss 7.737 |avg tokens 4754.200 |tokens/s 31260.377 |walltime 1347.710 |
Transformer | epoch 0 | step 9110 |avg loss 7.984 |avg tokens 4656.300 |tokens/s 30882.777 |walltime 1349.218 |
Transformer | epoch 0 | step 9120 |avg loss 8.074 |avg tokens 4552.900 |tokens/s 30800.800 |walltime 1350.696 |
Transformer | epoch 0 | step 9130 |avg loss 7.910 |avg tokens 4669.500 |tokens/s 31789.105 |walltime 1352.165 |
Transformer | epoch 0 | step 9140 |avg loss 8.012 |avg tokens 4510.900 |tokens/s 31427.226 |walltime 1353.600 |
Transformer | epoch 0 | step 9150 |avg loss 7.966 |avg tokens 4637.500 |tokens/s 31593.680 |walltime 1355.068 |
Transformer | epoch 0 | step 9160 |avg loss 8.201 |avg tokens 4687.000 |tokens/s 31502.943 |walltime 1356.556 |
Transformer | epoch 0 | step 9170 |avg loss 8.210 |avg tokens 4101.100 |tokens/s 29296.493 |walltime 1357.956 |
Transformer | epoch 0 | step 9180 |avg loss 7.937 |avg tokens 4736.000 |tokens/s 31886.663 |walltime 1359.441 |
Transformer | epoch 0 | step 9190 |avg loss 8.095 |avg tokens 4327.400 |tokens/s 29820.684 |walltime 1360.892 |
Transformer | epoch 0 | step 9200 |avg loss 8.288 |avg tokens 4582.300 |tokens/s 31798.061 |walltime 1362.333 |
Transformer | epoch 0 | step 9210 |avg loss 8.024 |avg tokens 4454.700 |tokens/s 29196.872 |walltime 1363.859 |
Transformer | epoch 0 | step 9220 |avg loss 8.228 |avg tokens 4247.600 |tokens/s 29619.789 |walltime 1365.293 |
Transformer | epoch 0 | step 9230 |avg loss 7.934 |avg tokens 4915.300 |tokens/s 32968.723 |walltime 1366.784 |
Transformer | epoch 0 | step 9240 |avg loss 8.154 |avg tokens 4790.000 |tokens/s 32012.053 |walltime 1368.280 |
Transformer | epoch 0 | step 9250 |avg loss 7.731 |avg tokens 4300.200 |tokens/s 29489.223 |walltime 1369.739 |
Transformer | epoch 0 | step 9260 |avg loss 8.090 |avg tokens 4790.800 |tokens/s 31654.292 |walltime 1371.252 |
Transformer | epoch 0 | step 9270 |avg loss 7.856 |avg tokens 4639.600 |tokens/s 31046.266 |walltime 1372.746 |
Transformer | epoch 0 | step 9280 |avg loss 8.214 |avg tokens 4674.900 |tokens/s 31236.467 |walltime 1374.243 |
Transformer | epoch 0 | step 9290 |avg loss 8.174 |avg tokens 3874.700 |tokens/s 27079.595 |walltime 1375.674 |
Transformer | epoch 0 | step 9300 |avg loss 7.964 |avg tokens 4565.700 |tokens/s 31077.535 |walltime 1377.143 |
Transformer | epoch 0 | step 9310 |avg loss 7.876 |avg tokens 4422.800 |tokens/s 29445.519 |walltime 1378.645 |
Transformer | epoch 0 | step 9320 |avg loss 8.301 |avg tokens 4645.400 |tokens/s 30723.645 |walltime 1380.157 |
Transformer | epoch 0 | step 9330 |avg loss 8.213 |avg tokens 4216.800 |tokens/s 28776.070 |walltime 1381.622 |
Transformer | epoch 0 | step 9340 |avg loss 8.330 |avg tokens 3889.600 |tokens/s 27175.704 |walltime 1383.054 |
Transformer | epoch 0 | step 9350 |avg loss 8.054 |avg tokens 4536.400 |tokens/s 31579.878 |walltime 1384.490 |
Transformer | epoch 0 | step 9360 |avg loss 7.846 |avg tokens 4628.000 |tokens/s 31192.790 |walltime 1385.974 |
Transformer | epoch 0 | step 9370 |avg loss 8.172 |avg tokens 4883.300 |tokens/s 33521.257 |walltime 1387.431 |
Transformer | epoch 0 | step 9380 |avg loss 8.257 |avg tokens 3769.600 |tokens/s 25740.334 |walltime 1388.895 |
Transformer | epoch 0 | step 9390 |avg loss 8.161 |avg tokens 4657.000 |tokens/s 31486.023 |walltime 1390.374 |
Transformer | epoch 0 | step 9400 |avg loss 8.168 |avg tokens 4698.400 |tokens/s 31677.666 |walltime 1391.857 |
Transformer | epoch 0 | step 9410 |avg loss 8.108 |avg tokens 4346.000 |tokens/s 30030.185 |walltime 1393.305 |
Transformer | epoch 0 | step 9420 |avg loss 7.877 |avg tokens 4664.800 |tokens/s 31795.148 |walltime 1394.772 |
Transformer | epoch 0 | step 9430 |avg loss 7.792 |avg tokens 4725.800 |tokens/s 31621.007 |walltime 1396.266 |
Transformer | epoch 0 | step 9440 |avg loss 8.059 |avg tokens 4245.000 |tokens/s 29028.540 |walltime 1397.729 |
Transformer | epoch 0 | step 9450 |avg loss 7.820 |avg tokens 4689.500 |tokens/s 31217.396 |walltime 1399.231 |
Transformer | epoch 0 | step 9460 |avg loss 8.276 |avg tokens 4163.300 |tokens/s 28897.700 |walltime 1400.672 |
Transformer | epoch 0 | step 9470 |avg loss 8.163 |avg tokens 4450.500 |tokens/s 31242.432 |walltime 1402.096 |
Transformer | epoch 0 | step 9480 |avg loss 8.253 |avg tokens 4099.600 |tokens/s 29122.938 |walltime 1403.504 |
Transformer | epoch 0 | step 9490 |avg loss 8.028 |avg tokens 4171.700 |tokens/s 28670.746 |walltime 1404.959 |
Transformer | epoch 0 | step 9500 |avg loss 8.023 |avg tokens 4590.600 |tokens/s 30367.794 |walltime 1406.470 |
Transformer | epoch 0 | step 9510 |avg loss 7.893 |avg tokens 4910.900 |tokens/s 32606.037 |walltime 1407.977 |
Transformer | epoch 0 | step 9520 |avg loss 7.941 |avg tokens 4896.900 |tokens/s 32514.674 |walltime 1409.483 |
Transformer | epoch 0 | step 9530 |avg loss 7.821 |avg tokens 4264.400 |tokens/s 28654.788 |walltime 1410.971 |
Transformer | epoch 0 | step 9540 |avg loss 7.782 |avg tokens 4464.500 |tokens/s 30541.585 |walltime 1412.433 |
Transformer | epoch 0 | step 9550 |avg loss 7.696 |avg tokens 4574.100 |tokens/s 31078.003 |walltime 1413.904 |
Transformer | epoch 0 | step 9560 |avg loss 8.282 |avg tokens 4440.800 |tokens/s 29289.987 |walltime 1415.421 |
Transformer | epoch 0 | step 9570 |avg loss 8.034 |avg tokens 4733.700 |tokens/s 31033.624 |walltime 1416.946 |
Transformer | epoch 0 | step 9580 |avg loss 7.886 |avg tokens 4775.300 |tokens/s 32051.260 |walltime 1418.436 |
Transformer | epoch 0 | step 9590 |avg loss 8.067 |avg tokens 4394.300 |tokens/s 29909.423 |walltime 1419.905 |
Transformer | epoch 0 | step 9600 |avg loss 8.083 |avg tokens 4241.000 |tokens/s 29214.045 |walltime 1421.357 |
Transformer | epoch 0 | step 9610 |avg loss 7.956 |avg tokens 4386.700 |tokens/s 30421.352 |walltime 1422.799 |
Transformer | epoch 0 | step 9620 |avg loss 8.002 |avg tokens 4877.700 |tokens/s 32583.878 |walltime 1424.296 |
Transformer | epoch 0 | step 9630 |avg loss 7.975 |avg tokens 4847.200 |tokens/s 31738.194 |walltime 1425.823 |
Transformer | epoch 0 | step 9640 |avg loss 7.783 |avg tokens 4846.900 |tokens/s 33216.810 |walltime 1427.282 |
Transformer | epoch 0 | step 9650 |avg loss 8.109 |avg tokens 4474.100 |tokens/s 31361.419 |walltime 1428.709 |
Transformer | epoch 0 | step 9660 |avg loss 7.930 |avg tokens 4238.100 |tokens/s 29570.421 |walltime 1430.142 |
Transformer | epoch 0 | step 9670 |avg loss 7.673 |avg tokens 4339.600 |tokens/s 30467.154 |walltime 1431.566 |
Transformer | epoch 0 | step 9680 |avg loss 8.023 |avg tokens 4600.600 |tokens/s 30121.110 |walltime 1433.094 |
Transformer | epoch 0 | step 9690 |avg loss 8.276 |avg tokens 4322.200 |tokens/s 31266.059 |walltime 1434.476 |
Transformer | epoch 0 | step 9700 |avg loss 8.041 |avg tokens 4614.400 |tokens/s 30424.788 |walltime 1435.993 |
Transformer | epoch 0 | step 9710 |avg loss 8.239 |avg tokens 4753.000 |tokens/s 32476.023 |walltime 1437.456 |
Transformer | epoch 0 | step 9720 |avg loss 8.065 |avg tokens 4421.600 |tokens/s 30492.138 |walltime 1438.906 |
Transformer | epoch 0 | step 9730 |avg loss 8.247 |avg tokens 4339.500 |tokens/s 31022.213 |walltime 1440.305 |
Transformer | epoch 0 | step 9740 |avg loss 8.049 |avg tokens 4454.100 |tokens/s 30451.118 |walltime 1441.768 |
Transformer | epoch 0 | step 9750 |avg loss 7.982 |avg tokens 4522.800 |tokens/s 30343.981 |walltime 1443.258 |
Transformer | epoch 0 | step 9760 |avg loss 8.200 |avg tokens 4627.900 |tokens/s 31515.066 |walltime 1444.727 |
Transformer | epoch 0 | step 9770 |avg loss 7.507 |avg tokens 4665.300 |tokens/s 31270.289 |walltime 1446.219 |
Transformer | epoch 0 | step 9780 |avg loss 8.226 |avg tokens 4509.900 |tokens/s 30136.140 |walltime 1447.715 |
Transformer | epoch 0 | step 9790 |avg loss 8.075 |avg tokens 4380.200 |tokens/s 29895.742 |walltime 1449.181 |
Transformer | epoch 0 | step 9800 |avg loss 8.277 |avg tokens 4310.900 |tokens/s 30335.351 |walltime 1450.602 |
Transformer | epoch 0 | step 9810 |avg loss 8.003 |avg tokens 4766.300 |tokens/s 32059.470 |walltime 1452.088 |
Transformer | epoch 0 | step 9820 |avg loss 7.974 |avg tokens 4710.000 |tokens/s 31440.083 |walltime 1453.586 |
Transformer | epoch 0 | step 9830 |avg loss 8.318 |avg tokens 4271.000 |tokens/s 30450.089 |walltime 1454.989 |
Transformer | epoch 0 | step 9840 |avg loss 8.224 |avg tokens 4867.100 |tokens/s 32777.270 |walltime 1456.474 |
Transformer | epoch 0 | step 9850 |avg loss 8.410 |avg tokens 4001.700 |tokens/s 28950.263 |walltime 1457.856 |
Transformer | epoch 0 | step 9860 |avg loss 8.235 |avg tokens 4672.600 |tokens/s 32019.471 |walltime 1459.316 |
Transformer | epoch 0 | step 9870 |avg loss 7.820 |avg tokens 4797.200 |tokens/s 31907.244 |walltime 1460.819 |
Transformer | epoch 0 | step 9880 |avg loss 8.283 |avg tokens 4411.900 |tokens/s 30799.447 |walltime 1462.251 |
Transformer | epoch 0 | step 9890 |avg loss 7.986 |avg tokens 4673.700 |tokens/s 31193.961 |walltime 1463.750 |
Transformer | epoch 0 | step 9900 |avg loss 8.006 |avg tokens 4716.700 |tokens/s 31821.367 |walltime 1465.232 |
Transformer | epoch 0 | step 9910 |avg loss 8.069 |avg tokens 4828.800 |tokens/s 31892.910 |walltime 1466.746 |
Transformer | epoch 0 | step 9920 |avg loss 8.085 |avg tokens 4502.400 |tokens/s 30630.936 |walltime 1468.216 |
Transformer | epoch 0 | step 9930 |avg loss 8.170 |avg tokens 4589.900 |tokens/s 28386.267 |walltime 1469.833 |
Transformer | epoch 0 | step 9940 |avg loss 7.937 |avg tokens 4376.900 |tokens/s 30459.534 |walltime 1471.270 |
Transformer | epoch 0 | step 9950 |avg loss 8.169 |avg tokens 4781.500 |tokens/s 32457.165 |walltime 1472.743 |
Transformer | epoch 0 | step 9960 |avg loss 8.084 |avg tokens 4720.100 |tokens/s 31500.970 |walltime 1474.241 |
Transformer | epoch 0 | step 9970 |avg loss 8.304 |avg tokens 4628.900 |tokens/s 31265.104 |walltime 1475.722 |
Transformer | epoch 0 | step 9980 |avg loss 7.658 |avg tokens 4792.400 |tokens/s 32058.731 |walltime 1477.217 |
Transformer | epoch 0 | step 9990 |avg loss 7.617 |avg tokens 4552.700 |tokens/s 30323.412 |walltime 1478.718 |
Transformer | epoch 0 | step 10000 |avg loss 8.099 |avg tokens 4465.500 |tokens/s 30705.621 |walltime 1480.173 |
Transformer | epoch 0 | step 10010 |avg loss 8.058 |avg tokens 4976.700 |tokens/s 32056.162 |walltime 1481.725 |
Transformer | epoch 0 | step 10020 |avg loss 7.982 |avg tokens 4674.100 |tokens/s 31114.881 |walltime 1483.227 |
Transformer | epoch 0 | step 10030 |avg loss 8.238 |avg tokens 4162.700 |tokens/s 29730.769 |walltime 1484.627 |
Transformer | epoch 0 | step 10040 |avg loss 7.650 |avg tokens 4833.700 |tokens/s 31550.392 |walltime 1486.159 |
Transformer | epoch 0 | step 10050 |avg loss 8.390 |avg tokens 4436.700 |tokens/s 31691.970 |walltime 1487.559 |
Transformer | epoch 0 | step 10060 |avg loss 8.135 |avg tokens 4812.300 |tokens/s 32166.862 |walltime 1489.055 |
Transformer | epoch 0 | step 10070 |avg loss 7.781 |avg tokens 4848.000 |tokens/s 31758.053 |walltime 1490.582 |
Transformer | epoch 0 | step 10080 |avg loss 7.919 |avg tokens 4390.100 |tokens/s 30509.061 |walltime 1492.021 |
Transformer | epoch 0 | step 10090 |avg loss 8.661 |avg tokens 3619.300 |tokens/s 26509.911 |walltime 1493.386 |
Transformer | epoch 0 | step 10100 |avg loss 8.154 |avg tokens 4332.600 |tokens/s 29932.790 |walltime 1494.834 |
Transformer | epoch 0 | step 10110 |avg loss 8.384 |avg tokens 4043.200 |tokens/s 29156.823 |walltime 1496.220 |
Transformer | epoch 0 | step 10120 |avg loss 8.261 |avg tokens 4457.500 |tokens/s 30764.303 |walltime 1497.669 |
Transformer | epoch 0 | step 10130 |avg loss 8.189 |avg tokens 4709.000 |tokens/s 31121.810 |walltime 1499.182 |
Transformer | epoch 0 | step 10140 |avg loss 7.874 |avg tokens 4418.400 |tokens/s 30392.396 |walltime 1500.636 |
Transformer | epoch 0 | step 10150 |avg loss 8.225 |avg tokens 4724.200 |tokens/s 32225.986 |walltime 1502.102 |
Transformer | epoch 0 | step 10160 |avg loss 8.220 |avg tokens 4321.800 |tokens/s 28696.602 |walltime 1503.608 |
Transformer | epoch 0 | step 10170 |avg loss 8.374 |avg tokens 4105.300 |tokens/s 29116.916 |walltime 1505.018 |
Transformer | epoch 0 | step 10180 |avg loss 8.008 |avg tokens 4577.600 |tokens/s 30488.330 |walltime 1506.520 |
Transformer | epoch 0 | step 10190 |avg loss 7.916 |avg tokens 4745.600 |tokens/s 31744.027 |walltime 1508.014 |
Transformer | epoch 0 | step 10200 |avg loss 8.168 |avg tokens 4619.900 |tokens/s 31487.125 |walltime 1509.482 |
Transformer | epoch 0 | step 10210 |avg loss 8.144 |avg tokens 4082.500 |tokens/s 29027.066 |walltime 1510.888 |
Transformer | epoch 0 | step 10220 |avg loss 7.863 |avg tokens 4845.600 |tokens/s 31866.317 |walltime 1512.409 |
Transformer | epoch 0 | step 10230 |avg loss 8.505 |avg tokens 4455.900 |tokens/s 30713.668 |walltime 1513.860 |
Transformer | epoch 0 | step 10240 |avg loss 8.227 |avg tokens 4325.300 |tokens/s 30367.815 |walltime 1515.284 |
Transformer | epoch 0 | step 10250 |avg loss 7.980 |avg tokens 4347.500 |tokens/s 29764.176 |walltime 1516.745 |
Transformer | epoch 0 | step 10260 |avg loss 8.291 |avg tokens 4220.500 |tokens/s 30048.381 |walltime 1518.149 |
Transformer | epoch 0 | step 10270 |avg loss 7.848 |avg tokens 4637.800 |tokens/s 30974.783 |walltime 1519.646 |
Transformer | epoch 0 | step 10280 |avg loss 8.084 |avg tokens 4380.200 |tokens/s 30302.360 |walltime 1521.092 |
Transformer | epoch 0 | step 10290 |avg loss 8.307 |avg tokens 4564.100 |tokens/s 30973.833 |walltime 1522.565 |
Transformer | epoch 0 | step 10300 |avg loss 7.718 |avg tokens 4845.600 |tokens/s 31575.360 |walltime 1524.100 |
Transformer | epoch 0 | step 10310 |avg loss 7.770 |avg tokens 4655.600 |tokens/s 30362.288 |walltime 1525.633 |
Transformer | epoch 0 | step 10320 |avg loss 8.166 |avg tokens 4871.800 |tokens/s 33084.140 |walltime 1527.106 |
Transformer | epoch 0 | step 10330 |avg loss 8.071 |avg tokens 4669.700 |tokens/s 30905.871 |walltime 1528.617 |
Transformer | epoch 0 | step 10340 |avg loss 7.972 |avg tokens 4009.200 |tokens/s 27079.840 |walltime 1530.097 |
Transformer | epoch 0 | step 10350 |avg loss 7.969 |avg tokens 4841.500 |tokens/s 31772.738 |walltime 1531.621 |
Transformer | epoch 0 | step 10360 |avg loss 7.842 |avg tokens 4895.900 |tokens/s 31818.554 |walltime 1533.160 |
Transformer | epoch 0 | step 10370 |avg loss 8.111 |avg tokens 4269.700 |tokens/s 29016.428 |walltime 1534.631 |
Transformer | epoch 0 | step 10380 |avg loss 8.453 |avg tokens 4456.000 |tokens/s 31119.874 |walltime 1536.063 |
Transformer | epoch 0 | step 10390 |avg loss 8.285 |avg tokens 4165.800 |tokens/s 28584.582 |walltime 1537.521 |
Transformer | epoch 0 | step 10400 |avg loss 8.054 |avg tokens 4443.100 |tokens/s 29866.551 |walltime 1539.008 |
Transformer | epoch 0 | step 10410 |avg loss 8.195 |avg tokens 4763.200 |tokens/s 31935.888 |walltime 1540.500 |
Transformer | epoch 0 | step 10420 |avg loss 7.898 |avg tokens 4777.900 |tokens/s 32005.346 |walltime 1541.993 |
Transformer | epoch 0 | step 10430 |avg loss 7.885 |avg tokens 4361.000 |tokens/s 29368.359 |walltime 1543.478 |
Transformer | epoch 0 | step 10440 |avg loss 8.370 |avg tokens 4274.900 |tokens/s 31081.909 |walltime 1544.853 |
Transformer | epoch 0 | step 10450 |avg loss 8.206 |avg tokens 4390.200 |tokens/s 31042.594 |walltime 1546.267 |
Transformer | epoch 0 | step 10460 |avg loss 8.045 |avg tokens 4791.600 |tokens/s 32249.360 |walltime 1547.753 |
Transformer | epoch 0 | step 10470 |avg loss 8.062 |avg tokens 4975.700 |tokens/s 32333.999 |walltime 1549.292 |
Transformer | epoch 0 | step 10480 |avg loss 8.055 |avg tokens 4391.000 |tokens/s 30059.295 |walltime 1550.753 |
Transformer | epoch 0 | step 10490 |avg loss 8.443 |avg tokens 4587.900 |tokens/s 32149.257 |walltime 1552.180 |
Transformer | epoch 0 | step 10500 |avg loss 7.925 |avg tokens 4443.400 |tokens/s 30555.466 |walltime 1553.634 |
Transformer | epoch 0 | step 10510 |avg loss 8.067 |avg tokens 4271.000 |tokens/s 29484.871 |walltime 1555.082 |
Transformer | epoch 0 | step 10520 |avg loss 8.187 |avg tokens 4381.500 |tokens/s 29638.648 |walltime 1556.561 |
Transformer | epoch 0 | step 10530 |avg loss 7.720 |avg tokens 4713.600 |tokens/s 30826.045 |walltime 1558.090 |
Transformer | epoch 0 | step 10540 |avg loss 8.134 |avg tokens 4410.900 |tokens/s 30512.443 |walltime 1559.535 |
Transformer | epoch 0 | step 10550 |avg loss 8.172 |avg tokens 4564.500 |tokens/s 31074.844 |walltime 1561.004 |
Transformer | epoch 0 | step 10560 |avg loss 8.429 |avg tokens 4595.600 |tokens/s 31491.327 |walltime 1562.464 |
Transformer | epoch 0 | step 10570 |avg loss 7.849 |avg tokens 4567.100 |tokens/s 31305.856 |walltime 1563.923 |
Transformer | epoch 0 | step 10580 |avg loss 8.032 |avg tokens 5002.300 |tokens/s 33392.975 |walltime 1565.421 |
Transformer | epoch 0 | step 10590 |avg loss 8.068 |avg tokens 4560.000 |tokens/s 31670.923 |walltime 1566.860 |
Transformer | epoch 0 | step 10600 |avg loss 8.256 |avg tokens 3996.600 |tokens/s 28507.841 |walltime 1568.262 |
Transformer | epoch 0 | step 10610 |avg loss 7.992 |avg tokens 4505.500 |tokens/s 30271.648 |walltime 1569.751 |
Transformer | epoch 0 | step 10620 |avg loss 8.102 |avg tokens 4790.300 |tokens/s 32575.879 |walltime 1571.221 |
Transformer | epoch 0 | step 10630 |avg loss 8.345 |avg tokens 4474.200 |tokens/s 30960.723 |walltime 1572.666 |
Transformer | epoch 0 | step 10640 |avg loss 8.432 |avg tokens 4486.000 |tokens/s 31254.803 |walltime 1574.102 |
Transformer | epoch 0 | step 10650 |avg loss 8.078 |avg tokens 4628.200 |tokens/s 31641.095 |walltime 1575.564 |
Transformer | epoch 0 | step 10660 |avg loss 7.878 |avg tokens 4440.900 |tokens/s 30415.334 |walltime 1577.024 |
Transformer | epoch 0 | step 10670 |avg loss 7.711 |avg tokens 4821.300 |tokens/s 32112.855 |walltime 1578.526 |
Transformer | epoch 0 | step 10680 |avg loss 8.089 |avg tokens 4553.900 |tokens/s 30005.149 |walltime 1580.043 |
Transformer | epoch 0 | step 10690 |avg loss 8.051 |avg tokens 4671.100 |tokens/s 31201.751 |walltime 1581.541 |
Transformer | epoch 0 | step 10700 |avg loss 8.059 |avg tokens 4130.900 |tokens/s 28961.652 |walltime 1582.967 |
Transformer | epoch 0 | step 10710 |avg loss 8.432 |avg tokens 4113.100 |tokens/s 29828.085 |walltime 1584.346 |
Transformer | epoch 0 | step 10720 |avg loss 8.298 |avg tokens 4595.700 |tokens/s 32296.068 |walltime 1585.769 |
Transformer | epoch 0 | step 10730 |avg loss 8.114 |avg tokens 4647.600 |tokens/s 30785.040 |walltime 1587.278 |
Transformer | epoch 0 | step 10740 |avg loss 7.966 |avg tokens 4548.000 |tokens/s 30705.566 |walltime 1588.760 |
Transformer | epoch 0 | step 10750 |avg loss 7.901 |avg tokens 4500.300 |tokens/s 30610.904 |walltime 1590.230 |
Transformer | epoch 0 | step 10760 |avg loss 7.593 |avg tokens 4619.300 |tokens/s 30955.997 |walltime 1591.722 |
Transformer | epoch 0 | step 10770 |avg loss 8.401 |avg tokens 4531.200 |tokens/s 30836.938 |walltime 1593.191 |
Transformer | epoch 0 | step 10780 |avg loss 8.298 |avg tokens 4396.600 |tokens/s 30287.255 |walltime 1594.643 |
Transformer | epoch 0 | step 10790 |avg loss 7.798 |avg tokens 4309.300 |tokens/s 29601.066 |walltime 1596.099 |
Transformer | epoch 0 | step 10800 |avg loss 8.121 |avg tokens 4734.800 |tokens/s 31891.970 |walltime 1597.584 |
Transformer | epoch 0 | step 10810 |avg loss 8.204 |avg tokens 4194.200 |tokens/s 29796.188 |walltime 1598.991 |
Transformer | epoch 0 | step 10820 |avg loss 7.804 |avg tokens 4426.800 |tokens/s 30590.088 |walltime 1600.438 |
Transformer | epoch 0 | step 10830 |avg loss 8.187 |avg tokens 4547.900 |tokens/s 30940.779 |walltime 1601.908 |
Transformer | epoch 0 | step 10840 |avg loss 8.005 |avg tokens 4768.000 |tokens/s 31348.237 |walltime 1603.429 |
Transformer | epoch 0 | step 10850 |avg loss 8.049 |avg tokens 4733.600 |tokens/s 33134.663 |walltime 1604.858 |
Transformer | epoch 0 | step 10860 |avg loss 8.094 |avg tokens 3946.100 |tokens/s 27988.658 |walltime 1606.268 |
Transformer | epoch 0 | step 10870 |avg loss 7.986 |avg tokens 4883.900 |tokens/s 31729.327 |walltime 1607.807 |
Transformer | epoch 0 | step 10880 |avg loss 8.225 |avg tokens 4399.100 |tokens/s 31478.418 |walltime 1609.204 |
Transformer | epoch 0 | step 10890 |avg loss 8.100 |avg tokens 4352.900 |tokens/s 30131.772 |walltime 1610.649 |
Transformer | epoch 0 | step 10900 |avg loss 8.510 |avg tokens 4621.200 |tokens/s 31094.782 |walltime 1612.135 |
Transformer | epoch 0 | step 10910 |avg loss 8.505 |avg tokens 4323.800 |tokens/s 31486.831 |walltime 1613.508 |
Transformer | epoch 0 | step 10920 |avg loss 8.187 |avg tokens 4428.200 |tokens/s 28843.068 |walltime 1615.044 |
Transformer | epoch 0 | step 10930 |avg loss 7.865 |avg tokens 4717.700 |tokens/s 32195.706 |walltime 1616.509 |
Transformer | epoch 0 | step 10940 |avg loss 8.036 |avg tokens 4358.300 |tokens/s 30504.693 |walltime 1617.938 |
Transformer | epoch 0 | step 10950 |avg loss 8.242 |avg tokens 4354.600 |tokens/s 29539.927 |walltime 1619.412 |
Transformer | epoch 0 | step 10960 |avg loss 8.082 |avg tokens 4630.300 |tokens/s 30484.898 |walltime 1620.931 |
Transformer | epoch 0 | step 10970 |avg loss 8.170 |avg tokens 4439.500 |tokens/s 30431.267 |walltime 1622.390 |
Transformer | epoch 0 | step 10980 |avg loss 8.208 |avg tokens 4342.300 |tokens/s 29793.844 |walltime 1623.847 |
Transformer | epoch 0 | step 10990 |avg loss 8.405 |avg tokens 4330.500 |tokens/s 30300.660 |walltime 1625.276 |
Transformer | epoch 0 | step 11000 |avg loss 8.053 |avg tokens 4611.400 |tokens/s 31400.599 |walltime 1626.745 |
Transformer | epoch 0 | step 11010 |avg loss 8.054 |avg tokens 4720.800 |tokens/s 31937.376 |walltime 1628.223 |
Transformer | epoch 0 | step 11020 |avg loss 8.000 |avg tokens 4350.200 |tokens/s 28975.298 |walltime 1629.724 |
Transformer | epoch 0 | step 11030 |avg loss 7.965 |avg tokens 4580.500 |tokens/s 30769.009 |walltime 1631.213 |
Transformer | epoch 0 | step 11040 |avg loss 8.040 |avg tokens 4142.600 |tokens/s 28221.951 |walltime 1632.681 |
Transformer | epoch 0 | step 11050 |avg loss 8.294 |avg tokens 4422.800 |tokens/s 30776.875 |walltime 1634.118 |
Transformer | epoch 0 | step 11060 |avg loss 8.667 |avg tokens 4627.000 |tokens/s 32498.451 |walltime 1635.542 |
Transformer | epoch 0 | step 11070 |avg loss 8.198 |avg tokens 4770.700 |tokens/s 33059.338 |walltime 1636.985 |
Transformer | epoch 0 | step 11080 |avg loss 8.171 |avg tokens 4659.700 |tokens/s 31298.285 |walltime 1638.474 |
Transformer | epoch 0 | step 11090 |avg loss 8.198 |avg tokens 3906.700 |tokens/s 26631.915 |walltime 1639.940 |
Transformer | epoch 0 | step 11100 |avg loss 8.290 |avg tokens 3946.700 |tokens/s 28531.333 |walltime 1641.324 |
Transformer | epoch 0 | step 11110 |avg loss 8.364 |avg tokens 4772.000 |tokens/s 31802.324 |walltime 1642.824 |
Transformer | epoch 0 | step 11120 |avg loss 7.953 |avg tokens 4643.700 |tokens/s 30150.097 |walltime 1644.364 |
Transformer | epoch 0 | step 11130 |avg loss 8.268 |avg tokens 4243.000 |tokens/s 29130.096 |walltime 1645.821 |
Transformer | epoch 0 | step 11140 |avg loss 8.180 |avg tokens 4572.500 |tokens/s 30875.827 |walltime 1647.302 |
Transformer | epoch 0 | step 11150 |avg loss 8.143 |avg tokens 4653.600 |tokens/s 31527.532 |walltime 1648.778 |
Transformer | epoch 0 | step 11160 |avg loss 7.989 |avg tokens 4906.800 |tokens/s 31579.492 |walltime 1650.332 |
Transformer | epoch 0 | step 11170 |avg loss 8.078 |avg tokens 4819.600 |tokens/s 33364.623 |walltime 1651.776 |
Transformer | epoch 0 | step 11180 |avg loss 7.978 |avg tokens 4745.100 |tokens/s 31689.825 |walltime 1653.274 |
Transformer | epoch 0 | step 11190 |avg loss 8.392 |avg tokens 4503.300 |tokens/s 30889.484 |walltime 1654.732 |
Transformer | epoch 0 | step 11200 |avg loss 8.335 |avg tokens 4298.700 |tokens/s 29884.303 |walltime 1656.170 |
Transformer | epoch 0 | step 11210 |avg loss 8.423 |avg tokens 4026.900 |tokens/s 28849.006 |walltime 1657.566 |
Transformer | epoch 0 | step 11220 |avg loss 8.070 |avg tokens 4750.800 |tokens/s 32438.511 |walltime 1659.030 |
Transformer | epoch 0 | step 11230 |avg loss 7.737 |avg tokens 4504.700 |tokens/s 30780.271 |walltime 1660.494 |
Transformer | epoch 0 | step 11240 |avg loss 7.970 |avg tokens 4768.300 |tokens/s 31386.607 |walltime 1662.013 |
Transformer | epoch 0 | step 11250 |avg loss 8.155 |avg tokens 4361.400 |tokens/s 29150.318 |walltime 1663.509 |
Transformer | epoch 0 | step 11260 |avg loss 7.885 |avg tokens 4532.600 |tokens/s 30245.052 |walltime 1665.008 |
Transformer | epoch 0 | step 11270 |avg loss 8.412 |avg tokens 4884.900 |tokens/s 33423.287 |walltime 1666.470 |
Transformer | epoch 0 | step 11280 |avg loss 7.996 |avg tokens 4552.300 |tokens/s 30608.575 |walltime 1667.957 |
Transformer | epoch 0 | step 11290 |avg loss 8.142 |avg tokens 4699.000 |tokens/s 31249.311 |walltime 1669.461 |
Transformer | epoch 0 | step 11300 |avg loss 8.048 |avg tokens 4578.400 |tokens/s 30825.827 |walltime 1670.946 |
Transformer | epoch 0 | step 11310 |avg loss 8.232 |avg tokens 4690.800 |tokens/s 32031.078 |walltime 1672.410 |
Transformer | epoch 0 | step 11320 |avg loss 7.883 |avg tokens 4518.400 |tokens/s 30441.830 |walltime 1673.895 |
Transformer | epoch 0 | step 11330 |avg loss 8.112 |avg tokens 4674.100 |tokens/s 31226.430 |walltime 1675.391 |
Transformer | epoch 0 | step 11340 |avg loss 7.867 |avg tokens 4529.800 |tokens/s 30428.082 |walltime 1676.880 |
Transformer | epoch 0 | step 11350 |avg loss 8.015 |avg tokens 4502.800 |tokens/s 30099.380 |walltime 1678.376 |
Transformer | epoch 0 | step 11360 |avg loss 8.356 |avg tokens 4589.200 |tokens/s 32011.613 |walltime 1679.810 |
Transformer | epoch 0 | step 11370 |avg loss 7.984 |avg tokens 4768.300 |tokens/s 31421.041 |walltime 1681.327 |
Transformer | epoch 0 | step 11380 |avg loss 7.996 |avg tokens 4748.200 |tokens/s 31512.493 |walltime 1682.834 |
Transformer | epoch 0 | step 11390 |avg loss 8.391 |avg tokens 4629.800 |tokens/s 31452.820 |walltime 1684.306 |
Transformer | epoch 0 | step 11400 |avg loss 8.079 |avg tokens 4573.500 |tokens/s 32257.431 |walltime 1685.724 |
Transformer | epoch 0 | step 11410 |avg loss 8.257 |avg tokens 4407.800 |tokens/s 30888.958 |walltime 1687.151 |
Transformer | epoch 0 | step 11420 |avg loss 8.024 |avg tokens 4619.400 |tokens/s 31058.405 |walltime 1688.638 |
Transformer | epoch 0 | step 11430 |avg loss 8.050 |avg tokens 4600.900 |tokens/s 31281.161 |walltime 1690.109 |
Transformer | epoch 0 | step 11440 |avg loss 8.100 |avg tokens 4122.500 |tokens/s 29223.054 |walltime 1691.520 |
Transformer | epoch 0 | step 11450 |avg loss 8.416 |avg tokens 4795.900 |tokens/s 32646.307 |walltime 1692.989 |
Transformer | epoch 0 | step 11460 |avg loss 8.432 |avg tokens 4248.200 |tokens/s 30033.182 |walltime 1694.403 |
Transformer | epoch 0 | step 11470 |avg loss 8.079 |avg tokens 4820.000 |tokens/s 32117.156 |walltime 1695.904 |
Transformer | epoch 0 | step 11480 |avg loss 7.903 |avg tokens 4606.400 |tokens/s 30910.633 |walltime 1697.394 |
Transformer | epoch 0 | step 11490 |avg loss 8.487 |avg tokens 4629.000 |tokens/s 31025.626 |walltime 1698.886 |
Transformer | epoch 0 | step 11500 |avg loss 8.153 |avg tokens 4640.400 |tokens/s 31905.050 |walltime 1700.341 |
Transformer | epoch 0 | step 11510 |avg loss 8.207 |avg tokens 4579.100 |tokens/s 31467.803 |walltime 1701.796 |
Transformer | epoch 0 | step 11520 |avg loss 8.334 |avg tokens 3906.200 |tokens/s 28984.172 |walltime 1703.143 |
Transformer | epoch 0 | step 11530 |avg loss 8.479 |avg tokens 4505.500 |tokens/s 31304.872 |walltime 1704.583 |
Transformer | epoch 0 | step 11540 |avg loss 8.576 |avg tokens 4330.800 |tokens/s 30247.264 |walltime 1706.015 |
Transformer | epoch 0 | step 11550 |avg loss 8.369 |avg tokens 4557.800 |tokens/s 32322.946 |walltime 1707.425 |
Transformer | epoch 0 | step 11560 |avg loss 8.277 |avg tokens 4662.800 |tokens/s 31053.423 |walltime 1708.926 |
Transformer | epoch 0 | step 11570 |avg loss 8.471 |avg tokens 4328.000 |tokens/s 30414.880 |walltime 1710.349 |
Transformer | epoch 0 | step 11580 |avg loss 8.370 |avg tokens 4221.900 |tokens/s 30112.791 |walltime 1711.751 |
Transformer | epoch 0 | step 11590 |avg loss 8.768 |avg tokens 3810.800 |tokens/s 28440.857 |walltime 1713.091 |
Transformer | epoch 0 | step 11600 |avg loss 8.194 |avg tokens 4540.200 |tokens/s 31006.670 |walltime 1714.555 |
Transformer | epoch 0 | step 11610 |avg loss 8.335 |avg tokens 4219.600 |tokens/s 29599.121 |walltime 1715.981 |
Transformer | epoch 0 | step 11620 |avg loss 7.976 |avg tokens 4497.300 |tokens/s 31126.852 |walltime 1717.426 |
Transformer | epoch 0 | step 11630 |avg loss 8.410 |avg tokens 4602.300 |tokens/s 32048.802 |walltime 1718.862 |
Transformer | epoch 0 | step 11640 |avg loss 8.216 |avg tokens 4527.800 |tokens/s 30797.469 |walltime 1720.332 |
Transformer | epoch 0 | step 11650 |avg loss 8.133 |avg tokens 4918.500 |tokens/s 32118.676 |walltime 1721.863 |
Transformer | epoch 0 | step 11660 |avg loss 8.212 |avg tokens 4834.500 |tokens/s 32689.875 |walltime 1723.342 |
Transformer | epoch 0 | step 11670 |avg loss 7.918 |avg tokens 4726.500 |tokens/s 31393.775 |walltime 1724.848 |
Transformer | epoch 0 | step 11680 |avg loss 8.597 |avg tokens 4541.000 |tokens/s 31505.166 |walltime 1726.289 |
Transformer | epoch 0 | step 11690 |avg loss 8.337 |avg tokens 3855.100 |tokens/s 26947.427 |walltime 1727.720 |
Transformer | epoch 0 | step 11700 |avg loss 8.298 |avg tokens 4750.300 |tokens/s 31978.661 |walltime 1729.205 |
Transformer | epoch 0 | step 11710 |avg loss 8.285 |avg tokens 4828.100 |tokens/s 33023.436 |walltime 1730.667 |
Transformer | epoch 0 | step 11720 |avg loss 7.996 |avg tokens 4664.200 |tokens/s 31199.834 |walltime 1732.162 |
Transformer | epoch 0 | step 11730 |avg loss 8.510 |avg tokens 4155.700 |tokens/s 29469.425 |walltime 1733.572 |
Transformer | epoch 0 | step 11740 |avg loss 8.122 |avg tokens 4346.500 |tokens/s 29857.990 |walltime 1735.028 |
Transformer | epoch 0 | step 11750 |avg loss 8.355 |avg tokens 4072.200 |tokens/s 29204.066 |walltime 1736.422 |
Transformer | epoch 0 | step 11760 |avg loss 8.396 |avg tokens 4446.600 |tokens/s 31527.276 |walltime 1737.833 |
Transformer | epoch 0 | step 11770 |avg loss 8.142 |avg tokens 4724.500 |tokens/s 32591.353 |walltime 1739.282 |
Transformer | epoch 0 | step 11780 |avg loss 8.195 |avg tokens 4499.900 |tokens/s 31182.904 |walltime 1740.726 |
Transformer | epoch 0 | step 11790 |avg loss 8.187 |avg tokens 4474.300 |tokens/s 30034.237 |walltime 1742.215 |
Transformer | epoch 0 | step 11800 |avg loss 7.954 |avg tokens 4977.600 |tokens/s 33607.560 |walltime 1743.696 |
Transformer | epoch 0 | step 11810 |avg loss 8.211 |avg tokens 4438.200 |tokens/s 30601.819 |walltime 1745.147 |
Transformer | epoch 0 | step 11820 |avg loss 8.105 |avg tokens 4310.200 |tokens/s 29303.427 |walltime 1746.618 |
Transformer | epoch 0 | step 11830 |avg loss 7.741 |avg tokens 4787.000 |tokens/s 31132.639 |walltime 1748.155 |
Transformer | epoch 0 | step 11840 |avg loss 8.099 |avg tokens 4412.900 |tokens/s 31139.410 |walltime 1749.572 |
Transformer | epoch 0 | step 11850 |avg loss 8.570 |avg tokens 4379.100 |tokens/s 31117.233 |walltime 1750.980 |
Transformer | epoch 0 | step 11860 |avg loss 8.033 |avg tokens 4243.400 |tokens/s 30515.111 |walltime 1752.370 |
Transformer | epoch 0 | step 11870 |avg loss 8.854 |avg tokens 4417.900 |tokens/s 30933.250 |walltime 1753.798 |
Transformer | epoch 0 | step 11880 |avg loss 8.227 |avg tokens 4563.400 |tokens/s 31266.534 |walltime 1755.258 |
Transformer | epoch 0 | step 11890 |avg loss 8.330 |avg tokens 4452.000 |tokens/s 31390.417 |walltime 1756.676 |
Transformer | epoch 0 | step 11900 |avg loss 8.292 |avg tokens 4612.500 |tokens/s 31248.479 |walltime 1758.152 |
Transformer | epoch 0 | step 11910 |avg loss 8.427 |avg tokens 4666.400 |tokens/s 32383.421 |walltime 1759.593 |
Transformer | epoch 0 | step 11920 |avg loss 8.272 |avg tokens 4679.300 |tokens/s 31940.616 |walltime 1761.058 |
Transformer | epoch 0 | step 11930 |avg loss 8.388 |avg tokens 4244.200 |tokens/s 30376.926 |walltime 1762.455 |
Transformer | epoch 0 | step 11940 |avg loss 8.414 |avg tokens 4541.000 |tokens/s 31180.189 |walltime 1763.912 |
Transformer | epoch 0 | step 11950 |avg loss 8.115 |avg tokens 4639.900 |tokens/s 30026.904 |walltime 1765.457 |
Transformer | epoch 0 | step 11960 |avg loss 8.076 |avg tokens 4543.700 |tokens/s 30796.659 |walltime 1766.933 |
Transformer | epoch 0 | step 11970 |avg loss 8.427 |avg tokens 3989.500 |tokens/s 29136.961 |walltime 1768.302 |
Transformer | epoch 0 | step 11980 |avg loss 8.358 |avg tokens 4768.600 |tokens/s 32217.234 |walltime 1769.782 |
Transformer | epoch 0 | step 11990 |avg loss 8.274 |avg tokens 4542.900 |tokens/s 30971.277 |walltime 1771.249 |
Transformer | epoch 0 | step 12000 |avg loss 8.238 |avg tokens 4267.200 |tokens/s 30457.072 |walltime 1772.650 |
Transformer | epoch 0 | step 12010 |avg loss 7.993 |avg tokens 4359.500 |tokens/s 29448.979 |walltime 1774.130 |
Transformer | epoch 0 | step 12020 |avg loss 7.967 |avg tokens 4481.400 |tokens/s 30501.628 |walltime 1775.599 |
Transformer | epoch 0 | step 12030 |avg loss 8.518 |avg tokens 3904.000 |tokens/s 28855.996 |walltime 1776.952 |
Transformer | epoch 0 | step 12040 |avg loss 8.376 |avg tokens 4195.200 |tokens/s 29624.328 |walltime 1778.368 |
Transformer | epoch 0 | step 12050 |avg loss 8.127 |avg tokens 4613.400 |tokens/s 31328.152 |walltime 1779.841 |
Transformer | epoch 0 | step 12060 |avg loss 7.952 |avg tokens 4670.800 |tokens/s 32186.893 |walltime 1781.292 |
Transformer | epoch 0 | step 12070 |avg loss 8.677 |avg tokens 4308.300 |tokens/s 28913.835 |walltime 1782.782 |
Transformer | epoch 0 | step 12080 |avg loss 8.633 |avg tokens 4009.200 |tokens/s 28277.667 |walltime 1784.200 |
Transformer | epoch 0 | step 12090 |avg loss 8.663 |avg tokens 4187.000 |tokens/s 29989.781 |walltime 1785.596 |
Transformer | epoch 0 | step 12100 |avg loss 8.227 |avg tokens 4662.500 |tokens/s 31691.334 |walltime 1787.067 |
Transformer | epoch 0 | step 12110 |avg loss 8.268 |avg tokens 4335.800 |tokens/s 29722.965 |walltime 1788.526 |
Transformer | epoch 0 | step 12120 |avg loss 8.253 |avg tokens 4848.500 |tokens/s 32601.449 |walltime 1790.013 |
Transformer | epoch 0 | step 12130 |avg loss 8.257 |avg tokens 4807.900 |tokens/s 33077.159 |walltime 1791.467 |
Transformer | epoch 0 | step 12140 |avg loss 8.115 |avg tokens 4347.400 |tokens/s 28849.627 |walltime 1792.974 |
Transformer | epoch 0 | step 12150 |avg loss 8.469 |avg tokens 4512.800 |tokens/s 31007.771 |walltime 1794.429 |
Transformer | epoch 0 | step 12160 |avg loss 8.233 |avg tokens 4698.600 |tokens/s 32741.074 |walltime 1795.864 |
Transformer | epoch 0 | step 12170 |avg loss 8.443 |avg tokens 4112.000 |tokens/s 29081.034 |walltime 1797.278 |
Transformer | epoch 0 | step 12180 |avg loss 8.328 |avg tokens 3654.800 |tokens/s 26732.811 |walltime 1798.645 |
Transformer | epoch 0 | step 12190 |avg loss 8.483 |avg tokens 4510.200 |tokens/s 31267.345 |walltime 1800.088 |
Transformer | epoch 0 | step 12200 |avg loss 8.536 |avg tokens 3772.300 |tokens/s 26954.978 |walltime 1801.487 |
Transformer | epoch 0 | step 12210 |avg loss 8.343 |avg tokens 4375.200 |tokens/s 30852.448 |walltime 1802.905 |
Transformer | epoch 0 | step 12220 |avg loss 8.239 |avg tokens 4536.800 |tokens/s 30691.323 |walltime 1804.384 |
Transformer | epoch 0 | step 12230 |avg loss 8.466 |avg tokens 4701.200 |tokens/s 32220.519 |walltime 1805.843 |
Transformer | epoch 0 | step 12240 |avg loss 8.244 |avg tokens 4663.200 |tokens/s 31869.104 |walltime 1807.306 |
Transformer | epoch 0 | step 12250 |avg loss 8.315 |avg tokens 4209.000 |tokens/s 30344.228 |walltime 1808.693 |
Transformer | epoch 0 | step 12260 |avg loss 8.151 |avg tokens 4991.700 |tokens/s 33563.738 |walltime 1810.180 |
Transformer | epoch 0 | step 12270 |avg loss 7.639 |avg tokens 4914.400 |tokens/s 32005.751 |walltime 1811.716 |
Transformer | epoch 0 | step 12280 |avg loss 8.200 |avg tokens 4726.300 |tokens/s 32147.675 |walltime 1813.186 |
Transformer | epoch 0 | step 12290 |avg loss 8.450 |avg tokens 4410.000 |tokens/s 31037.477 |walltime 1814.607 |
Transformer | epoch 0 | step 12300 |avg loss 8.296 |avg tokens 4080.900 |tokens/s 29032.137 |walltime 1816.012 |
Transformer | epoch 0 | step 12310 |avg loss 8.243 |avg tokens 4775.400 |tokens/s 32240.512 |walltime 1817.494 |
Transformer | epoch 0 | step 12320 |avg loss 8.507 |avg tokens 4331.800 |tokens/s 31132.318 |walltime 1818.885 |
Transformer | epoch 0 | step 12330 |avg loss 8.159 |avg tokens 4599.400 |tokens/s 31021.241 |walltime 1820.368 |
Transformer | epoch 0 | step 12340 |avg loss 8.353 |avg tokens 4558.900 |tokens/s 30957.102 |walltime 1821.840 |
Transformer | epoch 0 | step 12350 |avg loss 8.146 |avg tokens 4620.300 |tokens/s 31425.255 |walltime 1823.311 |
Transformer | epoch 0 | step 12360 |avg loss 7.783 |avg tokens 4742.500 |tokens/s 31058.270 |walltime 1824.838 |
Transformer | epoch 0 | step 12370 |avg loss 8.204 |avg tokens 4302.300 |tokens/s 30191.939 |walltime 1826.263 |
Transformer | epoch 0 | step 12380 |avg loss 8.040 |avg tokens 4400.800 |tokens/s 29887.041 |walltime 1827.735 |
Transformer | epoch 0 | step 12390 |avg loss 8.147 |avg tokens 4336.200 |tokens/s 30074.819 |walltime 1829.177 |
Transformer | epoch 0 | step 12400 |avg loss 8.484 |avg tokens 4409.700 |tokens/s 29530.509 |walltime 1830.670 |
Transformer | epoch 0 | step 12410 |avg loss 8.178 |avg tokens 4550.500 |tokens/s 30692.900 |walltime 1832.153 |
Transformer | epoch 0 | step 12420 |avg loss 8.061 |avg tokens 4883.500 |tokens/s 32694.070 |walltime 1833.646 |
Transformer | epoch 0 | step 12430 |avg loss 8.303 |avg tokens 4664.500 |tokens/s 31728.091 |walltime 1835.117 |
Transformer | epoch 0 | step 12440 |avg loss 8.418 |avg tokens 4328.300 |tokens/s 30614.800 |walltime 1836.530 |
Transformer | epoch 0 | step 12450 |avg loss 8.106 |avg tokens 4719.100 |tokens/s 31271.165 |walltime 1838.039 |
Transformer | epoch 0 | step 12460 |avg loss 8.165 |avg tokens 4097.900 |tokens/s 28004.967 |walltime 1839.503 |
Transformer | epoch 0 | step 12470 |avg loss 8.215 |avg tokens 4403.900 |tokens/s 29977.800 |walltime 1840.972 |
Transformer | epoch 0 | step 12480 |avg loss 8.242 |avg tokens 4509.700 |tokens/s 29388.185 |walltime 1842.506 |
Transformer | epoch 0 | step 12490 |avg loss 8.059 |avg tokens 4199.300 |tokens/s 28937.113 |walltime 1843.958 |
Transformer | epoch 0 | step 12500 |avg loss 8.125 |avg tokens 4255.300 |tokens/s 29122.173 |walltime 1845.419 |
Transformer | epoch 0 | step 12510 |avg loss 8.184 |avg tokens 3838.800 |tokens/s 27440.985 |walltime 1846.818 |
Transformer | epoch 0 | step 12520 |avg loss 8.307 |avg tokens 4270.800 |tokens/s 29495.093 |walltime 1848.266 |
Transformer | epoch 0 | step 12530 |avg loss 8.122 |avg tokens 4399.300 |tokens/s 30089.614 |walltime 1849.728 |
Transformer | epoch 0 | step 12540 |avg loss 8.491 |avg tokens 4179.600 |tokens/s 29874.660 |walltime 1851.127 |
Transformer | epoch 0 | step 12550 |avg loss 8.211 |avg tokens 4896.800 |tokens/s 32077.073 |walltime 1852.653 |
Transformer | epoch 0 | step 12560 |avg loss 7.841 |avg tokens 4295.500 |tokens/s 29414.147 |walltime 1854.114 |
Transformer | epoch 0 | step 12570 |avg loss 8.553 |avg tokens 4402.200 |tokens/s 31356.249 |walltime 1855.518 |
Transformer | epoch 0 | step 12580 |avg loss 8.122 |avg tokens 4762.800 |tokens/s 30735.156 |walltime 1857.067 |
Transformer | epoch 0 | step 12590 |avg loss 8.193 |avg tokens 4682.800 |tokens/s 32279.678 |walltime 1858.518 |
Transformer | epoch 0 | step 12600 |avg loss 8.160 |avg tokens 4764.800 |tokens/s 31450.135 |walltime 1860.033 |
Transformer | epoch 0 | step 12610 |avg loss 8.053 |avg tokens 4572.500 |tokens/s 31152.613 |walltime 1861.501 |
Transformer | epoch 0 | step 12620 |avg loss 8.204 |avg tokens 4518.400 |tokens/s 31098.917 |walltime 1862.954 |
Transformer | epoch 0 | step 12630 |avg loss 8.122 |avg tokens 4612.800 |tokens/s 30803.566 |walltime 1864.451 |
Transformer | epoch 0 | step 12640 |avg loss 7.977 |avg tokens 4755.700 |tokens/s 31248.906 |walltime 1865.973 |
Transformer | epoch 0 | step 12650 |avg loss 8.160 |avg tokens 4681.700 |tokens/s 31378.629 |walltime 1867.465 |
Transformer | epoch 0 | step 12660 |avg loss 8.274 |avg tokens 4348.200 |tokens/s 30998.125 |walltime 1868.868 |
Transformer | epoch 0 | step 12670 |avg loss 8.195 |avg tokens 4448.600 |tokens/s 31034.621 |walltime 1870.301 |
Transformer | epoch 0 | step 12680 |avg loss 8.284 |avg tokens 4784.300 |tokens/s 31603.787 |walltime 1871.815 |
Transformer | epoch 0 | step 12690 |avg loss 8.352 |avg tokens 3652.900 |tokens/s 25903.880 |walltime 1873.225 |
Transformer | epoch 0 | step 12700 |avg loss 8.224 |avg tokens 4738.200 |tokens/s 33101.261 |walltime 1874.657 |
Transformer | epoch 0 | step 12710 |avg loss 8.147 |avg tokens 4789.100 |tokens/s 32960.914 |walltime 1876.110 |
Transformer | epoch 0 | step 12720 |avg loss 8.406 |avg tokens 4432.500 |tokens/s 29995.388 |walltime 1877.587 |
Transformer | epoch 0 | step 12730 |avg loss 8.385 |avg tokens 4804.700 |tokens/s 32137.531 |walltime 1879.082 |
Transformer | epoch 0 | step 12740 |avg loss 8.588 |avg tokens 4467.000 |tokens/s 31140.138 |walltime 1880.517 |
Transformer | epoch 0 | step 12750 |avg loss 8.251 |avg tokens 4709.300 |tokens/s 31453.316 |walltime 1882.014 |
Transformer | epoch 0 | step 12760 |avg loss 8.489 |avg tokens 4511.100 |tokens/s 31311.711 |walltime 1883.455 |
Transformer | epoch 0 | step 12770 |avg loss 8.095 |avg tokens 4810.300 |tokens/s 30784.492 |walltime 1885.017 |
Transformer | epoch 0 | step 12780 |avg loss 8.138 |avg tokens 4607.700 |tokens/s 30684.562 |walltime 1886.519 |
Transformer | epoch 0 | step 12790 |avg loss 8.208 |avg tokens 4683.700 |tokens/s 31102.180 |walltime 1888.025 |
Transformer | epoch 0 | step 12800 |avg loss 8.263 |avg tokens 4866.400 |tokens/s 32066.455 |walltime 1889.543 |
Transformer | epoch 0 | step 12810 |avg loss 8.207 |avg tokens 4761.800 |tokens/s 31377.538 |walltime 1891.060 |
Transformer | epoch 0 | step 12820 |avg loss 8.209 |avg tokens 4244.200 |tokens/s 29067.095 |walltime 1892.520 |
Transformer | epoch 0 | step 12830 |avg loss 8.263 |avg tokens 4498.600 |tokens/s 30384.223 |walltime 1894.001 |
Transformer | epoch 0 | step 12840 |avg loss 8.324 |avg tokens 4525.400 |tokens/s 30577.179 |walltime 1895.481 |
Transformer | epoch 0 | step 12850 |avg loss 8.201 |avg tokens 4659.600 |tokens/s 30414.849 |walltime 1897.013 |
Transformer | epoch 0 | step 12860 |avg loss 8.303 |avg tokens 4546.800 |tokens/s 30166.677 |walltime 1898.520 |
Transformer | epoch 0 | step 12870 |avg loss 8.412 |avg tokens 4216.200 |tokens/s 30149.255 |walltime 1899.919 |
Transformer | epoch 0 | step 12880 |avg loss 8.273 |avg tokens 4532.800 |tokens/s 30603.003 |walltime 1901.400 |
Transformer | epoch 0 | step 12890 |avg loss 8.080 |avg tokens 4514.300 |tokens/s 30657.130 |walltime 1902.872 |
Transformer | epoch 0 | step 12900 |avg loss 8.106 |avg tokens 4762.000 |tokens/s 31728.814 |walltime 1904.373 |
Transformer | epoch 0 | step 12910 |avg loss 8.031 |avg tokens 4760.800 |tokens/s 32136.381 |walltime 1905.855 |
Transformer | epoch 0 | step 12920 |avg loss 8.523 |avg tokens 4062.300 |tokens/s 29060.914 |walltime 1907.252 |
Transformer | epoch 0 | step 12930 |avg loss 8.519 |avg tokens 4657.400 |tokens/s 31953.008 |walltime 1908.710 |
Transformer | epoch 0 | step 12940 |avg loss 8.259 |avg tokens 4633.600 |tokens/s 31242.885 |walltime 1910.193 |
Transformer | epoch 0 | step 12950 |avg loss 8.339 |avg tokens 4871.200 |tokens/s 32647.638 |walltime 1911.685 |
Transformer | epoch 0 | step 12960 |avg loss 8.336 |avg tokens 4586.400 |tokens/s 30195.054 |walltime 1913.204 |
Transformer | epoch 0 | step 12970 |avg loss 8.160 |avg tokens 4715.800 |tokens/s 31466.520 |walltime 1914.703 |
Transformer | epoch 0 | step 12980 |avg loss 8.343 |avg tokens 4449.400 |tokens/s 30085.391 |walltime 1916.182 |
Transformer | epoch 0 | step 12990 |avg loss 8.428 |avg tokens 4396.300 |tokens/s 29726.101 |walltime 1917.661 |
Transformer | epoch 0 | step 13000 |avg loss 8.292 |avg tokens 4827.600 |tokens/s 33396.826 |walltime 1919.106 |
Transformer | epoch 0 | step 13010 |avg loss 7.827 |avg tokens 4485.800 |tokens/s 30993.562 |walltime 1920.553 |
Transformer | epoch 0 | step 13020 |avg loss 8.170 |avg tokens 4319.900 |tokens/s 29146.331 |walltime 1922.036 |
Transformer | epoch 0 | step 13030 |avg loss 8.192 |avg tokens 4058.500 |tokens/s 28444.595 |walltime 1923.462 |
Transformer | epoch 0 | step 13040 |avg loss 8.455 |avg tokens 4791.500 |tokens/s 32539.675 |walltime 1924.935 |
Transformer | epoch 0 | step 13050 |avg loss 8.256 |avg tokens 4294.100 |tokens/s 29265.918 |walltime 1926.402 |
Transformer | epoch 0 | step 13060 |avg loss 8.450 |avg tokens 4597.800 |tokens/s 30990.163 |walltime 1927.886 |
Transformer | epoch 0 | step 13070 |avg loss 8.439 |avg tokens 4670.500 |tokens/s 31566.453 |walltime 1929.365 |
Transformer | epoch 0 | step 13080 |avg loss 8.408 |avg tokens 4119.900 |tokens/s 28672.210 |walltime 1930.802 |
Transformer | epoch 0 | step 13090 |avg loss 8.292 |avg tokens 4483.200 |tokens/s 30806.593 |walltime 1932.258 |
Transformer | epoch 0 | step 13100 |avg loss 8.129 |avg tokens 4271.900 |tokens/s 29952.959 |walltime 1933.684 |
Transformer | epoch 0 | step 13110 |avg loss 8.164 |avg tokens 4812.300 |tokens/s 32351.338 |walltime 1935.171 |
Transformer | epoch 0 | step 13120 |avg loss 8.648 |avg tokens 3831.600 |tokens/s 29116.604 |walltime 1936.487 |
Transformer | epoch 0 | step 13130 |avg loss 8.482 |avg tokens 4729.600 |tokens/s 32264.171 |walltime 1937.953 |
Transformer | epoch 0 | step 13140 |avg loss 8.361 |avg tokens 4187.400 |tokens/s 28992.880 |walltime 1939.397 |
Transformer | epoch 0 | step 13150 |avg loss 8.113 |avg tokens 4208.400 |tokens/s 29332.606 |walltime 1940.832 |
Transformer | epoch 0 | step 13160 |avg loss 8.182 |avg tokens 4578.100 |tokens/s 30798.094 |walltime 1942.319 |
Transformer | epoch 0 | step 13170 |avg loss 8.289 |avg tokens 4310.800 |tokens/s 29629.896 |walltime 1943.774 |
Transformer | epoch 0 | step 13180 |avg loss 8.390 |avg tokens 4549.600 |tokens/s 30733.723 |walltime 1945.254 |
Transformer | epoch 0 | step 13190 |avg loss 8.140 |avg tokens 4436.400 |tokens/s 30313.350 |walltime 1946.717 |
Transformer | epoch 0 | step 13200 |avg loss 8.629 |avg tokens 3979.200 |tokens/s 27001.359 |walltime 1948.191 |
Transformer | epoch 0 | step 13210 |avg loss 8.502 |avg tokens 4787.700 |tokens/s 32966.712 |walltime 1949.643 |
Transformer | epoch 0 | step 13220 |avg loss 8.127 |avg tokens 4473.300 |tokens/s 29767.787 |walltime 1951.146 |
Transformer | epoch 0 | step 13230 |avg loss 8.144 |avg tokens 4832.500 |tokens/s 32024.245 |walltime 1952.655 |
Transformer | epoch 0 | step 13240 |avg loss 8.410 |avg tokens 4653.200 |tokens/s 31432.033 |walltime 1954.136 |
Transformer | epoch 0 | step 13250 |avg loss 8.072 |avg tokens 4381.500 |tokens/s 29739.010 |walltime 1955.609 |
Transformer | epoch 0 | step 13260 |avg loss 8.051 |avg tokens 4992.000 |tokens/s 33238.918 |walltime 1957.111 |
Transformer | epoch 0 | step 13270 |avg loss 8.347 |avg tokens 4532.100 |tokens/s 31700.949 |walltime 1958.540 |
Transformer | epoch 0 | step 13280 |avg loss 8.302 |avg tokens 4657.300 |tokens/s 31612.950 |walltime 1960.014 |
Transformer | epoch 0 | step 13290 |avg loss 8.330 |avg tokens 4556.800 |tokens/s 30358.545 |walltime 1961.515 |
Transformer | epoch 0 | step 13300 |avg loss 8.147 |avg tokens 4834.700 |tokens/s 31947.604 |walltime 1963.028 |
Transformer | epoch 0 | step 13310 |avg loss 8.525 |avg tokens 4072.200 |tokens/s 28769.458 |walltime 1964.443 |
Transformer | epoch 0 | step 13320 |avg loss 8.195 |avg tokens 4315.600 |tokens/s 29641.249 |walltime 1965.899 |
Transformer | epoch 0 | step 13330 |avg loss 8.341 |avg tokens 4642.300 |tokens/s 31747.478 |walltime 1967.362 |
Transformer | epoch 0 | step 13340 |avg loss 8.294 |avg tokens 4308.000 |tokens/s 29457.926 |walltime 1968.824 |
Transformer | epoch 0 | step 13350 |avg loss 8.199 |avg tokens 4546.700 |tokens/s 32112.607 |walltime 1970.240 |
Transformer | epoch 0 | step 13360 |avg loss 8.307 |avg tokens 4709.400 |tokens/s 32268.747 |walltime 1971.699 |
Transformer | epoch 0 | step 13370 |avg loss 8.149 |avg tokens 4776.300 |tokens/s 32082.903 |walltime 1973.188 |
Transformer | epoch 0 | step 13380 |avg loss 8.561 |avg tokens 4482.500 |tokens/s 29771.529 |walltime 1974.694 |
Transformer | epoch 0 | step 13390 |avg loss 8.338 |avg tokens 4300.500 |tokens/s 29677.309 |walltime 1976.143 |
Transformer | epoch 0 | step 13400 |avg loss 8.419 |avg tokens 4778.200 |tokens/s 32928.245 |walltime 1977.594 |
Transformer | epoch 0 | step 13410 |avg loss 8.362 |avg tokens 4417.800 |tokens/s 30103.223 |walltime 1979.061 |
Transformer | epoch 0 | step 13420 |avg loss 8.044 |avg tokens 4591.200 |tokens/s 31106.383 |walltime 1980.537 |
Transformer | epoch 0 | step 13430 |avg loss 8.506 |avg tokens 3976.100 |tokens/s 25706.533 |walltime 1982.084 |
Transformer | epoch 0 | step 13440 |avg loss 8.151 |avg tokens 4646.800 |tokens/s 31701.000 |walltime 1983.550 |
Transformer | epoch 0 | step 13450 |avg loss 8.653 |avg tokens 4342.100 |tokens/s 31001.936 |walltime 1984.951 |
Transformer | epoch 0 | step 13460 |avg loss 8.119 |avg tokens 4710.900 |tokens/s 31358.888 |walltime 1986.453 |
Transformer | epoch 0 | step 13470 |avg loss 8.045 |avg tokens 4849.700 |tokens/s 32490.759 |walltime 1987.945 |
Transformer | epoch 0 | step 13480 |avg loss 8.224 |avg tokens 4687.300 |tokens/s 32202.067 |walltime 1989.401 |
Transformer | epoch 0 | step 13490 |avg loss 8.157 |avg tokens 4480.000 |tokens/s 30261.895 |walltime 1990.881 |
Transformer | epoch 0 | step 13500 |avg loss 8.331 |avg tokens 4168.500 |tokens/s 28933.037 |walltime 1992.322 |
Transformer | epoch 0 | step 13510 |avg loss 8.237 |avg tokens 4632.800 |tokens/s 31569.777 |walltime 1993.790 |
Transformer | epoch 0 | step 13520 |avg loss 8.288 |avg tokens 4336.300 |tokens/s 30426.979 |walltime 1995.215 |
Transformer | epoch 0 | step 13530 |avg loss 8.103 |avg tokens 4387.500 |tokens/s 30405.675 |walltime 1996.658 |
Transformer | epoch 0 | step 13540 |avg loss 8.705 |avg tokens 4262.500 |tokens/s 30241.479 |walltime 1998.067 |
Transformer | epoch 0 | step 13550 |avg loss 8.540 |avg tokens 3607.700 |tokens/s 26617.757 |walltime 1999.423 |
Transformer | epoch 0 | step 13560 |avg loss 8.233 |avg tokens 4685.100 |tokens/s 30995.733 |walltime 2000.934 |
Transformer | epoch 0 | step 13570 |avg loss 8.224 |avg tokens 4581.200 |tokens/s 31040.391 |walltime 2002.410 |
Transformer | epoch 0 | step 13580 |avg loss 8.213 |avg tokens 4563.100 |tokens/s 30801.370 |walltime 2003.892 |
Transformer | epoch 0 | step 13590 |avg loss 8.277 |avg tokens 4397.600 |tokens/s 30404.369 |walltime 2005.338 |
Transformer | epoch 0 | step 13600 |avg loss 8.529 |avg tokens 3968.700 |tokens/s 29205.991 |walltime 2006.697 |
Transformer | epoch 0 | step 13610 |avg loss 8.229 |avg tokens 4350.000 |tokens/s 30376.818 |walltime 2008.129 |
Transformer | epoch 0 | step 13620 |avg loss 8.237 |avg tokens 4590.300 |tokens/s 30977.948 |walltime 2009.611 |
Transformer | epoch 0 | step 13630 |avg loss 8.319 |avg tokens 4419.400 |tokens/s 30619.014 |walltime 2011.054 |
Transformer | epoch 0 | step 13640 |avg loss 8.347 |avg tokens 4622.800 |tokens/s 32454.098 |walltime 2012.478 |
Transformer | epoch 0 | step 13650 |avg loss 8.136 |avg tokens 4666.200 |tokens/s 31154.587 |walltime 2013.976 |
Transformer | epoch 0 | step 13660 |avg loss 8.073 |avg tokens 4416.900 |tokens/s 30515.442 |walltime 2015.424 |
Transformer | epoch 0 | step 13670 |avg loss 8.063 |avg tokens 4668.000 |tokens/s 31373.413 |walltime 2016.911 |
Transformer | epoch 0 | step 13680 |avg loss 8.455 |avg tokens 4594.700 |tokens/s 32449.656 |walltime 2018.327 |
Transformer | epoch 0 | step 13690 |avg loss 8.648 |avg tokens 4357.600 |tokens/s 30181.855 |walltime 2019.771 |
Transformer | epoch 0 | step 13700 |avg loss 8.045 |avg tokens 4835.800 |tokens/s 31660.244 |walltime 2021.299 |
Transformer | epoch 0 | step 13710 |avg loss 8.260 |avg tokens 4884.200 |tokens/s 31726.720 |walltime 2022.838 |
Transformer | epoch 0 | step 13720 |avg loss 8.176 |avg tokens 4631.200 |tokens/s 31130.860 |walltime 2024.326 |
Transformer | epoch 0 | step 13730 |avg loss 8.647 |avg tokens 4620.100 |tokens/s 32204.654 |walltime 2025.760 |
Transformer | epoch 0 | step 13740 |avg loss 8.462 |avg tokens 4798.000 |tokens/s 32613.621 |walltime 2027.232 |
Transformer | epoch 0 | step 13750 |avg loss 8.711 |avg tokens 4258.300 |tokens/s 30549.879 |walltime 2028.625 |
Transformer | epoch 0 | step 13760 |avg loss 8.136 |avg tokens 4431.500 |tokens/s 30675.945 |walltime 2030.070 |
Transformer | epoch 0 | step 13770 |avg loss 8.288 |avg tokens 4483.700 |tokens/s 30124.917 |walltime 2031.558 |
Transformer | epoch 0 | step 13780 |avg loss 8.197 |avg tokens 4733.800 |tokens/s 31676.760 |walltime 2033.053 |
Transformer | epoch 0 | step 13790 |avg loss 8.018 |avg tokens 4773.900 |tokens/s 31814.400 |walltime 2034.553 |
Transformer | epoch 0 | step 13800 |avg loss 8.160 |avg tokens 4649.800 |tokens/s 30708.777 |walltime 2036.068 |
Transformer | epoch 0 | step 13810 |avg loss 8.669 |avg tokens 4250.900 |tokens/s 30488.873 |walltime 2037.462 |
Transformer | epoch 0 | step 13820 |avg loss 8.733 |avg tokens 4518.900 |tokens/s 31892.153 |walltime 2038.879 |
Transformer | epoch 0 | step 13830 |avg loss 8.426 |avg tokens 4763.700 |tokens/s 32444.225 |walltime 2040.347 |
Transformer | epoch 0 | step 13840 |avg loss 8.410 |avg tokens 4453.500 |tokens/s 29961.414 |walltime 2041.833 |
Transformer | epoch 0 | step 13850 |avg loss 8.153 |avg tokens 4929.000 |tokens/s 31897.447 |walltime 2043.379 |
Transformer | epoch 0 | step 13860 |avg loss 8.317 |avg tokens 4743.500 |tokens/s 30975.480 |walltime 2044.910 |
Transformer | epoch 0 | step 13870 |avg loss 8.246 |avg tokens 4862.100 |tokens/s 31731.670 |walltime 2046.442 |
Transformer | epoch 0 | step 13880 |avg loss 8.306 |avg tokens 4073.200 |tokens/s 29130.970 |walltime 2047.841 |
Transformer | epoch 0 | step 13890 |avg loss 8.295 |avg tokens 4256.200 |tokens/s 28950.003 |walltime 2049.311 |
Transformer | epoch 0 | step 13900 |avg loss 8.336 |avg tokens 4504.700 |tokens/s 31241.851 |walltime 2050.753 |
Transformer | epoch 0 | step 13910 |avg loss 8.363 |avg tokens 4862.900 |tokens/s 33237.532 |walltime 2052.216 |
Transformer | epoch 0 | step 13920 |avg loss 8.111 |avg tokens 4477.000 |tokens/s 30713.384 |walltime 2053.673 |
Transformer | epoch 0 | step 13930 |avg loss 8.172 |avg tokens 4585.500 |tokens/s 31479.420 |walltime 2055.130 |
Transformer | epoch 0 | step 13940 |avg loss 8.108 |avg tokens 4721.200 |tokens/s 31377.708 |walltime 2056.635 |
Transformer | epoch 0 | step 13950 |avg loss 8.142 |avg tokens 4321.200 |tokens/s 29611.641 |walltime 2058.094 |
Transformer | epoch 0 | step 13960 |avg loss 8.270 |avg tokens 4461.500 |tokens/s 30231.270 |walltime 2059.570 |
Transformer | epoch 0 | step 13970 |avg loss 8.069 |avg tokens 4744.000 |tokens/s 31021.938 |walltime 2061.099 |
Transformer | epoch 0 | step 13980 |avg loss 8.124 |avg tokens 4888.100 |tokens/s 31910.058 |walltime 2062.631 |
Transformer | epoch 0 | step 13990 |avg loss 8.280 |avg tokens 4233.400 |tokens/s 28518.854 |walltime 2064.115 |
Transformer | epoch 0 | step 14000 |avg loss 8.254 |avg tokens 4190.200 |tokens/s 28621.781 |walltime 2065.579 |
Transformer | epoch 0 | step 14010 |avg loss 8.276 |avg tokens 4863.800 |tokens/s 31789.137 |walltime 2067.109 |
Transformer | epoch 0 | step 14020 |avg loss 8.543 |avg tokens 4451.500 |tokens/s 30903.968 |walltime 2068.550 |
Transformer | epoch 0 | step 14030 |avg loss 8.179 |avg tokens 4933.600 |tokens/s 32426.554 |walltime 2070.071 |
Transformer | epoch 0 | step 14040 |avg loss 8.251 |avg tokens 4477.900 |tokens/s 30652.517 |walltime 2071.532 |
Transformer | epoch 0 | step 14050 |avg loss 8.095 |avg tokens 4781.100 |tokens/s 31430.808 |walltime 2073.053 |
Transformer | epoch 0 | step 14060 |avg loss 8.280 |avg tokens 4725.600 |tokens/s 31542.655 |walltime 2074.551 |
Transformer | epoch 0 | step 14070 |avg loss 8.374 |avg tokens 4691.800 |tokens/s 31925.301 |walltime 2076.021 |
Transformer | epoch 0 | step 14080 |avg loss 8.009 |avg tokens 4675.900 |tokens/s 31181.261 |walltime 2077.521 |
Transformer | epoch 0 | step 14090 |avg loss 8.465 |avg tokens 4049.700 |tokens/s 28090.169 |walltime 2078.962 |
Transformer | epoch 0 | step 14100 |avg loss 8.741 |avg tokens 4112.900 |tokens/s 29373.413 |walltime 2080.362 |
Transformer | epoch 0 | step 14110 |avg loss 8.408 |avg tokens 4593.700 |tokens/s 31483.240 |walltime 2081.822 |
Transformer | epoch 0 | step 14120 |avg loss 8.510 |avg tokens 4485.800 |tokens/s 29353.444 |walltime 2083.350 |
Transformer | epoch 0 | step 14130 |avg loss 8.560 |avg tokens 4559.500 |tokens/s 31593.554 |walltime 2084.793 |
Transformer | epoch 0 | step 14140 |avg loss 8.046 |avg tokens 4659.200 |tokens/s 30417.923 |walltime 2086.325 |
Transformer | epoch 0 | step 14150 |avg loss 7.880 |avg tokens 4964.900 |tokens/s 33372.131 |walltime 2087.812 |
Transformer | epoch 0 | step 14160 |avg loss 8.043 |avg tokens 4171.900 |tokens/s 28469.356 |walltime 2089.278 |
Transformer | epoch 0 | step 14170 |avg loss 8.561 |avg tokens 4305.800 |tokens/s 29677.580 |walltime 2090.729 |
Transformer | epoch 0 | step 14180 |avg loss 8.189 |avg tokens 4644.100 |tokens/s 31012.567 |walltime 2092.226 |
Transformer | epoch 0 | step 14190 |avg loss 8.257 |avg tokens 4566.400 |tokens/s 30601.925 |walltime 2093.718 |
Transformer | epoch 0 | step 14200 |avg loss 8.357 |avg tokens 4142.600 |tokens/s 28299.697 |walltime 2095.182 |
Transformer | epoch 0 | step 14210 |avg loss 8.483 |avg tokens 4362.300 |tokens/s 30543.593 |walltime 2096.610 |
Transformer | epoch 0 | step 14220 |avg loss 8.443 |avg tokens 4100.100 |tokens/s 28349.450 |walltime 2098.057 |
Transformer | epoch 0 | step 14230 |avg loss 8.233 |avg tokens 4697.600 |tokens/s 31623.843 |walltime 2099.542 |
Transformer | epoch 0 | step 14240 |avg loss 8.066 |avg tokens 4424.600 |tokens/s 30286.423 |walltime 2101.003 |
Transformer | epoch 0 | step 14250 |avg loss 8.214 |avg tokens 4733.900 |tokens/s 30894.028 |walltime 2102.535 |
Transformer | epoch 0 | step 14260 |avg loss 8.064 |avg tokens 4628.800 |tokens/s 30877.488 |walltime 2104.035 |
Transformer | epoch 0 | step 14270 |avg loss 7.929 |avg tokens 4790.400 |tokens/s 31932.965 |walltime 2105.535 |
Transformer | epoch 0 | step 14280 |avg loss 8.273 |avg tokens 4243.500 |tokens/s 29544.589 |walltime 2106.971 |
Transformer | epoch 0 | step 14290 |avg loss 8.007 |avg tokens 4936.000 |tokens/s 32562.346 |walltime 2108.487 |
Transformer | epoch 0 | step 14300 |avg loss 8.266 |avg tokens 4504.500 |tokens/s 29827.763 |walltime 2109.997 |
Transformer | epoch 0 | step 14310 |avg loss 8.190 |avg tokens 4435.500 |tokens/s 30376.530 |walltime 2111.457 |
Transformer | epoch 0 | step 14320 |avg loss 8.198 |avg tokens 4907.500 |tokens/s 31906.886 |walltime 2112.995 |
Transformer | epoch 0 | step 14330 |avg loss 8.031 |avg tokens 4786.400 |tokens/s 31479.737 |walltime 2114.516 |
Transformer | epoch 0 | step 14340 |avg loss 8.188 |avg tokens 4509.600 |tokens/s 30868.974 |walltime 2115.977 |
Transformer | epoch 0 | step 14350 |avg loss 8.247 |avg tokens 4397.800 |tokens/s 31080.933 |walltime 2117.392 |
Transformer | epoch 0 | step 14360 |avg loss 8.147 |avg tokens 4636.700 |tokens/s 30952.492 |walltime 2118.890 |
Transformer | epoch 0 | step 14370 |avg loss 8.667 |avg tokens 4789.400 |tokens/s 33270.479 |walltime 2120.329 |
Transformer | epoch 0 | step 14380 |avg loss 8.246 |avg tokens 4741.000 |tokens/s 31893.643 |walltime 2121.816 |
Transformer | epoch 0 | step 14390 |avg loss 8.381 |avg tokens 4344.500 |tokens/s 30356.864 |walltime 2123.247 |
Transformer | epoch 0 | step 14400 |avg loss 8.202 |avg tokens 4273.300 |tokens/s 29177.419 |walltime 2124.711 |
Transformer | epoch 0 | step 14410 |avg loss 8.355 |avg tokens 3978.700 |tokens/s 27510.500 |walltime 2126.158 |
Transformer | epoch 0 | step 14420 |avg loss 8.211 |avg tokens 4690.700 |tokens/s 31920.364 |walltime 2127.627 |
Transformer | epoch 0 | step 14430 |avg loss 8.411 |avg tokens 4695.000 |tokens/s 33519.974 |walltime 2129.028 |
Transformer | epoch 0 | step 14440 |avg loss 8.391 |avg tokens 3559.200 |tokens/s 26056.138 |walltime 2130.394 |
Transformer | epoch 0 | step 14450 |avg loss 8.277 |avg tokens 4805.600 |tokens/s 31618.513 |walltime 2131.914 |
Transformer | epoch 0 | step 14460 |avg loss 8.399 |avg tokens 4652.300 |tokens/s 32169.125 |walltime 2133.360 |
Transformer | epoch 0 | step 14470 |avg loss 8.421 |avg tokens 4518.400 |tokens/s 31196.475 |walltime 2134.808 |
Transformer | epoch 0 | step 14480 |avg loss 8.266 |avg tokens 4549.600 |tokens/s 31200.013 |walltime 2136.266 |
Transformer | epoch 0 | step 14490 |avg loss 8.511 |avg tokens 4031.300 |tokens/s 27971.755 |walltime 2137.708 |
Transformer | epoch 0 | step 14500 |avg loss 8.472 |avg tokens 4489.900 |tokens/s 30947.108 |walltime 2139.158 |
Transformer | epoch 0 | step 14510 |avg loss 8.357 |avg tokens 4768.100 |tokens/s 31714.065 |walltime 2140.662 |
Transformer | epoch 0 | step 14520 |avg loss 8.603 |avg tokens 4473.900 |tokens/s 32210.434 |walltime 2142.051 |
Transformer | epoch 0 | step 14530 |avg loss 8.412 |avg tokens 4470.500 |tokens/s 29219.337 |walltime 2143.581 |
Transformer | epoch 0 | step 14540 |avg loss 8.328 |avg tokens 4442.000 |tokens/s 30292.622 |walltime 2145.047 |
Transformer | epoch 0 | step 14550 |avg loss 8.405 |avg tokens 4304.600 |tokens/s 29668.787 |walltime 2146.498 |
Transformer | epoch 0 | step 14560 |avg loss 8.402 |avg tokens 4463.500 |tokens/s 31042.496 |walltime 2147.936 |
Transformer | epoch 0 | step 14570 |avg loss 8.485 |avg tokens 4620.600 |tokens/s 31272.784 |walltime 2149.414 |
Transformer | epoch 0 | step 14580 |avg loss 8.259 |avg tokens 4748.400 |tokens/s 32235.704 |walltime 2150.887 |
Transformer | epoch 0 | step 14590 |avg loss 8.354 |avg tokens 4153.600 |tokens/s 30082.883 |walltime 2152.267 |
Transformer | epoch 0 | step 14600 |avg loss 8.410 |avg tokens 4630.700 |tokens/s 30881.651 |walltime 2153.767 |
Transformer | epoch 0 | step 14610 |avg loss 8.649 |avg tokens 4141.100 |tokens/s 29300.289 |walltime 2155.180 |
Transformer | epoch 0 | step 14620 |avg loss 8.391 |avg tokens 4907.400 |tokens/s 33657.546 |walltime 2156.638 |
Transformer | epoch 0 | step 14630 |avg loss 8.483 |avg tokens 4391.200 |tokens/s 29631.729 |walltime 2158.120 |
Transformer | epoch 0 | step 14640 |avg loss 8.177 |avg tokens 4649.500 |tokens/s 31410.854 |walltime 2159.600 |
Transformer | epoch 0 | step 14650 |avg loss 8.525 |avg tokens 4281.100 |tokens/s 30790.722 |walltime 2160.991 |
Transformer | epoch 0 | step 14660 |avg loss 8.402 |avg tokens 4459.600 |tokens/s 31433.034 |walltime 2162.409 |
Transformer | epoch 0 | step 14670 |avg loss 8.471 |avg tokens 4367.500 |tokens/s 30687.467 |walltime 2163.833 |
Transformer | epoch 0 | step 14680 |avg loss 8.319 |avg tokens 4331.600 |tokens/s 30019.638 |walltime 2165.276 |
Transformer | epoch 0 | step 14690 |avg loss 8.373 |avg tokens 4393.500 |tokens/s 30324.386 |walltime 2166.724 |
Transformer | epoch 0 | step 14700 |avg loss 8.366 |avg tokens 4359.100 |tokens/s 30446.661 |walltime 2168.156 |
Transformer | epoch 0 | step 14710 |avg loss 8.430 |avg tokens 4589.700 |tokens/s 31718.467 |walltime 2169.603 |
Transformer | epoch 0 | step 14720 |avg loss 8.477 |avg tokens 4428.000 |tokens/s 29989.588 |walltime 2171.080 |
Transformer | epoch 0 | step 14730 |avg loss 8.397 |avg tokens 4517.700 |tokens/s 31390.555 |walltime 2172.519 |
Transformer | epoch 0 | step 14740 |avg loss 8.422 |avg tokens 4786.500 |tokens/s 32466.373 |walltime 2173.993 |
Transformer | epoch 0 | step 14750 |avg loss 8.380 |avg tokens 4495.200 |tokens/s 31156.156 |walltime 2175.436 |
Transformer | epoch 0 | step 14760 |avg loss 8.120 |avg tokens 4681.700 |tokens/s 32910.994 |walltime 2176.859 |
Transformer | epoch 0 | step 14770 |avg loss 8.341 |avg tokens 4681.000 |tokens/s 32001.806 |walltime 2178.321 |
Transformer | epoch 0 | step 14780 |avg loss 8.575 |avg tokens 4117.200 |tokens/s 29049.362 |walltime 2179.739 |
Transformer | epoch 0 | step 14790 |avg loss 8.514 |avg tokens 3985.400 |tokens/s 28714.544 |walltime 2181.126 |
Transformer | epoch 0 | step 14800 |avg loss 8.383 |avg tokens 4890.900 |tokens/s 33108.293 |walltime 2182.604 |
Transformer | epoch 0 | step 14810 |avg loss 8.359 |avg tokens 4523.900 |tokens/s 31371.945 |walltime 2184.046 |
Transformer | epoch 0 | step 14820 |avg loss 8.269 |avg tokens 4404.000 |tokens/s 30078.539 |walltime 2185.510 |
Transformer | epoch 0 | step 14830 |avg loss 8.363 |avg tokens 4694.800 |tokens/s 31895.254 |walltime 2186.982 |
Transformer | epoch 0 | step 14840 |avg loss 8.136 |avg tokens 4570.000 |tokens/s 30865.067 |walltime 2188.463 |
Transformer | epoch 0 | step 14850 |avg loss 8.361 |avg tokens 4628.800 |tokens/s 31142.283 |walltime 2189.949 |
Transformer | epoch 0 | step 14860 |avg loss 8.166 |avg tokens 4560.000 |tokens/s 31203.536 |walltime 2191.410 |
Transformer | epoch 0 | step 14870 |avg loss 8.293 |avg tokens 4656.000 |tokens/s 31618.945 |walltime 2192.883 |
Transformer | epoch 0 | step 14880 |avg loss 8.180 |avg tokens 4490.600 |tokens/s 30117.205 |walltime 2194.374 |
Transformer | epoch 0 | step 14890 |avg loss 8.701 |avg tokens 4137.700 |tokens/s 29991.507 |walltime 2195.753 |
Transformer | epoch 0 | step 14900 |avg loss 8.376 |avg tokens 4939.200 |tokens/s 34045.690 |walltime 2197.204 |
Transformer | epoch 0 | step 14910 |avg loss 8.193 |avg tokens 4665.100 |tokens/s 30936.823 |walltime 2198.712 |
Transformer | epoch 0 | step 14920 |avg loss 8.476 |avg tokens 4317.100 |tokens/s 29623.546 |walltime 2200.169 |
Transformer | epoch 0 | step 14930 |avg loss 8.442 |avg tokens 4750.600 |tokens/s 32298.467 |walltime 2201.640 |
Transformer | epoch 0 | step 14940 |avg loss 8.283 |avg tokens 4550.100 |tokens/s 30481.279 |walltime 2203.133 |
Transformer | epoch 0 | step 14950 |avg loss 8.564 |avg tokens 4455.500 |tokens/s 30845.709 |walltime 2204.578 |
Transformer | epoch 0 | step 14960 |avg loss 8.667 |avg tokens 4373.200 |tokens/s 30784.441 |walltime 2205.998 |
Transformer | epoch 0 | step 14970 |avg loss 8.272 |avg tokens 4863.000 |tokens/s 32702.643 |walltime 2207.485 |
Transformer | epoch 0 | step 14980 |avg loss 8.328 |avg tokens 4631.300 |tokens/s 31321.656 |walltime 2208.964 |
Transformer | epoch 0 | step 14990 |avg loss 8.063 |avg tokens 4847.200 |tokens/s 31738.977 |walltime 2210.491 |
Transformer | epoch 0 | step 15000 |avg loss 8.350 |avg tokens 4246.200 |tokens/s 29667.544 |walltime 2211.922 |
Transformer | epoch 0 | step 15010 |avg loss 8.513 |avg tokens 4037.700 |tokens/s 28876.961 |walltime 2213.321 |
Transformer | epoch 0 | step 15020 |avg loss 8.150 |avg tokens 4879.100 |tokens/s 31801.571 |walltime 2214.855 |
Transformer | epoch 0 | step 15030 |avg loss 8.437 |avg tokens 4334.900 |tokens/s 30747.074 |walltime 2216.265 |
Transformer | epoch 0 | step 15040 |avg loss 8.085 |avg tokens 4773.600 |tokens/s 32587.368 |walltime 2217.729 |
Transformer | epoch 0 | step 15050 |avg loss 8.472 |avg tokens 4724.100 |tokens/s 32111.022 |walltime 2219.201 |
Transformer | epoch 0 | step 15060 |avg loss 8.262 |avg tokens 4574.700 |tokens/s 31250.531 |walltime 2220.665 |
Transformer | epoch 0 | step 15070 |avg loss 8.267 |avg tokens 3989.400 |tokens/s 28387.148 |walltime 2222.070 |
Transformer | epoch 0 | step 15080 |avg loss 8.190 |avg tokens 4440.500 |tokens/s 31337.869 |walltime 2223.487 |
Transformer | epoch 0 | step 15090 |avg loss 8.271 |avg tokens 4797.300 |tokens/s 32200.601 |walltime 2224.977 |
Transformer | epoch 0 | step 15100 |avg loss 8.270 |avg tokens 4801.600 |tokens/s 32147.217 |walltime 2226.470 |
Transformer | epoch 0 | step 15110 |avg loss 8.623 |avg tokens 4407.300 |tokens/s 30552.724 |walltime 2227.913 |
Transformer | epoch 0 | step 15120 |avg loss 8.384 |avg tokens 4652.000 |tokens/s 32138.159 |walltime 2229.360 |
Transformer | epoch 0 | step 15130 |avg loss 8.030 |avg tokens 4776.000 |tokens/s 31630.613 |walltime 2230.870 |
Transformer | epoch 0 | step 15140 |avg loss 8.490 |avg tokens 4246.700 |tokens/s 29578.087 |walltime 2232.306 |
Transformer | epoch 0 | step 15150 |avg loss 8.251 |avg tokens 4407.600 |tokens/s 29905.780 |walltime 2233.780 |
Transformer | epoch 0 | step 15160 |avg loss 8.298 |avg tokens 4630.400 |tokens/s 30926.305 |walltime 2235.277 |
Transformer | epoch 0 | step 15170 |avg loss 8.438 |avg tokens 4393.200 |tokens/s 30862.062 |walltime 2236.701 |
Transformer | epoch 0 | step 15180 |avg loss 8.386 |avg tokens 4612.300 |tokens/s 31391.785 |walltime 2238.170 |
Transformer | epoch 0 | step 15190 |avg loss 8.516 |avg tokens 4737.500 |tokens/s 32856.535 |walltime 2239.612 |
Transformer | epoch 0 | step 15200 |avg loss 8.222 |avg tokens 4569.800 |tokens/s 31178.740 |walltime 2241.077 |
Transformer | epoch 0 | step 15210 |avg loss 8.248 |avg tokens 4984.000 |tokens/s 33484.507 |walltime 2242.566 |
Transformer | epoch 0 | step 15220 |avg loss 8.340 |avg tokens 4530.600 |tokens/s 31429.517 |walltime 2244.007 |
Transformer | epoch 0 | step 15230 |avg loss 8.563 |avg tokens 2894.900 |tokens/s 22289.167 |walltime 2245.306 |
Transformer | epoch 0 | step 15240 |avg loss 8.210 |avg tokens 4064.900 |tokens/s 28451.868 |walltime 2246.735 |
Transformer | epoch 0 | step 15250 |avg loss 8.393 |avg tokens 4310.700 |tokens/s 30173.902 |walltime 2248.164 |
Transformer | epoch 0 | step 15260 |avg loss 8.335 |avg tokens 4663.200 |tokens/s 31184.774 |walltime 2249.659 |
Transformer | epoch 0 | step 15270 |avg loss 8.421 |avg tokens 4582.700 |tokens/s 32259.426 |walltime 2251.079 |
Transformer | epoch 0 | step 15280 |avg loss 8.347 |avg tokens 4307.400 |tokens/s 29642.541 |walltime 2252.533 |
Transformer | epoch 0 | step 15290 |avg loss 8.320 |avg tokens 4550.000 |tokens/s 31269.353 |walltime 2253.988 |
Transformer | epoch 0 | step 15300 |avg loss 8.654 |avg tokens 4632.100 |tokens/s 32608.956 |walltime 2255.408 |
Transformer | epoch 0 | step 15310 |avg loss 8.129 |avg tokens 4852.000 |tokens/s 32467.289 |walltime 2256.903 |
Transformer | epoch 0 | step 15320 |avg loss 8.270 |avg tokens 4638.600 |tokens/s 31529.962 |walltime 2258.374 |
Transformer | epoch 0 | step 15330 |avg loss 8.369 |avg tokens 4807.000 |tokens/s 32740.400 |walltime 2259.842 |
Transformer | epoch 0 | step 15340 |avg loss 8.252 |avg tokens 4563.700 |tokens/s 31530.957 |walltime 2261.289 |
Transformer | epoch 0 | step 15350 |avg loss 8.194 |avg tokens 4398.600 |tokens/s 30157.201 |walltime 2262.748 |
Transformer | epoch 0 | step 15360 |avg loss 8.380 |avg tokens 4867.500 |tokens/s 33077.341 |walltime 2264.219 |
Transformer | epoch 0 | step 15370 |avg loss 8.342 |avg tokens 4246.800 |tokens/s 29283.344 |walltime 2265.670 |
Transformer | epoch 0 | step 15380 |avg loss 8.358 |avg tokens 4377.300 |tokens/s 30412.513 |walltime 2267.109 |
Transformer | epoch 0 | step 15390 |avg loss 8.385 |avg tokens 4378.600 |tokens/s 29955.913 |walltime 2268.571 |
Transformer | epoch 0 | step 15400 |avg loss 8.387 |avg tokens 3998.200 |tokens/s 28542.301 |walltime 2269.972 |
Transformer | epoch 0 | step 15410 |avg loss 8.504 |avg tokens 4322.300 |tokens/s 29846.257 |walltime 2271.420 |
Transformer | epoch 0 | step 15420 |avg loss 8.241 |avg tokens 4712.000 |tokens/s 31541.923 |walltime 2272.914 |
Transformer | epoch 0 | step 15430 |avg loss 8.365 |avg tokens 4665.900 |tokens/s 31978.339 |walltime 2274.373 |
Transformer | epoch 0 | step 15440 |avg loss 8.311 |avg tokens 4556.600 |tokens/s 32005.148 |walltime 2275.796 |
Transformer | epoch 0 | step 15450 |avg loss 8.412 |avg tokens 4639.000 |tokens/s 31200.893 |walltime 2277.283 |
Transformer | epoch 0 | step 15460 |avg loss 8.144 |avg tokens 4563.700 |tokens/s 31147.700 |walltime 2278.748 |
Transformer | epoch 0 | step 15470 |avg loss 8.164 |avg tokens 4744.000 |tokens/s 32002.016 |walltime 2280.231 |
Transformer | epoch 0 | step 15480 |avg loss 8.436 |avg tokens 4801.300 |tokens/s 32633.328 |walltime 2281.702 |
Transformer | epoch 0 | step 15490 |avg loss 8.539 |avg tokens 4111.100 |tokens/s 29552.443 |walltime 2283.093 |
Transformer | epoch 0 | step 15500 |avg loss 8.314 |avg tokens 4413.800 |tokens/s 30547.230 |walltime 2284.538 |
Transformer | epoch 0 | step 15510 |avg loss 8.513 |avg tokens 3881.100 |tokens/s 27856.265 |walltime 2285.931 |
Transformer | epoch 0 | step 15520 |avg loss 8.504 |avg tokens 4735.100 |tokens/s 32689.024 |walltime 2287.380 |
Transformer | epoch 0 | step 15530 |avg loss 8.402 |avg tokens 4067.900 |tokens/s 28737.255 |walltime 2288.795 |
Transformer | epoch 0 | step 15540 |avg loss 8.278 |avg tokens 4897.600 |tokens/s 33185.371 |walltime 2290.271 |
Transformer | epoch 0 | step 15550 |avg loss 8.465 |avg tokens 4574.800 |tokens/s 30679.164 |walltime 2291.763 |
Transformer | epoch 0 | step 15560 |avg loss 8.353 |avg tokens 4689.800 |tokens/s 32241.756 |walltime 2293.217 |
Transformer | epoch 0 | step 15570 |avg loss 8.494 |avg tokens 4498.700 |tokens/s 30786.960 |walltime 2294.678 |
Transformer | epoch 0 | step 15580 |avg loss 8.478 |avg tokens 3921.700 |tokens/s 26923.148 |walltime 2296.135 |
Transformer | epoch 0 | step 15590 |avg loss 8.831 |avg tokens 4612.500 |tokens/s 32287.677 |walltime 2297.564 |
Transformer | epoch 0 | step 15600 |avg loss 8.389 |avg tokens 4582.800 |tokens/s 30776.420 |walltime 2299.053 |
Transformer | epoch 0 | step 15610 |avg loss 8.190 |avg tokens 4835.800 |tokens/s 30905.871 |walltime 2300.617 |
Transformer | epoch 0 | step 15620 |avg loss 8.103 |avg tokens 4486.600 |tokens/s 30020.562 |walltime 2302.112 |
Transformer | epoch 0 | step 15630 |avg loss 8.318 |avg tokens 4562.400 |tokens/s 30546.007 |walltime 2303.605 |
Transformer | epoch 0 | step 15640 |avg loss 8.405 |avg tokens 4657.400 |tokens/s 31103.220 |walltime 2305.103 |
Transformer | epoch 0 | step 15650 |avg loss 8.418 |avg tokens 4610.000 |tokens/s 30604.929 |walltime 2306.609 |
Transformer | epoch 0 | step 15660 |avg loss 8.261 |avg tokens 4366.600 |tokens/s 29696.780 |walltime 2308.080 |
Transformer | epoch 0 | step 15670 |avg loss 8.447 |avg tokens 4660.600 |tokens/s 31837.484 |walltime 2309.543 |
Transformer | epoch 0 | step 15680 |avg loss 8.259 |avg tokens 4645.300 |tokens/s 30838.952 |walltime 2311.050 |
Transformer | epoch 0 | step 15690 |avg loss 8.379 |avg tokens 4223.300 |tokens/s 28950.156 |walltime 2312.509 |
Transformer | epoch 0 | step 15700 |avg loss 8.326 |avg tokens 4584.600 |tokens/s 32239.746 |walltime 2313.931 |
Transformer | epoch 0 | step 15710 |avg loss 8.492 |avg tokens 4802.200 |tokens/s 32202.081 |walltime 2315.422 |
Transformer | epoch 0 | step 15720 |avg loss 8.304 |avg tokens 4883.300 |tokens/s 31897.798 |walltime 2316.953 |
Transformer | epoch 0 | step 15730 |avg loss 8.525 |avg tokens 4395.300 |tokens/s 31072.228 |walltime 2318.367 |
Transformer | epoch 0 | step 15740 |avg loss 8.021 |avg tokens 4818.600 |tokens/s 32154.425 |walltime 2319.866 |
Transformer | epoch 0 | step 15750 |avg loss 8.413 |avg tokens 4760.500 |tokens/s 32664.006 |walltime 2321.323 |
Transformer | epoch 0 | step 15760 |avg loss 8.497 |avg tokens 4121.400 |tokens/s 28639.477 |walltime 2322.762 |
Transformer | epoch 0 | step 15770 |avg loss 8.384 |avg tokens 4797.100 |tokens/s 32262.516 |walltime 2324.249 |
Transformer | epoch 0 | step 15780 |avg loss 8.131 |avg tokens 4738.600 |tokens/s 31034.784 |walltime 2325.776 |
Transformer | epoch 0 | step 15790 |avg loss 8.519 |avg tokens 4289.300 |tokens/s 30664.770 |walltime 2327.175 |
Transformer | epoch 0 | step 15800 |avg loss 8.618 |avg tokens 4395.200 |tokens/s 31323.539 |walltime 2328.578 |
Transformer | epoch 0 | step 15810 |avg loss 8.334 |avg tokens 4749.900 |tokens/s 32317.889 |walltime 2330.048 |
Transformer | epoch 0 | step 15820 |avg loss 8.408 |avg tokens 4663.400 |tokens/s 32357.677 |walltime 2331.489 |
Transformer | epoch 0 | step 15830 |avg loss 8.039 |avg tokens 4840.900 |tokens/s 32430.659 |walltime 2332.982 |
Transformer | epoch 0 | step 15840 |avg loss 8.283 |avg tokens 4474.100 |tokens/s 30968.466 |walltime 2334.426 |
Transformer | epoch 0 | step 15850 |avg loss 8.479 |avg tokens 4702.900 |tokens/s 31546.767 |walltime 2335.917 |
Transformer | epoch 0 | step 15860 |avg loss 8.695 |avg tokens 4082.700 |tokens/s 29704.772 |walltime 2337.292 |
Transformer | epoch 0 | step 15870 |avg loss 7.901 |avg tokens 4550.800 |tokens/s 31065.025 |walltime 2338.757 |
Transformer | epoch 0 | step 15880 |avg loss 8.405 |avg tokens 4373.000 |tokens/s 30197.070 |walltime 2340.205 |
Transformer | epoch 0 | step 15890 |avg loss 8.317 |avg tokens 4464.200 |tokens/s 30313.352 |walltime 2341.677 |
Transformer | epoch 0 | step 15900 |avg loss 8.074 |avg tokens 4450.100 |tokens/s 30333.025 |walltime 2343.145 |
Transformer | epoch 0 | step 15910 |avg loss 8.596 |avg tokens 4624.800 |tokens/s 32473.319 |walltime 2344.569 |
Transformer | epoch 0 | step 15920 |avg loss 8.439 |avg tokens 4876.000 |tokens/s 32495.381 |walltime 2346.069 |
Transformer | epoch 0 | step 15930 |avg loss 8.521 |avg tokens 4642.300 |tokens/s 32066.610 |walltime 2347.517 |
Transformer | epoch 0 | step 15940 |avg loss 8.537 |avg tokens 4797.600 |tokens/s 33682.287 |walltime 2348.941 |
Transformer | epoch 0 | step 15950 |avg loss 8.376 |avg tokens 4310.000 |tokens/s 30001.263 |walltime 2350.378 |
Transformer | epoch 0 | step 15960 |avg loss 8.165 |avg tokens 4732.800 |tokens/s 32281.023 |walltime 2351.844 |
Transformer | epoch 0 | step 15970 |avg loss 8.192 |avg tokens 4423.900 |tokens/s 29994.789 |walltime 2353.319 |
Transformer | epoch 0 | step 15980 |avg loss 8.578 |avg tokens 4613.000 |tokens/s 32578.158 |walltime 2354.735 |
Transformer | epoch 0 | step 15990 |avg loss 8.196 |avg tokens 4711.000 |tokens/s 31827.379 |walltime 2356.215 |
Transformer | epoch 0 | step 16000 |avg loss 8.120 |avg tokens 4673.900 |tokens/s 31713.342 |walltime 2357.689 |
Transformer | epoch 0 | step 16010 |avg loss 8.335 |avg tokens 4759.500 |tokens/s 31280.549 |walltime 2359.210 |
Transformer | epoch 0 | step 16020 |avg loss 8.092 |avg tokens 4666.200 |tokens/s 31366.049 |walltime 2360.698 |
Transformer | epoch 0 | step 16030 |avg loss 8.349 |avg tokens 4540.300 |tokens/s 31557.735 |walltime 2362.137 |
Transformer | epoch 0 | step 16040 |avg loss 8.373 |avg tokens 4872.700 |tokens/s 32901.891 |walltime 2363.618 |
Transformer | epoch 0 | step 16050 |avg loss 8.398 |avg tokens 4461.900 |tokens/s 30874.845 |walltime 2365.063 |
Transformer | epoch 0 | step 16060 |avg loss 8.499 |avg tokens 4407.700 |tokens/s 31089.279 |walltime 2366.481 |
Transformer | epoch 0 | step 16070 |avg loss 8.507 |avg tokens 3808.900 |tokens/s 27934.426 |walltime 2367.844 |
Transformer | epoch 0 | step 16080 |avg loss 8.907 |avg tokens 4129.400 |tokens/s 30443.886 |walltime 2369.201 |
Transformer | epoch 0 | step 16090 |avg loss 8.355 |avg tokens 4354.900 |tokens/s 30332.260 |walltime 2370.636 |
Transformer | epoch 0 | step 16100 |avg loss 8.279 |avg tokens 4537.700 |tokens/s 30389.935 |walltime 2372.130 |
Transformer | epoch 0 | step 16110 |avg loss 8.365 |avg tokens 4651.600 |tokens/s 31636.025 |walltime 2373.600 |
Transformer | epoch 0 | step 16120 |avg loss 8.241 |avg tokens 4422.300 |tokens/s 30679.222 |walltime 2375.041 |
Transformer | epoch 0 | step 16130 |avg loss 8.182 |avg tokens 4170.100 |tokens/s 28793.364 |walltime 2376.490 |
Transformer | epoch 0 | step 16140 |avg loss 8.270 |avg tokens 4750.900 |tokens/s 32677.943 |walltime 2377.944 |
Transformer | epoch 0 | step 16150 |avg loss 8.303 |avg tokens 4890.200 |tokens/s 33978.624 |walltime 2379.383 |
Transformer | epoch 0 | step 16160 |avg loss 8.279 |avg tokens 4200.400 |tokens/s 29122.845 |walltime 2380.825 |
Transformer | epoch 0 | step 16170 |avg loss 8.188 |avg tokens 4660.000 |tokens/s 31740.494 |walltime 2382.293 |
Transformer | epoch 0 | step 16180 |avg loss 8.330 |avg tokens 4450.900 |tokens/s 30344.302 |walltime 2383.760 |
Transformer | epoch 0 | step 16190 |avg loss 8.369 |avg tokens 4723.500 |tokens/s 31484.355 |walltime 2385.260 |
Transformer | epoch 0 | step 16200 |avg loss 8.269 |avg tokens 4472.200 |tokens/s 30700.390 |walltime 2386.717 |
Transformer | epoch 0 | step 16210 |avg loss 8.420 |avg tokens 4315.600 |tokens/s 29946.700 |walltime 2388.158 |
Transformer | epoch 0 | step 16220 |avg loss 8.469 |avg tokens 4254.800 |tokens/s 30216.693 |walltime 2389.566 |
Transformer | epoch 0 | step 16230 |avg loss 8.492 |avg tokens 3805.300 |tokens/s 28228.019 |walltime 2390.914 |
Transformer | epoch 0 | step 16240 |avg loss 8.551 |avg tokens 4480.900 |tokens/s 31427.940 |walltime 2392.340 |
Transformer | epoch 0 | step 16250 |avg loss 8.469 |avg tokens 4242.400 |tokens/s 29538.754 |walltime 2393.776 |
Transformer | epoch 0 | step 16260 |avg loss 8.205 |avg tokens 4310.800 |tokens/s 29595.772 |walltime 2395.233 |
Transformer | epoch 0 | step 16270 |avg loss 8.303 |avg tokens 4273.600 |tokens/s 30094.000 |walltime 2396.653 |
Transformer | epoch 0 | step 16280 |avg loss 8.496 |avg tokens 4316.000 |tokens/s 30509.006 |walltime 2398.068 |
Transformer | epoch 0 | step 16290 |avg loss 8.761 |avg tokens 4269.100 |tokens/s 31512.400 |walltime 2399.422 |
Transformer | epoch 0 | step 16300 |avg loss 8.474 |avg tokens 4833.700 |tokens/s 32956.284 |walltime 2400.889 |
Transformer | epoch 0 | step 16310 |avg loss 8.263 |avg tokens 4650.900 |tokens/s 31431.410 |walltime 2402.369 |
Transformer | epoch 0 | step 16320 |avg loss 8.518 |avg tokens 3866.000 |tokens/s 28479.470 |walltime 2403.726 |
Transformer | epoch 0 | step 16330 |avg loss 8.399 |avg tokens 4466.600 |tokens/s 30718.141 |walltime 2405.180 |
Transformer | epoch 0 | step 16340 |avg loss 8.171 |avg tokens 4556.400 |tokens/s 30387.693 |walltime 2406.680 |
Transformer | epoch 0 | step 16350 |avg loss 8.240 |avg tokens 4370.500 |tokens/s 29361.118 |walltime 2408.168 |
Transformer | epoch 0 | step 16360 |avg loss 8.365 |avg tokens 4304.600 |tokens/s 30066.250 |walltime 2409.600 |
Transformer | epoch 0 | step 16370 |avg loss 8.430 |avg tokens 4518.600 |tokens/s 31687.754 |walltime 2411.026 |
Transformer | epoch 0 | step 16380 |avg loss 8.132 |avg tokens 4908.800 |tokens/s 32710.219 |walltime 2412.527 |
Transformer | epoch 0 | step 16390 |avg loss 8.194 |avg tokens 4445.600 |tokens/s 30045.412 |walltime 2414.006 |
Transformer | epoch 0 | step 16400 |avg loss 8.253 |avg tokens 4639.300 |tokens/s 31418.208 |walltime 2415.483 |
Transformer | epoch 0 | step 16410 |avg loss 8.016 |avg tokens 4775.200 |tokens/s 31841.297 |walltime 2416.983 |
Transformer | epoch 0 | step 16420 |avg loss 8.016 |avg tokens 4597.000 |tokens/s 31194.557 |walltime 2418.456 |
Transformer | epoch 0 | step 16430 |avg loss 8.456 |avg tokens 4864.800 |tokens/s 33221.155 |walltime 2419.921 |
Transformer | epoch 0 | step 16440 |avg loss 8.350 |avg tokens 4195.900 |tokens/s 29239.756 |walltime 2421.356 |
Transformer | epoch 0 | step 16450 |avg loss 8.505 |avg tokens 4718.200 |tokens/s 32837.538 |walltime 2422.792 |
Transformer | epoch 0 | step 16460 |avg loss 8.360 |avg tokens 4366.400 |tokens/s 31046.850 |walltime 2424.199 |
Transformer | epoch 0 | step 16470 |avg loss 7.980 |avg tokens 4718.100 |tokens/s 31384.835 |walltime 2425.702 |
Transformer | epoch 0 | step 16480 |avg loss 7.931 |avg tokens 4673.100 |tokens/s 30372.139 |walltime 2427.241 |
Transformer | epoch 0 | step 16490 |avg loss 8.054 |avg tokens 4537.200 |tokens/s 30040.283 |walltime 2428.751 |
Transformer | epoch 0 | step 16500 |avg loss 8.405 |avg tokens 4680.800 |tokens/s 31268.178 |walltime 2430.248 |
Transformer | epoch 0 | step 16510 |avg loss 8.392 |avg tokens 4696.800 |tokens/s 32277.669 |walltime 2431.703 |
Transformer | epoch 0 | step 16520 |avg loss 8.179 |avg tokens 4785.600 |tokens/s 31199.908 |walltime 2433.237 |
Transformer | epoch 0 | step 16530 |avg loss 8.440 |avg tokens 4488.900 |tokens/s 30356.162 |walltime 2434.716 |
Transformer | epoch 0 | step 16540 |avg loss 8.265 |avg tokens 4619.300 |tokens/s 31040.760 |walltime 2436.204 |
Transformer | epoch 0 | step 16550 |avg loss 8.240 |avg tokens 4830.800 |tokens/s 32468.094 |walltime 2437.692 |
Transformer | epoch 0 | step 16560 |avg loss 8.300 |avg tokens 4612.500 |tokens/s 31955.658 |walltime 2439.135 |
Transformer | epoch 0 | step 16570 |avg loss 8.267 |avg tokens 4845.400 |tokens/s 31903.023 |walltime 2440.654 |
Transformer | epoch 0 | step 16580 |avg loss 8.259 |avg tokens 4567.000 |tokens/s 30343.936 |walltime 2442.159 |
Transformer | epoch 0 | step 16590 |avg loss 8.421 |avg tokens 4651.600 |tokens/s 31831.554 |walltime 2443.620 |
Transformer | epoch 0 | step 16600 |avg loss 8.275 |avg tokens 4526.400 |tokens/s 31494.002 |walltime 2445.058 |
Transformer | epoch 0 | step 16610 |avg loss 7.987 |avg tokens 4751.600 |tokens/s 31917.026 |walltime 2446.546 |
Transformer | epoch 0 | step 16620 |avg loss 8.416 |avg tokens 4612.100 |tokens/s 31963.948 |walltime 2447.989 |
Transformer | epoch 0 | step 16630 |avg loss 8.132 |avg tokens 4970.400 |tokens/s 32775.795 |walltime 2449.506 |
Transformer | epoch 0 | step 16640 |avg loss 8.386 |avg tokens 4002.100 |tokens/s 28702.993 |walltime 2450.900 |
Transformer | epoch 0 | step 16650 |avg loss 8.422 |avg tokens 4492.500 |tokens/s 30783.245 |walltime 2452.359 |
Transformer | epoch 0 | step 16660 |avg loss 7.959 |avg tokens 4399.900 |tokens/s 30470.126 |walltime 2453.804 |
Transformer | epoch 0 | step 16670 |avg loss 8.395 |avg tokens 4737.400 |tokens/s 31894.558 |walltime 2455.289 |
Transformer | epoch 0 | step 16680 |avg loss 8.245 |avg tokens 4963.300 |tokens/s 32741.428 |walltime 2456.805 |
Transformer | epoch 0 | step 16690 |avg loss 8.344 |avg tokens 4555.200 |tokens/s 30728.930 |walltime 2458.287 |
Transformer | epoch 0 | step 16700 |avg loss 8.290 |avg tokens 4387.500 |tokens/s 30199.435 |walltime 2459.740 |
Transformer | epoch 0 | step 16710 |avg loss 8.445 |avg tokens 4340.400 |tokens/s 30431.724 |walltime 2461.166 |
Transformer | epoch 0 | step 16720 |avg loss 8.247 |avg tokens 4762.000 |tokens/s 31595.605 |walltime 2462.673 |
Transformer | epoch 0 | step 16730 |avg loss 8.179 |avg tokens 4488.700 |tokens/s 30913.985 |walltime 2464.125 |
Transformer | epoch 0 | step 16740 |avg loss 7.987 |avg tokens 4752.800 |tokens/s 31196.851 |walltime 2465.649 |
Transformer | epoch 0 | step 16750 |avg loss 8.243 |avg tokens 4768.000 |tokens/s 31959.258 |walltime 2467.141 |
Transformer | epoch 0 | step 16760 |avg loss 8.333 |avg tokens 4500.000 |tokens/s 30234.070 |walltime 2468.629 |
Transformer | epoch 0 | step 16770 |avg loss 8.384 |avg tokens 4295.300 |tokens/s 29733.802 |walltime 2470.074 |
Transformer | epoch 0 | step 16780 |avg loss 8.671 |avg tokens 4659.200 |tokens/s 32493.878 |walltime 2471.508 |
Transformer | epoch 0 | step 16790 |avg loss 8.032 |avg tokens 4591.300 |tokens/s 31174.782 |walltime 2472.980 |
Transformer | epoch 0 | step 16800 |avg loss 8.458 |avg tokens 4405.900 |tokens/s 29954.652 |walltime 2474.451 |
Transformer | epoch 0 | step 16810 |avg loss 8.487 |avg tokens 4789.300 |tokens/s 31471.835 |walltime 2475.973 |
Transformer | epoch 0 | step 16820 |avg loss 8.319 |avg tokens 4324.100 |tokens/s 30121.352 |walltime 2477.409 |
Transformer | epoch 0 | step 16830 |avg loss 8.321 |avg tokens 4842.500 |tokens/s 32925.830 |walltime 2478.879 |
Transformer | epoch 0 | step 16840 |avg loss 8.420 |avg tokens 4958.500 |tokens/s 32391.129 |walltime 2480.410 |
Transformer | epoch 0 | step 16850 |avg loss 8.485 |avg tokens 4447.700 |tokens/s 30222.581 |walltime 2481.882 |
Transformer | epoch 0 | step 16860 |avg loss 8.632 |avg tokens 4481.300 |tokens/s 31034.446 |walltime 2483.326 |
Transformer | epoch 0 | step 16870 |avg loss 8.494 |avg tokens 4015.500 |tokens/s 27997.940 |walltime 2484.760 |
Transformer | epoch 0 | step 16880 |avg loss 8.444 |avg tokens 4505.700 |tokens/s 30636.744 |walltime 2486.231 |
Transformer | epoch 0 | step 16890 |avg loss 8.394 |avg tokens 4629.400 |tokens/s 31036.257 |walltime 2487.722 |
Transformer | epoch 0 | step 16900 |avg loss 8.184 |avg tokens 4918.400 |tokens/s 32199.532 |walltime 2489.250 |
Transformer | epoch 0 | step 16910 |avg loss 8.366 |avg tokens 4943.900 |tokens/s 32626.018 |walltime 2490.765 |
Transformer | epoch 0 | step 16920 |avg loss 8.423 |avg tokens 4720.800 |tokens/s 31465.173 |walltime 2492.265 |
Transformer | epoch 0 | step 16930 |avg loss 8.329 |avg tokens 4456.500 |tokens/s 31296.856 |walltime 2493.689 |
Transformer | epoch 0 | step 16940 |avg loss 8.391 |avg tokens 4448.600 |tokens/s 30079.584 |walltime 2495.168 |
Transformer | epoch 0 | step 16950 |avg loss 8.279 |avg tokens 4624.000 |tokens/s 30921.720 |walltime 2496.664 |
Transformer | epoch 0 | step 16960 |avg loss 8.404 |avg tokens 4623.900 |tokens/s 31610.693 |walltime 2498.127 |
Transformer | epoch 0 | step 16970 |avg loss 8.151 |avg tokens 4905.200 |tokens/s 32793.713 |walltime 2499.622 |
Transformer | epoch 0 | step 16980 |avg loss 8.366 |avg tokens 4673.200 |tokens/s 31963.632 |walltime 2501.084 |
Transformer | epoch 0 | step 16990 |avg loss 8.497 |avg tokens 4270.500 |tokens/s 29713.725 |walltime 2502.522 |
Transformer | epoch 0 | step 17000 |avg loss 8.443 |avg tokens 4662.300 |tokens/s 31584.581 |walltime 2503.998 |
Transformer | epoch 0 | step 17010 |avg loss 8.496 |avg tokens 4017.600 |tokens/s 29387.899 |walltime 2505.365 |
Transformer | epoch 0 | step 17020 |avg loss 8.385 |avg tokens 3739.400 |tokens/s 26930.639 |walltime 2506.753 |
Transformer | epoch 0 | step 17030 |avg loss 8.296 |avg tokens 4865.600 |tokens/s 32688.169 |walltime 2508.242 |
Transformer | epoch 0 | step 17040 |avg loss 8.230 |avg tokens 4206.800 |tokens/s 29690.655 |walltime 2509.659 |
Transformer | epoch 0 | step 17050 |avg loss 8.397 |avg tokens 4438.000 |tokens/s 29944.335 |walltime 2511.141 |
Transformer | epoch 0 | step 17060 |avg loss 8.398 |avg tokens 4522.200 |tokens/s 31039.541 |walltime 2512.598 |
Transformer | epoch 0 | step 17070 |avg loss 8.157 |avg tokens 4797.600 |tokens/s 31685.106 |walltime 2514.112 |
Transformer | epoch 0 | step 17080 |avg loss 8.671 |avg tokens 4540.600 |tokens/s 31080.738 |walltime 2515.573 |
Transformer | epoch 0 | step 17090 |avg loss 8.936 |avg tokens 4537.800 |tokens/s 30336.798 |walltime 2517.069 |
Transformer | epoch 0 | step 17100 |avg loss 8.556 |avg tokens 4453.000 |tokens/s 31629.773 |walltime 2518.476 |
Transformer | epoch 0 | step 17110 |avg loss 8.136 |avg tokens 4908.800 |tokens/s 32791.354 |walltime 2519.973 |
Transformer | epoch 0 | step 17120 |avg loss 8.270 |avg tokens 4516.600 |tokens/s 30374.375 |walltime 2521.460 |
Transformer | epoch 0 | step 17130 |avg loss 8.213 |avg tokens 4962.900 |tokens/s 33257.743 |walltime 2522.953 |
Transformer | epoch 0 | step 17140 |avg loss 8.433 |avg tokens 4374.200 |tokens/s 30312.423 |walltime 2524.396 |
Transformer | epoch 0 | step 17150 |avg loss 8.299 |avg tokens 4632.300 |tokens/s 31694.238 |walltime 2525.857 |
Transformer | epoch 0 | step 17160 |avg loss 8.265 |avg tokens 4482.400 |tokens/s 30495.850 |walltime 2527.327 |
Transformer | epoch 0 | step 17170 |avg loss 8.152 |avg tokens 4348.100 |tokens/s 30724.243 |walltime 2528.742 |
Transformer | epoch 0 | step 17180 |avg loss 8.434 |avg tokens 4226.900 |tokens/s 29459.886 |walltime 2530.177 |
Transformer | epoch 0 | step 17190 |avg loss 8.372 |avg tokens 4324.300 |tokens/s 29687.236 |walltime 2531.634 |
Transformer | epoch 0 | step 17200 |avg loss 8.416 |avg tokens 4332.500 |tokens/s 30294.651 |walltime 2533.064 |
Transformer | epoch 0 | step 17210 |avg loss 8.201 |avg tokens 4211.900 |tokens/s 29002.615 |walltime 2534.516 |
Transformer | epoch 0 | step 17220 |avg loss 8.407 |avg tokens 4671.400 |tokens/s 32163.918 |walltime 2535.968 |
Transformer | epoch 0 | step 17230 |avg loss 8.305 |avg tokens 4521.800 |tokens/s 30308.736 |walltime 2537.460 |
Transformer | epoch 0 | step 17240 |avg loss 8.221 |avg tokens 4597.200 |tokens/s 31289.170 |walltime 2538.930 |
Transformer | epoch 0 | step 17250 |avg loss 8.267 |avg tokens 4402.300 |tokens/s 30951.469 |walltime 2540.352 |
Transformer | epoch 0 | step 17260 |avg loss 8.150 |avg tokens 4385.800 |tokens/s 30119.366 |walltime 2541.808 |
Transformer | epoch 0 | step 17270 |avg loss 8.648 |avg tokens 3697.900 |tokens/s 26998.833 |walltime 2543.178 |
Transformer | epoch 0 | step 17280 |avg loss 8.400 |avg tokens 4600.800 |tokens/s 30789.772 |walltime 2544.672 |
Transformer | epoch 0 | step 17290 |avg loss 8.456 |avg tokens 4552.900 |tokens/s 30902.966 |walltime 2546.145 |
Transformer | epoch 0 | step 17300 |avg loss 8.336 |avg tokens 4789.600 |tokens/s 32192.006 |walltime 2547.633 |
Transformer | epoch 0 | step 17310 |avg loss 8.397 |avg tokens 4754.400 |tokens/s 32514.859 |walltime 2549.095 |
Transformer | epoch 0 | step 17320 |avg loss 8.404 |avg tokens 4726.800 |tokens/s 31116.067 |walltime 2550.614 |
Transformer | epoch 0 | step 17330 |avg loss 8.290 |avg tokens 4271.600 |tokens/s 30266.885 |walltime 2552.026 |
Transformer | epoch 0 | step 17340 |avg loss 8.201 |avg tokens 4947.200 |tokens/s 33388.784 |walltime 2553.507 |
Transformer | epoch 0 | step 17350 |avg loss 8.183 |avg tokens 4815.700 |tokens/s 32968.364 |walltime 2554.968 |
Transformer | epoch 0 | step 17360 |avg loss 8.159 |avg tokens 4241.300 |tokens/s 29486.527 |walltime 2556.407 |
Transformer | epoch 0 | step 17370 |avg loss 8.362 |avg tokens 4282.900 |tokens/s 30097.800 |walltime 2557.830 |
Transformer | epoch 0 | step 17380 |avg loss 8.235 |avg tokens 4740.100 |tokens/s 31995.627 |walltime 2559.311 |
Transformer | epoch 0 | step 17390 |avg loss 8.481 |avg tokens 4176.400 |tokens/s 29336.323 |walltime 2560.735 |
Transformer | epoch 0 | step 17400 |avg loss 8.202 |avg tokens 4649.900 |tokens/s 31778.768 |walltime 2562.198 |
Transformer | epoch 0 | step 17410 |avg loss 8.223 |avg tokens 4653.800 |tokens/s 31072.084 |walltime 2563.696 |
Transformer | epoch 0 | step 17420 |avg loss 8.488 |avg tokens 4075.300 |tokens/s 29250.850 |walltime 2565.089 |
Transformer | epoch 0 | step 17430 |avg loss 8.164 |avg tokens 4454.900 |tokens/s 30501.756 |walltime 2566.549 |
Transformer | epoch 0 | step 17440 |avg loss 8.525 |avg tokens 4315.300 |tokens/s 29933.946 |walltime 2567.991 |
Transformer | epoch 0 | step 17450 |avg loss 8.221 |avg tokens 4706.900 |tokens/s 32200.223 |walltime 2569.453 |
Transformer | epoch 0 | step 17460 |avg loss 8.281 |avg tokens 4658.800 |tokens/s 32638.034 |walltime 2570.880 |
Transformer | epoch 0 | step 17470 |avg loss 8.349 |avg tokens 4249.900 |tokens/s 29451.431 |walltime 2572.323 |
Transformer | epoch 0 | step 17480 |avg loss 8.263 |avg tokens 4737.600 |tokens/s 32104.560 |walltime 2573.799 |
Transformer | epoch 0 | step 17490 |avg loss 8.286 |avg tokens 4232.200 |tokens/s 29835.873 |walltime 2575.217 |
Transformer | epoch 0 | step 17500 |avg loss 8.519 |avg tokens 4627.000 |tokens/s 32068.929 |walltime 2576.660 |
Transformer | epoch 0 | step 17510 |avg loss 8.329 |avg tokens 4358.200 |tokens/s 30363.242 |walltime 2578.096 |
Transformer | epoch 0 | step 17520 |avg loss 8.516 |avg tokens 3584.100 |tokens/s 26069.034 |walltime 2579.470 |
Transformer | epoch 0 | step 17530 |avg loss 8.476 |avg tokens 4105.900 |tokens/s 27631.477 |walltime 2580.956 |
Transformer | epoch 0 | step 17540 |avg loss 8.400 |avg tokens 4188.100 |tokens/s 29132.942 |walltime 2582.394 |
Transformer | epoch 0 | step 17550 |avg loss 8.213 |avg tokens 4782.500 |tokens/s 31986.551 |walltime 2583.889 |
Transformer | epoch 0 | step 17560 |avg loss 8.296 |avg tokens 4695.000 |tokens/s 31303.020 |walltime 2585.389 |
Transformer | epoch 0 | step 17570 |avg loss 8.065 |avg tokens 4802.200 |tokens/s 32669.007 |walltime 2586.859 |
Transformer | epoch 0 | step 17580 |avg loss 8.189 |avg tokens 4208.800 |tokens/s 29476.136 |walltime 2588.287 |
Transformer | epoch 0 | step 17590 |avg loss 8.411 |avg tokens 4755.800 |tokens/s 31939.325 |walltime 2589.776 |
Transformer | epoch 0 | step 17600 |avg loss 8.262 |avg tokens 4314.900 |tokens/s 29935.275 |walltime 2591.217 |
Transformer | epoch 0 | step 17610 |avg loss 8.534 |avg tokens 4435.600 |tokens/s 31349.242 |walltime 2592.632 |
Transformer | epoch 0 | step 17620 |avg loss 8.436 |avg tokens 4471.800 |tokens/s 30670.678 |walltime 2594.090 |
Transformer | epoch 0 | step 17630 |avg loss 8.623 |avg tokens 4402.600 |tokens/s 30833.107 |walltime 2595.518 |
Transformer | epoch 0 | step 17640 |avg loss 8.423 |avg tokens 4146.500 |tokens/s 29110.647 |walltime 2596.942 |
Transformer | epoch 0 | step 17650 |avg loss 8.405 |avg tokens 4537.300 |tokens/s 31508.515 |walltime 2598.382 |
Transformer | epoch 0 | step 17660 |avg loss 8.606 |avg tokens 4470.900 |tokens/s 30857.324 |walltime 2599.831 |
Transformer | epoch 0 | step 17670 |avg loss 8.492 |avg tokens 4317.600 |tokens/s 30495.990 |walltime 2601.247 |
Transformer | epoch 0 | step 17680 |avg loss 8.451 |avg tokens 3821.300 |tokens/s 27230.909 |walltime 2602.650 |
Transformer | epoch 0 | step 17690 |avg loss 8.466 |avg tokens 4867.300 |tokens/s 31962.942 |walltime 2604.173 |
Transformer | epoch 0 | step 17700 |avg loss 8.358 |avg tokens 4578.400 |tokens/s 32162.218 |walltime 2605.597 |
Transformer | epoch 0 | step 17710 |avg loss 8.099 |avg tokens 4819.400 |tokens/s 31421.656 |walltime 2607.131 |
Transformer | epoch 0 | step 17720 |avg loss 8.499 |avg tokens 3828.800 |tokens/s 27318.195 |walltime 2608.532 |
Transformer | epoch 0 | step 17730 |avg loss 8.314 |avg tokens 4695.800 |tokens/s 32374.489 |walltime 2609.983 |
Transformer | epoch 0 | step 17740 |avg loss 8.436 |avg tokens 4631.000 |tokens/s 31845.761 |walltime 2611.437 |
Transformer | epoch 0 | step 17750 |avg loss 8.240 |avg tokens 4426.900 |tokens/s 29826.958 |walltime 2612.921 |
Transformer | epoch 0 | step 17760 |avg loss 8.296 |avg tokens 4472.500 |tokens/s 30700.550 |walltime 2614.378 |
Transformer | epoch 0 | step 17770 |avg loss 8.313 |avg tokens 4606.400 |tokens/s 31551.838 |walltime 2615.838 |
Transformer | epoch 0 | step 17780 |avg loss 8.504 |avg tokens 4904.900 |tokens/s 32530.051 |walltime 2617.346 |
Transformer | epoch 0 | step 17790 |avg loss 8.545 |avg tokens 4678.200 |tokens/s 32098.593 |walltime 2618.803 |
Transformer | epoch 0 | step 17800 |avg loss 8.455 |avg tokens 4240.800 |tokens/s 29910.261 |walltime 2620.221 |
Transformer | epoch 0 | step 17810 |avg loss 8.652 |avg tokens 4783.800 |tokens/s 33065.414 |walltime 2621.668 |
Transformer | epoch 0 | step 17820 |avg loss 8.263 |avg tokens 4173.500 |tokens/s 28573.484 |walltime 2623.128 |
Transformer | epoch 0 | step 17830 |avg loss 8.277 |avg tokens 4695.500 |tokens/s 31827.783 |walltime 2624.604 |
Transformer | epoch 0 | step 17840 |avg loss 8.374 |avg tokens 4649.600 |tokens/s 31707.663 |walltime 2626.070 |
Transformer | epoch 0 | step 17850 |avg loss 8.424 |avg tokens 4152.700 |tokens/s 28712.240 |walltime 2627.516 |
Transformer | epoch 0 | step 17860 |avg loss 8.103 |avg tokens 4795.000 |tokens/s 32214.616 |walltime 2629.005 |
Transformer | epoch 0 | step 17870 |avg loss 8.415 |avg tokens 4628.300 |tokens/s 31951.236 |walltime 2630.453 |
Transformer | epoch 0 | step 17880 |avg loss 8.461 |avg tokens 4716.500 |tokens/s 31351.647 |walltime 2631.958 |
Transformer | epoch 0 | step 17890 |avg loss 8.384 |avg tokens 4494.400 |tokens/s 30792.869 |walltime 2633.417 |
Transformer | epoch 0 | step 17900 |avg loss 8.271 |avg tokens 4261.700 |tokens/s 30692.239 |walltime 2634.806 |
Transformer | epoch 0 | step 17910 |avg loss 8.587 |avg tokens 4337.400 |tokens/s 28841.717 |walltime 2636.310 |
Transformer | epoch 0 | step 17920 |avg loss 8.419 |avg tokens 4700.000 |tokens/s 31526.871 |walltime 2637.800 |
Transformer | epoch 0 | step 17930 |avg loss 8.391 |avg tokens 4130.400 |tokens/s 29050.107 |walltime 2639.222 |
Transformer | epoch 0 | step 17940 |avg loss 8.424 |avg tokens 4872.200 |tokens/s 32881.596 |walltime 2640.704 |
Transformer | epoch 0 | step 17950 |avg loss 8.593 |avg tokens 4528.300 |tokens/s 30385.902 |walltime 2642.194 |
Transformer | epoch 0 | step 17960 |avg loss 8.530 |avg tokens 4629.900 |tokens/s 31287.336 |walltime 2643.674 |
Transformer | epoch 0 | step 17970 |avg loss 8.226 |avg tokens 4907.200 |tokens/s 32782.786 |walltime 2645.171 |
Transformer | epoch 0 | step 17980 |avg loss 8.356 |avg tokens 4356.000 |tokens/s 30442.762 |walltime 2646.602 |
Transformer | epoch 0 | step 17990 |avg loss 8.269 |avg tokens 4868.500 |tokens/s 32383.761 |walltime 2648.105 |
Transformer | epoch 0 | step 18000 |avg loss 8.377 |avg tokens 4205.800 |tokens/s 29321.851 |walltime 2649.540 |
Transformer | epoch 0 | step 18010 |avg loss 8.397 |avg tokens 4778.400 |tokens/s 31729.821 |walltime 2651.046 |
Transformer | epoch 0 | step 18020 |avg loss 8.152 |avg tokens 4692.200 |tokens/s 31235.454 |walltime 2652.548 |
Transformer | epoch 0 | step 18030 |avg loss 8.364 |avg tokens 4200.500 |tokens/s 28993.678 |walltime 2653.997 |
Transformer | epoch 0 | step 18040 |avg loss 8.076 |avg tokens 4128.000 |tokens/s 29230.677 |walltime 2655.409 |
Transformer | epoch 0 | step 18050 |avg loss 8.382 |avg tokens 4628.700 |tokens/s 30530.074 |walltime 2656.925 |
Transformer | epoch 0 | step 18060 |avg loss 8.178 |avg tokens 4324.800 |tokens/s 30125.727 |walltime 2658.360 |
Transformer | epoch 0 | step 18070 |avg loss 8.326 |avg tokens 4626.800 |tokens/s 31965.603 |walltime 2659.808 |
Transformer | epoch 0 | step 18080 |avg loss 8.222 |avg tokens 4370.400 |tokens/s 30358.152 |walltime 2661.248 |
Transformer | epoch 0 | step 18090 |avg loss 8.298 |avg tokens 4665.300 |tokens/s 31279.291 |walltime 2662.739 |
Transformer | epoch 0 | step 18100 |avg loss 8.386 |avg tokens 4238.600 |tokens/s 29695.614 |walltime 2664.166 |
Transformer | epoch 0 | step 18110 |avg loss 8.212 |avg tokens 4897.800 |tokens/s 32088.921 |walltime 2665.693 |
Transformer | epoch 0 | step 18120 |avg loss 8.408 |avg tokens 4537.900 |tokens/s 31480.190 |walltime 2667.134 |
Transformer | epoch 0 | step 18130 |avg loss 8.364 |avg tokens 4733.800 |tokens/s 31994.319 |walltime 2668.614 |
Transformer | epoch 0 | step 18140 |avg loss 8.700 |avg tokens 4680.800 |tokens/s 31990.709 |walltime 2670.077 |
Transformer | epoch 0 | step 18150 |avg loss 8.384 |avg tokens 4535.700 |tokens/s 30574.136 |walltime 2671.560 |
Transformer | epoch 0 | step 18160 |avg loss 8.619 |avg tokens 4442.000 |tokens/s 31436.291 |walltime 2672.973 |
Transformer | epoch 0 | step 18170 |avg loss 8.228 |avg tokens 4773.300 |tokens/s 31800.028 |walltime 2674.475 |
Transformer | epoch 0 | step 18180 |avg loss 8.329 |avg tokens 4330.200 |tokens/s 29948.086 |walltime 2675.920 |
Transformer | epoch 0 | step 18190 |avg loss 8.573 |avg tokens 3922.200 |tokens/s 28109.111 |walltime 2677.316 |
Transformer | epoch 0 | step 18200 |avg loss 8.196 |avg tokens 4903.900 |tokens/s 32574.679 |walltime 2678.821 |
Transformer | epoch 0 | step 18210 |avg loss 8.384 |avg tokens 4641.100 |tokens/s 31866.240 |walltime 2680.278 |
Transformer | epoch 0 | step 18220 |avg loss 8.378 |avg tokens 4680.700 |tokens/s 31589.746 |walltime 2681.759 |
Transformer | epoch 0 | step 18230 |avg loss 8.306 |avg tokens 4558.800 |tokens/s 31091.814 |walltime 2683.226 |
Transformer | epoch 0 | step 18240 |avg loss 8.237 |avg tokens 4491.800 |tokens/s 30800.287 |walltime 2684.684 |
Transformer | epoch 0 | step 18250 |avg loss 8.569 |avg tokens 4609.300 |tokens/s 32210.611 |walltime 2686.115 |
Transformer | epoch 0 | step 18260 |avg loss 8.218 |avg tokens 4566.000 |tokens/s 30943.610 |walltime 2687.591 |
Transformer | epoch 0 | step 18270 |avg loss 8.678 |avg tokens 4728.200 |tokens/s 32605.578 |walltime 2689.041 |
Transformer | epoch 0 | step 18280 |avg loss 8.450 |avg tokens 3785.400 |tokens/s 28153.944 |walltime 2690.385 |
Transformer | epoch 0 | step 18290 |avg loss 8.473 |avg tokens 3787.700 |tokens/s 27438.007 |walltime 2691.766 |
Transformer | epoch 0 | step 18300 |avg loss 8.367 |avg tokens 4831.600 |tokens/s 31770.642 |walltime 2693.286 |
Transformer | epoch 0 | step 18310 |avg loss 8.373 |avg tokens 4188.200 |tokens/s 28539.569 |walltime 2694.754 |
Transformer | epoch 0 | step 18320 |avg loss 8.465 |avg tokens 4474.400 |tokens/s 31976.803 |walltime 2696.153 |
Transformer | epoch 0 | step 18330 |avg loss 8.326 |avg tokens 4974.800 |tokens/s 34345.288 |walltime 2697.602 |
Transformer | epoch 0 | step 18340 |avg loss 8.556 |avg tokens 4773.700 |tokens/s 32245.815 |walltime 2699.082 |
Transformer | epoch 0 | step 18350 |avg loss 8.527 |avg tokens 3895.000 |tokens/s 28448.998 |walltime 2700.451 |
Transformer | epoch 0 | step 18360 |avg loss 8.239 |avg tokens 4609.300 |tokens/s 31093.623 |walltime 2701.934 |
Transformer | epoch 0 | step 18370 |avg loss 8.286 |avg tokens 4611.800 |tokens/s 30834.104 |walltime 2703.429 |
Transformer | epoch 0 | step 18380 |avg loss 8.285 |avg tokens 4741.400 |tokens/s 32042.706 |walltime 2704.909 |
Transformer | epoch 0 | step 18390 |avg loss 8.248 |avg tokens 4591.500 |tokens/s 31119.077 |walltime 2706.384 |
Transformer | epoch 0 | step 18400 |avg loss 8.523 |avg tokens 4426.500 |tokens/s 30509.596 |walltime 2707.835 |
Transformer | epoch 0 | step 18410 |avg loss 8.536 |avg tokens 3799.400 |tokens/s 25216.606 |walltime 2709.342 |
Transformer | epoch 0 | step 18420 |avg loss 8.243 |avg tokens 4884.800 |tokens/s 32709.555 |walltime 2710.835 |
Transformer | epoch 0 | step 18430 |avg loss 8.163 |avg tokens 4818.600 |tokens/s 33444.159 |walltime 2712.276 |
Transformer | epoch 0 | step 18440 |avg loss 8.415 |avg tokens 4834.400 |tokens/s 32898.327 |walltime 2713.746 |
Transformer | epoch 0 | step 18450 |avg loss 8.606 |avg tokens 4567.200 |tokens/s 32451.084 |walltime 2715.153 |
Transformer | epoch 0 | step 18460 |avg loss 8.291 |avg tokens 4593.400 |tokens/s 30790.227 |walltime 2716.645 |
Transformer | epoch 0 | step 18470 |avg loss 8.076 |avg tokens 4574.000 |tokens/s 31387.829 |walltime 2718.102 |
Transformer | epoch 0 | step 18480 |avg loss 8.410 |avg tokens 3988.300 |tokens/s 28124.693 |walltime 2719.520 |
Transformer | epoch 0 | step 18490 |avg loss 8.347 |avg tokens 4681.800 |tokens/s 33468.443 |walltime 2720.919 |
Transformer | epoch 0 | step 18500 |avg loss 8.467 |avg tokens 4483.300 |tokens/s 30870.727 |walltime 2722.371 |
Transformer | epoch 0 | step 18510 |avg loss 8.434 |avg tokens 4153.100 |tokens/s 29551.964 |walltime 2723.777 |
Transformer | epoch 0 | step 18520 |avg loss 8.121 |avg tokens 4564.800 |tokens/s 31052.340 |walltime 2725.247 |
Transformer | epoch 0 | step 18530 |avg loss 8.362 |avg tokens 4684.700 |tokens/s 31761.193 |walltime 2726.722 |
Transformer | epoch 0 | step 18540 |avg loss 8.356 |avg tokens 4632.000 |tokens/s 30149.525 |walltime 2728.258 |
Transformer | epoch 0 | step 18550 |avg loss 8.484 |avg tokens 4396.200 |tokens/s 30272.895 |walltime 2729.710 |
Transformer | epoch 0 | step 18560 |avg loss 8.463 |avg tokens 4510.600 |tokens/s 31334.230 |walltime 2731.150 |
Transformer | epoch 0 | step 18570 |avg loss 8.215 |avg tokens 4786.400 |tokens/s 31335.934 |walltime 2732.677 |
Transformer | epoch 0 | step 18580 |avg loss 8.462 |avg tokens 4183.700 |tokens/s 29083.022 |walltime 2734.116 |
Transformer | epoch 0 | step 18590 |avg loss 8.453 |avg tokens 4548.700 |tokens/s 31613.150 |walltime 2735.555 |
Transformer | epoch 0 | step 18600 |avg loss 8.356 |avg tokens 4326.400 |tokens/s 29961.654 |walltime 2736.999 |
Transformer | epoch 0 | step 18610 |avg loss 8.472 |avg tokens 4652.200 |tokens/s 33214.697 |walltime 2738.399 |
Transformer | epoch 0 | step 18620 |avg loss 8.228 |avg tokens 4497.200 |tokens/s 30462.461 |walltime 2739.876 |
Transformer | epoch 0 | step 18630 |avg loss 8.374 |avg tokens 4698.400 |tokens/s 31948.670 |walltime 2741.346 |
Transformer | epoch 0 | step 18640 |avg loss 8.375 |avg tokens 4806.400 |tokens/s 32405.999 |walltime 2742.830 |
Transformer | epoch 0 | step 18650 |avg loss 8.255 |avg tokens 4897.900 |tokens/s 32550.578 |walltime 2744.334 |
Transformer | epoch 0 | step 18660 |avg loss 8.282 |avg tokens 4409.100 |tokens/s 30827.289 |walltime 2745.764 |
Transformer | epoch 0 | step 18670 |avg loss 8.305 |avg tokens 4409.600 |tokens/s 30418.101 |walltime 2747.214 |
Transformer | epoch 0 | step 18680 |avg loss 8.275 |avg tokens 4461.300 |tokens/s 30775.859 |walltime 2748.664 |
Transformer | epoch 0 | step 18690 |avg loss 8.148 |avg tokens 4477.200 |tokens/s 30340.404 |walltime 2750.139 |
Transformer | epoch 0 | step 18700 |avg loss 8.497 |avg tokens 4871.900 |tokens/s 32755.389 |walltime 2751.627 |
Transformer | epoch 0 | step 18710 |avg loss 8.191 |avg tokens 4661.700 |tokens/s 31400.381 |walltime 2753.111 |
Transformer | epoch 0 | step 18720 |avg loss 8.284 |avg tokens 4843.200 |tokens/s 31655.761 |walltime 2754.641 |
Transformer | epoch 0 | step 18730 |avg loss 8.245 |avg tokens 4498.100 |tokens/s 29929.182 |walltime 2756.144 |
Transformer | epoch 0 | step 18740 |avg loss 8.423 |avg tokens 4632.400 |tokens/s 31817.500 |walltime 2757.600 |
Transformer | epoch 0 | step 18750 |avg loss 8.356 |avg tokens 4181.300 |tokens/s 27895.805 |walltime 2759.099 |
Transformer | epoch 0 | step 18760 |avg loss 8.331 |avg tokens 4286.500 |tokens/s 29642.914 |walltime 2760.545 |
Transformer | epoch 0 | step 18770 |avg loss 8.501 |avg tokens 4445.600 |tokens/s 31048.153 |walltime 2761.977 |
Transformer | epoch 0 | step 18780 |avg loss 8.572 |avg tokens 4451.800 |tokens/s 30957.291 |walltime 2763.415 |
Transformer | epoch 0 | step 18790 |avg loss 8.240 |avg tokens 4902.100 |tokens/s 33229.492 |walltime 2764.890 |
Transformer | epoch 0 | step 18800 |avg loss 8.207 |avg tokens 4692.100 |tokens/s 30871.607 |walltime 2766.410 |
Transformer | epoch 0 | step 18810 |avg loss 8.550 |avg tokens 4530.800 |tokens/s 31779.701 |walltime 2767.836 |
Transformer | epoch 0 | step 18820 |avg loss 8.194 |avg tokens 4553.600 |tokens/s 30771.528 |walltime 2769.316 |
Transformer | epoch 0 | step 18830 |avg loss 8.172 |avg tokens 4672.100 |tokens/s 31894.724 |walltime 2770.781 |
Transformer | epoch 0 | step 18840 |avg loss 8.562 |avg tokens 4250.700 |tokens/s 29850.721 |walltime 2772.205 |
Transformer | epoch 0 | step 18850 |avg loss 8.473 |avg tokens 3953.800 |tokens/s 28558.349 |walltime 2773.589 |
Transformer | epoch 0 | step 18860 |avg loss 8.384 |avg tokens 4858.700 |tokens/s 32176.928 |walltime 2775.099 |
Transformer | epoch 0 | step 18870 |avg loss 7.971 |avg tokens 4634.400 |tokens/s 30975.780 |walltime 2776.595 |
Transformer | epoch 0 | step 18880 |avg loss 8.237 |avg tokens 4387.600 |tokens/s 29743.548 |walltime 2778.070 |
Transformer | epoch 0 | step 18890 |avg loss 8.002 |avg tokens 4632.800 |tokens/s 30996.254 |walltime 2779.565 |
Transformer | epoch 0 | step 18900 |avg loss 8.219 |avg tokens 4865.000 |tokens/s 32161.668 |walltime 2781.078 |
Transformer | epoch 0 | step 18910 |avg loss 8.346 |avg tokens 4914.700 |tokens/s 33236.361 |walltime 2782.556 |
Transformer | epoch 0 | step 18920 |avg loss 8.073 |avg tokens 4825.400 |tokens/s 32056.550 |walltime 2784.062 |
Transformer | epoch 0 | step 18930 |avg loss 8.312 |avg tokens 4554.700 |tokens/s 30916.544 |walltime 2785.535 |
Transformer | epoch 0 | step 18940 |avg loss 8.256 |avg tokens 4226.800 |tokens/s 29198.012 |walltime 2786.982 |
Transformer | epoch 0 | step 18950 |avg loss 8.244 |avg tokens 4410.300 |tokens/s 30315.847 |walltime 2788.437 |
Transformer | epoch 0 | step 18960 |avg loss 8.330 |avg tokens 4243.000 |tokens/s 29574.972 |walltime 2789.872 |
Transformer | epoch 0 | step 18970 |avg loss 8.492 |avg tokens 4696.800 |tokens/s 32349.378 |walltime 2791.324 |
Transformer | epoch 0 | step 18980 |avg loss 8.522 |avg tokens 4437.700 |tokens/s 30396.659 |walltime 2792.784 |
Transformer | epoch 0 | step 18990 |avg loss 8.327 |avg tokens 4535.800 |tokens/s 31307.081 |walltime 2794.233 |
Transformer | epoch 0 | step 19000 |avg loss 8.236 |avg tokens 4545.500 |tokens/s 31324.196 |walltime 2795.684 |
Transformer | epoch 0 | step 19010 |avg loss 8.224 |avg tokens 4507.400 |tokens/s 30653.596 |walltime 2797.154 |
Transformer | epoch 0 | step 19020 |avg loss 8.347 |avg tokens 3558.900 |tokens/s 25783.253 |walltime 2798.534 |
Transformer | epoch 0 | step 19030 |avg loss 8.223 |avg tokens 4865.300 |tokens/s 32192.716 |walltime 2800.046 |
Transformer | epoch 0 | step 19040 |avg loss 8.354 |avg tokens 4590.200 |tokens/s 30899.050 |walltime 2801.531 |
Transformer | epoch 0 | step 19050 |avg loss 8.398 |avg tokens 4203.000 |tokens/s 29937.869 |walltime 2802.935 |
Transformer | epoch 0 | step 19060 |avg loss 8.215 |avg tokens 4656.500 |tokens/s 31159.125 |walltime 2804.430 |
Transformer | epoch 0 | step 19070 |avg loss 8.414 |avg tokens 4572.000 |tokens/s 31266.384 |walltime 2805.892 |
Transformer | epoch 0 | step 19080 |avg loss 8.396 |avg tokens 4668.600 |tokens/s 32640.531 |walltime 2807.322 |
Transformer | epoch 0 | step 19090 |avg loss 8.047 |avg tokens 4398.500 |tokens/s 30135.742 |walltime 2808.782 |
Transformer | epoch 0 | step 19100 |avg loss 8.133 |avg tokens 4492.800 |tokens/s 31525.737 |walltime 2810.207 |
Transformer | epoch 0 | step 19110 |avg loss 8.440 |avg tokens 4103.300 |tokens/s 29219.347 |walltime 2811.611 |
Transformer | epoch 0 | step 19120 |avg loss 8.354 |avg tokens 4166.600 |tokens/s 29946.993 |walltime 2813.003 |
Transformer | epoch 0 | step 19130 |avg loss 8.688 |avg tokens 3990.200 |tokens/s 29314.932 |walltime 2814.364 |
Transformer | epoch 0 | step 19140 |avg loss 8.315 |avg tokens 4597.600 |tokens/s 31821.461 |walltime 2815.808 |
Transformer | epoch 0 | step 19150 |avg loss 8.152 |avg tokens 4672.800 |tokens/s 31642.685 |walltime 2817.285 |
Transformer | epoch 0 | step 19160 |avg loss 8.297 |avg tokens 4503.300 |tokens/s 31836.396 |walltime 2818.700 |
Transformer | epoch 0 | step 19170 |avg loss 8.114 |avg tokens 4641.400 |tokens/s 31191.846 |walltime 2820.188 |
Transformer | epoch 0 | step 19180 |avg loss 8.220 |avg tokens 4832.800 |tokens/s 31507.552 |walltime 2821.722 |
Transformer | epoch 0 | step 19190 |avg loss 8.298 |avg tokens 4386.800 |tokens/s 29538.983 |walltime 2823.207 |
Transformer | epoch 0 | step 19200 |avg loss 8.474 |avg tokens 4477.000 |tokens/s 29755.090 |walltime 2824.711 |
Transformer | epoch 0 | step 19210 |avg loss 8.197 |avg tokens 4795.600 |tokens/s 31299.646 |walltime 2826.243 |
Transformer | epoch 0 | step 19220 |avg loss 8.279 |avg tokens 4088.200 |tokens/s 25910.534 |walltime 2827.821 |
Transformer | epoch 0 | step 19230 |avg loss 8.938 |avg tokens 4675.400 |tokens/s 33637.809 |walltime 2829.211 |
Transformer | epoch 0 | step 19240 |avg loss 8.459 |avg tokens 4552.300 |tokens/s 31060.933 |walltime 2830.677 |
Transformer | epoch 0 | step 19250 |avg loss 8.390 |avg tokens 4555.600 |tokens/s 30907.203 |walltime 2832.151 |
Transformer | epoch 0 | step 19260 |avg loss 8.696 |avg tokens 4266.300 |tokens/s 30372.434 |walltime 2833.555 |
Transformer | epoch 0 | step 19270 |avg loss 8.340 |avg tokens 4483.300 |tokens/s 30798.909 |walltime 2835.011 |
Transformer | epoch 0 | step 19280 |avg loss 8.274 |avg tokens 4503.700 |tokens/s 30699.310 |walltime 2836.478 |
Transformer | epoch 0 | step 19290 |avg loss 8.422 |avg tokens 4260.400 |tokens/s 30259.794 |walltime 2837.886 |
Transformer | epoch 0 | step 19300 |avg loss 8.443 |avg tokens 4457.800 |tokens/s 30566.309 |walltime 2839.345 |
Transformer | epoch 0 | step 19310 |avg loss 8.333 |avg tokens 4616.600 |tokens/s 30742.991 |walltime 2840.846 |
Transformer | epoch 0 | step 19320 |avg loss 8.301 |avg tokens 4627.800 |tokens/s 31216.770 |walltime 2842.329 |
Transformer | epoch 0 | step 19330 |avg loss 8.550 |avg tokens 4296.500 |tokens/s 28310.539 |walltime 2843.846 |
Transformer | epoch 0 | step 19340 |avg loss 8.431 |avg tokens 4593.200 |tokens/s 31217.459 |walltime 2845.318 |
Transformer | epoch 0 | step 19350 |avg loss 8.269 |avg tokens 4914.000 |tokens/s 32724.552 |walltime 2846.819 |
Transformer | epoch 0 | step 19360 |avg loss 8.376 |avg tokens 4553.600 |tokens/s 30654.103 |walltime 2848.305 |
Transformer | epoch 0 | step 19370 |avg loss 8.323 |avg tokens 4742.700 |tokens/s 31949.205 |walltime 2849.789 |
Transformer | epoch 0 | step 19380 |avg loss 8.314 |avg tokens 4660.100 |tokens/s 32131.345 |walltime 2851.240 |
Transformer | epoch 0 | step 19390 |avg loss 8.234 |avg tokens 4478.400 |tokens/s 30412.039 |walltime 2852.712 |
Transformer | epoch 0 | step 19400 |avg loss 8.043 |avg tokens 4757.600 |tokens/s 30728.524 |walltime 2854.260 |
Transformer | epoch 0 | step 19410 |avg loss 8.272 |avg tokens 4899.400 |tokens/s 32556.868 |walltime 2855.765 |
Transformer | epoch 0 | step 19420 |avg loss 8.570 |avg tokens 4472.700 |tokens/s 31400.283 |walltime 2857.190 |
Transformer | epoch 0 | step 19430 |avg loss 8.314 |avg tokens 4820.000 |tokens/s 31971.490 |walltime 2858.697 |
Transformer | epoch 0 | step 19440 |avg loss 8.492 |avg tokens 4786.300 |tokens/s 32664.692 |walltime 2860.163 |
Transformer | epoch 0 | step 19450 |avg loss 8.206 |avg tokens 4783.500 |tokens/s 31689.526 |walltime 2861.672 |
Transformer | epoch 0 | step 19460 |avg loss 8.177 |avg tokens 4928.300 |tokens/s 33007.848 |walltime 2863.165 |
Transformer | epoch 0 | step 19470 |avg loss 8.321 |avg tokens 4497.600 |tokens/s 30731.992 |walltime 2864.629 |
Transformer | epoch 0 | step 19480 |avg loss 8.160 |avg tokens 4877.600 |tokens/s 31831.535 |walltime 2866.161 |
Transformer | epoch 0 | step 19490 |avg loss 8.258 |avg tokens 4918.300 |tokens/s 31798.908 |walltime 2867.708 |
Transformer | epoch 0 | step 19500 |avg loss 8.452 |avg tokens 4574.300 |tokens/s 30987.887 |walltime 2869.184 |
Transformer | epoch 0 | step 19510 |avg loss 8.344 |avg tokens 4421.200 |tokens/s 29995.369 |walltime 2870.658 |
Transformer | epoch 0 | step 19520 |avg loss 7.992 |avg tokens 4646.200 |tokens/s 31239.566 |walltime 2872.145 |
Transformer | epoch 0 | step 19530 |avg loss 8.379 |avg tokens 4532.700 |tokens/s 31036.508 |walltime 2873.606 |
Transformer | epoch 0 | step 19540 |avg loss 8.444 |avg tokens 4278.000 |tokens/s 29671.016 |walltime 2875.047 |
Transformer | epoch 0 | step 19550 |avg loss 8.721 |avg tokens 4645.500 |tokens/s 33249.052 |walltime 2876.445 |
Transformer | epoch 0 | step 19560 |avg loss 8.370 |avg tokens 4561.700 |tokens/s 29870.536 |walltime 2877.972 |
Transformer | epoch 0 | step 19570 |avg loss 8.398 |avg tokens 4383.500 |tokens/s 31237.486 |walltime 2879.375 |
Transformer | epoch 0 | step 19580 |avg loss 8.498 |avg tokens 4475.500 |tokens/s 30528.042 |walltime 2880.841 |
Transformer | epoch 0 | step 19590 |avg loss 8.361 |avg tokens 4529.500 |tokens/s 29874.674 |walltime 2882.357 |
Transformer | epoch 0 | step 19600 |avg loss 8.601 |avg tokens 3882.500 |tokens/s 27883.997 |walltime 2883.750 |
Transformer | epoch 0 | step 19610 |avg loss 8.794 |avg tokens 4780.400 |tokens/s 32556.457 |walltime 2885.218 |
Transformer | epoch 0 | step 19620 |avg loss 8.226 |avg tokens 4797.600 |tokens/s 31999.340 |walltime 2886.717 |
Transformer | epoch 0 | step 19630 |avg loss 8.081 |avg tokens 4640.400 |tokens/s 31258.700 |walltime 2888.202 |
Transformer | epoch 0 | step 19640 |avg loss 8.403 |avg tokens 4805.100 |tokens/s 30997.561 |walltime 2889.752 |
Transformer | epoch 0 | step 19650 |avg loss 8.557 |avg tokens 4404.100 |tokens/s 30327.216 |walltime 2891.204 |
Transformer | epoch 0 | step 19660 |avg loss 8.399 |avg tokens 4115.100 |tokens/s 28644.508 |walltime 2892.641 |
Transformer | epoch 0 | step 19670 |avg loss 8.355 |avg tokens 4244.700 |tokens/s 29102.714 |walltime 2894.099 |
Transformer | epoch 0 | step 19680 |avg loss 8.587 |avg tokens 4279.200 |tokens/s 30051.769 |walltime 2895.523 |
Transformer | epoch 0 | step 19690 |avg loss 8.360 |avg tokens 4299.800 |tokens/s 29808.763 |walltime 2896.966 |
Transformer | epoch 0 | step 19700 |avg loss 8.497 |avg tokens 4546.700 |tokens/s 31046.521 |walltime 2898.430 |
Transformer | epoch 0 | step 19710 |avg loss 8.328 |avg tokens 4880.000 |tokens/s 32584.187 |walltime 2899.928 |
Transformer | epoch 0 | step 19720 |avg loss 8.513 |avg tokens 4632.700 |tokens/s 32085.933 |walltime 2901.372 |
Transformer | epoch 0 | step 19730 |avg loss 8.410 |avg tokens 4875.700 |tokens/s 32963.088 |walltime 2902.851 |
Transformer | epoch 0 | step 19740 |avg loss 8.362 |avg tokens 4664.700 |tokens/s 31256.607 |walltime 2904.343 |
Transformer | epoch 0 | step 19750 |avg loss 8.349 |avg tokens 4276.400 |tokens/s 29256.499 |walltime 2905.805 |
Transformer | epoch 0 | step 19760 |avg loss 8.183 |avg tokens 4673.600 |tokens/s 30888.650 |walltime 2907.318 |
Transformer | epoch 0 | step 19770 |avg loss 8.219 |avg tokens 4580.200 |tokens/s 30686.116 |walltime 2908.810 |
Transformer | epoch 0 | step 19780 |avg loss 8.382 |avg tokens 4587.100 |tokens/s 31667.689 |walltime 2910.259 |
Transformer | epoch 0 | step 19790 |avg loss 8.543 |avg tokens 4355.300 |tokens/s 31130.803 |walltime 2911.658 |
Transformer | epoch 0 | step 19800 |avg loss 8.489 |avg tokens 3935.200 |tokens/s 29185.552 |walltime 2913.006 |
Transformer | epoch 0 | step 19810 |avg loss 8.376 |avg tokens 4760.700 |tokens/s 32284.686 |walltime 2914.481 |
Transformer | epoch 0 | step 19820 |avg loss 8.242 |avg tokens 4439.300 |tokens/s 30420.451 |walltime 2915.940 |
Transformer | epoch 0 | step 19830 |avg loss 8.509 |avg tokens 4884.700 |tokens/s 33531.943 |walltime 2917.397 |
Transformer | epoch 0 | step 19840 |avg loss 8.285 |avg tokens 4767.900 |tokens/s 31412.058 |walltime 2918.915 |
Transformer | epoch 0 | step 19850 |avg loss 8.411 |avg tokens 4743.300 |tokens/s 32187.405 |walltime 2920.389 |
Transformer | epoch 0 | step 19860 |avg loss 8.537 |avg tokens 4619.300 |tokens/s 32322.999 |walltime 2921.818 |
Transformer | epoch 0 | step 19870 |avg loss 8.406 |avg tokens 4681.900 |tokens/s 31296.077 |walltime 2923.314 |
Transformer | epoch 0 | step 19880 |avg loss 8.353 |avg tokens 4430.700 |tokens/s 29410.647 |walltime 2924.820 |
Transformer | epoch 0 | step 19890 |avg loss 8.406 |avg tokens 4707.700 |tokens/s 32506.057 |walltime 2926.268 |
Transformer | epoch 0 | step 19900 |avg loss 8.201 |avg tokens 3806.900 |tokens/s 26178.298 |walltime 2927.723 |
Transformer | epoch 0 | step 19910 |avg loss 8.546 |avg tokens 4544.700 |tokens/s 31583.058 |walltime 2929.162 |
Transformer | epoch 0 | step 19920 |avg loss 8.067 |avg tokens 4624.100 |tokens/s 31174.940 |walltime 2930.645 |
Transformer | epoch 0 | step 19930 |avg loss 8.439 |avg tokens 3942.100 |tokens/s 27843.508 |walltime 2932.061 |
Transformer | epoch 0 | step 19940 |avg loss 8.476 |avg tokens 3854.300 |tokens/s 27928.695 |walltime 2933.441 |
Transformer | epoch 0 | step 19950 |avg loss 8.320 |avg tokens 4841.600 |tokens/s 32957.756 |walltime 2934.910 |
Transformer | epoch 0 | step 19960 |avg loss 8.602 |avg tokens 4405.800 |tokens/s 30702.833 |walltime 2936.345 |
Transformer | epoch 0 | step 19970 |avg loss 8.315 |avg tokens 4744.500 |tokens/s 31705.264 |walltime 2937.841 |
Transformer | epoch 0 | step 19980 |avg loss 8.637 |avg tokens 4144.000 |tokens/s 28936.265 |walltime 2939.273 |
Transformer | epoch 0 | step 19990 |avg loss 8.669 |avg tokens 4400.300 |tokens/s 31413.754 |walltime 2940.674 |
Transformer | epoch 0 | step 20000 |avg loss 8.342 |avg tokens 4732.600 |tokens/s 32387.106 |walltime 2942.135 |
Transformer | epoch 0 | step 20010 |avg loss 8.442 |avg tokens 4268.400 |tokens/s 29684.390 |walltime 2943.573 |
Transformer | epoch 0 | step 20020 |avg loss 8.259 |avg tokens 4445.400 |tokens/s 30485.077 |walltime 2945.032 |
Transformer | epoch 0 | step 20030 |avg loss 8.500 |avg tokens 4124.300 |tokens/s 29790.883 |walltime 2946.416 |
Transformer | epoch 0 | step 20040 |avg loss 8.227 |avg tokens 4746.400 |tokens/s 32840.336 |walltime 2947.861 |
Transformer | epoch 0 | step 20050 |avg loss 8.348 |avg tokens 4840.500 |tokens/s 31652.467 |walltime 2949.390 |
Transformer | epoch 0 | step 20060 |avg loss 8.300 |avg tokens 4450.400 |tokens/s 31138.030 |walltime 2950.820 |
Transformer | epoch 0 | step 20070 |avg loss 8.225 |avg tokens 4483.100 |tokens/s 30610.045 |walltime 2952.284 |
Transformer | epoch 0 | step 20080 |avg loss 8.431 |avg tokens 4191.000 |tokens/s 28307.621 |walltime 2953.765 |
Transformer | epoch 0 | step 20090 |avg loss 8.341 |avg tokens 4596.000 |tokens/s 30726.679 |walltime 2955.261 |
Transformer | epoch 0 | step 20100 |avg loss 8.684 |avg tokens 4166.100 |tokens/s 29391.366 |walltime 2956.678 |
Transformer | epoch 0 | step 20110 |avg loss 8.348 |avg tokens 4171.300 |tokens/s 29966.770 |walltime 2958.070 |
Transformer | epoch 0 | step 20120 |avg loss 8.288 |avg tokens 4435.400 |tokens/s 31622.006 |walltime 2959.473 |
Transformer | epoch 0 | step 20130 |avg loss 8.333 |avg tokens 4636.400 |tokens/s 32167.625 |walltime 2960.914 |
Transformer | epoch 0 | step 20140 |avg loss 8.230 |avg tokens 4628.000 |tokens/s 30753.469 |walltime 2962.419 |
Transformer | epoch 0 | step 20150 |avg loss 8.261 |avg tokens 4967.200 |tokens/s 32881.534 |walltime 2963.930 |
Transformer | epoch 0 | step 20160 |avg loss 8.685 |avg tokens 4617.500 |tokens/s 32239.730 |walltime 2965.362 |
Transformer | epoch 0 | step 20170 |avg loss 8.257 |avg tokens 4509.300 |tokens/s 30677.803 |walltime 2966.832 |
Transformer | epoch 0 | step 20180 |avg loss 8.277 |avg tokens 4684.200 |tokens/s 32441.168 |walltime 2968.276 |
Transformer | epoch 0 | step 20190 |avg loss 8.219 |avg tokens 4696.200 |tokens/s 31677.264 |walltime 2969.758 |
Transformer | epoch 0 | step 20200 |avg loss 8.208 |avg tokens 4803.400 |tokens/s 33252.953 |walltime 2971.203 |
Transformer | epoch 0 | step 20210 |avg loss 8.326 |avg tokens 4791.500 |tokens/s 33145.832 |walltime 2972.648 |
Transformer | epoch 0 | step 20220 |avg loss 8.369 |avg tokens 4479.900 |tokens/s 31352.732 |walltime 2974.077 |
Transformer | epoch 0 | step 20230 |avg loss 8.420 |avg tokens 4240.900 |tokens/s 29313.029 |walltime 2975.524 |
Transformer | epoch 0 | step 20240 |avg loss 8.344 |avg tokens 4201.500 |tokens/s 29724.143 |walltime 2976.937 |
Transformer | epoch 0 | step 20250 |avg loss 8.353 |avg tokens 4363.000 |tokens/s 30160.456 |walltime 2978.384 |
Transformer | epoch 0 | step 20260 |avg loss 8.424 |avg tokens 4420.300 |tokens/s 30671.339 |walltime 2979.825 |
Transformer | epoch 0 | step 20270 |avg loss 8.591 |avg tokens 4332.700 |tokens/s 31542.590 |walltime 2981.199 |
Transformer | epoch 0 | step 20280 |avg loss 8.407 |avg tokens 4762.700 |tokens/s 32072.278 |walltime 2982.684 |
Transformer | epoch 0 | step 20290 |avg loss 8.632 |avg tokens 4332.500 |tokens/s 31194.574 |walltime 2984.073 |
Transformer | epoch 0 | step 20300 |avg loss 8.437 |avg tokens 4903.000 |tokens/s 31964.025 |walltime 2985.606 |
Transformer | epoch 0 | step 20310 |avg loss 8.378 |avg tokens 4441.000 |tokens/s 30699.648 |walltime 2987.053 |
Transformer | epoch 0 | step 20320 |avg loss 8.596 |avg tokens 4347.400 |tokens/s 30583.270 |walltime 2988.475 |
Transformer | epoch 0 | step 20330 |avg loss 8.441 |avg tokens 4632.300 |tokens/s 31263.526 |walltime 2989.956 |
Transformer | epoch 0 | step 20340 |avg loss 8.325 |avg tokens 4748.000 |tokens/s 32246.318 |walltime 2991.429 |
Transformer | epoch 0 | step 20350 |avg loss 7.970 |avg tokens 4544.200 |tokens/s 30293.083 |walltime 2992.929 |
Transformer | epoch 0 | step 20360 |avg loss 8.237 |avg tokens 4694.300 |tokens/s 31819.449 |walltime 2994.404 |
Transformer | epoch 0 | step 20370 |avg loss 8.138 |avg tokens 4444.800 |tokens/s 30376.135 |walltime 2995.867 |
Transformer | epoch 0 | step 20380 |avg loss 8.324 |avg tokens 4437.400 |tokens/s 30177.148 |walltime 2997.338 |
Transformer | epoch 0 | step 20390 |avg loss 8.353 |avg tokens 4624.800 |tokens/s 31866.018 |walltime 2998.789 |
Transformer | epoch 0 | step 20400 |avg loss 8.192 |avg tokens 4379.900 |tokens/s 29770.149 |walltime 3000.260 |
Transformer | epoch 0 | step 20410 |avg loss 8.570 |avg tokens 4646.100 |tokens/s 31970.046 |walltime 3001.714 |
Transformer | epoch 0 | step 20420 |avg loss 8.585 |avg tokens 4567.200 |tokens/s 31920.121 |walltime 3003.144 |
Transformer | epoch 0 | step 20430 |avg loss 8.480 |avg tokens 4530.700 |tokens/s 31786.378 |walltime 3004.570 |
Transformer | epoch 0 | step 20440 |avg loss 8.535 |avg tokens 4503.300 |tokens/s 31444.408 |walltime 3006.002 |
Transformer | epoch 0 | step 20450 |avg loss 8.280 |avg tokens 4594.700 |tokens/s 31367.838 |walltime 3007.467 |
Transformer | epoch 0 | step 20460 |avg loss 8.379 |avg tokens 4638.200 |tokens/s 31058.671 |walltime 3008.960 |
Transformer | epoch 0 | step 20470 |avg loss 8.289 |avg tokens 4563.200 |tokens/s 31120.553 |walltime 3010.426 |
Transformer | epoch 0 | step 20480 |avg loss 8.113 |avg tokens 4642.200 |tokens/s 30637.031 |walltime 3011.942 |
Transformer | epoch 0 | step 20490 |avg loss 8.486 |avg tokens 4536.300 |tokens/s 32122.774 |walltime 3013.354 |
Transformer | epoch 0 | step 20500 |avg loss 8.399 |avg tokens 4179.300 |tokens/s 30096.680 |walltime 3014.742 |
Transformer | epoch 0 | step 20510 |avg loss 8.502 |avg tokens 4402.700 |tokens/s 30323.659 |walltime 3016.194 |
Transformer | epoch 0 | step 20520 |avg loss 8.372 |avg tokens 4864.800 |tokens/s 32322.637 |walltime 3017.699 |
Transformer | epoch 0 | step 20530 |avg loss 8.146 |avg tokens 4728.300 |tokens/s 31732.155 |walltime 3019.190 |
Transformer | epoch 0 | step 20540 |avg loss 8.466 |avg tokens 4690.700 |tokens/s 32104.683 |walltime 3020.651 |
Transformer | epoch 0 | step 20550 |avg loss 8.445 |avg tokens 4183.400 |tokens/s 29157.851 |walltime 3022.085 |
Transformer | epoch 0 | step 20560 |avg loss 8.294 |avg tokens 4394.500 |tokens/s 30408.063 |walltime 3023.531 |
Transformer | epoch 0 | step 20570 |avg loss 8.557 |avg tokens 4246.300 |tokens/s 29476.402 |walltime 3024.971 |
Transformer | epoch 0 | step 20580 |avg loss 8.332 |avg tokens 4237.600 |tokens/s 30079.182 |walltime 3026.380 |
Transformer | epoch 0 | step 20590 |avg loss 8.536 |avg tokens 4765.700 |tokens/s 32893.067 |walltime 3027.829 |
Transformer | epoch 0 | step 20600 |avg loss 8.247 |avg tokens 4916.800 |tokens/s 33040.752 |walltime 3029.317 |
Transformer | epoch 0 | step 20610 |avg loss 8.270 |avg tokens 4380.200 |tokens/s 29865.451 |walltime 3030.784 |
Transformer | epoch 0 | step 20620 |avg loss 8.190 |avg tokens 4725.300 |tokens/s 31899.588 |walltime 3032.265 |
Transformer | epoch 0 | step 20630 |avg loss 7.893 |avg tokens 4795.200 |tokens/s 31062.642 |walltime 3033.809 |
Transformer | epoch 0 | step 20640 |avg loss 8.281 |avg tokens 4243.300 |tokens/s 29188.087 |walltime 3035.262 |
Transformer | epoch 0 | step 20650 |avg loss 8.632 |avg tokens 4546.900 |tokens/s 32009.962 |walltime 3036.683 |
Transformer | epoch 0 | step 20660 |avg loss 8.289 |avg tokens 4740.000 |tokens/s 31917.657 |walltime 3038.168 |
Transformer | epoch 0 | step 20670 |avg loss 8.512 |avg tokens 4238.500 |tokens/s 29624.893 |walltime 3039.599 |
Transformer | epoch 0 | step 20680 |avg loss 8.145 |avg tokens 4612.300 |tokens/s 31656.707 |walltime 3041.056 |
Transformer | epoch 0 | step 20690 |avg loss 8.562 |avg tokens 4386.200 |tokens/s 29815.372 |walltime 3042.527 |
Transformer | epoch 0 | step 20700 |avg loss 8.373 |avg tokens 4446.300 |tokens/s 30566.237 |walltime 3043.981 |
Transformer | epoch 0 | step 20710 |avg loss 8.176 |avg tokens 4753.600 |tokens/s 32483.023 |walltime 3045.445 |
Transformer | epoch 0 | step 20720 |avg loss 8.362 |avg tokens 4921.700 |tokens/s 33390.630 |walltime 3046.919 |
Transformer | epoch 0 | step 20730 |avg loss 8.706 |avg tokens 3966.000 |tokens/s 27303.095 |walltime 3048.371 |
Transformer | epoch 0 | step 20740 |avg loss 8.347 |avg tokens 4825.900 |tokens/s 32151.985 |walltime 3049.872 |
Transformer | epoch 0 | step 20750 |avg loss 8.489 |avg tokens 4424.200 |tokens/s 30916.465 |walltime 3051.303 |
Transformer | epoch 0 | step 20760 |avg loss 8.776 |avg tokens 4586.400 |tokens/s 33362.497 |walltime 3052.678 |
Transformer | epoch 0 | step 20770 |avg loss 8.321 |avg tokens 4596.000 |tokens/s 31163.550 |walltime 3054.153 |
Transformer | epoch 0 | step 20780 |avg loss 7.902 |avg tokens 4193.900 |tokens/s 29577.735 |walltime 3055.571 |
Transformer | epoch 0 | step 20790 |avg loss 8.377 |avg tokens 4250.900 |tokens/s 29660.603 |walltime 3057.004 |
Transformer | epoch 0 | step 20800 |avg loss 8.340 |avg tokens 4716.300 |tokens/s 32011.432 |walltime 3058.477 |
Transformer | epoch 0 | step 20810 |avg loss 8.487 |avg tokens 4496.500 |tokens/s 28893.640 |walltime 3060.033 |
Transformer | epoch 0 | step 20820 |avg loss 8.473 |avg tokens 4701.200 |tokens/s 32860.090 |walltime 3061.464 |
Transformer | epoch 0 | step 20830 |avg loss 8.188 |avg tokens 4507.300 |tokens/s 30323.549 |walltime 3062.951 |
Transformer | epoch 0 | step 20840 |avg loss 8.409 |avg tokens 4590.400 |tokens/s 31072.704 |walltime 3064.428 |
Transformer | epoch 0 | step 20850 |avg loss 8.473 |avg tokens 4351.100 |tokens/s 30374.815 |walltime 3065.860 |
Transformer | epoch 0 | step 20860 |avg loss 8.301 |avg tokens 4841.700 |tokens/s 32344.700 |walltime 3067.357 |
Transformer | epoch 0 | step 20870 |avg loss 8.664 |avg tokens 4400.100 |tokens/s 31526.030 |walltime 3068.753 |
Transformer | epoch 0 | step 20880 |avg loss 8.564 |avg tokens 4445.600 |tokens/s 31506.887 |walltime 3070.164 |
Transformer | epoch 0 | step 20890 |avg loss 8.633 |avg tokens 4491.000 |tokens/s 31792.358 |walltime 3071.577 |
Transformer | epoch 0 | step 20900 |avg loss 8.308 |avg tokens 4585.800 |tokens/s 31127.192 |walltime 3073.050 |
Transformer | epoch 0 | step 20910 |avg loss 8.456 |avg tokens 4326.100 |tokens/s 30289.342 |walltime 3074.478 |
Transformer | epoch 0 | step 20920 |avg loss 8.457 |avg tokens 4793.800 |tokens/s 32603.010 |walltime 3075.948 |
Transformer | epoch 0 | step 20930 |avg loss 8.435 |avg tokens 4540.300 |tokens/s 30468.648 |walltime 3077.439 |
Transformer | epoch 0 | step 20940 |avg loss 8.362 |avg tokens 3943.100 |tokens/s 28382.448 |walltime 3078.828 |
Transformer | epoch 0 | step 20950 |avg loss 8.398 |avg tokens 4313.600 |tokens/s 30221.336 |walltime 3080.255 |
Transformer | epoch 0 | step 20960 |avg loss 8.388 |avg tokens 4535.100 |tokens/s 30739.223 |walltime 3081.731 |
Transformer | epoch 0 | step 20970 |avg loss 8.042 |avg tokens 4158.600 |tokens/s 28229.263 |walltime 3083.204 |
Transformer | epoch 0 | step 20980 |avg loss 8.359 |avg tokens 4660.800 |tokens/s 31562.531 |walltime 3084.680 |
Transformer | epoch 0 | step 20990 |avg loss 8.237 |avg tokens 4502.500 |tokens/s 30744.733 |walltime 3086.145 |
Transformer | epoch 0 | step 21000 |avg loss 8.498 |avg tokens 4497.100 |tokens/s 31301.342 |walltime 3087.582 |
Transformer | epoch 0 | step 21010 |avg loss 8.215 |avg tokens 4875.300 |tokens/s 32569.158 |walltime 3089.078 |
Transformer | epoch 0 | step 21020 |avg loss 8.275 |avg tokens 4562.700 |tokens/s 31190.152 |walltime 3090.541 |
Transformer | epoch 0 | step 21030 |avg loss 8.253 |avg tokens 3955.400 |tokens/s 27563.718 |walltime 3091.976 |
Transformer | epoch 0 | step 21040 |avg loss 8.308 |avg tokens 4574.700 |tokens/s 30483.261 |walltime 3093.477 |
Transformer | epoch 0 | step 21050 |avg loss 8.563 |avg tokens 3944.700 |tokens/s 27827.509 |walltime 3094.895 |
Transformer | epoch 0 | step 21060 |avg loss 8.404 |avg tokens 4425.500 |tokens/s 30237.477 |walltime 3096.358 |
Transformer | epoch 0 | step 21070 |avg loss 8.274 |avg tokens 4588.700 |tokens/s 30619.688 |walltime 3097.857 |
Transformer | epoch 0 | step 21080 |avg loss 8.483 |avg tokens 4097.600 |tokens/s 28687.527 |walltime 3099.285 |
Transformer | epoch 0 | step 21090 |avg loss 8.058 |avg tokens 4569.200 |tokens/s 30237.312 |walltime 3100.796 |
Transformer | epoch 0 | step 21100 |avg loss 8.403 |avg tokens 4377.500 |tokens/s 29746.206 |walltime 3102.268 |
Transformer | epoch 0 | step 21110 |avg loss 8.456 |avg tokens 3891.200 |tokens/s 26463.400 |walltime 3103.738 |
Transformer | epoch 0 | step 21120 |avg loss 8.435 |avg tokens 4835.200 |tokens/s 32414.523 |walltime 3105.230 |
Transformer | epoch 0 | step 21130 |avg loss 8.152 |avg tokens 4618.300 |tokens/s 31946.950 |walltime 3106.676 |
Transformer | epoch 0 | step 21140 |avg loss 8.309 |avg tokens 4538.100 |tokens/s 30510.041 |walltime 3108.163 |
Transformer | epoch 0 | step 21150 |avg loss 8.437 |avg tokens 4049.900 |tokens/s 28548.409 |walltime 3109.582 |
Transformer | epoch 0 | step 21160 |avg loss 8.269 |avg tokens 4792.800 |tokens/s 32218.579 |walltime 3111.069 |
Transformer | epoch 0 | step 21170 |avg loss 8.481 |avg tokens 3905.200 |tokens/s 28718.036 |walltime 3112.429 |
Transformer | epoch 0 | step 21180 |avg loss 8.264 |avg tokens 4676.000 |tokens/s 32465.690 |walltime 3113.869 |
Transformer | epoch 0 | step 21190 |avg loss 8.304 |avg tokens 4634.900 |tokens/s 32082.321 |walltime 3115.314 |
Transformer | epoch 0 | step 21200 |avg loss 8.300 |avg tokens 4813.900 |tokens/s 32119.204 |walltime 3116.813 |
Transformer | epoch 0 | step 21210 |avg loss 8.390 |avg tokens 4646.400 |tokens/s 32912.836 |walltime 3118.225 |
Transformer | epoch 0 | step 21220 |avg loss 8.465 |avg tokens 4425.500 |tokens/s 30371.198 |walltime 3119.682 |
Transformer | epoch 0 | step 21230 |avg loss 8.406 |avg tokens 4552.000 |tokens/s 31289.865 |walltime 3121.137 |
Transformer | epoch 0 | step 21240 |avg loss 8.393 |avg tokens 4652.800 |tokens/s 30578.894 |walltime 3122.658 |
Transformer | epoch 0 | step 21250 |avg loss 8.360 |avg tokens 4675.100 |tokens/s 31717.405 |walltime 3124.132 |
Transformer | epoch 0 | step 21260 |avg loss 8.252 |avg tokens 4725.000 |tokens/s 31850.328 |walltime 3125.616 |
Transformer | epoch 0 | step 21270 |avg loss 8.299 |avg tokens 4605.200 |tokens/s 31562.283 |walltime 3127.075 |
Transformer | epoch 0 | step 21280 |avg loss 8.271 |avg tokens 4828.000 |tokens/s 31861.772 |walltime 3128.590 |
Transformer | epoch 0 | step 21290 |avg loss 8.527 |avg tokens 4481.100 |tokens/s 31469.826 |walltime 3130.014 |
Transformer | epoch 0 | step 21300 |avg loss 8.211 |avg tokens 4566.700 |tokens/s 31324.051 |walltime 3131.472 |
Transformer | epoch 0 | step 21310 |avg loss 8.226 |avg tokens 4321.900 |tokens/s 29357.645 |walltime 3132.944 |
Transformer | epoch 0 | step 21320 |avg loss 8.231 |avg tokens 4749.400 |tokens/s 31987.034 |walltime 3134.429 |
Transformer | epoch 0 | step 21330 |avg loss 8.463 |avg tokens 4284.700 |tokens/s 30140.608 |walltime 3135.850 |
Transformer | epoch 0 | step 21340 |avg loss 8.502 |avg tokens 4709.900 |tokens/s 33045.915 |walltime 3137.276 |
Transformer | epoch 0 | step 21350 |avg loss 8.113 |avg tokens 4501.200 |tokens/s 30525.252 |walltime 3138.750 |
Transformer | epoch 0 | step 21360 |avg loss 8.460 |avg tokens 4930.600 |tokens/s 33284.559 |walltime 3140.232 |
Transformer | epoch 0 | step 21370 |avg loss 8.289 |avg tokens 4820.800 |tokens/s 32338.952 |walltime 3141.722 |
Transformer | epoch 0 | step 21380 |avg loss 8.350 |avg tokens 4653.300 |tokens/s 31494.376 |walltime 3143.200 |
Transformer | epoch 0 | step 21390 |avg loss 8.251 |avg tokens 4453.700 |tokens/s 30476.923 |walltime 3144.661 |
Transformer | epoch 0 | step 21400 |avg loss 8.423 |avg tokens 4450.000 |tokens/s 30012.397 |walltime 3146.144 |
Transformer | epoch 0 | step 21410 |avg loss 8.278 |avg tokens 4061.600 |tokens/s 28664.433 |walltime 3147.561 |
Transformer | epoch 0 | step 21420 |avg loss 8.430 |avg tokens 4492.600 |tokens/s 30771.127 |walltime 3149.021 |
Transformer | epoch 0 | step 21430 |avg loss 8.249 |avg tokens 4995.500 |tokens/s 34202.688 |walltime 3150.481 |
Transformer | epoch 0 | step 21440 |avg loss 8.353 |avg tokens 4536.900 |tokens/s 31219.260 |walltime 3151.935 |
Transformer | epoch 0 | step 21450 |avg loss 8.378 |avg tokens 4615.500 |tokens/s 31476.798 |walltime 3153.401 |
Transformer | epoch 0 | step 21460 |avg loss 8.244 |avg tokens 4802.400 |tokens/s 31680.327 |walltime 3154.917 |
Transformer | epoch 0 | step 21470 |avg loss 8.429 |avg tokens 3805.600 |tokens/s 27296.587 |walltime 3156.311 |
Transformer | epoch 0 | step 21480 |avg loss 8.467 |avg tokens 4630.500 |tokens/s 32152.964 |walltime 3157.751 |
Transformer | epoch 0 | step 21490 |avg loss 8.156 |avg tokens 4516.800 |tokens/s 30825.522 |walltime 3159.216 |
Transformer | epoch 0 | step 21500 |avg loss 8.399 |avg tokens 4766.800 |tokens/s 32742.194 |walltime 3160.672 |
Transformer | epoch 0 | step 21510 |avg loss 8.431 |avg tokens 4280.200 |tokens/s 29926.263 |walltime 3162.103 |
Transformer | epoch 0 | step 21520 |avg loss 8.478 |avg tokens 4332.800 |tokens/s 30395.139 |walltime 3163.528 |
Transformer | epoch 0 | step 21530 |avg loss 8.299 |avg tokens 4390.400 |tokens/s 30540.890 |walltime 3164.966 |
Transformer | epoch 0 | step 21540 |avg loss 8.564 |avg tokens 4544.700 |tokens/s 31249.252 |walltime 3166.420 |
Transformer | epoch 0 | step 21550 |avg loss 8.477 |avg tokens 4544.400 |tokens/s 29966.205 |walltime 3167.936 |
Transformer | epoch 0 | step 21560 |avg loss 8.643 |avg tokens 3895.000 |tokens/s 28594.167 |walltime 3169.299 |
Transformer | epoch 0 | step 21570 |avg loss 8.558 |avg tokens 4636.300 |tokens/s 32773.473 |walltime 3170.713 |
Transformer | epoch 0 | step 21580 |avg loss 8.401 |avg tokens 4835.300 |tokens/s 32244.343 |walltime 3172.213 |
Transformer | epoch 0 | step 21590 |avg loss 8.374 |avg tokens 4506.100 |tokens/s 31272.917 |walltime 3173.654 |
Transformer | epoch 0 | step 21600 |avg loss 8.560 |avg tokens 4368.000 |tokens/s 31142.640 |walltime 3175.056 |
Transformer | epoch 0 | step 21610 |avg loss 8.109 |avg tokens 4716.100 |tokens/s 32127.594 |walltime 3176.524 |
Transformer | epoch 0 | step 21620 |avg loss 8.554 |avg tokens 4131.800 |tokens/s 29673.492 |walltime 3177.917 |
Transformer | epoch 0 | step 21630 |avg loss 8.538 |avg tokens 4795.100 |tokens/s 32861.014 |walltime 3179.376 |
Transformer | epoch 0 | step 21640 |avg loss 8.207 |avg tokens 4504.000 |tokens/s 30642.487 |walltime 3180.846 |
Transformer | epoch 0 | step 21650 |avg loss 8.635 |avg tokens 4479.400 |tokens/s 31865.083 |walltime 3182.251 |
Transformer | epoch 0 | step 21660 |avg loss 8.340 |avg tokens 4490.100 |tokens/s 30882.183 |walltime 3183.705 |
Transformer | epoch 0 | step 21670 |avg loss 8.404 |avg tokens 4657.600 |tokens/s 31473.444 |walltime 3185.185 |
Transformer | epoch 0 | step 21680 |avg loss 8.499 |avg tokens 4551.900 |tokens/s 28702.261 |walltime 3186.771 |
Transformer | epoch 0 | step 21690 |avg loss 8.653 |avg tokens 4443.900 |tokens/s 31843.794 |walltime 3188.167 |
Transformer | epoch 0 | step 21700 |avg loss 8.457 |avg tokens 4140.200 |tokens/s 30032.958 |walltime 3189.545 |
Transformer | epoch 0 | step 21710 |avg loss 8.237 |avg tokens 4733.200 |tokens/s 31444.332 |walltime 3191.051 |
Transformer | epoch 0 | step 21720 |avg loss 8.329 |avg tokens 4259.500 |tokens/s 29198.855 |walltime 3192.509 |
Transformer | epoch 0 | step 21730 |avg loss 8.430 |avg tokens 4598.400 |tokens/s 32611.623 |walltime 3193.919 |
Transformer | epoch 0 | step 21740 |avg loss 8.038 |avg tokens 4717.600 |tokens/s 31705.776 |walltime 3195.407 |
Transformer | epoch 0 | step 21750 |avg loss 8.288 |avg tokens 4493.700 |tokens/s 30914.567 |walltime 3196.861 |
Transformer | epoch 0 | step 21760 |avg loss 8.493 |avg tokens 4733.300 |tokens/s 32539.052 |walltime 3198.316 |
Transformer | epoch 0 | step 21770 |avg loss 8.275 |avg tokens 4822.600 |tokens/s 32483.559 |walltime 3199.800 |
Transformer | epoch 0 | step 21780 |avg loss 8.440 |avg tokens 4611.400 |tokens/s 31434.889 |walltime 3201.267 |
Transformer | epoch 0 | step 21790 |avg loss 8.624 |avg tokens 3999.700 |tokens/s 28226.296 |walltime 3202.684 |
Transformer | epoch 0 | step 21800 |avg loss 8.387 |avg tokens 4585.600 |tokens/s 31392.076 |walltime 3204.145 |
Transformer | epoch 0 | step 21810 |avg loss 8.428 |avg tokens 4285.400 |tokens/s 29333.526 |walltime 3205.606 |
Transformer | epoch 0 | step 21820 |avg loss 8.278 |avg tokens 4799.900 |tokens/s 32182.655 |walltime 3207.097 |
Transformer | epoch 0 | step 21830 |avg loss 8.189 |avg tokens 4430.300 |tokens/s 29693.968 |walltime 3208.589 |
Transformer | epoch 0 | step 21840 |avg loss 8.197 |avg tokens 4307.900 |tokens/s 30385.084 |walltime 3210.007 |
Transformer | epoch 0 | step 21850 |avg loss 8.147 |avg tokens 4685.200 |tokens/s 31606.756 |walltime 3211.489 |
Transformer | epoch 0 | step 21860 |avg loss 8.650 |avg tokens 4621.300 |tokens/s 32254.020 |walltime 3212.922 |
Transformer | epoch 0 | step 21870 |avg loss 8.281 |avg tokens 4523.900 |tokens/s 31734.233 |walltime 3214.348 |
Transformer | epoch 0 | step 21880 |avg loss 8.286 |avg tokens 4644.700 |tokens/s 31598.899 |walltime 3215.818 |
Transformer | epoch 0 | step 21890 |avg loss 8.338 |avg tokens 4639.400 |tokens/s 31485.894 |walltime 3217.291 |
Transformer | epoch 0 | step 21900 |avg loss 8.185 |avg tokens 4838.800 |tokens/s 32425.099 |walltime 3218.783 |
Transformer | epoch 0 | step 21910 |avg loss 8.311 |avg tokens 4922.600 |tokens/s 32248.764 |walltime 3220.310 |
Transformer | epoch 0 | step 21920 |avg loss 8.375 |avg tokens 4565.000 |tokens/s 30721.669 |walltime 3221.796 |
Transformer | epoch 0 | step 21930 |avg loss 8.254 |avg tokens 4458.000 |tokens/s 29088.713 |walltime 3223.328 |
Transformer | epoch 0 | step 21940 |avg loss 8.657 |avg tokens 4566.000 |tokens/s 31191.745 |walltime 3224.792 |
Transformer | epoch 0 | step 21950 |avg loss 8.524 |avg tokens 4620.300 |tokens/s 31764.692 |walltime 3226.247 |
Transformer | epoch 0 | step 21960 |avg loss 8.408 |avg tokens 4629.900 |tokens/s 30936.518 |walltime 3227.743 |
Transformer | epoch 0 | step 21970 |avg loss 8.532 |avg tokens 4217.300 |tokens/s 29627.190 |walltime 3229.167 |
Transformer | epoch 0 | step 21980 |avg loss 8.363 |avg tokens 4556.900 |tokens/s 31627.192 |walltime 3230.608 |
Transformer | epoch 0 | step 21990 |avg loss 8.472 |avg tokens 4088.100 |tokens/s 28333.074 |walltime 3232.051 |
Transformer | epoch 0 | step 22000 |avg loss 8.564 |avg tokens 3770.900 |tokens/s 27834.021 |walltime 3233.405 |
Transformer | epoch 0 | step 22010 |avg loss 8.277 |avg tokens 4520.400 |tokens/s 30891.151 |walltime 3234.869 |
Transformer | epoch 0 | step 22020 |avg loss 8.564 |avg tokens 4509.800 |tokens/s 31125.298 |walltime 3236.318 |
Transformer | epoch 0 | step 22030 |avg loss 8.299 |avg tokens 4565.300 |tokens/s 31763.956 |walltime 3237.755 |
Transformer | epoch 0 | step 22040 |avg loss 8.299 |avg tokens 4396.300 |tokens/s 30537.742 |walltime 3239.194 |
Transformer | epoch 0 | step 22050 |avg loss 8.580 |avg tokens 4388.300 |tokens/s 31590.767 |walltime 3240.584 |
Transformer | epoch 0 | step 22060 |avg loss 8.497 |avg tokens 4648.100 |tokens/s 31032.890 |walltime 3242.081 |
Transformer | epoch 0 | step 22070 |avg loss 8.651 |avg tokens 4181.800 |tokens/s 30854.804 |walltime 3243.437 |
Transformer | epoch 0 | step 22080 |avg loss 8.458 |avg tokens 4613.000 |tokens/s 31413.080 |walltime 3244.905 |
Transformer | epoch 0 | step 22090 |avg loss 8.359 |avg tokens 4862.400 |tokens/s 33006.250 |walltime 3246.378 |
Transformer | epoch 0 | step 22100 |avg loss 8.732 |avg tokens 3975.800 |tokens/s 28113.690 |walltime 3247.793 |
Transformer | epoch 0 | step 22110 |avg loss 8.576 |avg tokens 4836.700 |tokens/s 33623.048 |walltime 3249.231 |
Transformer | epoch 0 | step 22120 |avg loss 8.298 |avg tokens 4487.100 |tokens/s 30872.590 |walltime 3250.684 |
Transformer | epoch 0 | step 22130 |avg loss 8.368 |avg tokens 4658.800 |tokens/s 31524.525 |walltime 3252.162 |
Transformer | epoch 0 | step 22140 |avg loss 8.554 |avg tokens 3337.000 |tokens/s 24926.337 |walltime 3253.501 |
Transformer | epoch 0 | step 22150 |avg loss 8.286 |avg tokens 4616.800 |tokens/s 31903.909 |walltime 3254.948 |
Transformer | epoch 0 | step 22160 |avg loss 8.430 |avg tokens 4635.600 |tokens/s 31585.507 |walltime 3256.416 |
Transformer | epoch 0 | step 22170 |avg loss 8.332 |avg tokens 4508.800 |tokens/s 30491.173 |walltime 3257.895 |
Transformer | epoch 0 | step 22180 |avg loss 8.473 |avg tokens 4708.900 |tokens/s 31919.862 |walltime 3259.370 |
Transformer | epoch 0 | step 22190 |avg loss 8.477 |avg tokens 4555.600 |tokens/s 32272.263 |walltime 3260.781 |
Transformer | epoch 0 | step 22200 |avg loss 8.357 |avg tokens 4426.300 |tokens/s 29674.428 |walltime 3262.273 |
Transformer | epoch 0 | step 22210 |avg loss 8.542 |avg tokens 4405.800 |tokens/s 30599.495 |walltime 3263.713 |
Transformer | epoch 0 | step 22220 |avg loss 8.288 |avg tokens 4657.000 |tokens/s 31454.759 |walltime 3265.193 |
Transformer | epoch 0 | step 22230 |avg loss 8.584 |avg tokens 4663.500 |tokens/s 32000.360 |walltime 3266.651 |
Transformer | epoch 0 | step 22240 |avg loss 8.351 |avg tokens 4646.100 |tokens/s 30602.391 |walltime 3268.169 |
Transformer | epoch 0 | step 22250 |avg loss 8.305 |avg tokens 4763.100 |tokens/s 32045.778 |walltime 3269.655 |
Transformer | epoch 0 | step 22260 |avg loss 8.159 |avg tokens 4217.100 |tokens/s 28675.314 |walltime 3271.126 |
Transformer | epoch 0 | step 22270 |avg loss 8.252 |avg tokens 4687.200 |tokens/s 31885.393 |walltime 3272.596 |
Transformer | epoch 0 | step 22280 |avg loss 8.107 |avg tokens 4373.600 |tokens/s 30062.936 |walltime 3274.051 |
Transformer | epoch 0 | step 22290 |avg loss 8.605 |avg tokens 4772.300 |tokens/s 32625.881 |walltime 3275.513 |
Transformer | epoch 0 | step 22300 |avg loss 8.536 |avg tokens 4768.300 |tokens/s 33212.824 |walltime 3276.949 |
Transformer | epoch 0 | step 22310 |avg loss 8.221 |avg tokens 4574.000 |tokens/s 30697.849 |walltime 3278.439 |
Transformer | epoch 0 | step 22320 |avg loss 8.226 |avg tokens 4695.200 |tokens/s 31203.410 |walltime 3279.944 |
Transformer | epoch 0 | step 22330 |avg loss 8.131 |avg tokens 4865.600 |tokens/s 32347.481 |walltime 3281.448 |
Transformer | epoch 0 | step 22340 |avg loss 8.378 |avg tokens 4933.400 |tokens/s 34187.489 |walltime 3282.891 |
Transformer | epoch 0 | step 22350 |avg loss 8.422 |avg tokens 4590.000 |tokens/s 31578.351 |walltime 3284.345 |
Transformer | epoch 0 | step 22360 |avg loss 8.275 |avg tokens 4579.400 |tokens/s 31382.634 |walltime 3285.804 |
Transformer | epoch 0 | step 22370 |avg loss 8.224 |avg tokens 4234.400 |tokens/s 28901.282 |walltime 3287.269 |
Transformer | epoch 0 | step 22380 |avg loss 8.190 |avg tokens 4890.400 |tokens/s 32199.812 |walltime 3288.788 |
Transformer | epoch 0 | step 22390 |avg loss 8.323 |avg tokens 4750.800 |tokens/s 31785.183 |walltime 3290.282 |
Transformer | epoch 0 | step 22400 |avg loss 8.318 |avg tokens 4497.400 |tokens/s 30460.908 |walltime 3291.759 |
Transformer | epoch 0 | step 22410 |avg loss 8.344 |avg tokens 4437.100 |tokens/s 30750.036 |walltime 3293.202 |
Transformer | epoch 0 | step 22420 |avg loss 8.540 |avg tokens 4370.800 |tokens/s 29097.680 |walltime 3294.704 |
Transformer | epoch 0 | step 22430 |avg loss 8.318 |avg tokens 4532.300 |tokens/s 29842.193 |walltime 3296.223 |
Transformer | epoch 0 | step 22440 |avg loss 8.453 |avg tokens 3830.400 |tokens/s 27539.917 |walltime 3297.614 |
Transformer | epoch 0 | step 22450 |avg loss 8.468 |avg tokens 4079.700 |tokens/s 27521.435 |walltime 3299.096 |
Transformer | epoch 0 | step 22460 |avg loss 8.358 |avg tokens 4654.400 |tokens/s 31434.383 |walltime 3300.577 |
Transformer | epoch 0 | step 22470 |avg loss 8.262 |avg tokens 4244.800 |tokens/s 29188.769 |walltime 3302.031 |
Transformer | epoch 0 | step 22480 |avg loss 8.410 |avg tokens 4592.400 |tokens/s 31934.281 |walltime 3303.469 |
Transformer | epoch 0 | step 22490 |avg loss 8.136 |avg tokens 4653.600 |tokens/s 31095.201 |walltime 3304.966 |
Transformer | epoch 0 | step 22500 |avg loss 8.220 |avg tokens 4796.800 |tokens/s 31344.293 |walltime 3306.496 |
Transformer | epoch 0 | step 22510 |avg loss 8.194 |avg tokens 4654.300 |tokens/s 31462.473 |walltime 3307.975 |
Transformer | epoch 0 | step 22520 |avg loss 8.196 |avg tokens 4915.500 |tokens/s 33028.714 |walltime 3309.463 |
Transformer | epoch 0 | step 22530 |avg loss 8.439 |avg tokens 4495.900 |tokens/s 30804.419 |walltime 3310.923 |
Transformer | epoch 0 | step 22540 |avg loss 8.452 |avg tokens 4323.500 |tokens/s 30154.703 |walltime 3312.357 |
Transformer | epoch 0 | step 22550 |avg loss 8.227 |avg tokens 4557.100 |tokens/s 30383.313 |walltime 3313.857 |
Transformer | epoch 0 | step 22560 |avg loss 8.605 |avg tokens 3786.300 |tokens/s 26987.095 |walltime 3315.260 |
Transformer | epoch 0 | step 22570 |avg loss 8.603 |avg tokens 4057.000 |tokens/s 28885.224 |walltime 3316.664 |
Transformer | epoch 0 | step 22580 |avg loss 8.350 |avg tokens 4655.300 |tokens/s 30757.030 |walltime 3318.178 |
Transformer | epoch 0 | step 22590 |avg loss 8.528 |avg tokens 4337.200 |tokens/s 29978.495 |walltime 3319.624 |
Transformer | epoch 0 | step 22600 |avg loss 8.385 |avg tokens 4697.900 |tokens/s 31318.711 |walltime 3321.125 |
Transformer | epoch 0 | step 22610 |avg loss 8.560 |avg tokens 4759.400 |tokens/s 32483.351 |walltime 3322.590 |
Transformer | epoch 0 | step 22620 |avg loss 8.423 |avg tokens 4244.500 |tokens/s 29975.361 |walltime 3324.006 |
Transformer | epoch 0 | step 22630 |avg loss 8.517 |avg tokens 4580.700 |tokens/s 31296.610 |walltime 3325.469 |
Transformer | epoch 0 | step 22640 |avg loss 8.209 |avg tokens 4712.000 |tokens/s 31739.937 |walltime 3326.954 |
Transformer | epoch 0 | step 22650 |avg loss 8.293 |avg tokens 4740.600 |tokens/s 31668.732 |walltime 3328.451 |
Transformer | epoch 0 | step 22660 |avg loss 8.393 |avg tokens 3819.600 |tokens/s 27262.033 |walltime 3329.852 |
Transformer | epoch 0 | step 22670 |avg loss 8.617 |avg tokens 4516.400 |tokens/s 30889.926 |walltime 3331.314 |
Transformer | epoch 0 | step 22680 |avg loss 8.251 |avg tokens 4925.900 |tokens/s 33049.942 |walltime 3332.804 |
Transformer | epoch 0 | step 22690 |avg loss 8.372 |avg tokens 4271.500 |tokens/s 29624.616 |walltime 3334.246 |
Transformer | epoch 0 | step 22700 |avg loss 8.470 |avg tokens 4671.100 |tokens/s 31937.711 |walltime 3335.709 |
Transformer | epoch 0 | step 22710 |avg loss 8.067 |avg tokens 4901.600 |tokens/s 32157.127 |walltime 3337.233 |
Transformer | epoch 0 | step 22720 |avg loss 8.119 |avg tokens 4153.300 |tokens/s 29075.166 |walltime 3338.662 |
Transformer | epoch 0 | step 22730 |avg loss 8.187 |avg tokens 4589.300 |tokens/s 30748.149 |walltime 3340.154 |
Transformer | epoch 0 | step 22740 |avg loss 8.450 |avg tokens 4317.500 |tokens/s 29922.142 |walltime 3341.597 |
Transformer | epoch 0 | step 22750 |avg loss 8.503 |avg tokens 4283.200 |tokens/s 28088.967 |walltime 3343.122 |
Transformer | epoch 0 | step 22760 |avg loss 8.194 |avg tokens 4851.200 |tokens/s 31680.581 |walltime 3344.653 |
Transformer | epoch 0 | step 22770 |avg loss 8.386 |avg tokens 4566.300 |tokens/s 31529.833 |walltime 3346.102 |
Transformer | epoch 0 | step 22780 |avg loss 8.206 |avg tokens 4556.000 |tokens/s 30524.436 |walltime 3347.594 |
Transformer | epoch 0 | step 22790 |avg loss 8.468 |avg tokens 4336.300 |tokens/s 31011.563 |walltime 3348.992 |
Transformer | epoch 0 | step 22800 |avg loss 8.553 |avg tokens 4287.500 |tokens/s 30517.382 |walltime 3350.397 |
Transformer | epoch 0 | step 22810 |avg loss 8.264 |avg tokens 4431.700 |tokens/s 30765.944 |walltime 3351.838 |
Transformer | epoch 0 | step 22820 |avg loss 8.060 |avg tokens 4925.600 |tokens/s 33353.358 |walltime 3353.315 |
Transformer | epoch 0 | step 22830 |avg loss 8.325 |avg tokens 4109.300 |tokens/s 29029.452 |walltime 3354.730 |
Transformer | epoch 0 | step 22840 |avg loss 8.364 |avg tokens 4541.900 |tokens/s 31266.859 |walltime 3356.183 |
Transformer | epoch 0 | step 22850 |avg loss 8.160 |avg tokens 4847.200 |tokens/s 32479.188 |walltime 3357.675 |
Transformer | epoch 0 | step 22860 |avg loss 8.408 |avg tokens 4887.900 |tokens/s 33558.376 |walltime 3359.132 |
Transformer | epoch 0 | step 22870 |avg loss 8.209 |avg tokens 4887.300 |tokens/s 32271.872 |walltime 3360.646 |
Transformer | epoch 0 | step 22880 |avg loss 8.256 |avg tokens 4400.100 |tokens/s 30601.240 |walltime 3362.084 |
Transformer | epoch 0 | step 22890 |avg loss 8.405 |avg tokens 4180.800 |tokens/s 29814.858 |walltime 3363.486 |
Transformer | epoch 0 | step 22900 |avg loss 8.194 |avg tokens 4490.500 |tokens/s 30536.985 |walltime 3364.957 |
Transformer | epoch 0 | step 22910 |avg loss 8.194 |avg tokens 4359.000 |tokens/s 29920.018 |walltime 3366.414 |
Transformer | epoch 0 | step 22920 |avg loss 8.262 |avg tokens 4698.100 |tokens/s 32248.216 |walltime 3367.871 |
Transformer | epoch 0 | step 22930 |avg loss 8.264 |avg tokens 4818.900 |tokens/s 32505.576 |walltime 3369.353 |
Transformer | epoch 0 | step 22940 |avg loss 8.431 |avg tokens 4847.700 |tokens/s 33180.417 |walltime 3370.814 |
Transformer | epoch 0 | step 22950 |avg loss 8.269 |avg tokens 4820.300 |tokens/s 31593.882 |walltime 3372.340 |
Transformer | epoch 0 | step 22960 |avg loss 8.539 |avg tokens 4381.700 |tokens/s 30980.188 |walltime 3373.754 |
Transformer | epoch 0 | step 22970 |avg loss 8.458 |avg tokens 4347.400 |tokens/s 29880.180 |walltime 3375.209 |
Transformer | epoch 0 | step 22980 |avg loss 8.455 |avg tokens 4248.400 |tokens/s 29052.690 |walltime 3376.671 |
Transformer | epoch 0 | step 22990 |avg loss 8.272 |avg tokens 4505.500 |tokens/s 30915.141 |walltime 3378.129 |
Transformer | epoch 0 | step 23000 |avg loss 8.588 |avg tokens 4311.300 |tokens/s 30814.798 |walltime 3379.528 |
Transformer | epoch 0 | step 23010 |avg loss 8.321 |avg tokens 4285.600 |tokens/s 30106.963 |walltime 3380.951 |
Transformer | epoch 0 | step 23020 |avg loss 8.653 |avg tokens 4184.000 |tokens/s 29771.787 |walltime 3382.357 |
Transformer | epoch 0 | step 23030 |avg loss 8.529 |avg tokens 4025.800 |tokens/s 28463.325 |walltime 3383.771 |
Transformer | epoch 0 | step 23040 |avg loss 8.549 |avg tokens 4336.300 |tokens/s 29959.112 |walltime 3385.218 |
Transformer | epoch 0 | step 23050 |avg loss 8.349 |avg tokens 4623.500 |tokens/s 31448.035 |walltime 3386.689 |
Transformer | epoch 0 | step 23060 |avg loss 8.259 |avg tokens 4705.300 |tokens/s 31306.486 |walltime 3388.192 |
Transformer | epoch 0 | step 23070 |avg loss 8.255 |avg tokens 4551.700 |tokens/s 30483.993 |walltime 3389.685 |
Transformer | epoch 0 | step 23080 |avg loss 8.259 |avg tokens 4768.700 |tokens/s 31896.980 |walltime 3391.180 |
Transformer | epoch 0 | step 23090 |avg loss 8.204 |avg tokens 4312.900 |tokens/s 29901.152 |walltime 3392.622 |
Transformer | epoch 0 | step 23100 |avg loss 7.927 |avg tokens 4707.300 |tokens/s 31333.770 |walltime 3394.125 |
Transformer | epoch 0 | step 23110 |avg loss 8.237 |avg tokens 4498.300 |tokens/s 30438.103 |walltime 3395.602 |
Transformer | epoch 0 | step 23120 |avg loss 8.264 |avg tokens 4648.400 |tokens/s 31672.429 |walltime 3397.070 |
Transformer | epoch 0 | step 23130 |avg loss 8.367 |avg tokens 4616.500 |tokens/s 30715.967 |walltime 3398.573 |
Transformer | epoch 0 | step 23140 |avg loss 8.504 |avg tokens 4856.200 |tokens/s 33061.547 |walltime 3400.042 |
Transformer | epoch 0 | step 23150 |avg loss 8.349 |avg tokens 4152.800 |tokens/s 29197.478 |walltime 3401.464 |
Transformer | epoch 0 | step 23160 |avg loss 8.505 |avg tokens 4484.100 |tokens/s 31096.560 |walltime 3402.906 |
Transformer | epoch 0 | step 23170 |avg loss 8.094 |avg tokens 4418.100 |tokens/s 29377.575 |walltime 3404.410 |
Transformer | epoch 0 | step 23180 |avg loss 8.307 |avg tokens 4790.400 |tokens/s 32171.508 |walltime 3405.899 |
Transformer | epoch 0 | step 23190 |avg loss 8.210 |avg tokens 4638.500 |tokens/s 30447.108 |walltime 3407.423 |
Transformer | epoch 0 | step 23200 |avg loss 8.531 |avg tokens 4336.400 |tokens/s 30378.897 |walltime 3408.850 |
Transformer | epoch 0 | step 23210 |avg loss 8.401 |avg tokens 4776.300 |tokens/s 32503.972 |walltime 3410.319 |
Transformer | epoch 0 | step 23220 |avg loss 8.439 |avg tokens 4165.200 |tokens/s 29578.800 |walltime 3411.728 |
Transformer | epoch 0 | step 23230 |avg loss 8.212 |avg tokens 4203.700 |tokens/s 29929.157 |walltime 3413.132 |
Transformer | epoch 0 | step 23240 |avg loss 8.447 |avg tokens 4403.200 |tokens/s 30229.743 |walltime 3414.589 |
Transformer | epoch 0 | step 23250 |avg loss 8.361 |avg tokens 3910.800 |tokens/s 27984.085 |walltime 3415.986 |
Transformer | epoch 0 | step 23260 |avg loss 8.567 |avg tokens 4121.300 |tokens/s 29893.955 |walltime 3417.365 |
Transformer | epoch 0 | step 23270 |avg loss 8.299 |avg tokens 4755.200 |tokens/s 31453.359 |walltime 3418.877 |
Transformer | epoch 0 | step 23280 |avg loss 8.507 |avg tokens 4435.300 |tokens/s 30203.372 |walltime 3420.345 |
Transformer | epoch 0 | step 23290 |avg loss 8.260 |avg tokens 4555.100 |tokens/s 30767.869 |walltime 3421.826 |
Transformer | epoch 0 | step 23300 |avg loss 8.271 |avg tokens 4508.800 |tokens/s 30333.933 |walltime 3423.312 |
Transformer | epoch 0 | step 23310 |avg loss 8.350 |avg tokens 4574.600 |tokens/s 32425.180 |walltime 3424.723 |
Transformer | epoch 0 | step 23320 |avg loss 8.226 |avg tokens 4694.700 |tokens/s 31654.069 |walltime 3426.206 |
Transformer | epoch 0 | step 23330 |avg loss 8.471 |avg tokens 4394.800 |tokens/s 30277.580 |walltime 3427.658 |
Transformer | epoch 0 | step 23340 |avg loss 8.341 |avg tokens 4970.200 |tokens/s 33054.393 |walltime 3429.161 |
Transformer | epoch 0 | step 23350 |avg loss 8.255 |avg tokens 4766.400 |tokens/s 31896.764 |walltime 3430.656 |
Transformer | epoch 0 | step 23360 |avg loss 8.276 |avg tokens 4831.200 |tokens/s 32252.076 |walltime 3432.153 |
Transformer | epoch 0 | step 23370 |avg loss 8.373 |avg tokens 4948.300 |tokens/s 34023.839 |walltime 3433.608 |
Transformer | epoch 0 | step 23380 |avg loss 8.433 |avg tokens 4474.600 |tokens/s 31125.489 |walltime 3435.045 |
Transformer | epoch 0 | step 23390 |avg loss 8.500 |avg tokens 4126.800 |tokens/s 29659.324 |walltime 3436.437 |
Transformer | epoch 0 | step 23400 |avg loss 8.277 |avg tokens 4533.400 |tokens/s 30732.821 |walltime 3437.912 |
Transformer | epoch 0 | step 23410 |avg loss 8.305 |avg tokens 4467.900 |tokens/s 29803.527 |walltime 3439.411 |
Transformer | epoch 0 | step 23420 |avg loss 8.151 |avg tokens 4707.000 |tokens/s 31004.893 |walltime 3440.929 |
Transformer | epoch 0 | step 23430 |avg loss 8.484 |avg tokens 4928.900 |tokens/s 33107.334 |walltime 3442.418 |
Transformer | epoch 0 | step 23440 |avg loss 8.005 |avg tokens 3999.500 |tokens/s 29767.804 |walltime 3443.762 |
Transformer | epoch 0 | step 23450 |avg loss 8.263 |avg tokens 4054.900 |tokens/s 28478.434 |walltime 3445.185 |
Transformer | epoch 0 | step 23460 |avg loss 8.241 |avg tokens 4422.000 |tokens/s 30303.342 |walltime 3446.645 |
Transformer | epoch 0 | step 23470 |avg loss 8.267 |avg tokens 4404.800 |tokens/s 30165.419 |walltime 3448.105 |
Transformer | epoch 0 | step 23480 |avg loss 8.315 |avg tokens 4660.400 |tokens/s 31083.127 |walltime 3449.604 |
Transformer | epoch 0 | step 23490 |avg loss 8.329 |avg tokens 4931.600 |tokens/s 32872.250 |walltime 3451.104 |
Transformer | epoch 0 | step 23500 |avg loss 8.263 |avg tokens 4398.900 |tokens/s 29854.386 |walltime 3452.578 |
Transformer | epoch 0 | step 23510 |avg loss 8.043 |avg tokens 4567.900 |tokens/s 29700.774 |walltime 3454.116 |
Transformer | epoch 0 | step 23520 |avg loss 8.427 |avg tokens 4264.100 |tokens/s 30062.058 |walltime 3455.534 |
Transformer | epoch 0 | step 23530 |avg loss 8.379 |avg tokens 4446.000 |tokens/s 30507.692 |walltime 3456.992 |
Transformer | epoch 0 | step 23540 |avg loss 8.566 |avg tokens 4702.000 |tokens/s 32217.958 |walltime 3458.451 |
Transformer | epoch 0 | step 23550 |avg loss 8.240 |avg tokens 4753.900 |tokens/s 32326.353 |walltime 3459.922 |
Transformer | epoch 0 | step 23560 |avg loss 8.308 |avg tokens 4466.600 |tokens/s 30456.729 |walltime 3461.388 |
Transformer | epoch 0 | step 23570 |avg loss 8.414 |avg tokens 4142.600 |tokens/s 28751.379 |walltime 3462.829 |
Transformer | epoch 0 | step 23580 |avg loss 8.565 |avg tokens 4578.800 |tokens/s 31152.513 |walltime 3464.299 |
Transformer | epoch 0 | step 23590 |avg loss 8.454 |avg tokens 4593.700 |tokens/s 30903.208 |walltime 3465.785 |
Transformer | epoch 0 | step 23600 |avg loss 8.354 |avg tokens 4675.700 |tokens/s 30043.060 |walltime 3467.342 |
Transformer | epoch 0 | step 23610 |avg loss 8.350 |avg tokens 4590.000 |tokens/s 31194.911 |walltime 3468.813 |
Transformer | epoch 0 | step 23620 |avg loss 7.985 |avg tokens 4106.900 |tokens/s 28425.638 |walltime 3470.258 |
Transformer | epoch 0 | step 23630 |avg loss 8.123 |avg tokens 4730.800 |tokens/s 31033.820 |walltime 3471.782 |
Transformer | epoch 0 | step 23640 |avg loss 8.449 |avg tokens 4034.800 |tokens/s 27497.198 |walltime 3473.250 |
Transformer | epoch 0 | step 23650 |avg loss 8.368 |avg tokens 4812.900 |tokens/s 32119.526 |walltime 3474.748 |
Transformer | epoch 0 | step 23660 |avg loss 8.220 |avg tokens 4796.000 |tokens/s 32653.299 |walltime 3476.217 |
Transformer | epoch 0 | step 23670 |avg loss 8.228 |avg tokens 4988.000 |tokens/s 33006.300 |walltime 3477.728 |
Transformer | epoch 0 | step 23680 |avg loss 8.493 |avg tokens 4420.100 |tokens/s 30488.084 |walltime 3479.178 |
Transformer | epoch 0 | step 23690 |avg loss 8.212 |avg tokens 4718.100 |tokens/s 31825.703 |walltime 3480.660 |
Transformer | epoch 0 | step 23700 |avg loss 8.385 |avg tokens 4220.200 |tokens/s 30022.558 |walltime 3482.066 |
Transformer | epoch 0 | step 23710 |avg loss 8.168 |avg tokens 4405.500 |tokens/s 29386.969 |walltime 3483.565 |
Transformer | epoch 0 | step 23720 |avg loss 8.244 |avg tokens 4832.600 |tokens/s 32605.157 |walltime 3485.047 |
Transformer | epoch 0 | step 23730 |avg loss 8.225 |avg tokens 4923.900 |tokens/s 32834.100 |walltime 3486.547 |
Transformer | epoch 0 | step 23740 |avg loss 8.391 |avg tokens 4630.800 |tokens/s 31450.129 |walltime 3488.019 |
Transformer | epoch 0 | step 23750 |avg loss 8.226 |avg tokens 4435.700 |tokens/s 30017.212 |walltime 3489.497 |
Transformer | epoch 0 | step 23760 |avg loss 8.555 |avg tokens 4303.400 |tokens/s 31114.630 |walltime 3490.880 |
Transformer | epoch 0 | step 23770 |avg loss 8.261 |avg tokens 4915.600 |tokens/s 33294.352 |walltime 3492.357 |
Transformer | epoch 0 | step 23780 |avg loss 8.190 |avg tokens 4739.300 |tokens/s 31947.122 |walltime 3493.840 |
Transformer | epoch 0 | step 23790 |avg loss 8.460 |avg tokens 4237.200 |tokens/s 29240.261 |walltime 3495.289 |
Transformer | epoch 0 | step 23800 |avg loss 8.375 |avg tokens 4592.500 |tokens/s 31934.585 |walltime 3496.727 |
Transformer | epoch 0 | step 23810 |avg loss 8.290 |avg tokens 4113.400 |tokens/s 28858.965 |walltime 3498.153 |
Transformer | epoch 0 | step 23820 |avg loss 8.367 |avg tokens 4189.700 |tokens/s 29207.018 |walltime 3499.587 |
Transformer | epoch 0 | step 23830 |avg loss 8.641 |avg tokens 3828.200 |tokens/s 29284.696 |walltime 3500.894 |
Transformer | epoch 0 | step 23840 |avg loss 8.605 |avg tokens 4560.800 |tokens/s 31674.056 |walltime 3502.334 |
Transformer | epoch 0 | step 23850 |avg loss 8.400 |avg tokens 4630.500 |tokens/s 31938.520 |walltime 3503.784 |
Transformer | epoch 0 | step 23860 |avg loss 8.209 |avg tokens 4922.600 |tokens/s 32539.918 |walltime 3505.297 |
Transformer | epoch 0 | step 23870 |avg loss 8.091 |avg tokens 4789.600 |tokens/s 32136.723 |walltime 3506.787 |
Transformer | epoch 0 | step 23880 |avg loss 8.212 |avg tokens 4608.400 |tokens/s 32084.723 |walltime 3508.224 |
Transformer | epoch 0 | step 23890 |avg loss 8.039 |avg tokens 4340.500 |tokens/s 30322.559 |walltime 3509.655 |
Transformer | epoch 0 | step 23900 |avg loss 8.557 |avg tokens 4097.100 |tokens/s 28678.053 |walltime 3511.084 |
Transformer | epoch 0 | step 23910 |avg loss 8.450 |avg tokens 4427.100 |tokens/s 30662.688 |walltime 3512.528 |
Transformer | epoch 0 | step 23920 |avg loss 8.484 |avg tokens 4467.100 |tokens/s 31660.252 |walltime 3513.938 |
Transformer | epoch 0 | step 23930 |avg loss 8.256 |avg tokens 4654.100 |tokens/s 30654.589 |walltime 3515.457 |
Transformer | epoch 0 | step 23940 |avg loss 8.823 |avg tokens 3670.900 |tokens/s 27733.929 |walltime 3516.780 |
Transformer | epoch 0 | step 23950 |avg loss 8.152 |avg tokens 4466.500 |tokens/s 29844.078 |walltime 3518.277 |
Transformer | epoch 0 | step 23960 |avg loss 8.419 |avg tokens 4608.400 |tokens/s 31795.745 |walltime 3519.726 |
Transformer | epoch 0 | step 23970 |avg loss 8.180 |avg tokens 3823.500 |tokens/s 27143.153 |walltime 3521.135 |
Transformer | epoch 0 | step 23980 |avg loss 8.427 |avg tokens 4299.400 |tokens/s 28906.519 |walltime 3522.622 |
Transformer | epoch 0 | step 23990 |avg loss 8.416 |avg tokens 4550.800 |tokens/s 32229.441 |walltime 3524.034 |
Transformer | epoch 0 | step 24000 |avg loss 8.532 |avg tokens 4470.700 |tokens/s 31206.434 |walltime 3525.467 |
Transformer | epoch 0 | step 24010 |avg loss 8.320 |avg tokens 4544.400 |tokens/s 31338.401 |walltime 3526.917 |
Transformer | epoch 0 | step 24020 |avg loss 8.144 |avg tokens 4230.400 |tokens/s 30028.436 |walltime 3528.326 |
Transformer | epoch 0 | step 24030 |avg loss 8.205 |avg tokens 4884.800 |tokens/s 31638.612 |walltime 3529.870 |
Transformer | epoch 0 | step 24040 |avg loss 8.476 |avg tokens 4625.700 |tokens/s 32550.762 |walltime 3531.291 |
Transformer | epoch 0 | step 24050 |avg loss 8.314 |avg tokens 4369.500 |tokens/s 30413.945 |walltime 3532.728 |
Transformer | epoch 0 | step 24060 |avg loss 8.180 |avg tokens 4814.100 |tokens/s 31792.221 |walltime 3534.242 |
Transformer | epoch 0 | step 24070 |avg loss 8.247 |avg tokens 4556.900 |tokens/s 30997.554 |walltime 3535.712 |
Transformer | epoch 0 | step 24080 |avg loss 8.472 |avg tokens 4312.400 |tokens/s 29059.622 |walltime 3537.196 |
Transformer | epoch 0 | step 24090 |avg loss 8.361 |avg tokens 4645.300 |tokens/s 31182.667 |walltime 3538.686 |
Transformer | epoch 0 | step 24100 |avg loss 8.400 |avg tokens 4602.600 |tokens/s 31839.701 |walltime 3540.131 |
Transformer | epoch 0 | step 24110 |avg loss 8.346 |avg tokens 4426.700 |tokens/s 27716.338 |walltime 3541.728 |
Transformer | epoch 0 | step 24120 |avg loss 8.175 |avg tokens 4714.900 |tokens/s 31726.011 |walltime 3543.214 |
Transformer | epoch 0 | step 24130 |avg loss 8.573 |avg tokens 4632.400 |tokens/s 32623.673 |walltime 3544.634 |
Transformer | epoch 0 | step 24140 |avg loss 8.398 |avg tokens 4149.700 |tokens/s 29288.553 |walltime 3546.051 |
Transformer | epoch 0 | step 24150 |avg loss 8.092 |avg tokens 4505.500 |tokens/s 30897.102 |walltime 3547.509 |
Transformer | epoch 0 | step 24160 |avg loss 8.293 |avg tokens 4662.400 |tokens/s 32144.926 |walltime 3548.960 |
Transformer | epoch 0 | step 24170 |avg loss 8.344 |avg tokens 4424.100 |tokens/s 31628.160 |walltime 3550.359 |
Transformer | epoch 0 | step 24180 |avg loss 8.627 |avg tokens 4028.900 |tokens/s 28026.907 |walltime 3551.796 |
Transformer | epoch 0 | step 24190 |avg loss 8.643 |avg tokens 4380.600 |tokens/s 30980.568 |walltime 3553.210 |
Transformer | epoch 0 | step 24200 |avg loss 8.108 |avg tokens 4539.200 |tokens/s 30959.840 |walltime 3554.676 |
Transformer | epoch 0 | step 24210 |avg loss 8.227 |avg tokens 4745.400 |tokens/s 31505.136 |walltime 3556.183 |
Transformer | epoch 0 | step 24220 |avg loss 8.376 |avg tokens 4635.500 |tokens/s 32059.464 |walltime 3557.628 |
Transformer | epoch 0 | step 24230 |avg loss 8.156 |avg tokens 4801.000 |tokens/s 32121.168 |walltime 3559.123 |
Transformer | epoch 0 | step 24240 |avg loss 8.163 |avg tokens 4771.800 |tokens/s 31892.488 |walltime 3560.619 |
Transformer | epoch 0 | step 24250 |avg loss 8.632 |avg tokens 4426.400 |tokens/s 30188.509 |walltime 3562.086 |
Transformer | epoch 0 | step 24260 |avg loss 8.342 |avg tokens 4613.500 |tokens/s 31961.314 |walltime 3563.529 |
Transformer | epoch 0 | step 24270 |avg loss 8.529 |avg tokens 4594.000 |tokens/s 31134.995 |walltime 3565.005 |
Transformer | epoch 0 | step 24280 |avg loss 8.525 |avg tokens 4240.500 |tokens/s 30054.616 |walltime 3566.416 |
Transformer | epoch 0 | step 24290 |avg loss 8.167 |avg tokens 4393.600 |tokens/s 30293.093 |walltime 3567.866 |
Transformer | epoch 0 | step 24300 |avg loss 8.311 |avg tokens 4059.700 |tokens/s 28483.019 |walltime 3569.291 |
Transformer | epoch 0 | step 24310 |avg loss 8.465 |avg tokens 4268.100 |tokens/s 30805.555 |walltime 3570.677 |
Transformer | epoch 0 | step 24320 |avg loss 8.111 |avg tokens 4854.400 |tokens/s 33274.103 |walltime 3572.136 |
Transformer | epoch 0 | step 24330 |avg loss 8.199 |avg tokens 4623.000 |tokens/s 32219.321 |walltime 3573.570 |
Transformer | epoch 0 | step 24340 |avg loss 8.236 |avg tokens 4057.600 |tokens/s 28388.791 |walltime 3575.000 |
Transformer | epoch 0 | step 24350 |avg loss 8.511 |avg tokens 4735.500 |tokens/s 33588.381 |walltime 3576.410 |
Transformer | epoch 0 | step 24360 |avg loss 8.190 |avg tokens 4776.900 |tokens/s 30626.098 |walltime 3577.969 |
Transformer | epoch 0 | step 24370 |avg loss 8.460 |avg tokens 4128.000 |tokens/s 29206.801 |walltime 3579.383 |
Transformer | epoch 0 | step 24380 |avg loss 8.235 |avg tokens 4683.100 |tokens/s 31182.741 |walltime 3580.885 |
Transformer | epoch 0 | step 24390 |avg loss 8.007 |avg tokens 4669.900 |tokens/s 31412.810 |walltime 3582.371 |
Transformer | epoch 0 | step 24400 |avg loss 8.284 |avg tokens 4408.900 |tokens/s 29554.176 |walltime 3583.863 |
Transformer | epoch 0 | step 24410 |avg loss 8.182 |avg tokens 4542.000 |tokens/s 30098.133 |walltime 3585.372 |
Transformer | epoch 0 | step 24420 |avg loss 8.216 |avg tokens 4522.400 |tokens/s 29679.464 |walltime 3586.896 |
Transformer | epoch 0 | step 24430 |avg loss 8.483 |avg tokens 4861.500 |tokens/s 33393.544 |walltime 3588.352 |
Transformer | epoch 0 | step 24440 |avg loss 8.266 |avg tokens 4205.400 |tokens/s 30185.988 |walltime 3589.745 |
Transformer | epoch 0 | step 24450 |avg loss 8.429 |avg tokens 3792.000 |tokens/s 27038.372 |walltime 3591.147 |
Transformer | epoch 0 | step 24460 |avg loss 8.147 |avg tokens 4863.400 |tokens/s 32525.084 |walltime 3592.643 |
Transformer | epoch 0 | step 24470 |avg loss 8.262 |avg tokens 4356.000 |tokens/s 29506.765 |walltime 3594.119 |
Transformer | epoch 0 | step 24480 |avg loss 8.102 |avg tokens 4812.800 |tokens/s 32446.812 |walltime 3595.602 |
Transformer | epoch 0 | step 24490 |avg loss 8.527 |avg tokens 4370.800 |tokens/s 30641.071 |walltime 3597.029 |
Transformer | epoch 0 | step 24500 |avg loss 8.310 |avg tokens 4727.900 |tokens/s 32049.966 |walltime 3598.504 |
Transformer | epoch 0 | step 24510 |avg loss 8.264 |avg tokens 4805.300 |tokens/s 32592.806 |walltime 3599.978 |
Transformer | epoch 0 | step 24520 |avg loss 8.399 |avg tokens 4404.000 |tokens/s 30195.008 |walltime 3601.437 |
Transformer | epoch 0 | step 24530 |avg loss 8.499 |avg tokens 4546.100 |tokens/s 32067.363 |walltime 3602.854 |
Transformer | epoch 0 | step 24540 |avg loss 8.554 |avg tokens 4323.300 |tokens/s 29163.012 |walltime 3604.337 |
Transformer | epoch 0 | step 24550 |avg loss 8.187 |avg tokens 4640.800 |tokens/s 30969.358 |walltime 3605.835 |
Transformer | epoch 0 | step 24560 |avg loss 8.407 |avg tokens 4436.300 |tokens/s 30992.728 |walltime 3607.267 |
Transformer | epoch 0 | step 24570 |avg loss 8.345 |avg tokens 4861.500 |tokens/s 32500.029 |walltime 3608.763 |
Transformer | epoch 0 | step 24580 |avg loss 8.339 |avg tokens 4500.300 |tokens/s 31136.220 |walltime 3610.208 |
Transformer | epoch 0 | step 24590 |avg loss 8.301 |avg tokens 4549.400 |tokens/s 31921.919 |walltime 3611.633 |
Transformer | epoch 0 | step 24600 |avg loss 8.619 |avg tokens 4211.700 |tokens/s 30192.374 |walltime 3613.028 |
Transformer | epoch 0 | step 24610 |avg loss 8.224 |avg tokens 4508.100 |tokens/s 32045.082 |walltime 3614.435 |
Transformer | epoch 0 | step 24620 |avg loss 8.136 |avg tokens 4876.700 |tokens/s 32447.711 |walltime 3615.938 |
Transformer | epoch 0 | step 24630 |avg loss 8.091 |avg tokens 4522.600 |tokens/s 31605.262 |walltime 3617.369 |
Transformer | epoch 0 | step 24640 |avg loss 8.253 |avg tokens 4410.200 |tokens/s 30244.394 |walltime 3618.827 |
Transformer | epoch 0 | step 24650 |avg loss 8.346 |avg tokens 3819.700 |tokens/s 27598.840 |walltime 3620.211 |
Transformer | epoch 0 | step 24660 |avg loss 8.046 |avg tokens 4449.000 |tokens/s 30665.962 |walltime 3621.662 |
Transformer | epoch 0 | step 24670 |avg loss 8.297 |avg tokens 4304.300 |tokens/s 29019.320 |walltime 3623.145 |
Transformer | epoch 0 | step 24680 |avg loss 8.459 |avg tokens 4591.400 |tokens/s 31501.052 |walltime 3624.603 |
Transformer | epoch 0 | step 24690 |avg loss 8.310 |avg tokens 4438.100 |tokens/s 30325.494 |walltime 3626.066 |
Transformer | epoch 0 | step 24700 |avg loss 8.503 |avg tokens 4458.900 |tokens/s 31192.117 |walltime 3627.495 |
Transformer | epoch 0 | step 24710 |avg loss 8.298 |avg tokens 4735.700 |tokens/s 31209.962 |walltime 3629.013 |
Transformer | epoch 0 | step 24720 |avg loss 8.413 |avg tokens 4403.300 |tokens/s 30503.432 |walltime 3630.456 |
Transformer | epoch 0 | step 24730 |avg loss 8.344 |avg tokens 4461.600 |tokens/s 30463.216 |walltime 3631.921 |
Transformer | epoch 0 | step 24740 |avg loss 8.373 |avg tokens 4570.600 |tokens/s 30712.743 |walltime 3633.409 |
Transformer | epoch 0 | step 24750 |avg loss 8.269 |avg tokens 4620.600 |tokens/s 31631.224 |walltime 3634.870 |
Transformer | epoch 0 | step 24760 |avg loss 8.263 |avg tokens 4796.700 |tokens/s 32324.252 |walltime 3636.354 |
Transformer | epoch 0 | step 24770 |avg loss 8.156 |avg tokens 4979.600 |tokens/s 32661.502 |walltime 3637.878 |
Transformer | epoch 0 | step 24780 |avg loss 8.231 |avg tokens 4803.700 |tokens/s 31673.509 |walltime 3639.395 |
Transformer | epoch 0 | step 24790 |avg loss 8.579 |avg tokens 4295.700 |tokens/s 29360.608 |walltime 3640.858 |
Transformer | epoch 0 | step 24800 |avg loss 8.490 |avg tokens 4229.000 |tokens/s 29267.814 |walltime 3642.303 |
Transformer | epoch 0 | step 24810 |avg loss 8.223 |avg tokens 4481.600 |tokens/s 30767.174 |walltime 3643.760 |
Transformer | epoch 0 | step 24820 |avg loss 8.328 |avg tokens 4530.400 |tokens/s 31137.032 |walltime 3645.215 |
Transformer | epoch 0 | step 24830 |avg loss 8.388 |avg tokens 4421.900 |tokens/s 30804.394 |walltime 3646.650 |
Transformer | epoch 0 | step 24840 |avg loss 8.430 |avg tokens 4439.200 |tokens/s 30161.070 |walltime 3648.122 |
Transformer | epoch 0 | step 24850 |avg loss 8.532 |avg tokens 4680.900 |tokens/s 31704.630 |walltime 3649.598 |
Transformer | epoch 0 | step 24860 |avg loss 8.300 |avg tokens 4464.500 |tokens/s 30087.490 |walltime 3651.082 |
Transformer | epoch 0 | step 24870 |avg loss 8.400 |avg tokens 4709.700 |tokens/s 32029.907 |walltime 3652.553 |
Transformer | epoch 0 | step 24880 |avg loss 8.260 |avg tokens 4634.200 |tokens/s 31704.052 |walltime 3654.014 |
Transformer | epoch 0 | step 24890 |avg loss 7.957 |avg tokens 4775.900 |tokens/s 31474.642 |walltime 3655.532 |
Transformer | epoch 0 | step 24900 |avg loss 8.313 |avg tokens 4684.500 |tokens/s 31357.529 |walltime 3657.026 |
Transformer | epoch 0 | step 24910 |avg loss 8.302 |avg tokens 4804.900 |tokens/s 32092.164 |walltime 3658.523 |
Transformer | epoch 0 | step 24920 |avg loss 8.222 |avg tokens 4672.200 |tokens/s 31932.890 |walltime 3659.986 |
Transformer | epoch 0 | step 24930 |avg loss 8.434 |avg tokens 4401.100 |tokens/s 28803.381 |walltime 3661.514 |
Transformer | epoch 0 | step 24940 |avg loss 8.350 |avg tokens 4753.800 |tokens/s 32561.943 |walltime 3662.974 |
Transformer | epoch 0 | step 24950 |avg loss 8.257 |avg tokens 4439.200 |tokens/s 30121.927 |walltime 3664.448 |
Transformer | epoch 0 | step 24960 |avg loss 8.253 |avg tokens 4265.200 |tokens/s 29461.973 |walltime 3665.895 |
Transformer | epoch 0 | step 24970 |avg loss 8.361 |avg tokens 4837.100 |tokens/s 32663.912 |walltime 3667.376 |
Transformer | epoch 0 | step 24980 |avg loss 8.624 |avg tokens 4843.100 |tokens/s 34142.245 |walltime 3668.795 |
Transformer | epoch 0 | step 24990 |avg loss 8.359 |avg tokens 4830.900 |tokens/s 31863.388 |walltime 3670.311 |
Transformer | epoch 0 | step 25000 |avg loss 8.285 |avg tokens 4557.700 |tokens/s 31646.821 |walltime 3671.751 |
Transformer | epoch 0 | step 25010 |avg loss 8.300 |avg tokens 4517.600 |tokens/s 30804.441 |walltime 3673.218 |
Transformer | epoch 0 | step 25020 |avg loss 8.164 |avg tokens 4844.400 |tokens/s 32253.086 |walltime 3674.720 |
Transformer | epoch 0 | step 25030 |avg loss 8.422 |avg tokens 4524.400 |tokens/s 30760.699 |walltime 3676.191 |
Transformer | epoch 0 | step 25040 |avg loss 8.521 |avg tokens 4007.300 |tokens/s 28292.293 |walltime 3677.607 |
Transformer | epoch 0 | step 25050 |avg loss 8.463 |avg tokens 4662.400 |tokens/s 31553.034 |walltime 3679.085 |
Transformer | epoch 0 | step 25060 |avg loss 8.339 |avg tokens 4164.000 |tokens/s 29805.415 |walltime 3680.482 |
Transformer | epoch 0 | step 25070 |avg loss 8.312 |avg tokens 4739.300 |tokens/s 31507.004 |walltime 3681.986 |
Transformer | epoch 0 | step 25080 |avg loss 8.724 |avg tokens 4430.200 |tokens/s 32899.146 |walltime 3683.332 |
Transformer | epoch 0 | step 25090 |avg loss 8.630 |avg tokens 3667.600 |tokens/s 26999.328 |walltime 3684.691 |
Transformer | epoch 0 | step 25100 |avg loss 8.298 |avg tokens 4125.400 |tokens/s 28863.584 |walltime 3686.120 |
Transformer | epoch 0 | step 25110 |avg loss 8.380 |avg tokens 4558.200 |tokens/s 31249.599 |walltime 3687.579 |
Transformer | epoch 0 | step 25120 |avg loss 8.413 |avg tokens 4436.000 |tokens/s 31040.012 |walltime 3689.008 |
Transformer | epoch 0 | step 25130 |avg loss 8.852 |avg tokens 4930.400 |tokens/s 34839.191 |walltime 3690.423 |
Transformer | epoch 0 | step 25140 |avg loss 8.457 |avg tokens 4526.900 |tokens/s 31056.885 |walltime 3691.881 |
Transformer | epoch 0 | step 25150 |avg loss 8.535 |avg tokens 3991.300 |tokens/s 28110.190 |walltime 3693.301 |
Transformer | epoch 0 | step 25160 |avg loss 8.383 |avg tokens 4279.500 |tokens/s 30094.817 |walltime 3694.723 |
Transformer | epoch 0 | step 25170 |avg loss 8.494 |avg tokens 4781.500 |tokens/s 33123.250 |walltime 3696.166 |
Transformer | epoch 0 | step 25180 |avg loss 8.295 |avg tokens 4757.600 |tokens/s 32327.514 |walltime 3697.638 |
Transformer | epoch 0 | step 25190 |avg loss 8.724 |avg tokens 4627.800 |tokens/s 33305.074 |walltime 3699.027 |
Transformer | epoch 0 | step 25200 |avg loss 8.219 |avg tokens 4258.400 |tokens/s 29177.872 |walltime 3700.487 |
Transformer | epoch 0 | step 25210 |avg loss 8.125 |avg tokens 4871.200 |tokens/s 33137.914 |walltime 3701.957 |
Transformer | epoch 0 | step 25220 |avg loss 8.477 |avg tokens 4300.300 |tokens/s 29281.371 |walltime 3703.425 |
Transformer | epoch 0 | step 25230 |avg loss 8.711 |avg tokens 4319.100 |tokens/s 30579.260 |walltime 3704.838 |
Transformer | epoch 0 | step 25240 |avg loss 8.270 |avg tokens 4603.600 |tokens/s 30503.803 |walltime 3706.347 |
Transformer | epoch 0 | step 25250 |avg loss 8.297 |avg tokens 4915.200 |tokens/s 32269.003 |walltime 3707.870 |
Transformer | epoch 0 | step 25260 |avg loss 8.238 |avg tokens 4599.200 |tokens/s 30795.707 |walltime 3709.364 |
Transformer | epoch 0 | step 25270 |avg loss 8.567 |avg tokens 4558.900 |tokens/s 31892.600 |walltime 3710.793 |
Transformer | epoch 0 | step 25280 |avg loss 8.409 |avg tokens 4131.000 |tokens/s 28873.438 |walltime 3712.224 |
Transformer | epoch 0 | step 25290 |avg loss 8.472 |avg tokens 4359.200 |tokens/s 30279.019 |walltime 3713.664 |
Transformer | epoch 0 | step 25300 |avg loss 8.359 |avg tokens 4624.100 |tokens/s 31753.815 |walltime 3715.120 |
Transformer | epoch 0 | step 25310 |avg loss 8.455 |avg tokens 4722.100 |tokens/s 32703.273 |walltime 3716.564 |
Transformer | epoch 0 | step 25320 |avg loss 8.270 |avg tokens 4533.400 |tokens/s 30691.246 |walltime 3718.041 |
Transformer | epoch 0 | step 25330 |avg loss 8.522 |avg tokens 4585.700 |tokens/s 31066.896 |walltime 3719.517 |
Transformer | epoch 0 | step 25340 |avg loss 8.669 |avg tokens 3884.900 |tokens/s 28585.032 |walltime 3720.876 |
Transformer | epoch 0 | step 25350 |avg loss 8.466 |avg tokens 4475.800 |tokens/s 31524.793 |walltime 3722.296 |
Transformer | epoch 0 | step 25360 |avg loss 8.216 |avg tokens 4821.500 |tokens/s 31651.798 |walltime 3723.819 |
Transformer | epoch 0 | step 25370 |avg loss 8.391 |avg tokens 4203.800 |tokens/s 29009.047 |walltime 3725.268 |
Transformer | epoch 0 | step 25380 |avg loss 8.338 |avg tokens 4555.700 |tokens/s 30765.802 |walltime 3726.749 |
Transformer | epoch 0 | step 25390 |avg loss 8.542 |avg tokens 4758.400 |tokens/s 33537.201 |walltime 3728.168 |
Transformer | epoch 0 | step 25400 |avg loss 8.182 |avg tokens 4794.600 |tokens/s 32529.310 |walltime 3729.642 |
Transformer | epoch 0 | step 25410 |avg loss 8.656 |avg tokens 4508.600 |tokens/s 31629.901 |walltime 3731.067 |
Transformer | epoch 0 | step 25420 |avg loss 8.359 |avg tokens 4711.300 |tokens/s 31386.397 |walltime 3732.568 |
Transformer | epoch 0 | step 25430 |avg loss 8.539 |avg tokens 4128.700 |tokens/s 29391.621 |walltime 3733.973 |
Transformer | epoch 0 | step 25440 |avg loss 8.554 |avg tokens 4371.500 |tokens/s 30675.618 |walltime 3735.398 |
Transformer | epoch 0 | step 25450 |avg loss 8.348 |avg tokens 4219.100 |tokens/s 28770.860 |walltime 3736.864 |
Transformer | epoch 0 | step 25460 |avg loss 8.462 |avg tokens 4242.300 |tokens/s 29989.353 |walltime 3738.279 |
Transformer | epoch 0 | step 25470 |avg loss 8.273 |avg tokens 4502.000 |tokens/s 31095.037 |walltime 3739.727 |
Transformer | epoch 0 | step 25480 |avg loss 8.178 |avg tokens 4385.300 |tokens/s 30123.399 |walltime 3741.183 |
Transformer | epoch 0 | step 25490 |avg loss 8.302 |avg tokens 4399.300 |tokens/s 30254.768 |walltime 3742.637 |
Transformer | epoch 0 | step 25500 |avg loss 8.382 |avg tokens 4671.300 |tokens/s 32166.731 |walltime 3744.089 |
Transformer | epoch 0 | step 25510 |avg loss 8.634 |avg tokens 4085.100 |tokens/s 28934.636 |walltime 3745.501 |
Transformer | epoch 0 | step 25520 |avg loss 8.270 |avg tokens 4458.400 |tokens/s 29586.562 |walltime 3747.008 |
Transformer | epoch 0 | step 25530 |avg loss 8.247 |avg tokens 4484.200 |tokens/s 29786.862 |walltime 3748.513 |
Transformer | epoch 0 | step 25540 |avg loss 8.427 |avg tokens 4480.200 |tokens/s 30661.306 |walltime 3749.974 |
Transformer | epoch 0 | step 25550 |avg loss 8.059 |avg tokens 4742.000 |tokens/s 30863.581 |walltime 3751.511 |
Transformer | epoch 0 | step 25560 |avg loss 8.198 |avg tokens 4231.800 |tokens/s 30210.251 |walltime 3752.912 |
Transformer | epoch 0 | step 25570 |avg loss 8.254 |avg tokens 4499.400 |tokens/s 30842.396 |walltime 3754.370 |
Transformer | epoch 0 | step 25580 |avg loss 8.483 |avg tokens 4590.500 |tokens/s 31717.102 |walltime 3755.818 |
Transformer | epoch 0 | step 25590 |avg loss 8.286 |avg tokens 4464.500 |tokens/s 31289.379 |walltime 3757.245 |
Transformer | epoch 0 | step 25600 |avg loss 8.668 |avg tokens 4132.000 |tokens/s 28786.731 |walltime 3758.680 |
Transformer | epoch 0 | step 25610 |avg loss 8.231 |avg tokens 4424.700 |tokens/s 30562.963 |walltime 3760.128 |
Transformer | epoch 0 | step 25620 |avg loss 8.012 |avg tokens 4524.000 |tokens/s 29884.513 |walltime 3761.642 |
Transformer | epoch 0 | step 25630 |avg loss 8.531 |avg tokens 4175.200 |tokens/s 28984.989 |walltime 3763.082 |
Transformer | epoch 0 | step 25640 |avg loss 8.238 |avg tokens 4570.500 |tokens/s 31201.778 |walltime 3764.547 |
Transformer | epoch 0 | step 25650 |avg loss 8.642 |avg tokens 4093.700 |tokens/s 28870.768 |walltime 3765.965 |
Transformer | epoch 0 | step 25660 |avg loss 8.332 |avg tokens 4639.200 |tokens/s 31447.936 |walltime 3767.440 |
Transformer | epoch 0 | step 25670 |avg loss 8.370 |avg tokens 4726.600 |tokens/s 32387.839 |walltime 3768.899 |
Transformer | epoch 0 | step 25680 |avg loss 8.496 |avg tokens 4250.500 |tokens/s 30413.340 |walltime 3770.297 |
Transformer | epoch 0 | step 25690 |avg loss 8.327 |avg tokens 4597.300 |tokens/s 31927.903 |walltime 3771.737 |
Transformer | epoch 0 | step 25700 |avg loss 8.376 |avg tokens 4627.900 |tokens/s 31865.748 |walltime 3773.189 |
Transformer | epoch 0 | step 25710 |avg loss 8.354 |avg tokens 4888.200 |tokens/s 32473.008 |walltime 3774.695 |
Transformer | epoch 0 | step 25720 |avg loss 8.425 |avg tokens 4208.100 |tokens/s 29018.158 |walltime 3776.145 |
Transformer | epoch 0 | step 25730 |avg loss 8.240 |avg tokens 4768.500 |tokens/s 32201.355 |walltime 3777.626 |
Transformer | epoch 0 | step 25740 |avg loss 8.401 |avg tokens 4719.300 |tokens/s 31729.673 |walltime 3779.113 |
Transformer | epoch 0 | step 25750 |avg loss 8.357 |avg tokens 4229.000 |tokens/s 29299.112 |walltime 3780.556 |
Transformer | epoch 0 | step 25760 |avg loss 8.514 |avg tokens 4810.200 |tokens/s 32430.118 |walltime 3782.040 |
Transformer | epoch 0 | step 25770 |avg loss 8.547 |avg tokens 4566.000 |tokens/s 31731.291 |walltime 3783.478 |
Transformer | epoch 0 | step 25780 |avg loss 8.311 |avg tokens 4337.500 |tokens/s 29885.232 |walltime 3784.930 |
Transformer | epoch 0 | step 25790 |avg loss 8.230 |avg tokens 4782.500 |tokens/s 31727.601 |walltime 3786.437 |
Transformer | epoch 0 | step 25800 |avg loss 8.260 |avg tokens 4582.100 |tokens/s 31237.051 |walltime 3787.904 |
Transformer | epoch 0 | step 25810 |avg loss 8.443 |avg tokens 4549.400 |tokens/s 29866.491 |walltime 3789.427 |
Transformer | epoch 0 | step 25820 |avg loss 8.556 |avg tokens 4246.200 |tokens/s 29497.350 |walltime 3790.867 |
Transformer | epoch 0 | step 25830 |avg loss 8.354 |avg tokens 4847.800 |tokens/s 32098.225 |walltime 3792.377 |
Transformer | epoch 0 | step 25840 |avg loss 8.259 |avg tokens 4341.200 |tokens/s 29718.677 |walltime 3793.838 |
Transformer | epoch 0 | step 25850 |avg loss 8.471 |avg tokens 4386.300 |tokens/s 30018.209 |walltime 3795.299 |
Transformer | epoch 0 | step 25860 |avg loss 8.214 |avg tokens 4413.100 |tokens/s 30393.577 |walltime 3796.751 |
Transformer | epoch 0 | step 25870 |avg loss 8.141 |avg tokens 4831.600 |tokens/s 31398.521 |walltime 3798.290 |
Transformer | epoch 0 | step 25880 |avg loss 8.075 |avg tokens 4746.400 |tokens/s 31095.201 |walltime 3799.816 |
Transformer | epoch 0 | step 25890 |avg loss 8.297 |avg tokens 4425.600 |tokens/s 30870.701 |walltime 3801.250 |
Transformer | epoch 0 | step 25900 |avg loss 8.334 |avg tokens 4755.200 |tokens/s 31539.337 |walltime 3802.758 |
Transformer | epoch 0 | step 25910 |avg loss 8.458 |avg tokens 4807.900 |tokens/s 32964.191 |walltime 3804.216 |
Transformer | epoch 0 | step 25920 |avg loss 8.696 |avg tokens 4087.900 |tokens/s 28086.808 |walltime 3805.672 |
Transformer | epoch 0 | step 25930 |avg loss 8.401 |avg tokens 4707.900 |tokens/s 31508.761 |walltime 3807.166 |
Transformer | epoch 0 | step 25940 |avg loss 8.270 |avg tokens 4730.400 |tokens/s 31727.501 |walltime 3808.657 |
Transformer | epoch 0 | step 25950 |avg loss 8.367 |avg tokens 4736.800 |tokens/s 30600.122 |walltime 3810.205 |
Transformer | epoch 0 | step 25960 |avg loss 8.322 |avg tokens 4511.400 |tokens/s 30402.400 |walltime 3811.689 |
Transformer | epoch 0 | step 25970 |avg loss 8.200 |avg tokens 4806.900 |tokens/s 31253.381 |walltime 3813.227 |
Transformer | epoch 0 | step 25980 |avg loss 8.548 |avg tokens 4140.600 |tokens/s 28594.453 |walltime 3814.675 |
Transformer | epoch 0 | step 25990 |avg loss 8.334 |avg tokens 4459.300 |tokens/s 30793.574 |walltime 3816.123 |
Transformer | epoch 0 | step 26000 |avg loss 8.316 |avg tokens 4599.600 |tokens/s 31415.808 |walltime 3817.587 |
Transformer | epoch 0 | step 26010 |avg loss 8.372 |avg tokens 4850.500 |tokens/s 32124.982 |walltime 3819.097 |
Transformer | epoch 0 | step 26020 |avg loss 8.528 |avg tokens 4189.800 |tokens/s 29619.901 |walltime 3820.511 |
Transformer | epoch 0 | step 26030 |avg loss 8.741 |avg tokens 4051.600 |tokens/s 29051.002 |walltime 3821.906 |
Transformer | epoch 0 | step 26040 |avg loss 8.038 |avg tokens 4989.000 |tokens/s 33446.892 |walltime 3823.398 |
Transformer | epoch 0 | step 26050 |avg loss 8.214 |avg tokens 4871.900 |tokens/s 32130.767 |walltime 3824.914 |
Transformer | epoch 0 | step 26060 |avg loss 8.544 |avg tokens 4266.500 |tokens/s 30891.633 |walltime 3826.295 |
Transformer | epoch 0 | step 26070 |avg loss 8.283 |avg tokens 4708.000 |tokens/s 31789.056 |walltime 3827.776 |
Transformer | epoch 0 | step 26080 |avg loss 8.332 |avg tokens 4741.600 |tokens/s 32709.778 |walltime 3829.226 |
Transformer | epoch 0 | step 26090 |avg loss 8.439 |avg tokens 4528.000 |tokens/s 29914.230 |walltime 3830.739 |
Transformer | epoch 0 | step 26100 |avg loss 8.236 |avg tokens 4451.400 |tokens/s 30398.926 |walltime 3832.204 |
Transformer | epoch 0 | step 26110 |avg loss 8.283 |avg tokens 4534.500 |tokens/s 30487.700 |walltime 3833.691 |
Transformer | epoch 0 | step 26120 |avg loss 8.499 |avg tokens 4276.800 |tokens/s 30052.939 |walltime 3835.114 |
Transformer | epoch 0 | step 26130 |avg loss 8.541 |avg tokens 4219.600 |tokens/s 28560.049 |walltime 3836.592 |
Transformer | epoch 0 | step 26140 |avg loss 8.541 |avg tokens 4341.000 |tokens/s 30367.649 |walltime 3838.021 |
Transformer | epoch 0 | step 26150 |avg loss 8.179 |avg tokens 4816.800 |tokens/s 32202.823 |walltime 3839.517 |
Transformer | epoch 0 | step 26160 |avg loss 8.640 |avg tokens 3671.700 |tokens/s 26776.883 |walltime 3840.888 |
Transformer | epoch 0 | step 26170 |avg loss 8.459 |avg tokens 4586.900 |tokens/s 31192.564 |walltime 3842.359 |
Transformer | epoch 0 | step 26180 |avg loss 8.480 |avg tokens 4664.800 |tokens/s 30562.977 |walltime 3843.885 |
Transformer | epoch 0 | step 26190 |avg loss 8.476 |avg tokens 4907.000 |tokens/s 32567.188 |walltime 3845.392 |
Transformer | epoch 0 | step 26200 |avg loss 8.403 |avg tokens 4731.400 |tokens/s 31895.853 |walltime 3846.875 |
Transformer | epoch 0 | step 26210 |avg loss 8.653 |avg tokens 4360.400 |tokens/s 30703.411 |walltime 3848.295 |
Transformer | epoch 0 | step 26220 |avg loss 8.363 |avg tokens 4918.000 |tokens/s 33527.697 |walltime 3849.762 |
Transformer | epoch 0 | step 26230 |avg loss 8.479 |avg tokens 4410.800 |tokens/s 29764.396 |walltime 3851.244 |
Transformer | epoch 0 | step 26240 |avg loss 8.493 |avg tokens 4329.100 |tokens/s 30199.943 |walltime 3852.677 |
Transformer | epoch 0 | step 26250 |avg loss 8.311 |avg tokens 4522.500 |tokens/s 31427.505 |walltime 3854.116 |
Transformer | epoch 0 | step 26260 |avg loss 8.468 |avg tokens 4718.000 |tokens/s 31786.519 |walltime 3855.601 |
Transformer | epoch 0 | step 26270 |avg loss 8.516 |avg tokens 4531.900 |tokens/s 31099.464 |walltime 3857.058 |
Transformer | epoch 0 | step 26280 |avg loss 8.518 |avg tokens 4402.400 |tokens/s 30630.385 |walltime 3858.495 |
Transformer | epoch 0 | step 26290 |avg loss 8.358 |avg tokens 4158.200 |tokens/s 29075.712 |walltime 3859.925 |
Transformer | epoch 0 | step 26300 |avg loss 8.260 |avg tokens 4351.600 |tokens/s 29680.928 |walltime 3861.391 |
Transformer | epoch 0 | step 26310 |avg loss 8.329 |avg tokens 3990.600 |tokens/s 27839.398 |walltime 3862.825 |
Transformer | epoch 0 | step 26320 |avg loss 8.117 |avg tokens 4457.800 |tokens/s 30173.008 |walltime 3864.302 |
Transformer | epoch 0 | step 26330 |avg loss 8.691 |avg tokens 4446.200 |tokens/s 32160.475 |walltime 3865.685 |
Transformer | epoch 0 | step 26340 |avg loss 8.364 |avg tokens 4076.900 |tokens/s 28845.819 |walltime 3867.098 |
Transformer | epoch 0 | step 26350 |avg loss 8.348 |avg tokens 4033.100 |tokens/s 29377.823 |walltime 3868.471 |
Transformer | epoch 0 | step 26360 |avg loss 8.406 |avg tokens 4530.200 |tokens/s 31206.285 |walltime 3869.923 |
Transformer | epoch 0 | step 26370 |avg loss 8.269 |avg tokens 4505.700 |tokens/s 30370.707 |walltime 3871.406 |
Transformer | epoch 0 | step 26380 |avg loss 8.124 |avg tokens 4653.200 |tokens/s 31404.742 |walltime 3872.888 |
Transformer | epoch 0 | step 26390 |avg loss 8.315 |avg tokens 4769.600 |tokens/s 32386.539 |walltime 3874.361 |
Transformer | epoch 0 | step 26400 |avg loss 8.111 |avg tokens 4913.800 |tokens/s 33126.805 |walltime 3875.844 |
Transformer | epoch 0 | step 26410 |avg loss 8.484 |avg tokens 4739.900 |tokens/s 32379.330 |walltime 3877.308 |
Transformer | epoch 0 | step 26420 |avg loss 8.272 |avg tokens 4647.200 |tokens/s 31191.158 |walltime 3878.798 |
Transformer | epoch 0 | step 26430 |avg loss 8.314 |avg tokens 4738.100 |tokens/s 32081.757 |walltime 3880.275 |
Transformer | epoch 0 | step 26440 |avg loss 8.507 |avg tokens 3938.900 |tokens/s 28163.660 |walltime 3881.673 |
Transformer | epoch 0 | step 26450 |avg loss 8.444 |avg tokens 4513.800 |tokens/s 31203.608 |walltime 3883.120 |
Transformer | epoch 0 | step 26460 |avg loss 8.338 |avg tokens 4540.800 |tokens/s 30809.768 |walltime 3884.594 |
Transformer | epoch 0 | step 26470 |avg loss 8.484 |avg tokens 4359.700 |tokens/s 30245.567 |walltime 3886.035 |
Transformer | epoch 0 | step 26480 |avg loss 8.647 |avg tokens 4282.400 |tokens/s 29870.386 |walltime 3887.469 |
Transformer | epoch 0 | step 26490 |avg loss 8.594 |avg tokens 4437.200 |tokens/s 30161.489 |walltime 3888.940 |
Transformer | epoch 0 | step 26500 |avg loss 8.043 |avg tokens 4216.100 |tokens/s 29841.325 |walltime 3890.353 |
Transformer | epoch 0 | step 26510 |avg loss 8.515 |avg tokens 4856.600 |tokens/s 33520.695 |walltime 3891.802 |
Transformer | epoch 0 | step 26520 |avg loss 8.291 |avg tokens 4762.100 |tokens/s 31460.826 |walltime 3893.315 |
Transformer | epoch 0 | step 26530 |avg loss 8.275 |avg tokens 4611.500 |tokens/s 30054.313 |walltime 3894.850 |
Transformer | epoch 0 | step 26540 |avg loss 8.463 |avg tokens 4225.100 |tokens/s 28013.154 |walltime 3896.358 |
Transformer | epoch 0 | step 26550 |avg loss 8.264 |avg tokens 4588.100 |tokens/s 31103.357 |walltime 3897.833 |
Transformer | epoch 0 | step 26560 |avg loss 8.288 |avg tokens 4867.600 |tokens/s 32419.270 |walltime 3899.334 |
Transformer | epoch 0 | step 26570 |avg loss 8.249 |avg tokens 4787.300 |tokens/s 31614.756 |walltime 3900.849 |
Transformer | epoch 0 | step 26580 |avg loss 8.519 |avg tokens 4223.900 |tokens/s 29442.818 |walltime 3902.283 |
Transformer | epoch 0 | step 26590 |avg loss 8.326 |avg tokens 4395.100 |tokens/s 30133.874 |walltime 3903.742 |
Transformer | epoch 0 | step 26600 |avg loss 8.316 |avg tokens 4143.900 |tokens/s 29054.839 |walltime 3905.168 |
Transformer | epoch 0 | step 26610 |avg loss 8.421 |avg tokens 4473.100 |tokens/s 30806.476 |walltime 3906.620 |
Transformer | epoch 0 | step 26620 |avg loss 8.429 |avg tokens 4394.200 |tokens/s 30716.512 |walltime 3908.051 |
Transformer | epoch 0 | step 26630 |avg loss 8.281 |avg tokens 4652.800 |tokens/s 32286.504 |walltime 3909.492 |
Transformer | epoch 0 | step 26640 |avg loss 8.203 |avg tokens 4757.500 |tokens/s 32007.332 |walltime 3910.978 |
Transformer | epoch 0 | step 26650 |avg loss 8.135 |avg tokens 4658.000 |tokens/s 30932.640 |walltime 3912.484 |
Transformer | epoch 0 | step 26660 |avg loss 8.222 |avg tokens 4617.600 |tokens/s 30524.577 |walltime 3913.997 |
Transformer | epoch 0 | step 26670 |avg loss 8.625 |avg tokens 4303.200 |tokens/s 29844.785 |walltime 3915.439 |
Transformer | epoch 0 | step 26680 |avg loss 8.439 |avg tokens 4713.900 |tokens/s 31691.167 |walltime 3916.926 |
Transformer | epoch 0 | step 26690 |avg loss 8.282 |avg tokens 4397.300 |tokens/s 30321.504 |walltime 3918.376 |
Transformer | epoch 0 | step 26700 |avg loss 7.748 |avg tokens 4907.000 |tokens/s 32790.151 |walltime 3919.873 |
Transformer | epoch 0 | step 26710 |avg loss 8.443 |avg tokens 4505.800 |tokens/s 30859.789 |walltime 3921.333 |
Transformer | epoch 0 | step 26720 |avg loss 8.349 |avg tokens 4441.900 |tokens/s 30698.270 |walltime 3922.780 |
Transformer | epoch 0 | step 26730 |avg loss 8.195 |avg tokens 4504.800 |tokens/s 30917.814 |walltime 3924.237 |
Transformer | epoch 0 | step 26740 |avg loss 8.433 |avg tokens 3977.500 |tokens/s 28695.047 |walltime 3925.623 |
Transformer | epoch 0 | step 26750 |avg loss 8.225 |avg tokens 4642.100 |tokens/s 30878.273 |walltime 3927.126 |
Transformer | epoch 0 | step 26760 |avg loss 8.360 |avg tokens 4687.900 |tokens/s 31374.916 |walltime 3928.620 |
Transformer | epoch 0 | step 26770 |avg loss 8.358 |avg tokens 4620.000 |tokens/s 31414.671 |walltime 3930.091 |
Transformer | epoch 0 | step 26780 |avg loss 8.260 |avg tokens 4715.700 |tokens/s 31451.753 |walltime 3931.590 |
Transformer | epoch 0 | step 26790 |avg loss 8.432 |avg tokens 4011.000 |tokens/s 29195.137 |walltime 3932.964 |
Transformer | epoch 0 | step 26800 |avg loss 8.533 |avg tokens 4593.700 |tokens/s 32063.458 |walltime 3934.397 |
Transformer | epoch 0 | step 26810 |avg loss 8.321 |avg tokens 4484.500 |tokens/s 31057.950 |walltime 3935.841 |
Transformer | epoch 0 | step 26820 |avg loss 8.305 |avg tokens 4719.000 |tokens/s 31850.178 |walltime 3937.323 |
Transformer | epoch 0 | step 26830 |avg loss 8.099 |avg tokens 4729.800 |tokens/s 31903.393 |walltime 3938.805 |
Transformer | epoch 0 | step 26840 |avg loss 8.296 |avg tokens 4717.800 |tokens/s 31954.102 |walltime 3940.282 |
Transformer | epoch 0 | step 26850 |avg loss 8.196 |avg tokens 4516.800 |tokens/s 30494.837 |walltime 3941.763 |
Transformer | epoch 0 | step 26860 |avg loss 8.587 |avg tokens 4374.900 |tokens/s 30349.175 |walltime 3943.204 |
Transformer | epoch 0 | step 26870 |avg loss 8.407 |avg tokens 4313.000 |tokens/s 30323.867 |walltime 3944.627 |
Transformer | epoch 0 | step 26880 |avg loss 8.295 |avg tokens 4460.200 |tokens/s 29985.827 |walltime 3946.114 |
Transformer | epoch 0 | step 26890 |avg loss 8.371 |avg tokens 3869.400 |tokens/s 26652.176 |walltime 3947.566 |
Transformer | epoch 0 | step 26900 |avg loss 8.116 |avg tokens 4551.200 |tokens/s 31433.000 |walltime 3949.014 |
Transformer | epoch 0 | step 26910 |avg loss 8.423 |avg tokens 4485.800 |tokens/s 30706.204 |walltime 3950.475 |
Transformer | epoch 0 | step 26920 |avg loss 8.151 |avg tokens 4706.400 |tokens/s 31733.538 |walltime 3951.958 |
Transformer | epoch 0 | step 26930 |avg loss 8.451 |avg tokens 4433.600 |tokens/s 31653.072 |walltime 3953.358 |
Transformer | epoch 0 | step 26940 |avg loss 8.292 |avg tokens 4423.300 |tokens/s 30263.399 |walltime 3954.820 |
Transformer | epoch 0 | step 26950 |avg loss 8.659 |avg tokens 4724.500 |tokens/s 33015.396 |walltime 3956.251 |
Transformer | epoch 0 | step 26960 |avg loss 8.401 |avg tokens 4587.900 |tokens/s 31397.326 |walltime 3957.712 |
Transformer | epoch 0 | step 26970 |avg loss 8.484 |avg tokens 4494.600 |tokens/s 31857.880 |walltime 3959.123 |
Transformer | epoch 0 | step 26980 |avg loss 8.640 |avg tokens 4429.500 |tokens/s 31766.913 |walltime 3960.517 |
Transformer | epoch 0 | step 26990 |avg loss 8.248 |avg tokens 4724.800 |tokens/s 31658.068 |walltime 3962.010 |
Transformer | epoch 0 | step 27000 |avg loss 8.204 |avg tokens 4836.800 |tokens/s 32740.041 |walltime 3963.487 |
Transformer | epoch 0 | step 27010 |avg loss 8.349 |avg tokens 4327.900 |tokens/s 29722.951 |walltime 3964.943 |
Transformer | epoch 0 | step 27020 |avg loss 8.265 |avg tokens 4744.800 |tokens/s 31668.710 |walltime 3966.442 |
Transformer | epoch 0 | step 27030 |avg loss 8.394 |avg tokens 4543.600 |tokens/s 31341.448 |walltime 3967.891 |
Transformer | epoch 0 | step 27040 |avg loss 8.372 |avg tokens 4656.900 |tokens/s 31344.003 |walltime 3969.377 |
Transformer | epoch 0 | step 27050 |avg loss 8.420 |avg tokens 3857.300 |tokens/s 28807.165 |walltime 3970.716 |
Transformer | epoch 0 | step 27060 |avg loss 8.341 |avg tokens 4541.800 |tokens/s 30504.889 |walltime 3972.205 |
Transformer | epoch 0 | step 27070 |avg loss 8.318 |avg tokens 4758.700 |tokens/s 31704.862 |walltime 3973.706 |
Transformer | epoch 0 | step 27080 |avg loss 8.223 |avg tokens 4898.400 |tokens/s 32454.560 |walltime 3975.215 |
Transformer | epoch 0 | step 27090 |avg loss 8.404 |avg tokens 4250.800 |tokens/s 29901.111 |walltime 3976.637 |
Transformer | epoch 0 | step 27100 |avg loss 8.524 |avg tokens 4029.600 |tokens/s 27982.165 |walltime 3978.077 |
Transformer | epoch 0 | step 27110 |avg loss 8.469 |avg tokens 4701.700 |tokens/s 31493.010 |walltime 3979.570 |
Transformer | epoch 0 | step 27120 |avg loss 8.197 |avg tokens 4356.500 |tokens/s 29960.110 |walltime 3981.024 |
Transformer | epoch 0 | step 27130 |avg loss 8.204 |avg tokens 4620.200 |tokens/s 31031.674 |walltime 3982.513 |
Transformer | epoch 0 | step 27140 |avg loss 8.087 |avg tokens 4341.600 |tokens/s 29607.338 |walltime 3983.979 |
Transformer | epoch 0 | step 27150 |avg loss 8.387 |avg tokens 4321.100 |tokens/s 28915.233 |walltime 3985.474 |
Transformer | epoch 0 | step 27160 |avg loss 8.613 |avg tokens 4508.600 |tokens/s 30840.295 |walltime 3986.936 |
Transformer | epoch 0 | step 27170 |avg loss 8.488 |avg tokens 4182.200 |tokens/s 29730.875 |walltime 3988.342 |
Transformer | epoch 0 | step 27180 |avg loss 8.710 |avg tokens 4239.800 |tokens/s 30742.280 |walltime 3989.721 |
Transformer | epoch 0 | step 27190 |avg loss 8.608 |avg tokens 3651.800 |tokens/s 26369.570 |walltime 3991.106 |
Transformer | epoch 0 | step 27200 |avg loss 8.209 |avg tokens 4903.200 |tokens/s 32687.366 |walltime 3992.606 |
Transformer | epoch 0 | step 27210 |avg loss 8.282 |avg tokens 4642.400 |tokens/s 31802.848 |walltime 3994.066 |
Transformer | epoch 0 | step 27220 |avg loss 8.303 |avg tokens 4737.000 |tokens/s 31832.129 |walltime 3995.554 |
Transformer | epoch 0 | step 27230 |avg loss 8.491 |avg tokens 4271.000 |tokens/s 29600.714 |walltime 3996.997 |
Transformer | epoch 0 | step 27240 |avg loss 8.552 |avg tokens 4224.600 |tokens/s 29875.483 |walltime 3998.411 |
Transformer | epoch 0 | step 27250 |avg loss 8.380 |avg tokens 4577.600 |tokens/s 31494.958 |walltime 3999.864 |
Transformer | epoch 0 | step 27260 |avg loss 8.301 |avg tokens 4402.800 |tokens/s 31104.525 |walltime 4001.280 |
Transformer | epoch 0 | step 27270 |avg loss 8.445 |avg tokens 4498.500 |tokens/s 30934.260 |walltime 4002.734 |
Transformer | epoch 0 | step 27280 |avg loss 8.302 |avg tokens 4734.700 |tokens/s 31191.487 |walltime 4004.252 |
Transformer | epoch 0 | step 27290 |avg loss 8.311 |avg tokens 4658.700 |tokens/s 32036.601 |walltime 4005.706 |
Transformer | epoch 0 | step 27300 |avg loss 8.303 |avg tokens 4834.600 |tokens/s 32978.293 |walltime 4007.172 |
Transformer | epoch 0 | step 27310 |avg loss 8.642 |avg tokens 3953.300 |tokens/s 29243.876 |walltime 4008.524 |
Transformer | epoch 0 | step 27320 |avg loss 8.399 |avg tokens 4355.500 |tokens/s 29673.803 |walltime 4009.992 |
Transformer | epoch 0 | step 27330 |avg loss 8.353 |avg tokens 4838.800 |tokens/s 32611.096 |walltime 4011.476 |
Transformer | epoch 0 | step 27340 |avg loss 8.490 |avg tokens 4852.600 |tokens/s 32810.835 |walltime 4012.955 |
Transformer | epoch 0 | step 27350 |avg loss 8.433 |avg tokens 4809.600 |tokens/s 32385.614 |walltime 4014.440 |
Transformer | epoch 0 | step 27360 |avg loss 8.421 |avg tokens 4688.500 |tokens/s 31866.761 |walltime 4015.911 |
Transformer | epoch 0 | step 27370 |avg loss 8.353 |avg tokens 4259.800 |tokens/s 30129.500 |walltime 4017.325 |
Transformer | epoch 0 | step 27380 |avg loss 8.332 |avg tokens 4494.000 |tokens/s 30767.587 |walltime 4018.786 |
Transformer | epoch 0 | step 27390 |avg loss 8.208 |avg tokens 4917.600 |tokens/s 32761.452 |walltime 4020.287 |
Transformer | epoch 0 | step 27400 |avg loss 8.358 |avg tokens 4746.000 |tokens/s 31812.369 |walltime 4021.778 |
Transformer | epoch 0 | step 27410 |avg loss 8.514 |avg tokens 4829.700 |tokens/s 33521.533 |walltime 4023.219 |
Transformer | epoch 0 | step 27420 |avg loss 8.331 |avg tokens 4732.600 |tokens/s 32079.012 |walltime 4024.695 |
Transformer | epoch 0 | step 27430 |avg loss 8.578 |avg tokens 4595.100 |tokens/s 31797.913 |walltime 4026.140 |
Transformer | epoch 0 | step 27440 |avg loss 8.209 |avg tokens 4804.800 |tokens/s 31743.239 |walltime 4027.653 |
Transformer | epoch 0 | step 27450 |avg loss 8.408 |avg tokens 4592.100 |tokens/s 31285.300 |walltime 4029.121 |
Transformer | epoch 0 | step 27460 |avg loss 8.333 |avg tokens 4504.700 |tokens/s 31148.052 |walltime 4030.567 |
Transformer | epoch 0 | step 27470 |avg loss 8.456 |avg tokens 4706.800 |tokens/s 32322.031 |walltime 4032.024 |
Transformer | epoch 0 | step 27480 |avg loss 8.166 |avg tokens 4749.600 |tokens/s 31873.358 |walltime 4033.514 |
Transformer | epoch 0 | step 27490 |avg loss 8.170 |avg tokens 4524.800 |tokens/s 30540.718 |walltime 4034.995 |
Transformer | epoch 0 | step 27500 |avg loss 8.333 |avg tokens 3964.600 |tokens/s 27950.385 |walltime 4036.414 |
Transformer | epoch 0 | step 27510 |avg loss 8.284 |avg tokens 4723.400 |tokens/s 31116.594 |walltime 4037.932 |
Transformer | epoch 0 | step 27520 |avg loss 8.257 |avg tokens 4515.800 |tokens/s 30392.181 |walltime 4039.418 |
Transformer | epoch 0 | step 27530 |avg loss 8.418 |avg tokens 4725.300 |tokens/s 31234.286 |walltime 4040.930 |
Transformer | epoch 0 | step 27540 |avg loss 8.404 |avg tokens 4221.400 |tokens/s 29673.364 |walltime 4042.353 |
Transformer | epoch 0 | step 27550 |avg loss 8.513 |avg tokens 3991.200 |tokens/s 28467.909 |walltime 4043.755 |
Transformer | epoch 0 | step 27560 |avg loss 8.288 |avg tokens 4912.700 |tokens/s 32577.398 |walltime 4045.263 |
Transformer | epoch 0 | step 27570 |avg loss 8.325 |avg tokens 4263.600 |tokens/s 29244.937 |walltime 4046.721 |
Transformer | epoch 0 | step 27580 |avg loss 8.320 |avg tokens 4426.500 |tokens/s 29297.939 |walltime 4048.232 |
Transformer | epoch 0 | step 27590 |avg loss 8.187 |avg tokens 4603.200 |tokens/s 31300.512 |walltime 4049.702 |
Transformer | epoch 0 | step 27600 |avg loss 8.382 |avg tokens 4546.100 |tokens/s 30546.762 |walltime 4051.191 |
Transformer | epoch 0 | step 27610 |avg loss 8.075 |avg tokens 4965.700 |tokens/s 33092.811 |walltime 4052.691 |
Transformer | epoch 0 | step 27620 |avg loss 8.271 |avg tokens 4818.200 |tokens/s 32121.471 |walltime 4054.191 |
Transformer | epoch 0 | step 27630 |avg loss 8.389 |avg tokens 4487.900 |tokens/s 31155.092 |walltime 4055.632 |
Transformer | epoch 0 | step 27640 |avg loss 8.356 |avg tokens 4806.100 |tokens/s 32129.388 |walltime 4057.128 |
Transformer | epoch 0 | step 27650 |avg loss 8.116 |avg tokens 4782.000 |tokens/s 31842.931 |walltime 4058.629 |
Transformer | epoch 0 | step 27660 |avg loss 8.411 |avg tokens 4627.100 |tokens/s 32058.894 |walltime 4060.073 |
Transformer | epoch 0 | step 27670 |avg loss 8.356 |avg tokens 4930.300 |tokens/s 33834.468 |walltime 4061.530 |
Transformer | epoch 0 | step 27680 |avg loss 8.324 |avg tokens 4390.100 |tokens/s 30183.678 |walltime 4062.984 |
Transformer | epoch 0 | step 27690 |avg loss 8.173 |avg tokens 4929.700 |tokens/s 32480.147 |walltime 4064.502 |
Transformer | epoch 0 | step 27700 |avg loss 8.348 |avg tokens 4740.000 |tokens/s 32601.698 |walltime 4065.956 |
Transformer | epoch 0 | step 27710 |avg loss 8.037 |avg tokens 4806.900 |tokens/s 31671.134 |walltime 4067.474 |
Transformer | epoch 0 | step 27720 |avg loss 8.632 |avg tokens 4865.700 |tokens/s 34995.327 |walltime 4068.864 |
Transformer | epoch 0 | step 27730 |avg loss 8.492 |avg tokens 3989.800 |tokens/s 29118.277 |walltime 4070.234 |
Transformer | epoch 0 | step 27740 |avg loss 8.170 |avg tokens 4530.400 |tokens/s 31149.072 |walltime 4071.689 |
Transformer | epoch 0 | step 27750 |avg loss 8.271 |avg tokens 4472.700 |tokens/s 31121.873 |walltime 4073.126 |
Transformer | epoch 0 | step 27760 |avg loss 8.421 |avg tokens 4635.400 |tokens/s 32093.658 |walltime 4074.570 |
Transformer | epoch 0 | step 27770 |avg loss 8.403 |avg tokens 4674.700 |tokens/s 31938.095 |walltime 4076.034 |
Transformer | epoch 0 | step 27780 |avg loss 8.204 |avg tokens 4436.800 |tokens/s 30470.445 |walltime 4077.490 |
Transformer | epoch 0 | step 27790 |avg loss 8.410 |avg tokens 4200.300 |tokens/s 29156.753 |walltime 4078.931 |
Transformer | epoch 0 | step 27800 |avg loss 8.380 |avg tokens 4765.700 |tokens/s 32453.499 |walltime 4080.399 |
Transformer | epoch 0 | step 27810 |avg loss 8.505 |avg tokens 4422.100 |tokens/s 30896.715 |walltime 4081.830 |
Transformer | epoch 0 | step 27820 |avg loss 8.493 |avg tokens 3981.500 |tokens/s 28644.045 |walltime 4083.220 |
Transformer | epoch 0 | step 27830 |avg loss 8.349 |avg tokens 4938.000 |tokens/s 33556.688 |walltime 4084.692 |
Transformer | epoch 0 | step 27840 |avg loss 8.348 |avg tokens 4490.200 |tokens/s 30683.837 |walltime 4086.155 |
Transformer | epoch 0 | step 27850 |avg loss 8.301 |avg tokens 4706.700 |tokens/s 31716.649 |walltime 4087.639 |
Transformer | epoch 0 | step 27860 |avg loss 8.570 |avg tokens 4793.500 |tokens/s 32841.775 |walltime 4089.099 |
Transformer | epoch 0 | step 27870 |avg loss 8.150 |avg tokens 4711.300 |tokens/s 31537.686 |walltime 4090.593 |
Transformer | epoch 0 | step 27880 |avg loss 8.408 |avg tokens 4883.000 |tokens/s 34471.035 |walltime 4092.009 |
Transformer | epoch 0 | step 27890 |avg loss 8.311 |avg tokens 4503.800 |tokens/s 31131.713 |walltime 4093.456 |
Transformer | epoch 0 | step 27900 |avg loss 8.290 |avg tokens 4442.300 |tokens/s 30892.443 |walltime 4094.894 |
Transformer | epoch 0 | step 27910 |avg loss 8.660 |avg tokens 4427.300 |tokens/s 31569.962 |walltime 4096.296 |
Transformer | epoch 0 | step 27920 |avg loss 8.386 |avg tokens 4730.900 |tokens/s 31492.246 |walltime 4097.799 |
Transformer | epoch 0 | step 27930 |avg loss 8.429 |avg tokens 4514.800 |tokens/s 31402.955 |walltime 4099.236 |
Transformer | epoch 0 | step 27940 |avg loss 8.327 |avg tokens 4760.600 |tokens/s 31788.663 |walltime 4100.734 |
Transformer | epoch 0 | step 27950 |avg loss 8.363 |avg tokens 4689.300 |tokens/s 31704.117 |walltime 4102.213 |
Transformer | epoch 0 | step 27960 |avg loss 8.226 |avg tokens 4443.000 |tokens/s 31403.448 |walltime 4103.628 |
Transformer | epoch 0 | step 27970 |avg loss 8.587 |avg tokens 4552.400 |tokens/s 31705.006 |walltime 4105.064 |
Transformer | epoch 0 | step 27980 |avg loss 8.313 |avg tokens 4290.600 |tokens/s 29635.735 |walltime 4106.511 |
Transformer | epoch 0 | step 27990 |avg loss 8.319 |avg tokens 4457.900 |tokens/s 30579.022 |walltime 4107.969 |
Transformer | epoch 0 | step 28000 |avg loss 8.364 |avg tokens 4868.900 |tokens/s 33086.303 |walltime 4109.441 |
Transformer | epoch 0 | step 28010 |avg loss 8.461 |avg tokens 4567.700 |tokens/s 30543.506 |walltime 4110.936 |
Transformer | epoch 0 | step 28020 |avg loss 8.388 |avg tokens 4543.900 |tokens/s 31666.904 |walltime 4112.371 |
Transformer | epoch 0 | step 28030 |avg loss 8.329 |avg tokens 4574.200 |tokens/s 30667.491 |walltime 4113.863 |
Transformer | epoch 0 | step 28040 |avg loss 8.418 |avg tokens 4263.900 |tokens/s 30046.406 |walltime 4115.282 |
Transformer | epoch 0 | step 28050 |avg loss 8.574 |avg tokens 4128.200 |tokens/s 29612.403 |walltime 4116.676 |
Transformer | epoch 0 | step 28060 |avg loss 8.421 |avg tokens 4059.200 |tokens/s 29217.785 |walltime 4118.065 |
Transformer | epoch 0 | step 28070 |avg loss 8.405 |avg tokens 4195.400 |tokens/s 28578.004 |walltime 4119.533 |
Transformer | epoch 0 | step 28080 |avg loss 8.365 |avg tokens 4345.300 |tokens/s 31111.214 |walltime 4120.930 |
Transformer | epoch 0 | step 28090 |avg loss 8.274 |avg tokens 4719.400 |tokens/s 32342.434 |walltime 4122.389 |
Transformer | epoch 0 | step 28100 |avg loss 8.359 |avg tokens 4711.000 |tokens/s 32115.385 |walltime 4123.856 |
Transformer | epoch 0 | step 28110 |avg loss 8.555 |avg tokens 4224.100 |tokens/s 30615.331 |walltime 4125.236 |
Transformer | epoch 0 | step 28120 |avg loss 8.651 |avg tokens 4530.100 |tokens/s 32443.992 |walltime 4126.632 |
Transformer | epoch 0 | step 28130 |avg loss 8.510 |avg tokens 4391.900 |tokens/s 29283.094 |walltime 4128.132 |
Transformer | epoch 0 | step 28140 |avg loss 8.403 |avg tokens 4812.600 |tokens/s 32992.672 |walltime 4129.591 |
Transformer | epoch 0 | step 28150 |avg loss 8.295 |avg tokens 4538.300 |tokens/s 30894.505 |walltime 4131.060 |
Transformer | epoch 0 | step 28160 |avg loss 8.301 |avg tokens 4474.600 |tokens/s 31071.588 |walltime 4132.500 |
Transformer | epoch 0 | step 28170 |avg loss 8.379 |avg tokens 4367.400 |tokens/s 30126.944 |walltime 4133.949 |
Transformer | epoch 0 | step 28180 |avg loss 8.630 |avg tokens 4644.000 |tokens/s 32338.714 |walltime 4135.385 |
Transformer | epoch 0 | step 28190 |avg loss 8.499 |avg tokens 4658.200 |tokens/s 32122.543 |walltime 4136.836 |
Transformer | epoch 0 | step 28200 |avg loss 8.319 |avg tokens 4601.700 |tokens/s 31273.359 |walltime 4138.307 |
Transformer | epoch 0 | step 28210 |avg loss 8.556 |avg tokens 4873.200 |tokens/s 33341.896 |walltime 4139.769 |
Transformer | epoch 0 | step 28220 |avg loss 8.427 |avg tokens 4781.100 |tokens/s 31864.443 |walltime 4141.269 |
Transformer | epoch 0 | step 28230 |avg loss 8.459 |avg tokens 3927.600 |tokens/s 28757.623 |walltime 4142.635 |
Transformer | epoch 0 | step 28240 |avg loss 8.517 |avg tokens 3893.900 |tokens/s 27625.312 |walltime 4144.044 |
Transformer | epoch 0 | step 28250 |avg loss 8.718 |avg tokens 3799.300 |tokens/s 25986.845 |walltime 4145.506 |
Transformer | epoch 0 | step 28260 |avg loss 8.477 |avg tokens 3983.000 |tokens/s 28751.520 |walltime 4146.892 |
Transformer | epoch 0 | step 28270 |avg loss 8.409 |avg tokens 4657.800 |tokens/s 31908.549 |walltime 4148.351 |
Transformer | epoch 0 | step 28280 |avg loss 8.247 |avg tokens 4419.800 |tokens/s 29987.636 |walltime 4149.825 |
Transformer | epoch 0 | step 28290 |avg loss 8.479 |avg tokens 4235.800 |tokens/s 28948.175 |walltime 4151.289 |
Transformer | epoch 0 | step 28300 |avg loss 8.316 |avg tokens 4557.800 |tokens/s 30822.550 |walltime 4152.767 |
Transformer | epoch 0 | step 28310 |avg loss 8.242 |avg tokens 4821.900 |tokens/s 32978.114 |walltime 4154.229 |
Transformer | epoch 0 | step 28320 |avg loss 8.386 |avg tokens 4688.500 |tokens/s 31267.208 |walltime 4155.729 |
Transformer | epoch 0 | step 28330 |avg loss 8.413 |avg tokens 3996.900 |tokens/s 28863.577 |walltime 4157.114 |
Transformer | epoch 0 | step 28340 |avg loss 8.360 |avg tokens 4372.400 |tokens/s 30574.295 |walltime 4158.544 |
Transformer | epoch 0 | step 28350 |avg loss 8.154 |avg tokens 4047.600 |tokens/s 28619.815 |walltime 4159.958 |
Transformer | epoch 0 | step 28360 |avg loss 8.535 |avg tokens 4573.800 |tokens/s 31926.747 |walltime 4161.391 |
Transformer | epoch 0 | step 28370 |avg loss 8.228 |avg tokens 4646.700 |tokens/s 31706.470 |walltime 4162.856 |
Transformer | epoch 0 | step 28380 |avg loss 8.502 |avg tokens 4399.900 |tokens/s 30201.155 |walltime 4164.313 |
Transformer | epoch 0 | step 28390 |avg loss 8.466 |avg tokens 4363.200 |tokens/s 30852.148 |walltime 4165.727 |
Transformer | epoch 0 | step 28400 |avg loss 8.478 |avg tokens 4263.800 |tokens/s 29042.413 |walltime 4167.195 |
Transformer | epoch 0 | step 28410 |avg loss 8.426 |avg tokens 4719.400 |tokens/s 31549.636 |walltime 4168.691 |
Transformer | epoch 0 | step 28420 |avg loss 8.377 |avg tokens 4183.600 |tokens/s 29843.594 |walltime 4170.093 |
Transformer | epoch 0 | step 28430 |avg loss 8.603 |avg tokens 4271.100 |tokens/s 31367.306 |walltime 4171.455 |
Transformer | epoch 0 | step 28440 |avg loss 8.401 |avg tokens 4674.800 |tokens/s 31467.769 |walltime 4172.940 |
Transformer | epoch 0 | step 28450 |avg loss 8.457 |avg tokens 4264.800 |tokens/s 30465.709 |walltime 4174.340 |
Transformer | epoch 0 | step 28460 |avg loss 8.623 |avg tokens 4585.100 |tokens/s 32330.853 |walltime 4175.758 |
Transformer | epoch 0 | step 28470 |avg loss 8.500 |avg tokens 3901.300 |tokens/s 28660.839 |walltime 4177.120 |
Transformer | epoch 0 | step 28480 |avg loss 8.512 |avg tokens 4642.600 |tokens/s 32001.676 |walltime 4178.570 |
Transformer | epoch 0 | step 28490 |avg loss 8.224 |avg tokens 4587.600 |tokens/s 31283.296 |walltime 4180.037 |
Transformer | epoch 0 | step 28500 |avg loss 8.377 |avg tokens 4609.800 |tokens/s 30948.402 |walltime 4181.526 |
Transformer | epoch 0 | step 28510 |avg loss 8.518 |avg tokens 4620.000 |tokens/s 31788.239 |walltime 4182.980 |
Transformer | epoch 0 | step 28520 |avg loss 8.309 |avg tokens 4538.800 |tokens/s 31445.487 |walltime 4184.423 |
Transformer | epoch 0 | step 28530 |avg loss 8.680 |avg tokens 4673.600 |tokens/s 33031.659 |walltime 4185.838 |
Transformer | epoch 0 | step 28540 |avg loss 8.309 |avg tokens 4483.500 |tokens/s 31024.325 |walltime 4187.283 |
Transformer | epoch 0 | step 28550 |avg loss 8.387 |avg tokens 4532.600 |tokens/s 31242.357 |walltime 4188.734 |
Transformer | epoch 0 | step 28560 |avg loss 7.953 |avg tokens 4655.700 |tokens/s 30825.559 |walltime 4190.244 |
Transformer | epoch 0 | step 28570 |avg loss 8.444 |avg tokens 4018.200 |tokens/s 28088.636 |walltime 4191.675 |
Transformer | epoch 0 | step 28580 |avg loss 8.671 |avg tokens 4177.700 |tokens/s 29900.819 |walltime 4193.072 |
Transformer | epoch 0 | step 28590 |avg loss 8.696 |avg tokens 3916.900 |tokens/s 28102.377 |walltime 4194.466 |
Transformer | epoch 0 | step 28600 |avg loss 8.576 |avg tokens 4889.200 |tokens/s 32790.179 |walltime 4195.957 |
Transformer | epoch 0 | step 28610 |avg loss 8.393 |avg tokens 4666.700 |tokens/s 32316.688 |walltime 4197.401 |
Transformer | epoch 0 | step 28620 |avg loss 8.700 |avg tokens 4046.500 |tokens/s 28995.305 |walltime 4198.796 |
Transformer | epoch 0 | step 28630 |avg loss 8.550 |avg tokens 4551.200 |tokens/s 31348.179 |walltime 4200.248 |
Transformer | epoch 0 | step 28640 |avg loss 8.176 |avg tokens 4863.200 |tokens/s 31775.550 |walltime 4201.779 |
Transformer | epoch 0 | step 28650 |avg loss 8.630 |avg tokens 4513.900 |tokens/s 31723.439 |walltime 4203.202 |
Transformer | epoch 0 | step 28660 |avg loss 8.300 |avg tokens 4805.000 |tokens/s 31335.846 |walltime 4204.735 |
Transformer | epoch 0 | step 28670 |avg loss 8.721 |avg tokens 3893.600 |tokens/s 27508.738 |walltime 4206.150 |
Transformer | epoch 0 | step 28680 |avg loss 8.440 |avg tokens 4460.400 |tokens/s 31039.706 |walltime 4207.587 |
Transformer | epoch 0 | step 28690 |avg loss 8.430 |avg tokens 4760.600 |tokens/s 31989.883 |walltime 4209.076 |
Transformer | epoch 0 | step 28700 |avg loss 8.340 |avg tokens 4822.100 |tokens/s 33093.006 |walltime 4210.533 |
Transformer | epoch 0 | step 28710 |avg loss 8.282 |avg tokens 4520.200 |tokens/s 31198.179 |walltime 4211.982 |
Transformer | epoch 0 | step 28720 |avg loss 8.429 |avg tokens 4722.800 |tokens/s 33219.998 |walltime 4213.403 |
Transformer | epoch 0 | step 28730 |avg loss 8.556 |avg tokens 4709.400 |tokens/s 32432.726 |walltime 4214.855 |
Transformer | epoch 0 | step 28740 |avg loss 8.449 |avg tokens 4149.700 |tokens/s 29583.626 |walltime 4216.258 |
Transformer | epoch 0 | step 28750 |avg loss 8.640 |avg tokens 4351.600 |tokens/s 31520.822 |walltime 4217.639 |
Transformer | epoch 0 | step 28760 |avg loss 8.169 |avg tokens 4407.600 |tokens/s 30085.171 |walltime 4219.104 |
Transformer | epoch 0 | step 28770 |avg loss 8.334 |avg tokens 4573.600 |tokens/s 31296.395 |walltime 4220.565 |
Transformer | epoch 0 | step 28780 |avg loss 8.434 |avg tokens 4540.500 |tokens/s 31211.542 |walltime 4222.020 |
Transformer | epoch 0 | step 28790 |avg loss 8.386 |avg tokens 4637.400 |tokens/s 31647.311 |walltime 4223.485 |
Transformer | epoch 0 | step 28800 |avg loss 8.282 |avg tokens 4650.400 |tokens/s 31603.036 |walltime 4224.957 |
Transformer | epoch 0 | step 28810 |avg loss 8.234 |avg tokens 4799.400 |tokens/s 32166.725 |walltime 4226.449 |
Transformer | epoch 0 | step 28820 |avg loss 8.450 |avg tokens 4959.300 |tokens/s 33698.404 |walltime 4227.920 |
Transformer | epoch 0 | step 28830 |avg loss 8.373 |avg tokens 4317.800 |tokens/s 29973.039 |walltime 4229.361 |
Transformer | epoch 0 | step 28840 |avg loss 8.232 |avg tokens 4807.800 |tokens/s 31278.244 |walltime 4230.898 |
Transformer | epoch 0 | step 28850 |avg loss 8.523 |avg tokens 4532.600 |tokens/s 30276.670 |walltime 4232.395 |
Transformer | epoch 0 | step 28860 |avg loss 8.712 |avg tokens 4268.100 |tokens/s 30746.239 |walltime 4233.783 |
Transformer | epoch 0 | step 28870 |avg loss 8.377 |avg tokens 4583.000 |tokens/s 31028.868 |walltime 4235.260 |
Transformer | epoch 0 | step 28880 |avg loss 8.580 |avg tokens 4143.200 |tokens/s 28819.995 |walltime 4236.698 |
Transformer | epoch 0 | step 28890 |avg loss 8.282 |avg tokens 4740.900 |tokens/s 31736.995 |walltime 4238.192 |
Transformer | epoch 0 | step 28900 |avg loss 8.382 |avg tokens 4752.100 |tokens/s 32275.345 |walltime 4239.664 |
Transformer | epoch 0 | step 28910 |avg loss 8.297 |avg tokens 4328.800 |tokens/s 29852.270 |walltime 4241.114 |
Transformer | epoch 0 | step 28920 |avg loss 8.500 |avg tokens 4631.800 |tokens/s 31624.881 |walltime 4242.579 |
Transformer | epoch 0 | step 28930 |avg loss 8.490 |avg tokens 4771.700 |tokens/s 32162.002 |walltime 4244.062 |
Transformer | epoch 0 | step 28940 |avg loss 8.290 |avg tokens 4566.500 |tokens/s 30703.964 |walltime 4245.550 |
Transformer | epoch 0 | step 28950 |avg loss 8.529 |avg tokens 4882.600 |tokens/s 32985.673 |walltime 4247.030 |
Transformer | epoch 0 | step 28960 |avg loss 8.235 |avg tokens 4677.100 |tokens/s 32875.660 |walltime 4248.453 |
Transformer | epoch 0 | step 28970 |avg loss 8.407 |avg tokens 4523.200 |tokens/s 31245.030 |walltime 4249.900 |
Transformer | epoch 0 | step 28980 |avg loss 8.427 |avg tokens 4747.900 |tokens/s 32870.566 |walltime 4251.345 |
Transformer | epoch 0 | step 28990 |avg loss 8.463 |avg tokens 4712.800 |tokens/s 33587.957 |walltime 4252.748 |
Transformer | epoch 0 | step 29000 |avg loss 8.396 |avg tokens 4557.100 |tokens/s 31201.182 |walltime 4254.208 |
Transformer | epoch 0 | step 29010 |avg loss 8.494 |avg tokens 4377.900 |tokens/s 30340.024 |walltime 4255.651 |
Transformer | epoch 0 | step 29020 |avg loss 8.335 |avg tokens 4427.300 |tokens/s 29748.438 |walltime 4257.140 |
Transformer | epoch 0 | step 29030 |avg loss 8.512 |avg tokens 4923.200 |tokens/s 33715.487 |walltime 4258.600 |
Transformer | epoch 0 | step 29040 |avg loss 8.320 |avg tokens 4773.200 |tokens/s 32698.388 |walltime 4260.060 |
Transformer | epoch 0 | step 29050 |avg loss 8.546 |avg tokens 4723.600 |tokens/s 32067.348 |walltime 4261.533 |
Transformer | epoch 0 | step 29060 |avg loss 8.349 |avg tokens 4456.300 |tokens/s 30539.803 |walltime 4262.992 |
Transformer | epoch 0 | step 29070 |avg loss 8.287 |avg tokens 4331.000 |tokens/s 30230.091 |walltime 4264.424 |
Transformer | epoch 0 | step 29080 |avg loss 8.432 |avg tokens 4221.600 |tokens/s 29397.550 |walltime 4265.860 |
Transformer | epoch 0 | step 29090 |avg loss 8.365 |avg tokens 4717.600 |tokens/s 31187.188 |walltime 4267.373 |
Transformer | epoch 0 | step 29100 |avg loss 8.396 |avg tokens 4376.700 |tokens/s 30566.221 |walltime 4268.805 |
Transformer | epoch 0 | step 29110 |avg loss 8.442 |avg tokens 4649.700 |tokens/s 31603.399 |walltime 4270.276 |
Transformer | epoch 0 | step 29120 |avg loss 8.502 |avg tokens 4610.600 |tokens/s 32055.078 |walltime 4271.715 |
Transformer | epoch 0 | step 29130 |avg loss 8.699 |avg tokens 4714.700 |tokens/s 33413.298 |walltime 4273.126 |
Transformer | epoch 0 | step 29140 |avg loss 8.533 |avg tokens 4613.500 |tokens/s 31538.161 |walltime 4274.589 |
Transformer | epoch 0 | step 29150 |avg loss 8.662 |avg tokens 4192.600 |tokens/s 30144.026 |walltime 4275.979 |
Transformer | epoch 0 | step 29160 |avg loss 8.217 |avg tokens 4444.500 |tokens/s 30786.797 |walltime 4277.423 |
Transformer | epoch 0 | step 29170 |avg loss 8.250 |avg tokens 4629.000 |tokens/s 30793.157 |walltime 4278.926 |
Transformer | epoch 0 | step 29180 |avg loss 8.422 |avg tokens 4206.900 |tokens/s 29960.123 |walltime 4280.330 |
Transformer | epoch 0 | step 29190 |avg loss 8.536 |avg tokens 4455.200 |tokens/s 30620.003 |walltime 4281.785 |
Transformer | epoch 0 | step 29200 |avg loss 8.484 |avg tokens 4730.100 |tokens/s 31455.669 |walltime 4283.289 |
Transformer | epoch 0 | step 29210 |avg loss 8.347 |avg tokens 4720.000 |tokens/s 32020.887 |walltime 4284.763 |
Transformer | epoch 0 | step 29220 |avg loss 8.499 |avg tokens 4709.300 |tokens/s 32152.574 |walltime 4286.228 |
Transformer | epoch 0 | step 29230 |avg loss 8.292 |avg tokens 4490.800 |tokens/s 31264.948 |walltime 4287.664 |
Transformer | epoch 0 | step 29240 |avg loss 8.475 |avg tokens 4389.900 |tokens/s 31492.936 |walltime 4289.058 |
Transformer | epoch 0 | step 29250 |avg loss 8.366 |avg tokens 4619.500 |tokens/s 30507.134 |walltime 4290.572 |
Transformer | epoch 0 | step 29260 |avg loss 8.622 |avg tokens 4257.100 |tokens/s 29727.255 |walltime 4292.005 |
Transformer | epoch 0 | step 29270 |avg loss 8.389 |avg tokens 4556.800 |tokens/s 30587.959 |walltime 4293.494 |
Transformer | epoch 0 | step 29280 |avg loss 8.576 |avg tokens 4375.900 |tokens/s 30705.621 |walltime 4294.919 |
Transformer | epoch 0 | step 29290 |avg loss 8.433 |avg tokens 4246.900 |tokens/s 29657.999 |walltime 4296.351 |
Transformer | epoch 0 | step 29300 |avg loss 8.392 |avg tokens 4630.300 |tokens/s 31818.234 |walltime 4297.807 |
Transformer | epoch 0 | step 29310 |avg loss 8.252 |avg tokens 4346.000 |tokens/s 30320.843 |walltime 4299.240 |
Transformer | epoch 0 | step 29320 |avg loss 8.344 |avg tokens 4540.000 |tokens/s 30672.246 |walltime 4300.720 |
Transformer | epoch 0 | step 29330 |avg loss 8.332 |avg tokens 4435.800 |tokens/s 30132.727 |walltime 4302.192 |
Transformer | epoch 0 | step 29340 |avg loss 8.478 |avg tokens 4648.200 |tokens/s 32556.367 |walltime 4303.620 |
Transformer | epoch 0 | step 29350 |avg loss 8.249 |avg tokens 4699.200 |tokens/s 31355.376 |walltime 4305.119 |
Transformer | epoch 0 | step 29360 |avg loss 8.466 |avg tokens 4632.200 |tokens/s 30814.644 |walltime 4306.622 |
Transformer | epoch 0 | step 29370 |avg loss 8.452 |avg tokens 4415.800 |tokens/s 30544.939 |walltime 4308.068 |
Transformer | epoch 0 | step 29380 |avg loss 8.447 |avg tokens 4591.600 |tokens/s 31332.069 |walltime 4309.533 |
Transformer | epoch 0 | step 29390 |avg loss 8.354 |avg tokens 4142.800 |tokens/s 29292.571 |walltime 4310.947 |
Transformer | epoch 0 | step 29400 |avg loss 8.483 |avg tokens 4325.500 |tokens/s 29137.870 |walltime 4312.432 |
Transformer | epoch 0 | step 29410 |avg loss 8.296 |avg tokens 4635.200 |tokens/s 30863.903 |walltime 4313.934 |
Transformer | epoch 0 | step 29420 |avg loss 8.392 |avg tokens 4735.700 |tokens/s 31828.396 |walltime 4315.421 |
Transformer | epoch 0 | step 29430 |avg loss 8.087 |avg tokens 4867.200 |tokens/s 32010.087 |walltime 4316.942 |
Transformer | epoch 0 | step 29440 |avg loss 8.371 |avg tokens 4705.500 |tokens/s 31732.514 |walltime 4318.425 |
Transformer | epoch 0 | step 29450 |avg loss 8.257 |avg tokens 4452.600 |tokens/s 30462.497 |walltime 4319.887 |
Transformer | epoch 0 | step 29460 |avg loss 8.262 |avg tokens 4603.600 |tokens/s 32392.539 |walltime 4321.308 |
Transformer | epoch 0 | step 29470 |avg loss 8.171 |avg tokens 4662.400 |tokens/s 32372.792 |walltime 4322.748 |
Transformer | epoch 0 | step 29480 |avg loss 8.546 |avg tokens 4810.200 |tokens/s 33111.839 |walltime 4324.201 |
Transformer | epoch 0 | step 29490 |avg loss 8.282 |avg tokens 4814.900 |tokens/s 32110.771 |walltime 4325.700 |
Transformer | epoch 0 | step 29500 |avg loss 8.431 |avg tokens 4685.000 |tokens/s 32014.547 |walltime 4327.164 |
Transformer | epoch 0 | step 29510 |avg loss 8.244 |avg tokens 4580.700 |tokens/s 30869.290 |walltime 4328.647 |
Transformer | epoch 0 | step 29520 |avg loss 8.352 |avg tokens 4792.600 |tokens/s 31994.518 |walltime 4330.145 |
Transformer | epoch 0 | step 29530 |avg loss 8.439 |avg tokens 4459.500 |tokens/s 31244.125 |walltime 4331.573 |
Transformer | epoch 0 | step 29540 |avg loss 8.378 |avg tokens 3982.400 |tokens/s 28306.245 |walltime 4332.980 |
Transformer | epoch 0 | step 29550 |avg loss 8.529 |avg tokens 4573.000 |tokens/s 31451.252 |walltime 4334.434 |
Transformer | epoch 0 | step 29560 |avg loss 8.442 |avg tokens 4593.200 |tokens/s 31682.747 |walltime 4335.883 |
Transformer | epoch 0 | step 29570 |avg loss 8.259 |avg tokens 4752.700 |tokens/s 31849.975 |walltime 4337.376 |
Transformer | epoch 0 | step 29580 |avg loss 8.419 |avg tokens 4618.100 |tokens/s 30782.563 |walltime 4338.876 |
Transformer | epoch 0 | step 29590 |avg loss 8.326 |avg tokens 4711.800 |tokens/s 31283.860 |walltime 4340.382 |
Transformer | epoch 0 | step 29600 |avg loss 8.222 |avg tokens 4452.900 |tokens/s 30373.188 |walltime 4341.848 |
Transformer | epoch 0 | step 29610 |avg loss 8.624 |avg tokens 4524.600 |tokens/s 31109.531 |walltime 4343.302 |
Transformer | epoch 0 | step 29620 |avg loss 8.427 |avg tokens 4541.100 |tokens/s 30983.743 |walltime 4344.768 |
Transformer | epoch 0 | step 29630 |avg loss 8.389 |avg tokens 4064.000 |tokens/s 27655.821 |walltime 4346.238 |
Transformer | epoch 0 | step 29640 |avg loss 8.431 |avg tokens 4230.000 |tokens/s 30137.298 |walltime 4347.641 |
Transformer | epoch 0 | step 29650 |avg loss 8.391 |avg tokens 4724.000 |tokens/s 32605.794 |walltime 4349.090 |
Transformer | epoch 0 | step 29660 |avg loss 8.160 |avg tokens 4374.400 |tokens/s 30760.317 |walltime 4350.512 |
Transformer | epoch 0 | step 29670 |avg loss 8.482 |avg tokens 4153.400 |tokens/s 29969.267 |walltime 4351.898 |
Transformer | epoch 0 | step 29680 |avg loss 8.775 |avg tokens 4234.600 |tokens/s 30333.279 |walltime 4353.294 |
Transformer | epoch 0 | step 29690 |avg loss 8.259 |avg tokens 4113.900 |tokens/s 28938.969 |walltime 4354.716 |
Transformer | epoch 0 | step 29700 |avg loss 8.351 |avg tokens 4836.700 |tokens/s 32169.406 |walltime 4356.219 |
Transformer | epoch 0 | step 29710 |avg loss 8.495 |avg tokens 4756.200 |tokens/s 32731.687 |walltime 4357.672 |
Transformer | epoch 0 | step 29720 |avg loss 8.469 |avg tokens 3896.300 |tokens/s 27550.540 |walltime 4359.086 |
Transformer | epoch 0 | step 29730 |avg loss 8.590 |avg tokens 4776.700 |tokens/s 32516.998 |walltime 4360.555 |
Transformer | epoch 0 | step 29740 |avg loss 8.285 |avg tokens 4856.300 |tokens/s 32678.534 |walltime 4362.041 |
Transformer | epoch 0 | step 29750 |avg loss 8.190 |avg tokens 4848.800 |tokens/s 32227.066 |walltime 4363.546 |
Transformer | epoch 0 | step 29760 |avg loss 8.605 |avg tokens 4346.600 |tokens/s 29707.617 |walltime 4365.009 |
Transformer | epoch 0 | step 29770 |avg loss 8.647 |avg tokens 4587.700 |tokens/s 32428.050 |walltime 4366.424 |
Transformer | epoch 0 | step 29780 |avg loss 8.398 |avg tokens 4500.200 |tokens/s 30285.047 |walltime 4367.910 |
Transformer | epoch 0 | step 29790 |avg loss 8.629 |avg tokens 4390.100 |tokens/s 30474.992 |walltime 4369.350 |
Transformer | epoch 0 | step 29800 |avg loss 8.215 |avg tokens 4533.600 |tokens/s 31792.270 |walltime 4370.776 |
Transformer | epoch 0 | step 29810 |avg loss 8.414 |avg tokens 4423.000 |tokens/s 30466.307 |walltime 4372.228 |
Transformer | epoch 0 | step 29820 |avg loss 8.324 |avg tokens 4803.200 |tokens/s 31437.643 |walltime 4373.756 |
Transformer | epoch 0 | step 29830 |avg loss 8.353 |avg tokens 4735.800 |tokens/s 31925.992 |walltime 4375.239 |
Transformer | epoch 0 | step 29840 |avg loss 8.252 |avg tokens 4856.200 |tokens/s 31697.445 |walltime 4376.772 |
Transformer | epoch 0 | step 29850 |avg loss 8.097 |avg tokens 4551.800 |tokens/s 30253.800 |walltime 4378.276 |
Transformer | epoch 0 | step 29860 |avg loss 8.530 |avg tokens 4172.400 |tokens/s 28945.899 |walltime 4379.718 |
Transformer | epoch 0 | step 29870 |avg loss 8.278 |avg tokens 4603.100 |tokens/s 30636.952 |walltime 4381.220 |
Transformer | epoch 0 | step 29880 |avg loss 8.375 |avg tokens 4733.600 |tokens/s 31612.051 |walltime 4382.717 |
Transformer | epoch 0 | step 29890 |avg loss 8.584 |avg tokens 4883.900 |tokens/s 33185.884 |walltime 4384.189 |
Transformer | epoch 0 | step 29900 |avg loss 8.302 |avg tokens 4628.000 |tokens/s 31275.434 |walltime 4385.669 |
Transformer | epoch 0 | step 29910 |avg loss 8.530 |avg tokens 4698.400 |tokens/s 31370.704 |walltime 4387.167 |
Transformer | epoch 0 | step 29920 |avg loss 8.527 |avg tokens 4349.800 |tokens/s 30662.294 |walltime 4388.585 |
Transformer | epoch 0 | step 29930 |avg loss 8.294 |avg tokens 4812.700 |tokens/s 31710.011 |walltime 4390.103 |
Transformer | epoch 0 | step 29940 |avg loss 8.155 |avg tokens 4369.500 |tokens/s 28904.077 |walltime 4391.615 |
Transformer | epoch 0 | step 29950 |avg loss 8.418 |avg tokens 4512.600 |tokens/s 30698.641 |walltime 4393.085 |
Transformer | epoch 0 | step 29960 |avg loss 8.556 |avg tokens 4456.600 |tokens/s 31021.945 |walltime 4394.521 |
Transformer | epoch 0 | step 29970 |avg loss 8.218 |avg tokens 4649.800 |tokens/s 31326.478 |walltime 4396.005 |
Transformer | epoch 0 | step 29980 |avg loss 8.595 |avg tokens 4283.100 |tokens/s 30899.825 |walltime 4397.392 |
Transformer | epoch 0 | step 29990 |avg loss 8.413 |avg tokens 4596.800 |tokens/s 31144.395 |walltime 4398.868 |
Transformer | epoch 0 | step 30000 |avg loss 8.807 |avg tokens 4297.700 |tokens/s 31113.332 |walltime 4400.249 |
Transformer | epoch 0 | step 30010 |avg loss 8.437 |avg tokens 4403.600 |tokens/s 29778.884 |walltime 4401.728 |
Transformer | epoch 0 | step 30020 |avg loss 8.539 |avg tokens 4488.100 |tokens/s 30864.333 |walltime 4403.182 |
Transformer | epoch 0 | step 30030 |avg loss 8.227 |avg tokens 4661.300 |tokens/s 31717.502 |walltime 4404.651 |
Transformer | epoch 0 | step 30040 |avg loss 8.813 |avg tokens 4725.100 |tokens/s 32777.289 |walltime 4406.093 |
Transformer | epoch 0 | step 30050 |avg loss 8.517 |avg tokens 4201.100 |tokens/s 26351.809 |walltime 4407.687 |
Transformer | epoch 0 | step 30060 |avg loss 8.331 |avg tokens 4724.100 |tokens/s 31199.484 |walltime 4409.201 |
Transformer | epoch 0 | step 30070 |avg loss 8.491 |avg tokens 3884.800 |tokens/s 27531.818 |walltime 4410.612 |
Transformer | epoch 0 | step 30080 |avg loss 8.720 |avg tokens 4686.700 |tokens/s 32900.039 |walltime 4412.037 |
Transformer | epoch 0 | step 30090 |avg loss 8.552 |avg tokens 4622.200 |tokens/s 32893.315 |walltime 4413.442 |
Transformer | epoch 0 | step 30100 |avg loss 8.354 |avg tokens 4597.200 |tokens/s 31275.371 |walltime 4414.912 |
Transformer | epoch 0 | step 30110 |avg loss 8.536 |avg tokens 4070.800 |tokens/s 29685.519 |walltime 4416.283 |
Transformer | epoch 0 | step 30120 |avg loss 8.296 |avg tokens 4649.800 |tokens/s 32219.088 |walltime 4417.727 |
Transformer | epoch 0 | step 30130 |avg loss 8.460 |avg tokens 4657.800 |tokens/s 30832.829 |walltime 4419.237 |
Transformer | epoch 0 | step 30140 |avg loss 8.051 |avg tokens 4446.900 |tokens/s 30201.067 |walltime 4420.710 |
Transformer | epoch 0 | step 30150 |avg loss 8.186 |avg tokens 4903.200 |tokens/s 33555.647 |walltime 4422.171 |
Transformer | epoch 0 | step 30160 |avg loss 8.412 |avg tokens 4763.900 |tokens/s 32013.981 |walltime 4423.659 |
Transformer | epoch 0 | step 30170 |avg loss 8.441 |avg tokens 4563.300 |tokens/s 30846.674 |walltime 4425.138 |
Transformer | epoch 0 | step 30180 |avg loss 8.162 |avg tokens 4733.100 |tokens/s 31787.735 |walltime 4426.627 |
Transformer | epoch 0 | step 30190 |avg loss 8.293 |avg tokens 4346.200 |tokens/s 29045.597 |walltime 4428.124 |
Transformer | epoch 0 | step 30200 |avg loss 8.522 |avg tokens 4313.600 |tokens/s 29936.650 |walltime 4429.565 |
Transformer | epoch 0 | step 30210 |avg loss 8.465 |avg tokens 4048.700 |tokens/s 28289.838 |walltime 4430.996 |
Transformer | epoch 0 | step 30220 |avg loss 8.202 |avg tokens 4744.800 |tokens/s 31099.846 |walltime 4432.521 |
Transformer | epoch 0 | step 30230 |avg loss 8.398 |avg tokens 4746.200 |tokens/s 31685.133 |walltime 4434.019 |
Transformer | epoch 0 | step 30240 |avg loss 8.277 |avg tokens 4622.400 |tokens/s 29914.314 |walltime 4435.565 |
Transformer | epoch 0 | step 30250 |avg loss 8.367 |avg tokens 4152.400 |tokens/s 29211.524 |walltime 4436.986 |
Transformer | epoch 0 | step 30260 |avg loss 8.036 |avg tokens 4775.200 |tokens/s 31417.106 |walltime 4438.506 |
Transformer | epoch 0 | step 30270 |avg loss 8.379 |avg tokens 4298.800 |tokens/s 29672.603 |walltime 4439.955 |
Transformer | epoch 0 | step 30280 |avg loss 8.432 |avg tokens 4340.900 |tokens/s 30325.919 |walltime 4441.386 |
Transformer | epoch 0 | step 30290 |avg loss 8.361 |avg tokens 4432.600 |tokens/s 30570.564 |walltime 4442.836 |
Transformer | epoch 0 | step 30300 |avg loss 8.389 |avg tokens 4814.800 |tokens/s 32708.468 |walltime 4444.308 |
Transformer | epoch 0 | step 30310 |avg loss 8.110 |avg tokens 4672.400 |tokens/s 31227.732 |walltime 4445.804 |
Transformer | epoch 0 | step 30320 |avg loss 8.632 |avg tokens 4628.300 |tokens/s 32126.684 |walltime 4447.245 |
Transformer | epoch 0 | step 30330 |avg loss 8.556 |avg tokens 4736.500 |tokens/s 31952.126 |walltime 4448.727 |
Transformer | epoch 0 | step 30340 |avg loss 8.386 |avg tokens 4770.400 |tokens/s 32382.624 |walltime 4450.201 |
Transformer | epoch 0 | step 30350 |avg loss 8.166 |avg tokens 4849.800 |tokens/s 32534.130 |walltime 4451.691 |
Transformer | epoch 0 | step 30360 |avg loss 8.365 |avg tokens 4239.700 |tokens/s 30535.009 |walltime 4453.080 |
Transformer | epoch 0 | step 30370 |avg loss 8.355 |avg tokens 4460.300 |tokens/s 29906.665 |walltime 4454.571 |
Transformer | epoch 0 | step 30380 |avg loss 8.373 |avg tokens 4665.100 |tokens/s 31445.826 |walltime 4456.055 |
Transformer | epoch 0 | step 30390 |avg loss 8.521 |avg tokens 4567.000 |tokens/s 31495.955 |walltime 4457.505 |
Transformer | epoch 0 | step 30400 |avg loss 8.333 |avg tokens 4778.000 |tokens/s 32698.068 |walltime 4458.966 |
Transformer | epoch 0 | step 30410 |avg loss 8.556 |avg tokens 4675.400 |tokens/s 31874.815 |walltime 4460.433 |
Transformer | epoch 0 | step 30420 |avg loss 8.405 |avg tokens 4400.200 |tokens/s 30778.428 |walltime 4461.862 |
Transformer | epoch 0 | step 30430 |avg loss 8.421 |avg tokens 4388.100 |tokens/s 29337.894 |walltime 4463.358 |
Transformer | epoch 0 | step 30440 |avg loss 8.316 |avg tokens 4530.400 |tokens/s 30670.060 |walltime 4464.835 |
Transformer | epoch 0 | step 30450 |avg loss 8.537 |avg tokens 4820.100 |tokens/s 33144.619 |walltime 4466.289 |
Transformer | epoch 0 | step 30460 |avg loss 8.433 |avg tokens 4328.100 |tokens/s 29316.199 |walltime 4467.766 |
Transformer | epoch 0 | step 30470 |avg loss 8.216 |avg tokens 4811.700 |tokens/s 32141.835 |walltime 4469.263 |
Transformer | epoch 0 | step 30480 |avg loss 8.501 |avg tokens 4138.700 |tokens/s 29781.699 |walltime 4470.653 |
Transformer | epoch 0 | step 30490 |avg loss 8.581 |avg tokens 4374.500 |tokens/s 30808.535 |walltime 4472.072 |
Transformer | epoch 0 | step 30500 |avg loss 8.401 |avg tokens 4746.400 |tokens/s 32098.627 |walltime 4473.551 |
Transformer | epoch 0 | step 30510 |avg loss 8.188 |avg tokens 4970.400 |tokens/s 32672.552 |walltime 4475.072 |
Transformer | epoch 0 | step 30520 |avg loss 8.348 |avg tokens 4940.700 |tokens/s 33458.816 |walltime 4476.549 |
Transformer | epoch 0 | step 30530 |avg loss 8.611 |avg tokens 4075.200 |tokens/s 28345.743 |walltime 4477.987 |
Transformer | epoch 0 | step 30540 |avg loss 8.124 |avg tokens 4301.300 |tokens/s 29316.013 |walltime 4479.454 |
Transformer | epoch 0 | step 30550 |avg loss 8.458 |avg tokens 4645.500 |tokens/s 32077.842 |walltime 4480.902 |
Transformer | epoch 0 | step 30560 |avg loss 8.331 |avg tokens 4559.100 |tokens/s 32128.750 |walltime 4482.321 |
Transformer | epoch 0 | step 30570 |avg loss 8.542 |avg tokens 4352.100 |tokens/s 30274.364 |walltime 4483.759 |
Transformer | epoch 0 | step 30580 |avg loss 8.316 |avg tokens 4706.400 |tokens/s 32056.063 |walltime 4485.227 |
Transformer | epoch 0 | step 30590 |avg loss 8.335 |avg tokens 3970.000 |tokens/s 28149.263 |walltime 4486.637 |
Transformer | epoch 0 | step 30600 |avg loss 8.393 |avg tokens 4052.500 |tokens/s 28328.377 |walltime 4488.068 |
Transformer | epoch 0 | step 30610 |avg loss 8.552 |avg tokens 4094.200 |tokens/s 29715.674 |walltime 4489.446 |
Transformer | epoch 0 | step 30620 |avg loss 8.558 |avg tokens 4345.800 |tokens/s 30091.817 |walltime 4490.890 |
Transformer | epoch 0 | step 30630 |avg loss 8.609 |avg tokens 4714.500 |tokens/s 32851.215 |walltime 4492.325 |
Transformer | epoch 0 | step 30640 |avg loss 8.161 |avg tokens 4635.200 |tokens/s 30741.058 |walltime 4493.833 |
Transformer | epoch 0 | step 30650 |avg loss 8.439 |avg tokens 4840.500 |tokens/s 32238.133 |walltime 4495.334 |
Transformer | epoch 0 | step 30660 |avg loss 8.529 |avg tokens 4244.400 |tokens/s 29547.172 |walltime 4496.771 |
Transformer | epoch 0 | step 30670 |avg loss 8.561 |avg tokens 4469.400 |tokens/s 31642.322 |walltime 4498.183 |
Transformer | epoch 0 | step 30680 |avg loss 8.414 |avg tokens 4526.200 |tokens/s 30987.236 |walltime 4499.644 |
Transformer | epoch 0 | step 30690 |avg loss 8.343 |avg tokens 4316.200 |tokens/s 29311.673 |walltime 4501.116 |
Transformer | epoch 0 | step 30700 |avg loss 8.489 |avg tokens 4419.200 |tokens/s 30056.769 |walltime 4502.587 |
Transformer | epoch 0 | step 30710 |avg loss 8.756 |avg tokens 4346.700 |tokens/s 31126.020 |walltime 4503.983 |
Transformer | epoch 0 | step 30720 |avg loss 8.464 |avg tokens 4170.500 |tokens/s 28864.008 |walltime 4505.428 |
Transformer | epoch 0 | step 30730 |avg loss 8.362 |avg tokens 4090.600 |tokens/s 28962.216 |walltime 4506.840 |
Transformer | epoch 0 | step 30740 |avg loss 8.553 |avg tokens 4807.700 |tokens/s 33307.162 |walltime 4508.284 |
Transformer | epoch 0 | step 30750 |avg loss 8.583 |avg tokens 4152.000 |tokens/s 29037.394 |walltime 4509.714 |
Transformer | epoch 0 | step 30760 |avg loss 8.489 |avg tokens 4089.600 |tokens/s 28811.023 |walltime 4511.133 |
Transformer | epoch 0 | step 30770 |avg loss 8.228 |avg tokens 4983.200 |tokens/s 32153.818 |walltime 4512.683 |
Transformer | epoch 0 | step 30780 |avg loss 8.185 |avg tokens 4831.400 |tokens/s 32112.294 |walltime 4514.188 |
Transformer | epoch 0 | step 30790 |avg loss 8.500 |avg tokens 4396.200 |tokens/s 31160.533 |walltime 4515.598 |
Transformer | epoch 0 | step 30800 |avg loss 8.384 |avg tokens 4478.200 |tokens/s 30233.595 |walltime 4517.080 |
Transformer | epoch 0 | step 30810 |avg loss 8.274 |avg tokens 4150.600 |tokens/s 28914.488 |walltime 4518.515 |
Transformer | epoch 0 | step 30820 |avg loss 8.095 |avg tokens 4687.600 |tokens/s 31120.500 |walltime 4520.021 |
Transformer | epoch 0 | step 30830 |avg loss 8.475 |avg tokens 4306.700 |tokens/s 29789.944 |walltime 4521.467 |
Transformer | epoch 0 | step 30840 |avg loss 8.319 |avg tokens 4649.800 |tokens/s 30279.961 |walltime 4523.003 |
Transformer | epoch 0 | step 30850 |avg loss 8.378 |avg tokens 4397.000 |tokens/s 30029.791 |walltime 4524.467 |
Transformer | epoch 0 | step 30860 |avg loss 8.334 |avg tokens 4922.900 |tokens/s 33009.321 |walltime 4525.958 |
Transformer | epoch 0 | step 30870 |avg loss 8.253 |avg tokens 4726.500 |tokens/s 30732.849 |walltime 4527.496 |
Transformer | epoch 0 | step 30880 |avg loss 8.052 |avg tokens 4740.400 |tokens/s 31773.532 |walltime 4528.988 |
Transformer | epoch 0 | step 30890 |avg loss 8.392 |avg tokens 4180.900 |tokens/s 29617.531 |walltime 4530.400 |
Transformer | epoch 0 | step 30900 |avg loss 8.533 |avg tokens 4683.500 |tokens/s 32158.210 |walltime 4531.856 |
Transformer | epoch 0 | step 30910 |avg loss 8.502 |avg tokens 4518.200 |tokens/s 30493.898 |walltime 4533.338 |
Transformer | epoch 0 | step 30920 |avg loss 8.401 |avg tokens 4710.200 |tokens/s 30151.307 |walltime 4534.900 |
Transformer | epoch 0 | step 30930 |avg loss 8.220 |avg tokens 4741.800 |tokens/s 31602.732 |walltime 4536.400 |
Transformer | epoch 0 | step 30940 |avg loss 8.226 |avg tokens 4518.800 |tokens/s 30612.767 |walltime 4537.877 |
Transformer | epoch 0 | step 30950 |avg loss 8.285 |avg tokens 4801.600 |tokens/s 31762.908 |walltime 4539.388 |
Transformer | epoch 0 | step 30960 |avg loss 8.350 |avg tokens 4640.500 |tokens/s 31610.647 |walltime 4540.856 |
Transformer | epoch 0 | step 30970 |avg loss 8.429 |avg tokens 4759.100 |tokens/s 31714.529 |walltime 4542.357 |
Transformer | epoch 0 | step 30980 |avg loss 8.724 |avg tokens 3754.900 |tokens/s 27723.512 |walltime 4543.711 |
Transformer | epoch 0 | step 30990 |avg loss 8.449 |avg tokens 4757.400 |tokens/s 32099.653 |walltime 4545.193 |
Transformer | epoch 0 | step 31000 |avg loss 8.471 |avg tokens 4737.200 |tokens/s 32092.641 |walltime 4546.669 |
Transformer | epoch 0 | step 31010 |avg loss 8.093 |avg tokens 4667.500 |tokens/s 31316.463 |walltime 4548.160 |
Transformer | epoch 0 | step 31020 |avg loss 8.374 |avg tokens 4582.100 |tokens/s 31233.157 |walltime 4549.627 |
Transformer | epoch 0 | step 31030 |avg loss 8.394 |avg tokens 4656.100 |tokens/s 32302.103 |walltime 4551.068 |
Transformer | epoch 0 | step 31040 |avg loss 8.148 |avg tokens 4684.800 |tokens/s 31470.591 |walltime 4552.557 |
Transformer | epoch 0 | step 31050 |avg loss 8.330 |avg tokens 4470.100 |tokens/s 30688.817 |walltime 4554.014 |
Transformer | epoch 0 | step 31060 |avg loss 8.362 |avg tokens 4774.600 |tokens/s 33206.215 |walltime 4555.451 |
Transformer | epoch 0 | step 31070 |avg loss 8.262 |avg tokens 4450.600 |tokens/s 31158.235 |walltime 4556.880 |
Transformer | epoch 0 | step 31080 |avg loss 8.647 |avg tokens 4411.300 |tokens/s 31227.177 |walltime 4558.293 |
Transformer | epoch 0 | step 31090 |avg loss 8.260 |avg tokens 4702.400 |tokens/s 30992.904 |walltime 4559.810 |
Transformer | epoch 0 | step 31100 |avg loss 8.073 |avg tokens 4411.500 |tokens/s 30396.258 |walltime 4561.261 |
Transformer | epoch 0 | step 31110 |avg loss 8.560 |avg tokens 4470.500 |tokens/s 31349.927 |walltime 4562.687 |
Transformer | epoch 0 | step 31120 |avg loss 8.388 |avg tokens 4876.200 |tokens/s 32699.364 |walltime 4564.178 |
Transformer | epoch 0 | step 31130 |avg loss 8.480 |avg tokens 4145.400 |tokens/s 29337.365 |walltime 4565.591 |
Transformer | epoch 0 | step 31140 |avg loss 8.213 |avg tokens 4435.400 |tokens/s 30348.323 |walltime 4567.053 |
Transformer | epoch 0 | step 31150 |avg loss 8.224 |avg tokens 4749.600 |tokens/s 31316.020 |walltime 4568.570 |
Transformer | epoch 0 | step 31160 |avg loss 8.187 |avg tokens 4374.900 |tokens/s 30206.132 |walltime 4570.018 |
Transformer | epoch 0 | step 31170 |avg loss 8.074 |avg tokens 4348.800 |tokens/s 29694.115 |walltime 4571.482 |
Transformer | epoch 0 | step 31180 |avg loss 8.398 |avg tokens 4757.700 |tokens/s 32188.150 |walltime 4572.961 |
Transformer | epoch 0 | step 31190 |avg loss 8.483 |avg tokens 4637.800 |tokens/s 32800.080 |walltime 4574.374 |
Transformer | epoch 0 | step 31200 |avg loss 8.631 |avg tokens 4266.600 |tokens/s 27484.801 |walltime 4575.927 |
Transformer | epoch 0 | step 31210 |avg loss 8.466 |avg tokens 4462.400 |tokens/s 30908.232 |walltime 4577.371 |
Transformer | epoch 0 | step 31220 |avg loss 8.301 |avg tokens 4759.500 |tokens/s 32030.914 |walltime 4578.856 |
Transformer | epoch 0 | step 31230 |avg loss 8.299 |avg tokens 4484.700 |tokens/s 30647.786 |walltime 4580.320 |
Transformer | epoch 0 | step 31240 |avg loss 8.252 |avg tokens 4776.000 |tokens/s 31974.174 |walltime 4581.814 |
Transformer | epoch 0 | step 31250 |avg loss 8.272 |avg tokens 4780.800 |tokens/s 31948.705 |walltime 4583.310 |
Transformer | epoch 0 | step 31260 |avg loss 8.368 |avg tokens 4218.800 |tokens/s 28342.280 |walltime 4584.798 |
Transformer | epoch 0 | step 31270 |avg loss 8.434 |avg tokens 4297.300 |tokens/s 29794.643 |walltime 4586.241 |
Transformer | epoch 0 | step 31280 |avg loss 8.355 |avg tokens 4646.700 |tokens/s 30642.414 |walltime 4587.757 |
Transformer | epoch 0 | step 31290 |avg loss 8.276 |avg tokens 4589.600 |tokens/s 30941.750 |walltime 4589.240 |
Transformer | epoch 0 | step 31300 |avg loss 8.509 |avg tokens 4459.000 |tokens/s 30309.854 |walltime 4590.712 |
Transformer | epoch 0 | step 31310 |avg loss 8.329 |avg tokens 4796.500 |tokens/s 32110.412 |walltime 4592.205 |
Transformer | epoch 0 | step 31320 |avg loss 8.291 |avg tokens 4823.300 |tokens/s 31614.192 |walltime 4593.731 |
Transformer | epoch 0 | step 31330 |avg loss 8.429 |avg tokens 4161.700 |tokens/s 29841.584 |walltime 4595.126 |
Transformer | epoch 0 | step 31340 |avg loss 8.203 |avg tokens 4619.400 |tokens/s 31034.208 |walltime 4596.614 |
Transformer | epoch 0 | step 31350 |avg loss 8.481 |avg tokens 4284.100 |tokens/s 29312.735 |walltime 4598.076 |
Transformer | epoch 0 | step 31360 |avg loss 8.405 |avg tokens 4272.200 |tokens/s 29484.497 |walltime 4599.525 |
Transformer | epoch 0 | step 31370 |avg loss 8.506 |avg tokens 4303.000 |tokens/s 30307.074 |walltime 4600.944 |
Transformer | epoch 0 | step 31380 |avg loss 8.334 |avg tokens 4485.600 |tokens/s 29873.853 |walltime 4602.446 |
Transformer | epoch 0 | step 31390 |avg loss 8.271 |avg tokens 4600.800 |tokens/s 31708.434 |walltime 4603.897 |
Transformer | epoch 0 | step 31400 |avg loss 8.322 |avg tokens 4280.100 |tokens/s 29423.680 |walltime 4605.352 |
Transformer | epoch 0 | step 31410 |avg loss 8.071 |avg tokens 4780.500 |tokens/s 31351.617 |walltime 4606.876 |
Transformer | epoch 0 | step 31420 |avg loss 8.269 |avg tokens 4155.000 |tokens/s 28682.624 |walltime 4608.325 |
Transformer | epoch 0 | step 31430 |avg loss 8.454 |avg tokens 3986.700 |tokens/s 28512.669 |walltime 4609.723 |
Transformer | epoch 0 | step 31440 |avg loss 8.577 |avg tokens 4487.100 |tokens/s 29797.480 |walltime 4611.229 |
Transformer | epoch 0 | step 31450 |avg loss 8.189 |avg tokens 4595.100 |tokens/s 30881.857 |walltime 4612.717 |
Transformer | epoch 0 | step 31460 |avg loss 8.313 |avg tokens 4754.400 |tokens/s 31828.333 |walltime 4614.211 |
Transformer | epoch 0 | step 31470 |avg loss 8.429 |avg tokens 4802.100 |tokens/s 32212.324 |walltime 4615.702 |
Transformer | epoch 0 | step 31480 |avg loss 8.198 |avg tokens 4997.600 |tokens/s 33786.508 |walltime 4617.181 |
Epoch time: 4609.37907075882
Transformer | epoch 0 | step 31487 |avg loss 8.245 |avg tokens 4388.143 |tokens/s 23642.199 |walltime 4618.480 |
Validation loss on subset valid: 8.427943842031835
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [135].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [135].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [135], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [135], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [108].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [93].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [108], which does not match the required output shape [93].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [89].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [93], which does not match the required output shape [89].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [89], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [52].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [52], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [74].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [74].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [74], which does not match the required output shape [80].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [80], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [46].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [46], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
| Translated 3000 sentences (73265 tokens) in 64.6s (46.45 sentences/s, 1134.32 tokens/s)
| Eval completed in: 75.22s | UNCASED BLEU 0.43
| done training in 4697.5 seconds
Transformer | epoch 0 | step () |avg loss 8.428 |walltime 4706.974 |
