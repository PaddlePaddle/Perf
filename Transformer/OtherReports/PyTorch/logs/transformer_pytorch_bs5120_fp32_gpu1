| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 8421, WORLD_SIZE: 1, RANK: 0
| distributed init done!
| initialized host 70bfc6a3004d as rank 0 and device id 0
Namespace(adam_betas=[0.9, 0.997], adam_eps=1e-09, amp=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=1, do_sanity_check=False, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, file=None, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=10, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=40, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_lr=0.0, momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, path=None, prefix_size=0, print_alignment=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='/workspace/checkpoint', save_interval=1, save_predictions=False, seed=1, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| /data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| /data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
| NOTICE: your device may support faster training with --amp
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 1 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
Transformer | epoch 0 | step 10 |avg loss 16.028 |avg tokens 4746.000 |tokens/s 9674.172 |walltime 14.631 |
Transformer | epoch 0 | step 20 |avg loss 15.409 |avg tokens 4684.900 |tokens/s 9026.540 |walltime 19.821 |
Transformer | epoch 0 | step 30 |avg loss 14.683 |avg tokens 4825.900 |tokens/s 9371.879 |walltime 24.971 |
Transformer | epoch 0 | step 40 |avg loss 14.153 |avg tokens 4474.100 |tokens/s 8999.070 |walltime 29.942 |
Transformer | epoch 0 | step 50 |avg loss 13.659 |avg tokens 4491.300 |tokens/s 8743.274 |walltime 35.079 |
Transformer | epoch 0 | step 60 |avg loss 13.378 |avg tokens 4605.600 |tokens/s 9123.200 |walltime 40.128 |
Transformer | epoch 0 | step 70 |avg loss 13.093 |avg tokens 4797.300 |tokens/s 9230.687 |walltime 45.325 |
Transformer | epoch 0 | step 80 |avg loss 13.042 |avg tokens 4635.400 |tokens/s 9218.328 |walltime 50.353 |
Transformer | epoch 0 | step 90 |avg loss 12.764 |avg tokens 4480.700 |tokens/s 9242.714 |walltime 55.201 |
Transformer | epoch 0 | step 100 |avg loss 12.566 |avg tokens 4863.700 |tokens/s 9310.286 |walltime 60.425 |
Transformer | epoch 0 | step 110 |avg loss 12.481 |avg tokens 4102.000 |tokens/s 8480.681 |walltime 65.262 |
Transformer | epoch 0 | step 120 |avg loss 12.200 |avg tokens 4813.100 |tokens/s 9293.995 |walltime 70.441 |
Transformer | epoch 0 | step 130 |avg loss 12.283 |avg tokens 4012.100 |tokens/s 8662.354 |walltime 75.072 |
Transformer | epoch 0 | step 140 |avg loss 12.052 |avg tokens 4501.700 |tokens/s 9099.389 |walltime 80.020 |
Transformer | epoch 0 | step 150 |avg loss 11.986 |avg tokens 4478.300 |tokens/s 9066.400 |walltime 84.959 |
Transformer | epoch 0 | step 160 |avg loss 11.880 |avg tokens 4764.200 |tokens/s 9310.473 |walltime 90.076 |
Transformer | epoch 0 | step 170 |avg loss 11.844 |avg tokens 4419.500 |tokens/s 8959.638 |walltime 95.009 |
Transformer | epoch 0 | step 180 |avg loss 11.597 |avg tokens 4704.800 |tokens/s 8978.318 |walltime 100.249 |
Transformer | epoch 0 | step 190 |avg loss 11.608 |avg tokens 4298.700 |tokens/s 8767.930 |walltime 105.152 |
Transformer | epoch 0 | step 200 |avg loss 11.646 |avg tokens 4392.700 |tokens/s 9007.799 |walltime 110.028 |
Transformer | epoch 0 | step 210 |avg loss 11.705 |avg tokens 4111.300 |tokens/s 8326.856 |walltime 114.966 |
Transformer | epoch 0 | step 220 |avg loss 11.520 |avg tokens 4313.800 |tokens/s 8738.225 |walltime 119.902 |
Transformer | epoch 0 | step 230 |avg loss 11.408 |avg tokens 4865.800 |tokens/s 9107.843 |walltime 125.245 |
Transformer | epoch 0 | step 240 |avg loss 11.418 |avg tokens 4560.800 |tokens/s 8991.795 |walltime 130.317 |
Transformer | epoch 0 | step 250 |avg loss 11.624 |avg tokens 4366.200 |tokens/s 9126.819 |walltime 135.101 |
Transformer | epoch 0 | step 260 |avg loss 11.390 |avg tokens 4891.600 |tokens/s 9329.045 |walltime 140.344 |
Transformer | epoch 0 | step 270 |avg loss 11.197 |avg tokens 4857.800 |tokens/s 9122.917 |walltime 145.669 |
Transformer | epoch 0 | step 280 |avg loss 11.401 |avg tokens 4607.100 |tokens/s 9128.057 |walltime 150.716 |
Transformer | epoch 0 | step 290 |avg loss 11.620 |avg tokens 4808.600 |tokens/s 10294.626 |walltime 155.387 |
Transformer | epoch 0 | step 300 |avg loss 11.379 |avg tokens 3985.000 |tokens/s 8174.208 |walltime 160.262 |
Transformer | epoch 0 | step 310 |avg loss 11.055 |avg tokens 4752.000 |tokens/s 8985.943 |walltime 165.551 |
Transformer | epoch 0 | step 320 |avg loss 11.404 |avg tokens 4491.200 |tokens/s 8889.723 |walltime 170.603 |
Transformer | epoch 0 | step 330 |avg loss 11.217 |avg tokens 4595.600 |tokens/s 8837.129 |walltime 175.803 |
Transformer | epoch 0 | step 340 |avg loss 11.216 |avg tokens 4527.900 |tokens/s 8715.134 |walltime 180.998 |
Transformer | epoch 0 | step 350 |avg loss 10.944 |avg tokens 4840.000 |tokens/s 9262.285 |walltime 186.224 |
Transformer | epoch 0 | step 360 |avg loss 11.433 |avg tokens 4818.300 |tokens/s 9460.901 |walltime 191.317 |
Transformer | epoch 0 | step 370 |avg loss 11.208 |avg tokens 4466.600 |tokens/s 8994.072 |walltime 196.283 |
Transformer | epoch 0 | step 380 |avg loss 11.260 |avg tokens 4497.300 |tokens/s 8927.508 |walltime 201.321 |
Transformer | epoch 0 | step 390 |avg loss 11.145 |avg tokens 4554.000 |tokens/s 8669.522 |walltime 206.573 |
Transformer | epoch 0 | step 400 |avg loss 11.538 |avg tokens 4280.600 |tokens/s 8778.202 |walltime 211.450 |
Transformer | epoch 0 | step 410 |avg loss 11.251 |avg tokens 4490.200 |tokens/s 9176.823 |walltime 216.343 |
Transformer | epoch 0 | step 420 |avg loss 10.983 |avg tokens 4122.700 |tokens/s 8271.083 |walltime 221.327 |
Transformer | epoch 0 | step 430 |avg loss 11.005 |avg tokens 4399.400 |tokens/s 8518.436 |walltime 226.492 |
Transformer | epoch 0 | step 440 |avg loss 11.402 |avg tokens 4768.500 |tokens/s 9371.605 |walltime 231.580 |
Transformer | epoch 0 | step 450 |avg loss 11.321 |avg tokens 3972.100 |tokens/s 8490.580 |walltime 236.258 |
Transformer | epoch 0 | step 460 |avg loss 10.819 |avg tokens 5023.200 |tokens/s 9411.428 |walltime 241.596 |
Transformer | epoch 0 | step 470 |avg loss 10.990 |avg tokens 4543.900 |tokens/s 8980.447 |walltime 246.656 |
Transformer | epoch 0 | step 480 |avg loss 11.085 |avg tokens 4486.900 |tokens/s 8735.764 |walltime 251.792 |
Transformer | epoch 0 | step 490 |avg loss 11.090 |avg tokens 4767.300 |tokens/s 9336.077 |walltime 256.898 |
Transformer | epoch 0 | step 500 |avg loss 10.857 |avg tokens 4836.000 |tokens/s 9404.619 |walltime 262.040 |
Transformer | epoch 0 | step 510 |avg loss 11.121 |avg tokens 4769.600 |tokens/s 9544.832 |walltime 267.037 |
Transformer | epoch 0 | step 520 |avg loss 10.853 |avg tokens 4485.800 |tokens/s 9102.560 |walltime 271.965 |
Transformer | epoch 0 | step 530 |avg loss 10.770 |avg tokens 4718.800 |tokens/s 9054.018 |walltime 277.177 |
Transformer | epoch 0 | step 540 |avg loss 10.866 |avg tokens 4225.400 |tokens/s 8553.621 |walltime 282.117 |
Transformer | epoch 0 | step 550 |avg loss 10.818 |avg tokens 4537.400 |tokens/s 9216.907 |walltime 287.040 |
Transformer | epoch 0 | step 560 |avg loss 10.561 |avg tokens 4657.600 |tokens/s 8860.670 |walltime 292.296 |
Transformer | epoch 0 | step 570 |avg loss 10.646 |avg tokens 4406.300 |tokens/s 8560.804 |walltime 297.444 |
Transformer | epoch 0 | step 580 |avg loss 10.489 |avg tokens 4371.300 |tokens/s 8666.772 |walltime 302.487 |
Transformer | epoch 0 | step 590 |avg loss 10.550 |avg tokens 4391.900 |tokens/s 8419.217 |walltime 307.704 |
Transformer | epoch 0 | step 600 |avg loss 10.459 |avg tokens 4589.800 |tokens/s 8855.445 |walltime 312.887 |
Transformer | epoch 0 | step 610 |avg loss 10.577 |avg tokens 4798.900 |tokens/s 9355.045 |walltime 318.017 |
Transformer | epoch 0 | step 620 |avg loss 10.415 |avg tokens 4643.200 |tokens/s 8865.622 |walltime 323.254 |
Transformer | epoch 0 | step 630 |avg loss 10.623 |avg tokens 4290.200 |tokens/s 8642.280 |walltime 328.218 |
Transformer | epoch 0 | step 640 |avg loss 10.242 |avg tokens 4483.200 |tokens/s 8714.730 |walltime 333.363 |
Transformer | epoch 0 | step 650 |avg loss 10.631 |avg tokens 4729.900 |tokens/s 9553.122 |walltime 338.314 |
Transformer | epoch 0 | step 660 |avg loss 10.052 |avg tokens 4681.700 |tokens/s 9278.704 |walltime 343.359 |
Transformer | epoch 0 | step 670 |avg loss 10.501 |avg tokens 4824.000 |tokens/s 9116.357 |walltime 348.651 |
Transformer | epoch 0 | step 680 |avg loss 10.351 |avg tokens 4452.400 |tokens/s 8857.798 |walltime 353.677 |
Transformer | epoch 0 | step 690 |avg loss 10.425 |avg tokens 4292.800 |tokens/s 8520.845 |walltime 358.715 |
Transformer | epoch 0 | step 700 |avg loss 10.362 |avg tokens 4044.200 |tokens/s 8337.351 |walltime 363.566 |
Transformer | epoch 0 | step 710 |avg loss 10.191 |avg tokens 4747.600 |tokens/s 9003.507 |walltime 368.839 |
Transformer | epoch 0 | step 720 |avg loss 10.342 |avg tokens 4380.400 |tokens/s 8556.307 |walltime 373.959 |
Transformer | epoch 0 | step 730 |avg loss 10.836 |avg tokens 3953.800 |tokens/s 8067.296 |walltime 378.860 |
Transformer | epoch 0 | step 740 |avg loss 10.649 |avg tokens 4533.800 |tokens/s 9140.459 |walltime 383.820 |
Transformer | epoch 0 | step 750 |avg loss 10.480 |avg tokens 4559.400 |tokens/s 8896.917 |walltime 388.945 |
Transformer | epoch 0 | step 760 |avg loss 10.198 |avg tokens 4836.800 |tokens/s 9098.208 |walltime 394.261 |
Transformer | epoch 0 | step 770 |avg loss 10.269 |avg tokens 4763.100 |tokens/s 9394.180 |walltime 399.331 |
Transformer | epoch 0 | step 780 |avg loss 10.279 |avg tokens 4412.300 |tokens/s 8841.456 |walltime 404.322 |
Transformer | epoch 0 | step 790 |avg loss 10.393 |avg tokens 4735.200 |tokens/s 9303.901 |walltime 409.411 |
Transformer | epoch 0 | step 800 |avg loss 9.969 |avg tokens 4525.600 |tokens/s 8859.405 |walltime 414.519 |
Transformer | epoch 0 | step 810 |avg loss 10.107 |avg tokens 4537.300 |tokens/s 9013.184 |walltime 419.553 |
Transformer | epoch 0 | step 820 |avg loss 10.145 |avg tokens 4601.600 |tokens/s 9178.878 |walltime 424.567 |
Transformer | epoch 0 | step 830 |avg loss 10.091 |avg tokens 4701.700 |tokens/s 9124.518 |walltime 429.719 |
Transformer | epoch 0 | step 840 |avg loss 10.224 |avg tokens 4362.600 |tokens/s 8883.496 |walltime 434.630 |
Transformer | epoch 0 | step 850 |avg loss 10.005 |avg tokens 4365.300 |tokens/s 8687.542 |walltime 439.655 |
Transformer | epoch 0 | step 860 |avg loss 9.854 |avg tokens 4655.900 |tokens/s 8909.574 |walltime 444.881 |
Transformer | epoch 0 | step 870 |avg loss 9.999 |avg tokens 4686.300 |tokens/s 9202.792 |walltime 449.973 |
Transformer | epoch 0 | step 880 |avg loss 9.892 |avg tokens 4841.900 |tokens/s 9487.136 |walltime 455.077 |
Transformer | epoch 0 | step 890 |avg loss 10.231 |avg tokens 4208.500 |tokens/s 9022.333 |walltime 459.741 |
Transformer | epoch 0 | step 900 |avg loss 10.142 |avg tokens 4439.500 |tokens/s 8800.755 |walltime 464.786 |
Transformer | epoch 0 | step 910 |avg loss 9.860 |avg tokens 4735.800 |tokens/s 8958.706 |walltime 470.072 |
Transformer | epoch 0 | step 920 |avg loss 9.878 |avg tokens 4521.000 |tokens/s 8836.863 |walltime 475.188 |
Transformer | epoch 0 | step 930 |avg loss 10.004 |avg tokens 4485.700 |tokens/s 9254.393 |walltime 480.035 |
Transformer | epoch 0 | step 940 |avg loss 9.804 |avg tokens 4760.800 |tokens/s 9162.160 |walltime 485.231 |
Transformer | epoch 0 | step 950 |avg loss 10.114 |avg tokens 3946.700 |tokens/s 8467.207 |walltime 489.892 |
Transformer | epoch 0 | step 960 |avg loss 10.002 |avg tokens 4745.200 |tokens/s 9326.228 |walltime 494.981 |
Transformer | epoch 0 | step 970 |avg loss 9.858 |avg tokens 4967.000 |tokens/s 9439.377 |walltime 500.243 |
Transformer | epoch 0 | step 980 |avg loss 9.817 |avg tokens 4290.400 |tokens/s 8602.738 |walltime 505.230 |
Transformer | epoch 0 | step 990 |avg loss 9.892 |avg tokens 4642.100 |tokens/s 9150.271 |walltime 510.303 |
Transformer | epoch 0 | step 1000 |avg loss 9.667 |avg tokens 4810.900 |tokens/s 9296.144 |walltime 515.478 |
Transformer | epoch 0 | step 1010 |avg loss 9.912 |avg tokens 3619.400 |tokens/s 7802.094 |walltime 520.117 |
Transformer | epoch 0 | step 1020 |avg loss 9.570 |avg tokens 4821.800 |tokens/s 9262.252 |walltime 525.323 |
Transformer | epoch 0 | step 1030 |avg loss 9.834 |avg tokens 4113.200 |tokens/s 8491.369 |walltime 530.167 |
Transformer | epoch 0 | step 1040 |avg loss 9.504 |avg tokens 4625.200 |tokens/s 8772.466 |walltime 535.439 |
Transformer | epoch 0 | step 1050 |avg loss 9.790 |avg tokens 4734.700 |tokens/s 9265.958 |walltime 540.549 |
Transformer | epoch 0 | step 1060 |avg loss 9.543 |avg tokens 4226.700 |tokens/s 8377.064 |walltime 545.595 |
Transformer | epoch 0 | step 1070 |avg loss 9.826 |avg tokens 4131.800 |tokens/s 8226.377 |walltime 550.617 |
Transformer | epoch 0 | step 1080 |avg loss 9.708 |avg tokens 4392.200 |tokens/s 8936.057 |walltime 555.532 |
Transformer | epoch 0 | step 1090 |avg loss 9.827 |avg tokens 4145.900 |tokens/s 8395.641 |walltime 560.471 |
Transformer | epoch 0 | step 1100 |avg loss 9.357 |avg tokens 4239.500 |tokens/s 8402.609 |walltime 565.516 |
Transformer | epoch 0 | step 1110 |avg loss 9.498 |avg tokens 4679.700 |tokens/s 9140.111 |walltime 570.636 |
Transformer | epoch 0 | step 1120 |avg loss 9.524 |avg tokens 4609.100 |tokens/s 9031.488 |walltime 575.739 |
Transformer | epoch 0 | step 1130 |avg loss 9.416 |avg tokens 4714.600 |tokens/s 8915.853 |walltime 581.027 |
Transformer | epoch 0 | step 1140 |avg loss 9.430 |avg tokens 4256.700 |tokens/s 8402.733 |walltime 586.093 |
Transformer | epoch 0 | step 1150 |avg loss 9.839 |avg tokens 4098.100 |tokens/s 8700.844 |walltime 590.803 |
Transformer | epoch 0 | step 1160 |avg loss 9.694 |avg tokens 3743.600 |tokens/s 7850.912 |walltime 595.572 |
Transformer | epoch 0 | step 1170 |avg loss 9.722 |avg tokens 4452.500 |tokens/s 8992.960 |walltime 600.523 |
Transformer | epoch 0 | step 1180 |avg loss 9.473 |avg tokens 4444.200 |tokens/s 8865.345 |walltime 605.536 |
Transformer | epoch 0 | step 1190 |avg loss 9.271 |avg tokens 4816.700 |tokens/s 9219.603 |walltime 610.760 |
Transformer | epoch 0 | step 1200 |avg loss 9.468 |avg tokens 4153.500 |tokens/s 8398.293 |walltime 615.706 |
Transformer | epoch 0 | step 1210 |avg loss 9.246 |avg tokens 4412.700 |tokens/s 8858.008 |walltime 620.687 |
Transformer | epoch 0 | step 1220 |avg loss 9.510 |avg tokens 4489.100 |tokens/s 8786.633 |walltime 625.796 |
Transformer | epoch 0 | step 1230 |avg loss 9.502 |avg tokens 4406.300 |tokens/s 8515.291 |walltime 630.971 |
Transformer | epoch 0 | step 1240 |avg loss 9.441 |avg tokens 4218.800 |tokens/s 8632.549 |walltime 635.858 |
Transformer | epoch 0 | step 1250 |avg loss 9.468 |avg tokens 4411.800 |tokens/s 8773.327 |walltime 640.887 |
Transformer | epoch 0 | step 1260 |avg loss 9.216 |avg tokens 4450.000 |tokens/s 8653.385 |walltime 646.029 |
Transformer | epoch 0 | step 1270 |avg loss 9.502 |avg tokens 4509.000 |tokens/s 8955.618 |walltime 651.064 |
Transformer | epoch 0 | step 1280 |avg loss 9.280 |avg tokens 4760.800 |tokens/s 9164.906 |walltime 656.259 |
Transformer | epoch 0 | step 1290 |avg loss 9.368 |avg tokens 4139.700 |tokens/s 8492.714 |walltime 661.133 |
Transformer | epoch 0 | step 1300 |avg loss 9.328 |avg tokens 4875.700 |tokens/s 9360.381 |walltime 666.342 |
Transformer | epoch 0 | step 1310 |avg loss 9.327 |avg tokens 4744.100 |tokens/s 9220.588 |walltime 671.487 |
Transformer | epoch 0 | step 1320 |avg loss 9.185 |avg tokens 4128.400 |tokens/s 8476.239 |walltime 676.358 |
Transformer | epoch 0 | step 1330 |avg loss 9.457 |avg tokens 4701.300 |tokens/s 9570.538 |walltime 681.270 |
Transformer | epoch 0 | step 1340 |avg loss 9.504 |avg tokens 4167.600 |tokens/s 8551.649 |walltime 686.143 |
Transformer | epoch 0 | step 1350 |avg loss 8.933 |avg tokens 4708.800 |tokens/s 8995.158 |walltime 691.378 |
Transformer | epoch 0 | step 1360 |avg loss 9.356 |avg tokens 4692.900 |tokens/s 9167.483 |walltime 696.497 |
Transformer | epoch 0 | step 1370 |avg loss 9.370 |avg tokens 4811.200 |tokens/s 9596.811 |walltime 701.510 |
Transformer | epoch 0 | step 1380 |avg loss 9.206 |avg tokens 4095.900 |tokens/s 8311.930 |walltime 706.438 |
Transformer | epoch 0 | step 1390 |avg loss 9.245 |avg tokens 4153.500 |tokens/s 8496.071 |walltime 711.327 |
Transformer | epoch 0 | step 1400 |avg loss 9.285 |avg tokens 4743.700 |tokens/s 9441.325 |walltime 716.351 |
Transformer | epoch 0 | step 1410 |avg loss 9.575 |avg tokens 4357.600 |tokens/s 9173.833 |walltime 721.101 |
Transformer | epoch 0 | step 1420 |avg loss 8.763 |avg tokens 4777.600 |tokens/s 9060.863 |walltime 726.374 |
Transformer | epoch 0 | step 1430 |avg loss 9.139 |avg tokens 4613.700 |tokens/s 8946.682 |walltime 731.531 |
Transformer | epoch 0 | step 1440 |avg loss 9.322 |avg tokens 4474.100 |tokens/s 8995.948 |walltime 736.505 |
Transformer | epoch 0 | step 1450 |avg loss 9.203 |avg tokens 4552.100 |tokens/s 9263.037 |walltime 741.419 |
Transformer | epoch 0 | step 1460 |avg loss 9.202 |avg tokens 4805.200 |tokens/s 9557.477 |walltime 746.447 |
Transformer | epoch 0 | step 1470 |avg loss 9.062 |avg tokens 4808.300 |tokens/s 9224.257 |walltime 751.659 |
Transformer | epoch 0 | step 1480 |avg loss 9.048 |avg tokens 4686.200 |tokens/s 8730.124 |walltime 757.027 |
Transformer | epoch 0 | step 1490 |avg loss 9.148 |avg tokens 4018.100 |tokens/s 8104.011 |walltime 761.985 |
Transformer | epoch 0 | step 1500 |avg loss 9.345 |avg tokens 4492.800 |tokens/s 8939.976 |walltime 767.011 |
Transformer | epoch 0 | step 1510 |avg loss 9.344 |avg tokens 4327.400 |tokens/s 8855.907 |walltime 771.897 |
Transformer | epoch 0 | step 1520 |avg loss 8.942 |avg tokens 4669.500 |tokens/s 9121.796 |walltime 777.016 |
Transformer | epoch 0 | step 1530 |avg loss 8.747 |avg tokens 4531.100 |tokens/s 8756.971 |walltime 782.191 |
Transformer | epoch 0 | step 1540 |avg loss 9.425 |avg tokens 4215.400 |tokens/s 8733.576 |walltime 787.017 |
Transformer | epoch 0 | step 1550 |avg loss 9.239 |avg tokens 4473.100 |tokens/s 8917.249 |walltime 792.033 |
Transformer | epoch 0 | step 1560 |avg loss 8.841 |avg tokens 4819.700 |tokens/s 9255.022 |walltime 797.241 |
Transformer | epoch 0 | step 1570 |avg loss 8.738 |avg tokens 4379.700 |tokens/s 8622.728 |walltime 802.320 |
Transformer | epoch 0 | step 1580 |avg loss 8.720 |avg tokens 4669.200 |tokens/s 9228.509 |walltime 807.380 |
Transformer | epoch 0 | step 1590 |avg loss 9.065 |avg tokens 4354.100 |tokens/s 8559.151 |walltime 812.467 |
Transformer | epoch 0 | step 1600 |avg loss 9.331 |avg tokens 3894.400 |tokens/s 8307.544 |walltime 817.155 |
Transformer | epoch 0 | step 1610 |avg loss 9.311 |avg tokens 4300.400 |tokens/s 8659.818 |walltime 822.121 |
Transformer | epoch 0 | step 1620 |avg loss 8.973 |avg tokens 4232.900 |tokens/s 8646.980 |walltime 827.016 |
Transformer | epoch 0 | step 1630 |avg loss 8.820 |avg tokens 4570.000 |tokens/s 8993.972 |walltime 832.097 |
Transformer | epoch 0 | step 1640 |avg loss 9.176 |avg tokens 4848.400 |tokens/s 9616.490 |walltime 837.139 |
Transformer | epoch 0 | step 1650 |avg loss 8.976 |avg tokens 4497.100 |tokens/s 8958.647 |walltime 842.159 |
Transformer | epoch 0 | step 1660 |avg loss 9.018 |avg tokens 4492.200 |tokens/s 8958.082 |walltime 847.173 |
Transformer | epoch 0 | step 1670 |avg loss 8.853 |avg tokens 4453.700 |tokens/s 9192.962 |walltime 852.018 |
Transformer | epoch 0 | step 1680 |avg loss 8.865 |avg tokens 4243.300 |tokens/s 8397.535 |walltime 857.071 |
Transformer | epoch 0 | step 1690 |avg loss 8.978 |avg tokens 4299.500 |tokens/s 8766.466 |walltime 861.976 |
Transformer | epoch 0 | step 1700 |avg loss 8.394 |avg tokens 4318.700 |tokens/s 8461.591 |walltime 867.079 |
Transformer | epoch 0 | step 1710 |avg loss 9.291 |avg tokens 4495.800 |tokens/s 8894.255 |walltime 872.134 |
Transformer | epoch 0 | step 1720 |avg loss 8.968 |avg tokens 4581.900 |tokens/s 9218.621 |walltime 877.104 |
Transformer | epoch 0 | step 1730 |avg loss 9.107 |avg tokens 4460.300 |tokens/s 9165.675 |walltime 881.971 |
Transformer | epoch 0 | step 1740 |avg loss 8.775 |avg tokens 4639.400 |tokens/s 9129.959 |walltime 887.052 |
Transformer | epoch 0 | step 1750 |avg loss 8.360 |avg tokens 4440.200 |tokens/s 8576.898 |walltime 892.229 |
Transformer | epoch 0 | step 1760 |avg loss 8.763 |avg tokens 4516.800 |tokens/s 8709.672 |walltime 897.415 |
Transformer | epoch 0 | step 1770 |avg loss 8.976 |avg tokens 4522.000 |tokens/s 9185.426 |walltime 902.338 |
Transformer | epoch 0 | step 1780 |avg loss 8.938 |avg tokens 4501.000 |tokens/s 8958.336 |walltime 907.363 |
Transformer | epoch 0 | step 1790 |avg loss 9.088 |avg tokens 4117.000 |tokens/s 8634.252 |walltime 912.131 |
Transformer | epoch 0 | step 1800 |avg loss 8.521 |avg tokens 4558.700 |tokens/s 8800.170 |walltime 917.311 |
Transformer | epoch 0 | step 1810 |avg loss 8.970 |avg tokens 4614.500 |tokens/s 9288.077 |walltime 922.279 |
Transformer | epoch 0 | step 1820 |avg loss 8.607 |avg tokens 4472.500 |tokens/s 8834.885 |walltime 927.342 |
Transformer | epoch 0 | step 1830 |avg loss 8.919 |avg tokens 4461.200 |tokens/s 8869.131 |walltime 932.372 |
Transformer | epoch 0 | step 1840 |avg loss 8.645 |avg tokens 4369.700 |tokens/s 8533.007 |walltime 937.493 |
Transformer | epoch 0 | step 1850 |avg loss 8.698 |avg tokens 4823.600 |tokens/s 9421.108 |walltime 942.613 |
Transformer | epoch 0 | step 1860 |avg loss 8.448 |avg tokens 4683.200 |tokens/s 9042.760 |walltime 947.792 |
Transformer | epoch 0 | step 1870 |avg loss 9.006 |avg tokens 4443.800 |tokens/s 9398.186 |walltime 952.520 |
Transformer | epoch 0 | step 1880 |avg loss 8.769 |avg tokens 4682.600 |tokens/s 9263.614 |walltime 957.575 |
Transformer | epoch 0 | step 1890 |avg loss 8.604 |avg tokens 4498.200 |tokens/s 8796.109 |walltime 962.689 |
Transformer | epoch 0 | step 1900 |avg loss 8.263 |avg tokens 4576.800 |tokens/s 8809.387 |walltime 967.884 |
Transformer | epoch 0 | step 1910 |avg loss 8.081 |avg tokens 4909.700 |tokens/s 9240.991 |walltime 973.197 |
Transformer | epoch 0 | step 1920 |avg loss 8.614 |avg tokens 4542.900 |tokens/s 9079.571 |walltime 978.200 |
Transformer | epoch 0 | step 1930 |avg loss 8.396 |avg tokens 4310.000 |tokens/s 8655.339 |walltime 983.180 |
Transformer | epoch 0 | step 1940 |avg loss 8.511 |avg tokens 4631.200 |tokens/s 8900.636 |walltime 988.383 |
Transformer | epoch 0 | step 1950 |avg loss 8.612 |avg tokens 4509.200 |tokens/s 8988.085 |walltime 993.400 |
Transformer | epoch 0 | step 1960 |avg loss 8.356 |avg tokens 4483.900 |tokens/s 8769.528 |walltime 998.513 |
Transformer | epoch 0 | step 1970 |avg loss 8.047 |avg tokens 4791.100 |tokens/s 9038.756 |walltime 1003.814 |
Transformer | epoch 0 | step 1980 |avg loss 9.005 |avg tokens 4380.000 |tokens/s 9056.813 |walltime 1008.650 |
Transformer | epoch 0 | step 1990 |avg loss 8.344 |avg tokens 4672.000 |tokens/s 9030.657 |walltime 1013.823 |
Transformer | epoch 0 | step 2000 |avg loss 8.412 |avg tokens 4797.000 |tokens/s 9462.831 |walltime 1018.893 |
Transformer | epoch 0 | step 2010 |avg loss 8.094 |avg tokens 4725.900 |tokens/s 9234.802 |walltime 1024.010 |
Transformer | epoch 0 | step 2020 |avg loss 8.053 |avg tokens 4814.000 |tokens/s 9183.582 |walltime 1029.252 |
Transformer | epoch 0 | step 2030 |avg loss 8.003 |avg tokens 4819.800 |tokens/s 8990.521 |walltime 1034.613 |
Transformer | epoch 0 | step 2040 |avg loss 8.611 |avg tokens 4704.800 |tokens/s 9466.951 |walltime 1039.583 |
Transformer | epoch 0 | step 2050 |avg loss 8.457 |avg tokens 4561.100 |tokens/s 8969.102 |walltime 1044.668 |
Transformer | epoch 0 | step 2060 |avg loss 8.041 |avg tokens 4978.400 |tokens/s 9393.567 |walltime 1049.968 |
Transformer | epoch 0 | step 2070 |avg loss 8.505 |avg tokens 4304.400 |tokens/s 8589.792 |walltime 1054.979 |
Transformer | epoch 0 | step 2080 |avg loss 8.130 |avg tokens 4396.100 |tokens/s 8663.765 |walltime 1060.053 |
Transformer | epoch 0 | step 2090 |avg loss 8.095 |avg tokens 4608.100 |tokens/s 8986.908 |walltime 1065.181 |
Transformer | epoch 0 | step 2100 |avg loss 8.477 |avg tokens 4488.200 |tokens/s 8839.640 |walltime 1070.258 |
Transformer | epoch 0 | step 2110 |avg loss 8.674 |avg tokens 4167.900 |tokens/s 8653.771 |walltime 1075.074 |
Transformer | epoch 0 | step 2120 |avg loss 8.095 |avg tokens 4437.600 |tokens/s 8655.781 |walltime 1080.201 |
Transformer | epoch 0 | step 2130 |avg loss 8.326 |avg tokens 4524.900 |tokens/s 8962.631 |walltime 1085.250 |
Transformer | epoch 0 | step 2140 |avg loss 8.275 |avg tokens 4858.100 |tokens/s 9268.313 |walltime 1090.491 |
Transformer | epoch 0 | step 2150 |avg loss 8.586 |avg tokens 4491.400 |tokens/s 9178.444 |walltime 1095.385 |
Transformer | epoch 0 | step 2160 |avg loss 8.264 |avg tokens 4675.100 |tokens/s 9029.608 |walltime 1100.562 |
Transformer | epoch 0 | step 2170 |avg loss 8.318 |avg tokens 4459.400 |tokens/s 8967.841 |walltime 1105.535 |
Transformer | epoch 0 | step 2180 |avg loss 8.294 |avg tokens 4088.200 |tokens/s 8684.548 |walltime 1110.242 |
Transformer | epoch 0 | step 2190 |avg loss 8.061 |avg tokens 4880.600 |tokens/s 9413.441 |walltime 1115.427 |
Transformer | epoch 0 | step 2200 |avg loss 8.376 |avg tokens 4615.800 |tokens/s 9222.830 |walltime 1120.432 |
Transformer | epoch 0 | step 2210 |avg loss 7.839 |avg tokens 4213.400 |tokens/s 8311.281 |walltime 1125.501 |
Transformer | epoch 0 | step 2220 |avg loss 8.269 |avg tokens 4809.100 |tokens/s 9297.433 |walltime 1130.674 |
Transformer | epoch 0 | step 2230 |avg loss 7.729 |avg tokens 4847.200 |tokens/s 9268.114 |walltime 1135.904 |
Transformer | epoch 0 | step 2240 |avg loss 7.875 |avg tokens 4871.200 |tokens/s 9196.874 |walltime 1141.200 |
Transformer | epoch 0 | step 2250 |avg loss 7.762 |avg tokens 4487.800 |tokens/s 8732.080 |walltime 1146.340 |
Transformer | epoch 0 | step 2260 |avg loss 7.626 |avg tokens 4374.500 |tokens/s 8667.022 |walltime 1151.387 |
Transformer | epoch 0 | step 2270 |avg loss 8.242 |avg tokens 4482.900 |tokens/s 8787.256 |walltime 1156.489 |
Transformer | epoch 0 | step 2280 |avg loss 7.969 |avg tokens 4619.200 |tokens/s 8966.974 |walltime 1161.640 |
Transformer | epoch 0 | step 2290 |avg loss 7.933 |avg tokens 4869.600 |tokens/s 9185.591 |walltime 1166.941 |
Transformer | epoch 0 | step 2300 |avg loss 8.085 |avg tokens 4270.100 |tokens/s 8505.613 |walltime 1171.962 |
Transformer | epoch 0 | step 2310 |avg loss 8.153 |avg tokens 4845.700 |tokens/s 9356.208 |walltime 1177.141 |
Transformer | epoch 0 | step 2320 |avg loss 8.346 |avg tokens 3956.200 |tokens/s 8230.958 |walltime 1181.947 |
Transformer | epoch 0 | step 2330 |avg loss 8.434 |avg tokens 4580.200 |tokens/s 9187.070 |walltime 1186.933 |
Transformer | epoch 0 | step 2340 |avg loss 8.433 |avg tokens 3958.800 |tokens/s 8072.725 |walltime 1191.837 |
Transformer | epoch 0 | step 2350 |avg loss 8.262 |avg tokens 4497.200 |tokens/s 8950.241 |walltime 1196.862 |
Transformer | epoch 0 | step 2360 |avg loss 8.210 |avg tokens 4488.300 |tokens/s 9145.047 |walltime 1201.769 |
Transformer | epoch 0 | step 2370 |avg loss 7.687 |avg tokens 4794.400 |tokens/s 9095.162 |walltime 1207.041 |
Transformer | epoch 0 | step 2380 |avg loss 8.015 |avg tokens 4752.000 |tokens/s 9297.207 |walltime 1212.152 |
Transformer | epoch 0 | step 2390 |avg loss 8.053 |avg tokens 4304.100 |tokens/s 8309.839 |walltime 1217.332 |
Transformer | epoch 0 | step 2400 |avg loss 8.065 |avg tokens 4257.000 |tokens/s 8625.449 |walltime 1222.267 |
Transformer | epoch 0 | step 2410 |avg loss 7.940 |avg tokens 4707.900 |tokens/s 9109.059 |walltime 1227.435 |
Transformer | epoch 0 | step 2420 |avg loss 7.963 |avg tokens 4799.400 |tokens/s 9437.371 |walltime 1232.521 |
Transformer | epoch 0 | step 2430 |avg loss 8.481 |avg tokens 4149.300 |tokens/s 8730.570 |walltime 1237.273 |
Transformer | epoch 0 | step 2440 |avg loss 7.691 |avg tokens 4544.300 |tokens/s 8708.921 |walltime 1242.491 |
Transformer | epoch 0 | step 2450 |avg loss 7.878 |avg tokens 4848.400 |tokens/s 9335.679 |walltime 1247.685 |
Transformer | epoch 0 | step 2460 |avg loss 8.282 |avg tokens 4652.000 |tokens/s 9238.618 |walltime 1252.720 |
Transformer | epoch 0 | step 2470 |avg loss 7.312 |avg tokens 4695.100 |tokens/s 8886.917 |walltime 1258.003 |
Transformer | epoch 0 | step 2480 |avg loss 8.221 |avg tokens 4430.800 |tokens/s 8847.264 |walltime 1263.011 |
Transformer | epoch 0 | step 2490 |avg loss 7.911 |avg tokens 4577.000 |tokens/s 8901.032 |walltime 1268.154 |
Transformer | epoch 0 | step 2500 |avg loss 8.160 |avg tokens 4749.400 |tokens/s 9556.902 |walltime 1273.123 |
Transformer | epoch 0 | step 2510 |avg loss 7.684 |avg tokens 4614.400 |tokens/s 8946.204 |walltime 1278.281 |
Transformer | epoch 0 | step 2520 |avg loss 7.628 |avg tokens 4528.500 |tokens/s 8745.042 |walltime 1283.460 |
Transformer | epoch 0 | step 2530 |avg loss 7.568 |avg tokens 4942.600 |tokens/s 9296.204 |walltime 1288.776 |
Transformer | epoch 0 | step 2540 |avg loss 8.331 |avg tokens 4209.700 |tokens/s 8840.347 |walltime 1293.538 |
Transformer | epoch 0 | step 2550 |avg loss 7.864 |avg tokens 4218.800 |tokens/s 8350.017 |walltime 1298.591 |
Transformer | epoch 0 | step 2560 |avg loss 8.038 |avg tokens 4196.600 |tokens/s 8509.017 |walltime 1303.523 |
Transformer | epoch 0 | step 2570 |avg loss 8.472 |avg tokens 4717.000 |tokens/s 9781.516 |walltime 1308.345 |
Transformer | epoch 0 | step 2580 |avg loss 8.134 |avg tokens 4259.000 |tokens/s 8589.718 |walltime 1313.303 |
Transformer | epoch 0 | step 2590 |avg loss 7.757 |avg tokens 4557.600 |tokens/s 9033.555 |walltime 1318.348 |
Transformer | epoch 0 | step 2600 |avg loss 8.022 |avg tokens 4531.900 |tokens/s 9073.754 |walltime 1323.343 |
Transformer | epoch 0 | step 2610 |avg loss 8.216 |avg tokens 4809.800 |tokens/s 9623.775 |walltime 1328.341 |
Transformer | epoch 0 | step 2620 |avg loss 7.550 |avg tokens 4689.500 |tokens/s 8980.229 |walltime 1333.563 |
Transformer | epoch 0 | step 2630 |avg loss 7.654 |avg tokens 4837.000 |tokens/s 9203.261 |walltime 1338.819 |
Transformer | epoch 0 | step 2640 |avg loss 7.534 |avg tokens 4858.600 |tokens/s 9319.661 |walltime 1344.032 |
Transformer | epoch 0 | step 2650 |avg loss 7.556 |avg tokens 4916.600 |tokens/s 9609.392 |walltime 1349.148 |
Transformer | epoch 0 | step 2660 |avg loss 7.886 |avg tokens 4636.300 |tokens/s 9298.818 |walltime 1354.134 |
Transformer | epoch 0 | step 2670 |avg loss 7.632 |avg tokens 4866.200 |tokens/s 9095.685 |walltime 1359.484 |
Transformer | epoch 0 | step 2680 |avg loss 7.896 |avg tokens 4668.700 |tokens/s 9233.730 |walltime 1364.540 |
Transformer | epoch 0 | step 2690 |avg loss 7.570 |avg tokens 4719.500 |tokens/s 8939.813 |walltime 1369.820 |
Transformer | epoch 0 | step 2700 |avg loss 7.077 |avg tokens 4715.200 |tokens/s 8874.510 |walltime 1375.133 |
Transformer | epoch 0 | step 2710 |avg loss 8.326 |avg tokens 4519.100 |tokens/s 9340.902 |walltime 1379.971 |
Transformer | epoch 0 | step 2720 |avg loss 8.013 |avg tokens 4620.000 |tokens/s 8999.133 |walltime 1385.105 |
Transformer | epoch 0 | step 2730 |avg loss 7.783 |avg tokens 4460.700 |tokens/s 8721.103 |walltime 1390.219 |
Transformer | epoch 0 | step 2740 |avg loss 7.440 |avg tokens 4468.000 |tokens/s 8826.025 |walltime 1395.282 |
Transformer | epoch 0 | step 2750 |avg loss 7.706 |avg tokens 4680.800 |tokens/s 9188.969 |walltime 1400.376 |
Transformer | epoch 0 | step 2760 |avg loss 7.160 |avg tokens 4638.400 |tokens/s 8768.942 |walltime 1405.665 |
Transformer | epoch 0 | step 2770 |avg loss 8.373 |avg tokens 4048.100 |tokens/s 8650.389 |walltime 1410.345 |
Transformer | epoch 0 | step 2780 |avg loss 7.632 |avg tokens 4778.100 |tokens/s 9138.982 |walltime 1415.573 |
Transformer | epoch 0 | step 2790 |avg loss 8.506 |avg tokens 3867.800 |tokens/s 8102.608 |walltime 1420.347 |
Transformer | epoch 0 | step 2800 |avg loss 8.660 |avg tokens 4538.000 |tokens/s 9224.511 |walltime 1425.266 |
Transformer | epoch 0 | step 2810 |avg loss 8.474 |avg tokens 4381.000 |tokens/s 9179.504 |walltime 1430.039 |
Transformer | epoch 0 | step 2820 |avg loss 8.209 |avg tokens 4419.200 |tokens/s 8534.481 |walltime 1435.217 |
Transformer | epoch 0 | step 2830 |avg loss 7.882 |avg tokens 4910.600 |tokens/s 9483.687 |walltime 1440.395 |
Transformer | epoch 0 | step 2840 |avg loss 7.380 |avg tokens 4520.400 |tokens/s 8764.981 |walltime 1445.552 |
Transformer | epoch 0 | step 2850 |avg loss 7.408 |avg tokens 4584.200 |tokens/s 8744.615 |walltime 1450.794 |
Transformer | epoch 0 | step 2860 |avg loss 7.646 |avg tokens 4672.600 |tokens/s 9279.806 |walltime 1455.830 |
Transformer | epoch 0 | step 2870 |avg loss 8.287 |avg tokens 4281.600 |tokens/s 8973.744 |walltime 1460.601 |
Transformer | epoch 0 | step 2880 |avg loss 7.868 |avg tokens 4483.200 |tokens/s 8616.451 |walltime 1465.804 |
Transformer | epoch 0 | step 2890 |avg loss 7.673 |avg tokens 4717.300 |tokens/s 9354.434 |walltime 1470.847 |
Transformer | epoch 0 | step 2900 |avg loss 7.317 |avg tokens 4648.500 |tokens/s 8934.954 |walltime 1476.049 |
Transformer | epoch 0 | step 2910 |avg loss 8.011 |avg tokens 4441.700 |tokens/s 9076.901 |walltime 1480.943 |
Transformer | epoch 0 | step 2920 |avg loss 7.641 |avg tokens 4958.700 |tokens/s 9571.105 |walltime 1486.124 |
Transformer | epoch 0 | step 2930 |avg loss 7.622 |avg tokens 4812.300 |tokens/s 9073.708 |walltime 1491.427 |
Transformer | epoch 0 | step 2940 |avg loss 7.259 |avg tokens 4592.800 |tokens/s 8739.308 |walltime 1496.683 |
Transformer | epoch 0 | step 2950 |avg loss 7.978 |avg tokens 4451.900 |tokens/s 8936.208 |walltime 1501.665 |
Transformer | epoch 0 | step 2960 |avg loss 7.582 |avg tokens 4556.400 |tokens/s 8936.708 |walltime 1506.763 |
Transformer | epoch 0 | step 2970 |avg loss 7.474 |avg tokens 4738.000 |tokens/s 9017.848 |walltime 1512.017 |
Transformer | epoch 0 | step 2980 |avg loss 7.962 |avg tokens 4520.100 |tokens/s 8946.481 |walltime 1517.069 |
Transformer | epoch 0 | step 2990 |avg loss 7.514 |avg tokens 4483.200 |tokens/s 8834.240 |walltime 1522.144 |
Transformer | epoch 0 | step 3000 |avg loss 7.369 |avg tokens 4767.100 |tokens/s 9107.478 |walltime 1527.379 |
Transformer | epoch 0 | step 3010 |avg loss 8.124 |avg tokens 4051.600 |tokens/s 8706.709 |walltime 1532.032 |
Transformer | epoch 0 | step 3020 |avg loss 7.622 |avg tokens 4708.000 |tokens/s 8935.520 |walltime 1537.301 |
Transformer | epoch 0 | step 3030 |avg loss 7.887 |avg tokens 4462.500 |tokens/s 8956.452 |walltime 1542.283 |
Transformer | epoch 0 | step 3040 |avg loss 8.166 |avg tokens 4632.900 |tokens/s 9278.135 |walltime 1547.277 |
Transformer | epoch 0 | step 3050 |avg loss 7.377 |avg tokens 4756.600 |tokens/s 8946.901 |walltime 1552.593 |
Transformer | epoch 0 | step 3060 |avg loss 7.306 |avg tokens 4733.800 |tokens/s 9092.806 |walltime 1557.799 |
Transformer | epoch 0 | step 3070 |avg loss 7.770 |avg tokens 3856.500 |tokens/s 8262.951 |walltime 1562.466 |
Transformer | epoch 0 | step 3080 |avg loss 7.616 |avg tokens 4400.200 |tokens/s 8557.626 |walltime 1567.608 |
Transformer | epoch 0 | step 3090 |avg loss 7.122 |avg tokens 4792.600 |tokens/s 9049.659 |walltime 1572.904 |
Transformer | epoch 0 | step 3100 |avg loss 7.085 |avg tokens 4781.700 |tokens/s 8998.296 |walltime 1578.218 |
Transformer | epoch 0 | step 3110 |avg loss 6.863 |avg tokens 4752.800 |tokens/s 8903.831 |walltime 1583.556 |
Transformer | epoch 0 | step 3120 |avg loss 7.839 |avg tokens 4234.800 |tokens/s 8601.661 |walltime 1588.479 |
Transformer | epoch 0 | step 3130 |avg loss 7.759 |avg tokens 4025.100 |tokens/s 8355.673 |walltime 1593.297 |
Transformer | epoch 0 | step 3140 |avg loss 8.030 |avg tokens 4137.600 |tokens/s 8633.153 |walltime 1598.089 |
Transformer | epoch 0 | step 3150 |avg loss 7.183 |avg tokens 4786.500 |tokens/s 8925.918 |walltime 1603.452 |
Transformer | epoch 0 | step 3160 |avg loss 7.299 |avg tokens 4760.600 |tokens/s 9095.091 |walltime 1608.686 |
Transformer | epoch 0 | step 3170 |avg loss 7.808 |avg tokens 4462.700 |tokens/s 9043.748 |walltime 1613.621 |
Transformer | epoch 0 | step 3180 |avg loss 8.099 |avg tokens 4450.100 |tokens/s 9330.304 |walltime 1618.390 |
Transformer | epoch 0 | step 3190 |avg loss 8.204 |avg tokens 3842.800 |tokens/s 8253.682 |walltime 1623.046 |
Transformer | epoch 0 | step 3200 |avg loss 7.574 |avg tokens 4288.300 |tokens/s 8572.733 |walltime 1628.048 |
Transformer | epoch 0 | step 3210 |avg loss 7.517 |avg tokens 4634.700 |tokens/s 9214.359 |walltime 1633.078 |
Transformer | epoch 0 | step 3220 |avg loss 7.237 |avg tokens 4686.100 |tokens/s 8974.841 |walltime 1638.299 |
Transformer | epoch 0 | step 3230 |avg loss 7.777 |avg tokens 4234.700 |tokens/s 8647.531 |walltime 1643.196 |
Transformer | epoch 0 | step 3240 |avg loss 8.080 |avg tokens 4545.400 |tokens/s 9380.212 |walltime 1648.042 |
Transformer | epoch 0 | step 3250 |avg loss 7.440 |avg tokens 4806.800 |tokens/s 9168.263 |walltime 1653.285 |
Transformer | epoch 0 | step 3260 |avg loss 7.563 |avg tokens 4752.200 |tokens/s 9284.827 |walltime 1658.403 |
Transformer | epoch 0 | step 3270 |avg loss 7.758 |avg tokens 4409.400 |tokens/s 8978.353 |walltime 1663.314 |
Transformer | epoch 0 | step 3280 |avg loss 7.083 |avg tokens 4706.300 |tokens/s 8951.071 |walltime 1668.572 |
Transformer | epoch 0 | step 3290 |avg loss 6.946 |avg tokens 4719.900 |tokens/s 8911.012 |walltime 1673.869 |
Transformer | epoch 0 | step 3300 |avg loss 7.757 |avg tokens 4489.000 |tokens/s 8999.615 |walltime 1678.857 |
Transformer | epoch 0 | step 3310 |avg loss 7.868 |avg tokens 4264.300 |tokens/s 8871.761 |walltime 1683.664 |
Transformer | epoch 0 | step 3320 |avg loss 8.060 |avg tokens 4098.600 |tokens/s 8438.516 |walltime 1688.521 |
Transformer | epoch 0 | step 3330 |avg loss 7.340 |avg tokens 4858.800 |tokens/s 9319.737 |walltime 1693.734 |
Transformer | epoch 0 | step 3340 |avg loss 8.032 |avg tokens 4445.100 |tokens/s 9439.563 |walltime 1698.443 |
Transformer | epoch 0 | step 3350 |avg loss 7.273 |avg tokens 4368.500 |tokens/s 8565.249 |walltime 1703.543 |
Transformer | epoch 0 | step 3360 |avg loss 7.402 |avg tokens 4671.500 |tokens/s 9074.697 |walltime 1708.691 |
Transformer | epoch 0 | step 3370 |avg loss 7.669 |avg tokens 4650.600 |tokens/s 9372.313 |walltime 1713.653 |
Transformer | epoch 0 | step 3380 |avg loss 7.838 |avg tokens 4269.000 |tokens/s 8480.482 |walltime 1718.687 |
Transformer | epoch 0 | step 3390 |avg loss 7.626 |avg tokens 4604.400 |tokens/s 8989.074 |walltime 1723.809 |
Transformer | epoch 0 | step 3400 |avg loss 7.578 |avg tokens 4721.500 |tokens/s 9119.896 |walltime 1728.987 |
Transformer | epoch 0 | step 3410 |avg loss 7.708 |avg tokens 4245.100 |tokens/s 8492.530 |walltime 1733.985 |
Transformer | epoch 0 | step 3420 |avg loss 7.822 |avg tokens 4493.800 |tokens/s 9087.677 |walltime 1738.930 |
Transformer | epoch 0 | step 3430 |avg loss 7.564 |avg tokens 4389.600 |tokens/s 8750.058 |walltime 1743.947 |
Transformer | epoch 0 | step 3440 |avg loss 7.427 |avg tokens 4570.700 |tokens/s 8864.660 |walltime 1749.103 |
Transformer | epoch 0 | step 3450 |avg loss 7.031 |avg tokens 4399.500 |tokens/s 8264.592 |walltime 1754.426 |
Transformer | epoch 0 | step 3460 |avg loss 7.355 |avg tokens 4854.500 |tokens/s 9277.747 |walltime 1759.659 |
Transformer | epoch 0 | step 3470 |avg loss 7.566 |avg tokens 4634.500 |tokens/s 9306.598 |walltime 1764.638 |
Transformer | epoch 0 | step 3480 |avg loss 8.140 |avg tokens 4730.900 |tokens/s 9786.772 |walltime 1769.472 |
Transformer | epoch 0 | step 3490 |avg loss 7.427 |avg tokens 4415.000 |tokens/s 8969.413 |walltime 1774.395 |
Transformer | epoch 0 | step 3500 |avg loss 7.464 |avg tokens 4884.900 |tokens/s 9379.481 |walltime 1779.603 |
Transformer | epoch 0 | step 3510 |avg loss 7.082 |avg tokens 4468.800 |tokens/s 8662.537 |walltime 1784.761 |
Transformer | epoch 0 | step 3520 |avg loss 7.805 |avg tokens 4194.200 |tokens/s 8861.749 |walltime 1789.494 |
Transformer | epoch 0 | step 3530 |avg loss 7.180 |avg tokens 4820.000 |tokens/s 9077.625 |walltime 1794.804 |
Transformer | epoch 0 | step 3540 |avg loss 7.480 |avg tokens 4648.200 |tokens/s 9196.914 |walltime 1799.858 |
Transformer | epoch 0 | step 3550 |avg loss 7.546 |avg tokens 4660.900 |tokens/s 9094.582 |walltime 1804.983 |
Transformer | epoch 0 | step 3560 |avg loss 7.186 |avg tokens 4922.800 |tokens/s 9256.329 |walltime 1810.301 |
Transformer | epoch 0 | step 3570 |avg loss 7.608 |avg tokens 3913.300 |tokens/s 8512.105 |walltime 1814.899 |
Transformer | epoch 0 | step 3580 |avg loss 7.573 |avg tokens 4667.700 |tokens/s 9250.345 |walltime 1819.945 |
Transformer | epoch 0 | step 3590 |avg loss 7.402 |avg tokens 4084.400 |tokens/s 8218.385 |walltime 1824.915 |
Transformer | epoch 0 | step 3600 |avg loss 7.402 |avg tokens 4491.500 |tokens/s 8828.804 |walltime 1830.002 |
Transformer | epoch 0 | step 3610 |avg loss 6.997 |avg tokens 4800.000 |tokens/s 9100.356 |walltime 1835.276 |
Transformer | epoch 0 | step 3620 |avg loss 7.024 |avg tokens 4767.200 |tokens/s 8869.781 |walltime 1840.651 |
Transformer | epoch 0 | step 3630 |avg loss 7.263 |avg tokens 4487.500 |tokens/s 8748.921 |walltime 1845.780 |
Transformer | epoch 0 | step 3640 |avg loss 7.327 |avg tokens 4341.900 |tokens/s 8406.413 |walltime 1850.945 |
Transformer | epoch 0 | step 3650 |avg loss 7.021 |avg tokens 4433.600 |tokens/s 8661.105 |walltime 1856.064 |
Transformer | epoch 0 | step 3660 |avg loss 7.980 |avg tokens 4456.700 |tokens/s 9040.167 |walltime 1860.994 |
Transformer | epoch 0 | step 3670 |avg loss 7.663 |avg tokens 4502.700 |tokens/s 8984.951 |walltime 1866.006 |
Transformer | epoch 0 | step 3680 |avg loss 7.918 |avg tokens 4347.000 |tokens/s 8852.948 |walltime 1870.916 |
Transformer | epoch 0 | step 3690 |avg loss 7.979 |avg tokens 3903.700 |tokens/s 8233.548 |walltime 1875.657 |
Transformer | epoch 0 | step 3700 |avg loss 8.177 |avg tokens 4325.200 |tokens/s 9427.015 |walltime 1880.245 |
Transformer | epoch 0 | step 3710 |avg loss 8.059 |avg tokens 4150.000 |tokens/s 8706.003 |walltime 1885.012 |
Transformer | epoch 0 | step 3720 |avg loss 8.131 |avg tokens 3794.700 |tokens/s 8449.725 |walltime 1889.503 |
Transformer | epoch 0 | step 3730 |avg loss 7.438 |avg tokens 4585.400 |tokens/s 8834.353 |walltime 1894.693 |
Transformer | epoch 0 | step 3740 |avg loss 7.665 |avg tokens 4395.900 |tokens/s 9024.642 |walltime 1899.564 |
Transformer | epoch 0 | step 3750 |avg loss 7.400 |avg tokens 4758.000 |tokens/s 9296.662 |walltime 1904.682 |
Transformer | epoch 0 | step 3760 |avg loss 7.232 |avg tokens 4435.200 |tokens/s 8759.845 |walltime 1909.745 |
Transformer | epoch 0 | step 3770 |avg loss 7.604 |avg tokens 4706.800 |tokens/s 9462.903 |walltime 1914.719 |
Transformer | epoch 0 | step 3780 |avg loss 7.806 |avg tokens 3980.400 |tokens/s 8127.131 |walltime 1919.617 |
Transformer | epoch 0 | step 3790 |avg loss 7.135 |avg tokens 4795.000 |tokens/s 9278.042 |walltime 1924.785 |
Transformer | epoch 0 | step 3800 |avg loss 7.421 |avg tokens 4566.500 |tokens/s 8986.064 |walltime 1929.867 |
Transformer | epoch 0 | step 3810 |avg loss 7.700 |avg tokens 4135.300 |tokens/s 8483.074 |walltime 1934.742 |
Transformer | epoch 0 | step 3820 |avg loss 7.787 |avg tokens 4482.500 |tokens/s 9145.826 |walltime 1939.643 |
Transformer | epoch 0 | step 3830 |avg loss 7.667 |avg tokens 4706.400 |tokens/s 9392.210 |walltime 1944.654 |
Transformer | epoch 0 | step 3840 |avg loss 7.181 |avg tokens 4627.200 |tokens/s 8766.362 |walltime 1949.932 |
Transformer | epoch 0 | step 3850 |avg loss 7.177 |avg tokens 4498.700 |tokens/s 8890.390 |walltime 1954.992 |
Transformer | epoch 0 | step 3860 |avg loss 7.625 |avg tokens 4257.300 |tokens/s 8580.186 |walltime 1959.954 |
Transformer | epoch 0 | step 3870 |avg loss 7.910 |avg tokens 4747.900 |tokens/s 9557.154 |walltime 1964.922 |
Transformer | epoch 0 | step 3880 |avg loss 7.003 |avg tokens 4851.800 |tokens/s 9632.464 |walltime 1969.959 |
Transformer | epoch 0 | step 3890 |avg loss 7.980 |avg tokens 4493.200 |tokens/s 9166.477 |walltime 1974.861 |
Transformer | epoch 0 | step 3900 |avg loss 7.886 |avg tokens 4631.700 |tokens/s 9210.408 |walltime 1979.889 |
Transformer | epoch 0 | step 3910 |avg loss 7.601 |avg tokens 4482.600 |tokens/s 8853.266 |walltime 1984.953 |
Transformer | epoch 0 | step 3920 |avg loss 7.031 |avg tokens 4790.700 |tokens/s 9041.386 |walltime 1990.251 |
Transformer | epoch 0 | step 3930 |avg loss 7.691 |avg tokens 3834.100 |tokens/s 8321.541 |walltime 1994.859 |
Transformer | epoch 0 | step 3940 |avg loss 7.360 |avg tokens 4577.900 |tokens/s 8746.692 |walltime 2000.093 |
Transformer | epoch 0 | step 3950 |avg loss 7.382 |avg tokens 4858.100 |tokens/s 9374.556 |walltime 2005.275 |
Transformer | epoch 0 | step 3960 |avg loss 7.929 |avg tokens 4213.900 |tokens/s 8759.919 |walltime 2010.085 |
Transformer | epoch 0 | step 3970 |avg loss 7.410 |avg tokens 4324.600 |tokens/s 8604.834 |walltime 2015.111 |
Transformer | epoch 0 | step 3980 |avg loss 7.915 |avg tokens 4116.600 |tokens/s 8507.524 |walltime 2019.950 |
Transformer | epoch 0 | step 3990 |avg loss 7.541 |avg tokens 4243.300 |tokens/s 8930.764 |walltime 2024.701 |
Transformer | epoch 0 | step 4000 |avg loss 7.326 |avg tokens 4820.800 |tokens/s 9294.951 |walltime 2029.888 |
Transformer | epoch 0 | step 4010 |avg loss 7.303 |avg tokens 4525.100 |tokens/s 8855.231 |walltime 2034.998 |
Transformer | epoch 0 | step 4020 |avg loss 7.078 |avg tokens 4737.600 |tokens/s 8993.932 |walltime 2040.265 |
Transformer | epoch 0 | step 4030 |avg loss 7.863 |avg tokens 4585.600 |tokens/s 9716.336 |walltime 2044.985 |
Transformer | epoch 0 | step 4040 |avg loss 7.765 |avg tokens 4099.900 |tokens/s 8254.681 |walltime 2049.952 |
Transformer | epoch 0 | step 4050 |avg loss 7.734 |avg tokens 4795.300 |tokens/s 9375.809 |walltime 2055.066 |
Transformer | epoch 0 | step 4060 |avg loss 7.296 |avg tokens 4415.300 |tokens/s 8631.811 |walltime 2060.181 |
Transformer | epoch 0 | step 4070 |avg loss 7.693 |avg tokens 4089.100 |tokens/s 8499.280 |walltime 2064.992 |
Transformer | epoch 0 | step 4080 |avg loss 7.620 |avg tokens 4679.100 |tokens/s 9085.925 |walltime 2070.142 |
Transformer | epoch 0 | step 4090 |avg loss 7.123 |avg tokens 4786.400 |tokens/s 9091.748 |walltime 2075.407 |
Transformer | epoch 0 | step 4100 |avg loss 7.723 |avg tokens 4735.300 |tokens/s 9334.716 |walltime 2080.480 |
Transformer | epoch 0 | step 4110 |avg loss 7.631 |avg tokens 4287.200 |tokens/s 8751.686 |walltime 2085.378 |
Transformer | epoch 0 | step 4120 |avg loss 7.627 |avg tokens 4061.600 |tokens/s 8255.652 |walltime 2090.298 |
Transformer | epoch 0 | step 4130 |avg loss 7.630 |avg tokens 4720.200 |tokens/s 9246.195 |walltime 2095.403 |
Transformer | epoch 0 | step 4140 |avg loss 7.715 |avg tokens 4554.500 |tokens/s 9075.241 |walltime 2100.422 |
Transformer | epoch 0 | step 4150 |avg loss 7.435 |avg tokens 4460.500 |tokens/s 8839.455 |walltime 2105.468 |
Transformer | epoch 0 | step 4160 |avg loss 7.584 |avg tokens 4707.200 |tokens/s 9242.010 |walltime 2110.561 |
Transformer | epoch 0 | step 4170 |avg loss 7.390 |avg tokens 4741.800 |tokens/s 9165.163 |walltime 2115.735 |
Transformer | epoch 0 | step 4180 |avg loss 6.979 |avg tokens 4937.400 |tokens/s 9302.803 |walltime 2121.042 |
Transformer | epoch 0 | step 4190 |avg loss 7.940 |avg tokens 4397.300 |tokens/s 9085.877 |walltime 2125.882 |
Transformer | epoch 0 | step 4200 |avg loss 7.713 |avg tokens 4302.000 |tokens/s 8712.558 |walltime 2130.820 |
Transformer | epoch 0 | step 4210 |avg loss 7.764 |avg tokens 4660.800 |tokens/s 9323.625 |walltime 2135.819 |
Transformer | epoch 0 | step 4220 |avg loss 7.529 |avg tokens 4817.500 |tokens/s 9462.024 |walltime 2140.910 |
Transformer | epoch 0 | step 4230 |avg loss 7.275 |avg tokens 4532.400 |tokens/s 8888.677 |walltime 2146.009 |
Transformer | epoch 0 | step 4240 |avg loss 7.562 |avg tokens 4726.500 |tokens/s 9124.878 |walltime 2151.189 |
Transformer | epoch 0 | step 4250 |avg loss 7.562 |avg tokens 4230.000 |tokens/s 8383.373 |walltime 2156.235 |
Transformer | epoch 0 | step 4260 |avg loss 7.678 |avg tokens 4632.500 |tokens/s 9007.112 |walltime 2161.378 |
Transformer | epoch 0 | step 4270 |avg loss 7.220 |avg tokens 4711.200 |tokens/s 8974.784 |walltime 2166.627 |
Transformer | epoch 0 | step 4280 |avg loss 7.518 |avg tokens 4783.600 |tokens/s 9298.031 |walltime 2171.772 |
Transformer | epoch 0 | step 4290 |avg loss 7.609 |avg tokens 4682.600 |tokens/s 9293.321 |walltime 2176.811 |
Transformer | epoch 0 | step 4300 |avg loss 7.420 |avg tokens 4710.200 |tokens/s 9066.280 |walltime 2182.006 |
Transformer | epoch 0 | step 4310 |avg loss 7.405 |avg tokens 4259.900 |tokens/s 8507.471 |walltime 2187.013 |
Transformer | epoch 0 | step 4320 |avg loss 7.165 |avg tokens 4786.200 |tokens/s 9047.456 |walltime 2192.303 |
Transformer | epoch 0 | step 4330 |avg loss 7.057 |avg tokens 4976.100 |tokens/s 9279.885 |walltime 2197.665 |
Transformer | epoch 0 | step 4340 |avg loss 7.251 |avg tokens 4897.300 |tokens/s 9461.829 |walltime 2202.841 |
Transformer | epoch 0 | step 4350 |avg loss 7.491 |avg tokens 4771.600 |tokens/s 8983.641 |walltime 2208.153 |
Transformer | epoch 0 | step 4360 |avg loss 7.456 |avg tokens 4546.000 |tokens/s 8981.244 |walltime 2213.214 |
Transformer | epoch 0 | step 4370 |avg loss 7.076 |avg tokens 4741.500 |tokens/s 8907.240 |walltime 2218.538 |
Transformer | epoch 0 | step 4380 |avg loss 6.879 |avg tokens 4860.000 |tokens/s 9360.751 |walltime 2223.729 |
Transformer | epoch 0 | step 4390 |avg loss 7.314 |avg tokens 4632.600 |tokens/s 9007.579 |walltime 2228.872 |
Transformer | epoch 0 | step 4400 |avg loss 7.363 |avg tokens 4809.600 |tokens/s 9306.804 |walltime 2234.040 |
Transformer | epoch 0 | step 4410 |avg loss 7.261 |avg tokens 4511.400 |tokens/s 8794.603 |walltime 2239.170 |
Transformer | epoch 0 | step 4420 |avg loss 7.512 |avg tokens 4736.800 |tokens/s 9136.121 |walltime 2244.355 |
Transformer | epoch 0 | step 4430 |avg loss 7.567 |avg tokens 4425.300 |tokens/s 8758.583 |walltime 2249.407 |
Transformer | epoch 0 | step 4440 |avg loss 7.641 |avg tokens 4341.600 |tokens/s 8834.128 |walltime 2254.322 |
Transformer | epoch 0 | step 4450 |avg loss 7.241 |avg tokens 4696.700 |tokens/s 9033.538 |walltime 2259.521 |
Transformer | epoch 0 | step 4460 |avg loss 7.661 |avg tokens 4009.700 |tokens/s 8425.115 |walltime 2264.280 |
Transformer | epoch 0 | step 4470 |avg loss 7.522 |avg tokens 4660.400 |tokens/s 8832.695 |walltime 2269.557 |
Transformer | epoch 0 | step 4480 |avg loss 7.670 |avg tokens 4635.900 |tokens/s 9118.706 |walltime 2274.641 |
Transformer | epoch 0 | step 4490 |avg loss 7.861 |avg tokens 4497.800 |tokens/s 9002.023 |walltime 2279.637 |
Transformer | epoch 0 | step 4500 |avg loss 7.283 |avg tokens 4759.200 |tokens/s 9066.845 |walltime 2284.886 |
Transformer | epoch 0 | step 4510 |avg loss 7.081 |avg tokens 4565.000 |tokens/s 8833.137 |walltime 2290.054 |
Transformer | epoch 0 | step 4520 |avg loss 7.644 |avg tokens 4777.600 |tokens/s 9352.783 |walltime 2295.162 |
Transformer | epoch 0 | step 4530 |avg loss 7.206 |avg tokens 4808.800 |tokens/s 9168.565 |walltime 2300.407 |
Transformer | epoch 0 | step 4540 |avg loss 7.172 |avg tokens 4684.800 |tokens/s 8837.850 |walltime 2305.708 |
Transformer | epoch 0 | step 4550 |avg loss 8.052 |avg tokens 4766.500 |tokens/s 9547.221 |walltime 2310.700 |
Transformer | epoch 0 | step 4560 |avg loss 7.345 |avg tokens 4534.300 |tokens/s 8987.664 |walltime 2315.746 |
Transformer | epoch 0 | step 4570 |avg loss 7.309 |avg tokens 4792.000 |tokens/s 9262.426 |walltime 2320.919 |
Transformer | epoch 0 | step 4580 |avg loss 7.376 |avg tokens 4493.700 |tokens/s 8755.801 |walltime 2326.051 |
Transformer | epoch 0 | step 4590 |avg loss 7.494 |avg tokens 4432.400 |tokens/s 8850.240 |walltime 2331.060 |
Transformer | epoch 0 | step 4600 |avg loss 7.724 |avg tokens 4740.000 |tokens/s 9218.687 |walltime 2336.201 |
Transformer | epoch 0 | step 4610 |avg loss 7.321 |avg tokens 4532.700 |tokens/s 8822.479 |walltime 2341.339 |
Transformer | epoch 0 | step 4620 |avg loss 7.633 |avg tokens 4344.100 |tokens/s 8672.763 |walltime 2346.348 |
Transformer | epoch 0 | step 4630 |avg loss 8.189 |avg tokens 4440.600 |tokens/s 9252.875 |walltime 2351.147 |
Transformer | epoch 0 | step 4640 |avg loss 8.151 |avg tokens 4544.200 |tokens/s 9335.245 |walltime 2356.015 |
Transformer | epoch 0 | step 4650 |avg loss 7.495 |avg tokens 4390.800 |tokens/s 8684.667 |walltime 2361.071 |
Transformer | epoch 0 | step 4660 |avg loss 7.391 |avg tokens 4532.100 |tokens/s 8907.195 |walltime 2366.159 |
Transformer | epoch 0 | step 4670 |avg loss 8.120 |avg tokens 4505.100 |tokens/s 9330.749 |walltime 2370.987 |
Transformer | epoch 0 | step 4680 |avg loss 7.685 |avg tokens 4507.200 |tokens/s 9158.099 |walltime 2375.909 |
Transformer | epoch 0 | step 4690 |avg loss 7.687 |avg tokens 4162.100 |tokens/s 8373.716 |walltime 2380.879 |
Transformer | epoch 0 | step 4700 |avg loss 8.199 |avg tokens 3964.900 |tokens/s 8547.609 |walltime 2385.518 |
Transformer | epoch 0 | step 4710 |avg loss 7.488 |avg tokens 4451.900 |tokens/s 8620.222 |walltime 2390.682 |
Transformer | epoch 0 | step 4720 |avg loss 7.382 |avg tokens 4472.900 |tokens/s 8836.266 |walltime 2395.744 |
Transformer | epoch 0 | step 4730 |avg loss 7.493 |avg tokens 4342.900 |tokens/s 8580.996 |walltime 2400.805 |
Transformer | epoch 0 | step 4740 |avg loss 7.410 |avg tokens 4516.100 |tokens/s 8748.057 |walltime 2405.968 |
Transformer | epoch 0 | step 4750 |avg loss 7.539 |avg tokens 4667.000 |tokens/s 8994.668 |walltime 2411.156 |
Transformer | epoch 0 | step 4760 |avg loss 7.555 |avg tokens 4367.900 |tokens/s 8362.501 |walltime 2416.379 |
Transformer | epoch 0 | step 4770 |avg loss 7.133 |avg tokens 4503.300 |tokens/s 8776.397 |walltime 2421.511 |
Transformer | epoch 0 | step 4780 |avg loss 7.990 |avg tokens 4380.900 |tokens/s 9112.957 |walltime 2426.318 |
Transformer | epoch 0 | step 4790 |avg loss 7.339 |avg tokens 4590.400 |tokens/s 8957.846 |walltime 2431.442 |
Transformer | epoch 0 | step 4800 |avg loss 8.147 |avg tokens 4484.400 |tokens/s 9030.990 |walltime 2436.408 |
Transformer | epoch 0 | step 4810 |avg loss 8.048 |avg tokens 4613.600 |tokens/s 9266.901 |walltime 2441.387 |
Transformer | epoch 0 | step 4820 |avg loss 7.018 |avg tokens 4752.800 |tokens/s 9049.930 |walltime 2446.638 |
Transformer | epoch 0 | step 4830 |avg loss 7.754 |avg tokens 4666.100 |tokens/s 9111.428 |walltime 2451.759 |
Transformer | epoch 0 | step 4840 |avg loss 7.202 |avg tokens 4764.800 |tokens/s 9061.237 |walltime 2457.018 |
Transformer | epoch 0 | step 4850 |avg loss 7.791 |avg tokens 4130.200 |tokens/s 8204.313 |walltime 2462.052 |
Transformer | epoch 0 | step 4860 |avg loss 8.134 |avg tokens 4481.900 |tokens/s 9055.597 |walltime 2467.001 |
Transformer | epoch 0 | step 4870 |avg loss 7.430 |avg tokens 4849.400 |tokens/s 9434.780 |walltime 2472.141 |
Transformer | epoch 0 | step 4880 |avg loss 7.786 |avg tokens 4705.900 |tokens/s 9429.041 |walltime 2477.132 |
Transformer | epoch 0 | step 4890 |avg loss 7.758 |avg tokens 3942.700 |tokens/s 7960.229 |walltime 2482.085 |
Transformer | epoch 0 | step 4900 |avg loss 8.114 |avg tokens 3877.800 |tokens/s 8191.431 |walltime 2486.819 |
Transformer | epoch 0 | step 4910 |avg loss 7.902 |avg tokens 4907.200 |tokens/s 9947.368 |walltime 2491.752 |
Transformer | epoch 0 | step 4920 |avg loss 8.082 |avg tokens 4641.700 |tokens/s 9269.644 |walltime 2496.760 |
Transformer | epoch 0 | step 4930 |avg loss 7.759 |avg tokens 4803.900 |tokens/s 9357.904 |walltime 2501.893 |
Transformer | epoch 0 | step 4940 |avg loss 7.996 |avg tokens 4372.600 |tokens/s 8914.091 |walltime 2506.799 |
Transformer | epoch 0 | step 4950 |avg loss 7.721 |avg tokens 4342.200 |tokens/s 8603.731 |walltime 2511.845 |
Transformer | epoch 0 | step 4960 |avg loss 7.956 |avg tokens 4705.800 |tokens/s 9110.143 |walltime 2517.011 |
Transformer | epoch 0 | step 4970 |avg loss 7.918 |avg tokens 4300.200 |tokens/s 8844.288 |walltime 2521.873 |
Transformer | epoch 0 | step 4980 |avg loss 8.359 |avg tokens 3810.800 |tokens/s 8178.317 |walltime 2526.533 |
Transformer | epoch 0 | step 4990 |avg loss 8.101 |avg tokens 4337.000 |tokens/s 8883.297 |walltime 2531.415 |
Transformer | epoch 0 | step 5000 |avg loss 7.626 |avg tokens 4908.200 |tokens/s 9365.452 |walltime 2536.656 |
Transformer | epoch 0 | step 5010 |avg loss 7.185 |avg tokens 4392.100 |tokens/s 8649.016 |walltime 2541.734 |
Transformer | epoch 0 | step 5020 |avg loss 7.194 |avg tokens 4881.600 |tokens/s 9096.699 |walltime 2547.100 |
Transformer | epoch 0 | step 5030 |avg loss 7.489 |avg tokens 4556.000 |tokens/s 9030.012 |walltime 2552.145 |
Transformer | epoch 0 | step 5040 |avg loss 7.811 |avg tokens 4568.500 |tokens/s 9237.469 |walltime 2557.091 |
Transformer | epoch 0 | step 5050 |avg loss 7.983 |avg tokens 4718.600 |tokens/s 9429.657 |walltime 2562.095 |
Transformer | epoch 0 | step 5060 |avg loss 7.968 |avg tokens 4252.500 |tokens/s 8568.698 |walltime 2567.058 |
Transformer | epoch 0 | step 5070 |avg loss 8.329 |avg tokens 4179.300 |tokens/s 8949.325 |walltime 2571.728 |
Transformer | epoch 0 | step 5080 |avg loss 7.580 |avg tokens 4591.000 |tokens/s 8891.226 |walltime 2576.891 |
Transformer | epoch 0 | step 5090 |avg loss 8.150 |avg tokens 4182.600 |tokens/s 8385.201 |walltime 2581.879 |
Transformer | epoch 0 | step 5100 |avg loss 7.758 |avg tokens 4366.500 |tokens/s 8856.176 |walltime 2586.810 |
Transformer | epoch 0 | step 5110 |avg loss 7.173 |avg tokens 4669.500 |tokens/s 9025.694 |walltime 2591.984 |
Transformer | epoch 0 | step 5120 |avg loss 7.866 |avg tokens 4357.700 |tokens/s 8720.553 |walltime 2596.981 |
Transformer | epoch 0 | step 5130 |avg loss 7.835 |avg tokens 4765.400 |tokens/s 9052.186 |walltime 2602.245 |
Transformer | epoch 0 | step 5140 |avg loss 7.664 |avg tokens 4533.800 |tokens/s 8760.245 |walltime 2607.420 |
Transformer | epoch 0 | step 5150 |avg loss 7.653 |avg tokens 4537.100 |tokens/s 9016.738 |walltime 2612.452 |
Transformer | epoch 0 | step 5160 |avg loss 7.609 |avg tokens 4629.900 |tokens/s 8921.878 |walltime 2617.642 |
Transformer | epoch 0 | step 5170 |avg loss 7.594 |avg tokens 4836.000 |tokens/s 9317.364 |walltime 2622.832 |
Transformer | epoch 0 | step 5180 |avg loss 8.019 |avg tokens 4406.600 |tokens/s 8908.827 |walltime 2627.778 |
Transformer | epoch 0 | step 5190 |avg loss 7.728 |avg tokens 4882.500 |tokens/s 9209.990 |walltime 2633.080 |
Transformer | epoch 0 | step 5200 |avg loss 8.103 |avg tokens 4506.200 |tokens/s 8959.768 |walltime 2638.109 |
Transformer | epoch 0 | step 5210 |avg loss 8.020 |avg tokens 4086.800 |tokens/s 8530.799 |walltime 2642.900 |
Transformer | epoch 0 | step 5220 |avg loss 7.589 |avg tokens 4533.300 |tokens/s 9032.030 |walltime 2647.919 |
Transformer | epoch 0 | step 5230 |avg loss 8.041 |avg tokens 4540.300 |tokens/s 8912.187 |walltime 2653.013 |
Transformer | epoch 0 | step 5240 |avg loss 7.658 |avg tokens 4854.900 |tokens/s 9286.597 |walltime 2658.241 |
Transformer | epoch 0 | step 5250 |avg loss 8.099 |avg tokens 4628.300 |tokens/s 9440.153 |walltime 2663.144 |
Transformer | epoch 0 | step 5260 |avg loss 8.037 |avg tokens 4877.400 |tokens/s 9696.154 |walltime 2668.174 |
Transformer | epoch 0 | step 5270 |avg loss 7.552 |avg tokens 4707.200 |tokens/s 8989.954 |walltime 2673.410 |
Transformer | epoch 0 | step 5280 |avg loss 7.778 |avg tokens 4299.100 |tokens/s 8623.860 |walltime 2678.395 |
Transformer | epoch 0 | step 5290 |avg loss 8.467 |avg tokens 4819.600 |tokens/s 10213.273 |walltime 2683.114 |
Transformer | epoch 0 | step 5300 |avg loss 8.034 |avg tokens 4576.400 |tokens/s 8845.680 |walltime 2688.288 |
Transformer | epoch 0 | step 5310 |avg loss 7.850 |avg tokens 4565.000 |tokens/s 8917.367 |walltime 2693.407 |
Transformer | epoch 0 | step 5320 |avg loss 7.536 |avg tokens 4465.000 |tokens/s 8645.159 |walltime 2698.572 |
Transformer | epoch 0 | step 5330 |avg loss 8.073 |avg tokens 4213.900 |tokens/s 8804.801 |walltime 2703.358 |
Transformer | epoch 0 | step 5340 |avg loss 7.852 |avg tokens 4311.200 |tokens/s 8710.118 |walltime 2708.307 |
Transformer | epoch 0 | step 5350 |avg loss 7.507 |avg tokens 4700.800 |tokens/s 8888.130 |walltime 2713.596 |
Transformer | epoch 0 | step 5360 |avg loss 7.750 |avg tokens 4734.300 |tokens/s 9002.821 |walltime 2718.855 |
Transformer | epoch 0 | step 5370 |avg loss 7.556 |avg tokens 4915.100 |tokens/s 9237.717 |walltime 2724.176 |
Transformer | epoch 0 | step 5380 |avg loss 7.736 |avg tokens 4554.600 |tokens/s 8968.904 |walltime 2729.254 |
Transformer | epoch 0 | step 5390 |avg loss 8.122 |avg tokens 4544.200 |tokens/s 9355.035 |walltime 2734.111 |
Transformer | epoch 0 | step 5400 |avg loss 8.281 |avg tokens 4242.700 |tokens/s 8956.766 |walltime 2738.848 |
Transformer | epoch 0 | step 5410 |avg loss 7.897 |avg tokens 4043.400 |tokens/s 8355.943 |walltime 2743.687 |
Transformer | epoch 0 | step 5420 |avg loss 7.860 |avg tokens 4282.200 |tokens/s 8642.909 |walltime 2748.642 |
Transformer | epoch 0 | step 5430 |avg loss 7.872 |avg tokens 4971.500 |tokens/s 9584.179 |walltime 2753.829 |
Transformer | epoch 0 | step 5440 |avg loss 7.113 |avg tokens 4505.600 |tokens/s 8684.113 |walltime 2759.017 |
Transformer | epoch 0 | step 5450 |avg loss 7.518 |avg tokens 4814.400 |tokens/s 9081.822 |walltime 2764.318 |
Transformer | epoch 0 | step 5460 |avg loss 8.135 |avg tokens 4751.300 |tokens/s 9540.629 |walltime 2769.298 |
Transformer | epoch 0 | step 5470 |avg loss 7.812 |avg tokens 4740.500 |tokens/s 8962.555 |walltime 2774.588 |
Transformer | epoch 0 | step 5480 |avg loss 7.908 |avg tokens 4551.500 |tokens/s 9336.493 |walltime 2779.463 |
Transformer | epoch 0 | step 5490 |avg loss 7.556 |avg tokens 4830.100 |tokens/s 9282.697 |walltime 2784.666 |
Transformer | epoch 0 | step 5500 |avg loss 8.013 |avg tokens 3980.900 |tokens/s 8023.459 |walltime 2789.628 |
Transformer | epoch 0 | step 5510 |avg loss 7.657 |avg tokens 4471.000 |tokens/s 8828.721 |walltime 2794.692 |
Transformer | epoch 0 | step 5520 |avg loss 7.425 |avg tokens 4612.600 |tokens/s 8799.805 |walltime 2799.933 |
Transformer | epoch 0 | step 5530 |avg loss 7.571 |avg tokens 4534.000 |tokens/s 8724.358 |walltime 2805.130 |
Transformer | epoch 0 | step 5540 |avg loss 8.022 |avg tokens 4762.200 |tokens/s 9509.682 |walltime 2810.138 |
Transformer | epoch 0 | step 5550 |avg loss 7.713 |avg tokens 4795.800 |tokens/s 9307.772 |walltime 2815.291 |
Transformer | epoch 0 | step 5560 |avg loss 7.446 |avg tokens 4810.800 |tokens/s 9301.034 |walltime 2820.463 |
Transformer | epoch 0 | step 5570 |avg loss 8.237 |avg tokens 4233.300 |tokens/s 8860.860 |walltime 2825.240 |
Transformer | epoch 0 | step 5580 |avg loss 7.990 |avg tokens 3960.100 |tokens/s 8412.789 |walltime 2829.948 |
Transformer | epoch 0 | step 5590 |avg loss 7.362 |avg tokens 4765.600 |tokens/s 9073.347 |walltime 2835.200 |
Transformer | epoch 0 | step 5600 |avg loss 7.642 |avg tokens 4353.600 |tokens/s 8528.704 |walltime 2840.305 |
Transformer | epoch 0 | step 5610 |avg loss 7.914 |avg tokens 4202.100 |tokens/s 8784.763 |walltime 2845.088 |
Transformer | epoch 0 | step 5620 |avg loss 7.775 |avg tokens 4680.900 |tokens/s 8957.149 |walltime 2850.314 |
Transformer | epoch 0 | step 5630 |avg loss 7.935 |avg tokens 4967.100 |tokens/s 9581.963 |walltime 2855.498 |
Transformer | epoch 0 | step 5640 |avg loss 7.738 |avg tokens 4590.700 |tokens/s 9031.779 |walltime 2860.581 |
Transformer | epoch 0 | step 5650 |avg loss 7.461 |avg tokens 4399.300 |tokens/s 8746.113 |walltime 2865.611 |
Transformer | epoch 0 | step 5660 |avg loss 8.137 |avg tokens 4178.000 |tokens/s 8832.674 |walltime 2870.341 |
Transformer | epoch 0 | step 5670 |avg loss 7.605 |avg tokens 4821.600 |tokens/s 8961.059 |walltime 2875.721 |
Transformer | epoch 0 | step 5680 |avg loss 7.275 |avg tokens 4656.000 |tokens/s 9059.324 |walltime 2880.861 |
Transformer | epoch 0 | step 5690 |avg loss 7.622 |avg tokens 4441.600 |tokens/s 8719.753 |walltime 2885.955 |
Transformer | epoch 0 | step 5700 |avg loss 8.079 |avg tokens 4514.700 |tokens/s 9260.012 |walltime 2890.830 |
Transformer | epoch 0 | step 5710 |avg loss 7.868 |avg tokens 4524.300 |tokens/s 8899.370 |walltime 2895.914 |
Transformer | epoch 0 | step 5720 |avg loss 7.691 |avg tokens 4968.000 |tokens/s 9603.770 |walltime 2901.087 |
Transformer | epoch 0 | step 5730 |avg loss 7.281 |avg tokens 4988.700 |tokens/s 9216.781 |walltime 2906.499 |
Transformer | epoch 0 | step 5740 |avg loss 7.902 |avg tokens 4547.000 |tokens/s 9178.615 |walltime 2911.453 |
Transformer | epoch 0 | step 5750 |avg loss 7.834 |avg tokens 4774.100 |tokens/s 9326.888 |walltime 2916.572 |
Transformer | epoch 0 | step 5760 |avg loss 7.987 |avg tokens 4508.100 |tokens/s 8836.338 |walltime 2921.674 |
Transformer | epoch 0 | step 5770 |avg loss 8.066 |avg tokens 4745.100 |tokens/s 9580.851 |walltime 2926.627 |
Transformer | epoch 0 | step 5780 |avg loss 7.412 |avg tokens 4813.100 |tokens/s 9152.540 |walltime 2931.885 |
Transformer | epoch 0 | step 5790 |avg loss 7.900 |avg tokens 4738.500 |tokens/s 9433.959 |walltime 2936.908 |
Transformer | epoch 0 | step 5800 |avg loss 7.756 |avg tokens 4603.000 |tokens/s 8776.767 |walltime 2942.153 |
Transformer | epoch 0 | step 5810 |avg loss 7.863 |avg tokens 3978.900 |tokens/s 8301.455 |walltime 2946.946 |
Transformer | epoch 0 | step 5820 |avg loss 7.872 |avg tokens 4246.100 |tokens/s 8408.147 |walltime 2951.996 |
Transformer | epoch 0 | step 5830 |avg loss 7.938 |avg tokens 4585.900 |tokens/s 9091.325 |walltime 2957.040 |
Transformer | epoch 0 | step 5840 |avg loss 8.067 |avg tokens 4145.200 |tokens/s 8422.553 |walltime 2961.961 |
Transformer | epoch 0 | step 5850 |avg loss 7.722 |avg tokens 4870.400 |tokens/s 9259.144 |walltime 2967.222 |
Transformer | epoch 0 | step 5860 |avg loss 7.962 |avg tokens 4675.600 |tokens/s 9199.751 |walltime 2972.304 |
Transformer | epoch 0 | step 5870 |avg loss 8.039 |avg tokens 4060.900 |tokens/s 8248.298 |walltime 2977.227 |
Transformer | epoch 0 | step 5880 |avg loss 7.966 |avg tokens 4357.100 |tokens/s 8861.967 |walltime 2982.144 |
Transformer | epoch 0 | step 5890 |avg loss 7.663 |avg tokens 4756.300 |tokens/s 9184.803 |walltime 2987.322 |
Transformer | epoch 0 | step 5900 |avg loss 7.728 |avg tokens 4540.600 |tokens/s 9341.679 |walltime 2992.183 |
Transformer | epoch 0 | step 5910 |avg loss 7.792 |avg tokens 4254.600 |tokens/s 8722.307 |walltime 2997.061 |
Transformer | epoch 0 | step 5920 |avg loss 7.600 |avg tokens 4638.100 |tokens/s 9182.011 |walltime 3002.112 |
Transformer | epoch 0 | step 5930 |avg loss 7.896 |avg tokens 4795.700 |tokens/s 9337.123 |walltime 3007.248 |
Transformer | epoch 0 | step 5940 |avg loss 8.203 |avg tokens 3767.200 |tokens/s 8064.993 |walltime 3011.919 |
Transformer | epoch 0 | step 5950 |avg loss 7.757 |avg tokens 4590.200 |tokens/s 9156.896 |walltime 3016.932 |
Transformer | epoch 0 | step 5960 |avg loss 8.171 |avg tokens 3981.200 |tokens/s 8157.677 |walltime 3021.812 |
Transformer | epoch 0 | step 5970 |avg loss 7.993 |avg tokens 4529.100 |tokens/s 8854.481 |walltime 3026.927 |
Transformer | epoch 0 | step 5980 |avg loss 7.813 |avg tokens 4394.700 |tokens/s 8732.456 |walltime 3031.960 |
Transformer | epoch 0 | step 5990 |avg loss 7.905 |avg tokens 4481.600 |tokens/s 8875.326 |walltime 3037.009 |
Transformer | epoch 0 | step 6000 |avg loss 7.842 |avg tokens 4864.300 |tokens/s 9501.596 |walltime 3042.129 |
Transformer | epoch 0 | step 6010 |avg loss 7.202 |avg tokens 4601.600 |tokens/s 8885.265 |walltime 3047.308 |
Transformer | epoch 0 | step 6020 |avg loss 7.554 |avg tokens 4697.600 |tokens/s 9051.114 |walltime 3052.498 |
Transformer | epoch 0 | step 6030 |avg loss 7.551 |avg tokens 4615.100 |tokens/s 8758.163 |walltime 3057.767 |
Transformer | epoch 0 | step 6040 |avg loss 7.987 |avg tokens 4142.700 |tokens/s 8373.975 |walltime 3062.715 |
Transformer | epoch 0 | step 6050 |avg loss 7.488 |avg tokens 4808.200 |tokens/s 8993.243 |walltime 3068.061 |
Transformer | epoch 0 | step 6060 |avg loss 7.810 |avg tokens 4390.100 |tokens/s 8721.581 |walltime 3073.095 |
Transformer | epoch 0 | step 6070 |avg loss 7.818 |avg tokens 4571.900 |tokens/s 8819.474 |walltime 3078.278 |
Transformer | epoch 0 | step 6080 |avg loss 8.178 |avg tokens 3983.800 |tokens/s 8325.962 |walltime 3083.063 |
Transformer | epoch 0 | step 6090 |avg loss 8.060 |avg tokens 4429.200 |tokens/s 8612.096 |walltime 3088.206 |
Transformer | epoch 0 | step 6100 |avg loss 8.150 |avg tokens 4248.800 |tokens/s 8603.635 |walltime 3093.145 |
Transformer | epoch 0 | step 6110 |avg loss 8.110 |avg tokens 4191.800 |tokens/s 8666.418 |walltime 3097.982 |
Transformer | epoch 0 | step 6120 |avg loss 7.845 |avg tokens 4347.300 |tokens/s 8572.257 |walltime 3103.053 |
Transformer | epoch 0 | step 6130 |avg loss 7.915 |avg tokens 4275.200 |tokens/s 8801.411 |walltime 3107.910 |
Transformer | epoch 0 | step 6140 |avg loss 8.039 |avg tokens 4477.800 |tokens/s 8793.717 |walltime 3113.002 |
Transformer | epoch 0 | step 6150 |avg loss 8.215 |avg tokens 4063.100 |tokens/s 8291.831 |walltime 3117.902 |
Transformer | epoch 0 | step 6160 |avg loss 7.859 |avg tokens 4832.200 |tokens/s 9217.757 |walltime 3123.145 |
Transformer | epoch 0 | step 6170 |avg loss 7.860 |avg tokens 4673.300 |tokens/s 8973.015 |walltime 3128.353 |
Transformer | epoch 0 | step 6180 |avg loss 8.672 |avg tokens 4442.300 |tokens/s 8643.490 |walltime 3133.492 |
Transformer | epoch 0 | step 6190 |avg loss 8.019 |avg tokens 4277.100 |tokens/s 8485.688 |walltime 3138.533 |
Transformer | epoch 0 | step 6200 |avg loss 8.411 |avg tokens 3508.700 |tokens/s 7369.312 |walltime 3143.294 |
Transformer | epoch 0 | step 6210 |avg loss 8.221 |avg tokens 4281.600 |tokens/s 9259.142 |walltime 3147.918 |
Transformer | epoch 0 | step 6220 |avg loss 7.695 |avg tokens 4976.400 |tokens/s 9351.812 |walltime 3153.239 |
Transformer | epoch 0 | step 6230 |avg loss 7.942 |avg tokens 4799.200 |tokens/s 9225.381 |walltime 3158.442 |
Transformer | epoch 0 | step 6240 |avg loss 7.841 |avg tokens 4651.200 |tokens/s 8892.106 |walltime 3163.672 |
Transformer | epoch 0 | step 6250 |avg loss 7.811 |avg tokens 4634.700 |tokens/s 8916.133 |walltime 3168.870 |
Transformer | epoch 0 | step 6260 |avg loss 7.745 |avg tokens 4682.400 |tokens/s 8893.753 |walltime 3174.135 |
Transformer | epoch 0 | step 6270 |avg loss 8.057 |avg tokens 4843.400 |tokens/s 9344.954 |walltime 3179.318 |
Transformer | epoch 0 | step 6280 |avg loss 7.812 |avg tokens 4779.200 |tokens/s 9219.725 |walltime 3184.502 |
Transformer | epoch 0 | step 6290 |avg loss 7.905 |avg tokens 4834.100 |tokens/s 9476.380 |walltime 3189.603 |
Transformer | epoch 0 | step 6300 |avg loss 8.106 |avg tokens 4579.900 |tokens/s 8968.315 |walltime 3194.710 |
Transformer | epoch 0 | step 6310 |avg loss 8.138 |avg tokens 4461.900 |tokens/s 9149.806 |walltime 3199.586 |
Transformer | epoch 0 | step 6320 |avg loss 8.232 |avg tokens 4766.100 |tokens/s 9351.367 |walltime 3204.683 |
Transformer | epoch 0 | step 6330 |avg loss 7.877 |avg tokens 4811.500 |tokens/s 9256.925 |walltime 3209.881 |
Transformer | epoch 0 | step 6340 |avg loss 7.899 |avg tokens 4386.400 |tokens/s 8540.397 |walltime 3215.017 |
Transformer | epoch 0 | step 6350 |avg loss 8.059 |avg tokens 4858.800 |tokens/s 9392.276 |walltime 3220.190 |
Transformer | epoch 0 | step 6360 |avg loss 8.033 |avg tokens 4440.800 |tokens/s 8709.354 |walltime 3225.289 |
Transformer | epoch 0 | step 6370 |avg loss 7.966 |avg tokens 4299.800 |tokens/s 8716.608 |walltime 3230.222 |
Transformer | epoch 0 | step 6380 |avg loss 7.997 |avg tokens 4332.900 |tokens/s 8643.760 |walltime 3235.235 |
Transformer | epoch 0 | step 6390 |avg loss 8.193 |avg tokens 4825.100 |tokens/s 9452.387 |walltime 3240.339 |
Transformer | epoch 0 | step 6400 |avg loss 7.673 |avg tokens 4639.200 |tokens/s 8846.890 |walltime 3245.583 |
Transformer | epoch 0 | step 6410 |avg loss 7.913 |avg tokens 4543.500 |tokens/s 8811.448 |walltime 3250.739 |
Transformer | epoch 0 | step 6420 |avg loss 8.346 |avg tokens 4013.500 |tokens/s 8484.581 |walltime 3255.470 |
Transformer | epoch 0 | step 6430 |avg loss 7.847 |avg tokens 4544.200 |tokens/s 8886.552 |walltime 3260.583 |
Transformer | epoch 0 | step 6440 |avg loss 8.021 |avg tokens 4657.400 |tokens/s 9098.047 |walltime 3265.702 |
Transformer | epoch 0 | step 6450 |avg loss 8.117 |avg tokens 4717.100 |tokens/s 9172.077 |walltime 3270.845 |
Transformer | epoch 0 | step 6460 |avg loss 8.122 |avg tokens 4130.900 |tokens/s 8328.554 |walltime 3275.805 |
Transformer | epoch 0 | step 6470 |avg loss 7.958 |avg tokens 4565.600 |tokens/s 8826.342 |walltime 3280.978 |
Transformer | epoch 0 | step 6480 |avg loss 7.631 |avg tokens 4682.600 |tokens/s 8809.349 |walltime 3286.294 |
Transformer | epoch 0 | step 6490 |avg loss 8.125 |avg tokens 4769.600 |tokens/s 9133.814 |walltime 3291.515 |
Transformer | epoch 0 | step 6500 |avg loss 8.008 |avg tokens 4646.300 |tokens/s 9056.137 |walltime 3296.646 |
Transformer | epoch 0 | step 6510 |avg loss 7.720 |avg tokens 4820.700 |tokens/s 9102.089 |walltime 3301.942 |
Transformer | epoch 0 | step 6520 |avg loss 8.240 |avg tokens 4183.700 |tokens/s 8613.046 |walltime 3306.800 |
Transformer | epoch 0 | step 6530 |avg loss 8.078 |avg tokens 4669.000 |tokens/s 9149.163 |walltime 3311.903 |
Transformer | epoch 0 | step 6540 |avg loss 8.092 |avg tokens 4303.600 |tokens/s 9002.595 |walltime 3316.683 |
Transformer | epoch 0 | step 6550 |avg loss 8.426 |avg tokens 4258.900 |tokens/s 8696.414 |walltime 3321.581 |
Transformer | epoch 0 | step 6560 |avg loss 8.132 |avg tokens 4704.900 |tokens/s 9273.090 |walltime 3326.654 |
Transformer | epoch 0 | step 6570 |avg loss 8.327 |avg tokens 4773.300 |tokens/s 9246.963 |walltime 3331.816 |
Transformer | epoch 0 | step 6580 |avg loss 8.093 |avg tokens 4481.600 |tokens/s 9150.188 |walltime 3336.714 |
Transformer | epoch 0 | step 6590 |avg loss 8.237 |avg tokens 4183.000 |tokens/s 8778.349 |walltime 3341.479 |
Transformer | epoch 0 | step 6600 |avg loss 8.224 |avg tokens 4447.100 |tokens/s 8813.804 |walltime 3346.525 |
Transformer | epoch 0 | step 6610 |avg loss 7.994 |avg tokens 4696.700 |tokens/s 9054.348 |walltime 3351.712 |
Transformer | epoch 0 | step 6620 |avg loss 7.880 |avg tokens 4371.500 |tokens/s 8534.431 |walltime 3356.834 |
Transformer | epoch 0 | step 6630 |avg loss 7.650 |avg tokens 4422.400 |tokens/s 8655.200 |walltime 3361.944 |
Transformer | epoch 0 | step 6640 |avg loss 8.099 |avg tokens 4593.800 |tokens/s 8923.877 |walltime 3367.092 |
Transformer | epoch 0 | step 6650 |avg loss 8.047 |avg tokens 4787.400 |tokens/s 9339.201 |walltime 3372.218 |
Transformer | epoch 0 | step 6660 |avg loss 8.073 |avg tokens 4567.900 |tokens/s 8912.378 |walltime 3377.343 |
Transformer | epoch 0 | step 6670 |avg loss 7.884 |avg tokens 4498.900 |tokens/s 8731.230 |walltime 3382.496 |
Transformer | epoch 0 | step 6680 |avg loss 8.152 |avg tokens 4368.000 |tokens/s 8911.172 |walltime 3387.397 |
Transformer | epoch 0 | step 6690 |avg loss 8.346 |avg tokens 4604.300 |tokens/s 9311.610 |walltime 3392.342 |
Transformer | epoch 0 | step 6700 |avg loss 8.263 |avg tokens 4301.800 |tokens/s 8496.204 |walltime 3397.405 |
Transformer | epoch 0 | step 6710 |avg loss 7.884 |avg tokens 4702.600 |tokens/s 8959.142 |walltime 3402.654 |
Transformer | epoch 0 | step 6720 |avg loss 8.185 |avg tokens 4368.700 |tokens/s 8738.835 |walltime 3407.653 |
Transformer | epoch 0 | step 6730 |avg loss 8.431 |avg tokens 4250.700 |tokens/s 8386.007 |walltime 3412.722 |
Transformer | epoch 0 | step 6740 |avg loss 8.169 |avg tokens 4324.000 |tokens/s 8683.797 |walltime 3417.702 |
Transformer | epoch 0 | step 6750 |avg loss 8.467 |avg tokens 4482.000 |tokens/s 9035.819 |walltime 3422.662 |
Transformer | epoch 0 | step 6760 |avg loss 8.114 |avg tokens 4498.600 |tokens/s 8700.856 |walltime 3427.832 |
Transformer | epoch 0 | step 6770 |avg loss 8.090 |avg tokens 4635.300 |tokens/s 9094.476 |walltime 3432.929 |
Transformer | epoch 0 | step 6780 |avg loss 8.357 |avg tokens 4844.200 |tokens/s 9320.330 |walltime 3438.127 |
Transformer | epoch 0 | step 6790 |avg loss 8.224 |avg tokens 4699.300 |tokens/s 9070.714 |walltime 3443.307 |
Transformer | epoch 0 | step 6800 |avg loss 8.321 |avg tokens 4374.000 |tokens/s 9010.779 |walltime 3448.161 |
Transformer | epoch 0 | step 6810 |avg loss 8.106 |avg tokens 4399.600 |tokens/s 8816.300 |walltime 3453.152 |
Transformer | epoch 0 | step 6820 |avg loss 8.192 |avg tokens 4745.700 |tokens/s 9335.021 |walltime 3458.236 |
Transformer | epoch 0 | step 6830 |avg loss 8.220 |avg tokens 4489.900 |tokens/s 9018.165 |walltime 3463.214 |
Transformer | epoch 0 | step 6840 |avg loss 8.163 |avg tokens 4652.100 |tokens/s 9064.806 |walltime 3468.346 |
Transformer | epoch 0 | step 6850 |avg loss 7.998 |avg tokens 4760.700 |tokens/s 9228.113 |walltime 3473.505 |
Transformer | epoch 0 | step 6860 |avg loss 7.974 |avg tokens 4758.200 |tokens/s 9116.126 |walltime 3478.725 |
Transformer | epoch 0 | step 6870 |avg loss 7.953 |avg tokens 4793.500 |tokens/s 9539.471 |walltime 3483.750 |
Transformer | epoch 0 | step 6880 |avg loss 7.949 |avg tokens 4641.100 |tokens/s 9025.067 |walltime 3488.892 |
Transformer | epoch 0 | step 6890 |avg loss 8.006 |avg tokens 4334.000 |tokens/s 8559.264 |walltime 3493.956 |
Transformer | epoch 0 | step 6900 |avg loss 8.245 |avg tokens 4516.400 |tokens/s 8769.547 |walltime 3499.106 |
Transformer | epoch 0 | step 6910 |avg loss 8.178 |avg tokens 4318.300 |tokens/s 8921.221 |walltime 3503.946 |
Transformer | epoch 0 | step 6920 |avg loss 8.207 |avg tokens 4509.500 |tokens/s 9091.609 |walltime 3508.906 |
Transformer | epoch 0 | step 6930 |avg loss 8.248 |avg tokens 4521.000 |tokens/s 9058.308 |walltime 3513.897 |
Transformer | epoch 0 | step 6940 |avg loss 8.184 |avg tokens 4183.500 |tokens/s 8287.864 |walltime 3518.945 |
Transformer | epoch 0 | step 6950 |avg loss 8.044 |avg tokens 4538.400 |tokens/s 8862.657 |walltime 3524.066 |
Transformer | epoch 0 | step 6960 |avg loss 8.339 |avg tokens 4481.400 |tokens/s 9047.302 |walltime 3529.019 |
Transformer | epoch 0 | step 6970 |avg loss 8.259 |avg tokens 4647.500 |tokens/s 9683.197 |walltime 3533.819 |
Transformer | epoch 0 | step 6980 |avg loss 8.228 |avg tokens 4731.300 |tokens/s 9094.369 |walltime 3539.021 |
Transformer | epoch 0 | step 6990 |avg loss 8.183 |avg tokens 4698.600 |tokens/s 9272.752 |walltime 3544.088 |
Transformer | epoch 0 | step 7000 |avg loss 7.970 |avg tokens 4260.100 |tokens/s 8338.649 |walltime 3549.197 |
Transformer | epoch 0 | step 7010 |avg loss 7.816 |avg tokens 4759.000 |tokens/s 9060.083 |walltime 3554.450 |
Transformer | epoch 0 | step 7020 |avg loss 7.818 |avg tokens 4563.900 |tokens/s 8804.056 |walltime 3559.634 |
Transformer | epoch 0 | step 7030 |avg loss 8.090 |avg tokens 4581.400 |tokens/s 8899.516 |walltime 3564.782 |
Transformer | epoch 0 | step 7040 |avg loss 8.626 |avg tokens 4292.000 |tokens/s 9049.785 |walltime 3569.524 |
Transformer | epoch 0 | step 7050 |avg loss 8.256 |avg tokens 4325.900 |tokens/s 8776.041 |walltime 3574.454 |
Transformer | epoch 0 | step 7060 |avg loss 8.286 |avg tokens 4818.200 |tokens/s 9172.008 |walltime 3579.707 |
Transformer | epoch 0 | step 7070 |avg loss 8.321 |avg tokens 4407.900 |tokens/s 9101.783 |walltime 3584.550 |
Transformer | epoch 0 | step 7080 |avg loss 8.772 |avg tokens 4781.900 |tokens/s 9625.802 |walltime 3589.517 |
Transformer | epoch 0 | step 7090 |avg loss 8.217 |avg tokens 4548.500 |tokens/s 8945.863 |walltime 3594.602 |
Transformer | epoch 0 | step 7100 |avg loss 8.324 |avg tokens 4652.300 |tokens/s 9263.121 |walltime 3599.624 |
Transformer | epoch 0 | step 7110 |avg loss 8.609 |avg tokens 3993.300 |tokens/s 9119.081 |walltime 3604.003 |
Transformer | epoch 0 | step 7120 |avg loss 8.048 |avg tokens 4723.800 |tokens/s 9034.652 |walltime 3609.232 |
Transformer | epoch 0 | step 7130 |avg loss 8.541 |avg tokens 4403.300 |tokens/s 8884.382 |walltime 3614.188 |
Transformer | epoch 0 | step 7140 |avg loss 8.301 |avg tokens 4635.200 |tokens/s 8962.791 |walltime 3619.360 |
Transformer | epoch 0 | step 7150 |avg loss 8.303 |avg tokens 4669.100 |tokens/s 9049.031 |walltime 3624.519 |
Transformer | epoch 0 | step 7160 |avg loss 8.208 |avg tokens 4276.500 |tokens/s 8647.974 |walltime 3629.465 |
Transformer | epoch 0 | step 7170 |avg loss 8.223 |avg tokens 4189.200 |tokens/s 8870.513 |walltime 3634.187 |
Transformer | epoch 0 | step 7180 |avg loss 8.629 |avg tokens 3872.000 |tokens/s 8272.821 |walltime 3638.868 |
Transformer | epoch 0 | step 7190 |avg loss 8.046 |avg tokens 4417.200 |tokens/s 8727.781 |walltime 3643.929 |
Transformer | epoch 0 | step 7200 |avg loss 8.299 |avg tokens 4595.700 |tokens/s 9032.123 |walltime 3649.017 |
Transformer | epoch 0 | step 7210 |avg loss 8.358 |avg tokens 4239.300 |tokens/s 8656.950 |walltime 3653.914 |
Transformer | epoch 0 | step 7220 |avg loss 8.428 |avg tokens 4537.500 |tokens/s 8948.111 |walltime 3658.985 |
Transformer | epoch 0 | step 7230 |avg loss 8.210 |avg tokens 4641.100 |tokens/s 9278.495 |walltime 3663.987 |
Transformer | epoch 0 | step 7240 |avg loss 8.359 |avg tokens 4740.400 |tokens/s 9101.463 |walltime 3669.195 |
Transformer | epoch 0 | step 7250 |avg loss 8.146 |avg tokens 4377.700 |tokens/s 8850.340 |walltime 3674.141 |
Transformer | epoch 0 | step 7260 |avg loss 8.123 |avg tokens 4420.000 |tokens/s 8829.199 |walltime 3679.148 |
Transformer | epoch 0 | step 7270 |avg loss 7.990 |avg tokens 4913.500 |tokens/s 9328.159 |walltime 3684.415 |
Transformer | epoch 0 | step 7280 |avg loss 8.190 |avg tokens 4689.600 |tokens/s 9062.570 |walltime 3689.590 |
Transformer | epoch 0 | step 7290 |avg loss 8.071 |avg tokens 4702.200 |tokens/s 9190.017 |walltime 3694.706 |
Transformer | epoch 0 | step 7300 |avg loss 8.045 |avg tokens 4470.300 |tokens/s 8797.959 |walltime 3699.787 |
Transformer | epoch 0 | step 7310 |avg loss 8.142 |avg tokens 4326.200 |tokens/s 8639.398 |walltime 3704.795 |
Transformer | epoch 0 | step 7320 |avg loss 8.109 |avg tokens 4582.800 |tokens/s 9056.115 |walltime 3709.855 |
Transformer | epoch 0 | step 7330 |avg loss 8.299 |avg tokens 4382.600 |tokens/s 8628.930 |walltime 3714.934 |
Transformer | epoch 0 | step 7340 |avg loss 8.069 |avg tokens 4686.300 |tokens/s 8970.071 |walltime 3720.159 |
Transformer | epoch 0 | step 7350 |avg loss 8.280 |avg tokens 4648.500 |tokens/s 9268.526 |walltime 3725.174 |
Transformer | epoch 0 | step 7360 |avg loss 7.829 |avg tokens 4927.200 |tokens/s 9227.051 |walltime 3730.514 |
Transformer | epoch 0 | step 7370 |avg loss 8.373 |avg tokens 4401.400 |tokens/s 9047.963 |walltime 3735.379 |
Transformer | epoch 0 | step 7380 |avg loss 7.907 |avg tokens 4727.200 |tokens/s 8899.884 |walltime 3740.690 |
Transformer | epoch 0 | step 7390 |avg loss 8.118 |avg tokens 4283.700 |tokens/s 8847.879 |walltime 3745.532 |
Transformer | epoch 0 | step 7400 |avg loss 7.845 |avg tokens 4539.300 |tokens/s 8418.979 |walltime 3750.923 |
Transformer | epoch 0 | step 7410 |avg loss 8.130 |avg tokens 4495.000 |tokens/s 8866.230 |walltime 3755.993 |
Transformer | epoch 0 | step 7420 |avg loss 8.012 |avg tokens 4688.500 |tokens/s 9211.850 |walltime 3761.083 |
Transformer | epoch 0 | step 7430 |avg loss 7.847 |avg tokens 4866.200 |tokens/s 9260.882 |walltime 3766.337 |
Transformer | epoch 0 | step 7440 |avg loss 8.081 |avg tokens 4888.200 |tokens/s 9518.420 |walltime 3771.473 |
Transformer | epoch 0 | step 7450 |avg loss 8.133 |avg tokens 4605.200 |tokens/s 9030.811 |walltime 3776.572 |
Transformer | epoch 0 | step 7460 |avg loss 8.080 |avg tokens 4847.200 |tokens/s 9220.710 |walltime 3781.829 |
Transformer | epoch 0 | step 7470 |avg loss 8.184 |avg tokens 4662.600 |tokens/s 9012.220 |walltime 3787.003 |
Transformer | epoch 0 | step 7480 |avg loss 8.150 |avg tokens 4209.700 |tokens/s 8442.231 |walltime 3791.989 |
Transformer | epoch 0 | step 7490 |avg loss 8.160 |avg tokens 4464.500 |tokens/s 8707.089 |walltime 3797.117 |
Transformer | epoch 0 | step 7500 |avg loss 8.078 |avg tokens 4699.700 |tokens/s 8975.295 |walltime 3802.353 |
Transformer | epoch 0 | step 7510 |avg loss 8.179 |avg tokens 4515.600 |tokens/s 8984.693 |walltime 3807.379 |
Transformer | epoch 0 | step 7520 |avg loss 8.142 |avg tokens 4696.500 |tokens/s 9048.013 |walltime 3812.570 |
Transformer | epoch 0 | step 7530 |avg loss 8.257 |avg tokens 4454.000 |tokens/s 8783.787 |walltime 3817.640 |
Transformer | epoch 0 | step 7540 |avg loss 8.109 |avg tokens 4552.300 |tokens/s 8742.295 |walltime 3822.847 |
Transformer | epoch 0 | step 7550 |avg loss 8.157 |avg tokens 4198.500 |tokens/s 8661.919 |walltime 3827.695 |
Transformer | epoch 0 | step 7560 |avg loss 8.211 |avg tokens 4670.900 |tokens/s 9097.119 |walltime 3832.829 |
Transformer | epoch 0 | step 7570 |avg loss 7.692 |avg tokens 4459.900 |tokens/s 8566.566 |walltime 3838.035 |
Transformer | epoch 0 | step 7580 |avg loss 8.207 |avg tokens 4442.400 |tokens/s 8683.390 |walltime 3843.151 |
Transformer | epoch 0 | step 7590 |avg loss 8.182 |avg tokens 4687.200 |tokens/s 8858.599 |walltime 3848.442 |
Transformer | epoch 0 | step 7600 |avg loss 8.130 |avg tokens 4768.900 |tokens/s 9076.334 |walltime 3853.697 |
Transformer | epoch 0 | step 7610 |avg loss 8.376 |avg tokens 4828.000 |tokens/s 9431.073 |walltime 3858.816 |
Transformer | epoch 0 | step 7620 |avg loss 8.073 |avg tokens 4369.000 |tokens/s 8621.006 |walltime 3863.884 |
Transformer | epoch 0 | step 7630 |avg loss 7.987 |avg tokens 4227.700 |tokens/s 8506.907 |walltime 3868.853 |
Transformer | epoch 0 | step 7640 |avg loss 8.250 |avg tokens 4225.600 |tokens/s 8764.393 |walltime 3873.675 |
Transformer | epoch 0 | step 7650 |avg loss 8.118 |avg tokens 4722.400 |tokens/s 9059.534 |walltime 3878.887 |
Transformer | epoch 0 | step 7660 |avg loss 8.182 |avg tokens 4489.500 |tokens/s 9042.956 |walltime 3883.852 |
Transformer | epoch 0 | step 7670 |avg loss 8.347 |avg tokens 4387.600 |tokens/s 8576.740 |walltime 3888.968 |
Transformer | epoch 0 | step 7680 |avg loss 8.374 |avg tokens 4654.100 |tokens/s 9334.848 |walltime 3893.953 |
Transformer | epoch 0 | step 7690 |avg loss 7.960 |avg tokens 4739.400 |tokens/s 8976.783 |walltime 3899.233 |
Transformer | epoch 0 | step 7700 |avg loss 8.127 |avg tokens 4348.300 |tokens/s 8538.800 |walltime 3904.325 |
Transformer | epoch 0 | step 7710 |avg loss 8.207 |avg tokens 4419.700 |tokens/s 9228.032 |walltime 3909.115 |
Transformer | epoch 0 | step 7720 |avg loss 8.700 |avg tokens 3701.600 |tokens/s 7723.224 |walltime 3913.908 |
Transformer | epoch 0 | step 7730 |avg loss 8.048 |avg tokens 4857.200 |tokens/s 9271.562 |walltime 3919.146 |
Transformer | epoch 0 | step 7740 |avg loss 8.133 |avg tokens 4926.300 |tokens/s 9466.252 |walltime 3924.351 |
Transformer | epoch 0 | step 7750 |avg loss 8.149 |avg tokens 4336.200 |tokens/s 8676.524 |walltime 3929.348 |
Transformer | epoch 0 | step 7760 |avg loss 8.532 |avg tokens 4675.800 |tokens/s 9765.108 |walltime 3934.136 |
Transformer | epoch 0 | step 7770 |avg loss 7.912 |avg tokens 4824.400 |tokens/s 9043.133 |walltime 3939.471 |
Transformer | epoch 0 | step 7780 |avg loss 8.035 |avg tokens 4780.400 |tokens/s 9415.467 |walltime 3944.548 |
Transformer | epoch 0 | step 7790 |avg loss 8.301 |avg tokens 4546.100 |tokens/s 8775.239 |walltime 3949.729 |
Transformer | epoch 0 | step 7800 |avg loss 8.328 |avg tokens 4678.500 |tokens/s 9152.515 |walltime 3954.841 |
Transformer | epoch 0 | step 7810 |avg loss 8.159 |avg tokens 4768.400 |tokens/s 9416.868 |walltime 3959.904 |
Transformer | epoch 0 | step 7820 |avg loss 8.352 |avg tokens 4447.400 |tokens/s 8811.239 |walltime 3964.952 |
Transformer | epoch 0 | step 7830 |avg loss 7.882 |avg tokens 4864.000 |tokens/s 9194.620 |walltime 3970.242 |
Transformer | epoch 0 | step 7840 |avg loss 8.213 |avg tokens 4629.300 |tokens/s 9138.897 |walltime 3975.307 |
Transformer | epoch 0 | step 7850 |avg loss 8.156 |avg tokens 4351.200 |tokens/s 8729.399 |walltime 3980.292 |
Transformer | epoch 0 | step 7860 |avg loss 8.283 |avg tokens 4784.400 |tokens/s 9495.652 |walltime 3985.331 |
Transformer | epoch 0 | step 7870 |avg loss 8.131 |avg tokens 4261.800 |tokens/s 8483.904 |walltime 3990.354 |
Transformer | epoch 0 | step 7880 |avg loss 8.453 |avg tokens 3737.100 |tokens/s 8157.847 |walltime 3994.935 |
Transformer | epoch 0 | step 7890 |avg loss 8.067 |avg tokens 4448.800 |tokens/s 8839.523 |walltime 3999.968 |
Transformer | epoch 0 | step 7900 |avg loss 8.459 |avg tokens 4413.300 |tokens/s 9061.093 |walltime 4004.838 |
Transformer | epoch 0 | step 7910 |avg loss 8.231 |avg tokens 4544.700 |tokens/s 9041.905 |walltime 4009.865 |
Transformer | epoch 0 | step 7920 |avg loss 8.068 |avg tokens 4587.500 |tokens/s 9008.402 |walltime 4014.957 |
Transformer | epoch 0 | step 7930 |avg loss 8.213 |avg tokens 4697.700 |tokens/s 9188.732 |walltime 4020.070 |
Transformer | epoch 0 | step 7940 |avg loss 8.170 |avg tokens 4615.300 |tokens/s 9006.999 |walltime 4025.194 |
Transformer | epoch 0 | step 7950 |avg loss 7.900 |avg tokens 4754.700 |tokens/s 8931.606 |walltime 4030.517 |
Transformer | epoch 0 | step 7960 |avg loss 8.087 |avg tokens 4309.300 |tokens/s 8455.574 |walltime 4035.614 |
Transformer | epoch 0 | step 7970 |avg loss 8.210 |avg tokens 4799.800 |tokens/s 9305.926 |walltime 4040.771 |
Transformer | epoch 0 | step 7980 |avg loss 8.004 |avg tokens 4689.600 |tokens/s 8897.818 |walltime 4046.042 |
Transformer | epoch 0 | step 7990 |avg loss 8.210 |avg tokens 4243.200 |tokens/s 8684.391 |walltime 4050.928 |
Transformer | epoch 0 | step 8000 |avg loss 8.167 |avg tokens 4434.600 |tokens/s 8850.279 |walltime 4055.939 |
Transformer | epoch 0 | step 8010 |avg loss 7.988 |avg tokens 4590.400 |tokens/s 8794.911 |walltime 4061.158 |
Transformer | epoch 0 | step 8020 |avg loss 8.097 |avg tokens 4910.500 |tokens/s 9260.029 |walltime 4066.461 |
Transformer | epoch 0 | step 8030 |avg loss 8.298 |avg tokens 4260.400 |tokens/s 8769.687 |walltime 4071.319 |
Transformer | epoch 0 | step 8040 |avg loss 8.070 |avg tokens 4849.900 |tokens/s 9094.335 |walltime 4076.652 |
Transformer | epoch 0 | step 8050 |avg loss 8.158 |avg tokens 4614.700 |tokens/s 8609.740 |walltime 4082.012 |
Transformer | epoch 0 | step 8060 |avg loss 8.224 |avg tokens 4746.900 |tokens/s 9052.381 |walltime 4087.256 |
Transformer | epoch 0 | step 8070 |avg loss 8.016 |avg tokens 4722.300 |tokens/s 8868.601 |walltime 4092.580 |
Transformer | epoch 0 | step 8080 |avg loss 7.908 |avg tokens 4718.100 |tokens/s 9033.843 |walltime 4097.803 |
Transformer | epoch 0 | step 8090 |avg loss 7.985 |avg tokens 4712.600 |tokens/s 8979.143 |walltime 4103.051 |
Transformer | epoch 0 | step 8100 |avg loss 8.337 |avg tokens 4954.200 |tokens/s 9476.281 |walltime 4108.279 |
Transformer | epoch 0 | step 8110 |avg loss 8.562 |avg tokens 3927.200 |tokens/s 8418.072 |walltime 4112.945 |
Transformer | epoch 0 | step 8120 |avg loss 8.489 |avg tokens 4561.900 |tokens/s 9077.546 |walltime 4117.970 |
Transformer | epoch 0 | step 8130 |avg loss 7.992 |avg tokens 4786.800 |tokens/s 9017.807 |walltime 4123.278 |
Transformer | epoch 0 | step 8140 |avg loss 7.800 |avg tokens 4452.000 |tokens/s 8710.415 |walltime 4128.389 |
Transformer | epoch 0 | step 8150 |avg loss 8.252 |avg tokens 4906.400 |tokens/s 9348.948 |walltime 4133.637 |
Transformer | epoch 0 | step 8160 |avg loss 8.014 |avg tokens 4530.900 |tokens/s 8890.377 |walltime 4138.734 |
Transformer | epoch 0 | step 8170 |avg loss 7.792 |avg tokens 4694.500 |tokens/s 8959.197 |walltime 4143.974 |
Transformer | epoch 0 | step 8180 |avg loss 8.101 |avg tokens 4698.200 |tokens/s 9228.428 |walltime 4149.065 |
Transformer | epoch 0 | step 8190 |avg loss 8.388 |avg tokens 4699.900 |tokens/s 9334.367 |walltime 4154.100 |
Transformer | epoch 0 | step 8200 |avg loss 8.340 |avg tokens 4298.400 |tokens/s 8759.075 |walltime 4159.007 |
Transformer | epoch 0 | step 8210 |avg loss 8.499 |avg tokens 4809.600 |tokens/s 9782.165 |walltime 4163.924 |
Transformer | epoch 0 | step 8220 |avg loss 8.309 |avg tokens 4400.100 |tokens/s 8837.642 |walltime 4168.903 |
Transformer | epoch 0 | step 8230 |avg loss 8.320 |avg tokens 4618.500 |tokens/s 9429.955 |walltime 4173.800 |
Transformer | epoch 0 | step 8240 |avg loss 8.154 |avg tokens 4556.000 |tokens/s 8669.456 |walltime 4179.056 |
Transformer | epoch 0 | step 8250 |avg loss 8.667 |avg tokens 4226.500 |tokens/s 8728.082 |walltime 4183.898 |
Transformer | epoch 0 | step 8260 |avg loss 8.192 |avg tokens 4690.200 |tokens/s 9143.046 |walltime 4189.028 |
Transformer | epoch 0 | step 8270 |avg loss 8.644 |avg tokens 4401.600 |tokens/s 9078.753 |walltime 4193.876 |
Transformer | epoch 0 | step 8280 |avg loss 8.222 |avg tokens 4590.900 |tokens/s 8732.697 |walltime 4199.133 |
Transformer | epoch 0 | step 8290 |avg loss 8.551 |avg tokens 4393.700 |tokens/s 9383.062 |walltime 4203.816 |
Transformer | epoch 0 | step 8300 |avg loss 8.477 |avg tokens 4829.600 |tokens/s 9540.406 |walltime 4208.878 |
Transformer | epoch 0 | step 8310 |avg loss 8.083 |avg tokens 4804.800 |tokens/s 9154.550 |walltime 4214.127 |
Transformer | epoch 0 | step 8320 |avg loss 8.234 |avg tokens 4570.700 |tokens/s 8794.579 |walltime 4219.324 |
Transformer | epoch 0 | step 8330 |avg loss 7.892 |avg tokens 4570.100 |tokens/s 8787.147 |walltime 4224.525 |
Transformer | epoch 0 | step 8340 |avg loss 8.586 |avg tokens 4416.000 |tokens/s 8960.788 |walltime 4229.453 |
Transformer | epoch 0 | step 8350 |avg loss 8.123 |avg tokens 4934.100 |tokens/s 9259.101 |walltime 4234.782 |
Transformer | epoch 0 | step 8360 |avg loss 8.194 |avg tokens 4322.300 |tokens/s 8442.229 |walltime 4239.902 |
Transformer | epoch 0 | step 8370 |avg loss 8.208 |avg tokens 4354.700 |tokens/s 8720.319 |walltime 4244.895 |
Transformer | epoch 0 | step 8380 |avg loss 8.283 |avg tokens 4810.200 |tokens/s 9585.767 |walltime 4249.913 |
Transformer | epoch 0 | step 8390 |avg loss 8.401 |avg tokens 4110.500 |tokens/s 8349.980 |walltime 4254.836 |
Transformer | epoch 0 | step 8400 |avg loss 8.354 |avg tokens 4469.300 |tokens/s 8901.057 |walltime 4259.857 |
Transformer | epoch 0 | step 8410 |avg loss 8.540 |avg tokens 4157.700 |tokens/s 8574.735 |walltime 4264.706 |
Transformer | epoch 0 | step 8420 |avg loss 8.059 |avg tokens 4892.400 |tokens/s 9324.614 |walltime 4269.953 |
Transformer | epoch 0 | step 8430 |avg loss 8.273 |avg tokens 4794.800 |tokens/s 9117.187 |walltime 4275.212 |
Transformer | epoch 0 | step 8440 |avg loss 8.577 |avg tokens 4210.800 |tokens/s 9047.666 |walltime 4279.866 |
Transformer | epoch 0 | step 8450 |avg loss 8.051 |avg tokens 4747.300 |tokens/s 9057.416 |walltime 4285.107 |
Transformer | epoch 0 | step 8460 |avg loss 8.346 |avg tokens 4334.500 |tokens/s 8548.749 |walltime 4290.178 |
Transformer | epoch 0 | step 8470 |avg loss 8.242 |avg tokens 4366.800 |tokens/s 8764.866 |walltime 4295.160 |
Transformer | epoch 0 | step 8480 |avg loss 8.363 |avg tokens 4595.700 |tokens/s 9087.524 |walltime 4300.217 |
Transformer | epoch 0 | step 8490 |avg loss 8.351 |avg tokens 4456.500 |tokens/s 8899.992 |walltime 4305.224 |
Transformer | epoch 0 | step 8500 |avg loss 8.393 |avg tokens 4643.200 |tokens/s 9174.892 |walltime 4310.285 |
Transformer | epoch 0 | step 8510 |avg loss 8.591 |avg tokens 4325.900 |tokens/s 9251.159 |walltime 4314.961 |
Transformer | epoch 0 | step 8520 |avg loss 8.189 |avg tokens 4443.200 |tokens/s 8647.096 |walltime 4320.099 |
Transformer | epoch 0 | step 8530 |avg loss 8.269 |avg tokens 4306.300 |tokens/s 8664.604 |walltime 4325.069 |
Transformer | epoch 0 | step 8540 |avg loss 8.429 |avg tokens 4428.700 |tokens/s 9012.018 |walltime 4329.984 |
Transformer | epoch 0 | step 8550 |avg loss 8.208 |avg tokens 4715.800 |tokens/s 9144.511 |walltime 4335.141 |
Transformer | epoch 0 | step 8560 |avg loss 8.432 |avg tokens 4739.000 |tokens/s 9232.282 |walltime 4340.274 |
Transformer | epoch 0 | step 8570 |avg loss 8.275 |avg tokens 4096.700 |tokens/s 8539.302 |walltime 4345.071 |
Transformer | epoch 0 | step 8580 |avg loss 8.457 |avg tokens 4534.700 |tokens/s 8925.343 |walltime 4350.152 |
Transformer | epoch 0 | step 8590 |avg loss 8.483 |avg tokens 4554.300 |tokens/s 9148.629 |walltime 4355.130 |
Transformer | epoch 0 | step 8600 |avg loss 8.578 |avg tokens 4676.400 |tokens/s 9543.377 |walltime 4360.030 |
Transformer | epoch 0 | step 8610 |avg loss 8.561 |avg tokens 4130.700 |tokens/s 8574.866 |walltime 4364.847 |
Transformer | epoch 0 | step 8620 |avg loss 8.360 |avg tokens 4657.100 |tokens/s 9227.952 |walltime 4369.894 |
Transformer | epoch 0 | step 8630 |avg loss 8.224 |avg tokens 4786.000 |tokens/s 9162.715 |walltime 4375.117 |
Transformer | epoch 0 | step 8640 |avg loss 8.344 |avg tokens 4842.300 |tokens/s 9641.064 |walltime 4380.140 |
Transformer | epoch 0 | step 8650 |avg loss 8.471 |avg tokens 4255.400 |tokens/s 8973.772 |walltime 4384.882 |
Transformer | epoch 0 | step 8660 |avg loss 8.331 |avg tokens 4533.900 |tokens/s 9016.036 |walltime 4389.911 |
Transformer | epoch 0 | step 8670 |avg loss 8.697 |avg tokens 4484.300 |tokens/s 9373.644 |walltime 4394.695 |
Transformer | epoch 0 | step 8680 |avg loss 8.122 |avg tokens 4660.400 |tokens/s 8969.532 |walltime 4399.891 |
Transformer | epoch 0 | step 8690 |avg loss 8.597 |avg tokens 4186.600 |tokens/s 8370.234 |walltime 4404.892 |
Transformer | epoch 0 | step 8700 |avg loss 8.443 |avg tokens 4248.600 |tokens/s 8573.108 |walltime 4409.848 |
Transformer | epoch 0 | step 8710 |avg loss 8.402 |avg tokens 4432.800 |tokens/s 8702.321 |walltime 4414.942 |
Transformer | epoch 0 | step 8720 |avg loss 8.664 |avg tokens 4410.000 |tokens/s 9200.899 |walltime 4419.735 |
Transformer | epoch 0 | step 8730 |avg loss 8.348 |avg tokens 4730.600 |tokens/s 9268.586 |walltime 4424.839 |
Transformer | epoch 0 | step 8740 |avg loss 8.414 |avg tokens 4615.300 |tokens/s 8999.677 |walltime 4429.967 |
Transformer | epoch 0 | step 8750 |avg loss 8.168 |avg tokens 4415.800 |tokens/s 8922.154 |walltime 4434.916 |
Transformer | epoch 0 | step 8760 |avg loss 8.418 |avg tokens 4257.000 |tokens/s 8469.823 |walltime 4439.942 |
Transformer | epoch 0 | step 8770 |avg loss 8.462 |avg tokens 4271.900 |tokens/s 8865.151 |walltime 4444.761 |
Transformer | epoch 0 | step 8780 |avg loss 8.408 |avg tokens 4470.200 |tokens/s 8880.453 |walltime 4449.795 |
Transformer | epoch 0 | step 8790 |avg loss 8.272 |avg tokens 4782.300 |tokens/s 9382.519 |walltime 4454.892 |
Transformer | epoch 0 | step 8800 |avg loss 8.550 |avg tokens 4785.900 |tokens/s 9420.969 |walltime 4459.972 |
Transformer | epoch 0 | step 8810 |avg loss 7.966 |avg tokens 4657.600 |tokens/s 8866.002 |walltime 4465.225 |
Transformer | epoch 0 | step 8820 |avg loss 8.295 |avg tokens 4842.400 |tokens/s 9274.167 |walltime 4470.447 |
Transformer | epoch 0 | step 8830 |avg loss 8.052 |avg tokens 4771.800 |tokens/s 9431.985 |walltime 4475.506 |
Transformer | epoch 0 | step 8840 |avg loss 8.300 |avg tokens 4463.800 |tokens/s 9245.016 |walltime 4480.334 |
Transformer | epoch 0 | step 8850 |avg loss 8.375 |avg tokens 4784.900 |tokens/s 9366.354 |walltime 4485.443 |
Transformer | epoch 0 | step 8860 |avg loss 8.140 |avg tokens 4425.600 |tokens/s 8770.299 |walltime 4490.489 |
Transformer | epoch 0 | step 8870 |avg loss 8.044 |avg tokens 4124.200 |tokens/s 8296.876 |walltime 4495.460 |
Transformer | epoch 0 | step 8880 |avg loss 7.943 |avg tokens 4390.900 |tokens/s 8629.442 |walltime 4500.548 |
Transformer | epoch 0 | step 8890 |avg loss 8.042 |avg tokens 4633.200 |tokens/s 8826.592 |walltime 4505.797 |
Transformer | epoch 0 | step 8900 |avg loss 8.616 |avg tokens 4403.900 |tokens/s 9000.411 |walltime 4510.690 |
Transformer | epoch 0 | step 8910 |avg loss 8.143 |avg tokens 4525.400 |tokens/s 8967.136 |walltime 4515.737 |
Transformer | epoch 0 | step 8920 |avg loss 8.579 |avg tokens 4452.700 |tokens/s 8809.883 |walltime 4520.791 |
Transformer | epoch 0 | step 8930 |avg loss 8.355 |avg tokens 4467.700 |tokens/s 8870.440 |walltime 4525.828 |
Transformer | epoch 0 | step 8940 |avg loss 8.466 |avg tokens 4813.500 |tokens/s 9261.665 |walltime 4531.025 |
Transformer | epoch 0 | step 8950 |avg loss 8.357 |avg tokens 4714.300 |tokens/s 9164.014 |walltime 4536.169 |
Transformer | epoch 0 | step 8960 |avg loss 8.611 |avg tokens 3953.700 |tokens/s 8224.205 |walltime 4540.977 |
Transformer | epoch 0 | step 8970 |avg loss 8.250 |avg tokens 4663.900 |tokens/s 8976.999 |walltime 4546.172 |
Transformer | epoch 0 | step 8980 |avg loss 8.684 |avg tokens 4226.900 |tokens/s 8803.256 |walltime 4550.974 |
Transformer | epoch 0 | step 8990 |avg loss 8.347 |avg tokens 4653.400 |tokens/s 9139.556 |walltime 4556.065 |
Transformer | epoch 0 | step 9000 |avg loss 8.396 |avg tokens 4026.000 |tokens/s 8643.289 |walltime 4560.723 |
Transformer | epoch 0 | step 9010 |avg loss 8.431 |avg tokens 4249.100 |tokens/s 8618.556 |walltime 4565.653 |
Transformer | epoch 0 | step 9020 |avg loss 8.452 |avg tokens 4639.800 |tokens/s 9051.688 |walltime 4570.779 |
Transformer | epoch 0 | step 9030 |avg loss 8.522 |avg tokens 4369.400 |tokens/s 8910.197 |walltime 4575.683 |
Transformer | epoch 0 | step 9040 |avg loss 8.373 |avg tokens 4680.900 |tokens/s 9478.759 |walltime 4580.621 |
Transformer | epoch 0 | step 9050 |avg loss 8.321 |avg tokens 4273.300 |tokens/s 8475.096 |walltime 4585.663 |
Transformer | epoch 0 | step 9060 |avg loss 8.088 |avg tokens 4755.400 |tokens/s 9009.859 |walltime 4590.941 |
Transformer | epoch 0 | step 9070 |avg loss 8.589 |avg tokens 4526.500 |tokens/s 9321.486 |walltime 4595.797 |
Transformer | epoch 0 | step 9080 |avg loss 8.228 |avg tokens 4270.400 |tokens/s 8732.442 |walltime 4600.688 |
Transformer | epoch 0 | step 9090 |avg loss 8.280 |avg tokens 4503.400 |tokens/s 8836.189 |walltime 4605.784 |
Transformer | epoch 0 | step 9100 |avg loss 8.109 |avg tokens 4754.200 |tokens/s 8903.643 |walltime 4611.124 |
Transformer | epoch 0 | step 9110 |avg loss 8.293 |avg tokens 4656.300 |tokens/s 9003.425 |walltime 4616.296 |
Transformer | epoch 0 | step 9120 |avg loss 8.372 |avg tokens 4552.900 |tokens/s 8872.870 |walltime 4621.427 |
Transformer | epoch 0 | step 9130 |avg loss 8.250 |avg tokens 4669.500 |tokens/s 9146.212 |walltime 4626.532 |
Transformer | epoch 0 | step 9140 |avg loss 8.362 |avg tokens 4510.900 |tokens/s 8946.611 |walltime 4631.574 |
Transformer | epoch 0 | step 9150 |avg loss 8.314 |avg tokens 4637.500 |tokens/s 8630.217 |walltime 4636.948 |
Transformer | epoch 0 | step 9160 |avg loss 8.544 |avg tokens 4687.000 |tokens/s 9064.802 |walltime 4642.118 |
Transformer | epoch 0 | step 9170 |avg loss 8.537 |avg tokens 4101.100 |tokens/s 8655.799 |walltime 4646.856 |
Transformer | epoch 0 | step 9180 |avg loss 8.329 |avg tokens 4736.000 |tokens/s 9189.199 |walltime 4652.010 |
Transformer | epoch 0 | step 9190 |avg loss 8.434 |avg tokens 4327.400 |tokens/s 8736.673 |walltime 4656.963 |
Transformer | epoch 0 | step 9200 |avg loss 8.622 |avg tokens 4582.300 |tokens/s 9191.821 |walltime 4661.949 |
Transformer | epoch 0 | step 9210 |avg loss 8.433 |avg tokens 4454.700 |tokens/s 8624.267 |walltime 4667.114 |
Transformer | epoch 0 | step 9220 |avg loss 8.565 |avg tokens 4247.600 |tokens/s 8739.704 |walltime 4671.974 |
Transformer | epoch 0 | step 9230 |avg loss 8.327 |avg tokens 4915.300 |tokens/s 9463.971 |walltime 4677.168 |
Transformer | epoch 0 | step 9240 |avg loss 8.504 |avg tokens 4790.000 |tokens/s 9270.500 |walltime 4682.335 |
Transformer | epoch 0 | step 9250 |avg loss 8.147 |avg tokens 4300.200 |tokens/s 8674.792 |walltime 4687.292 |
Transformer | epoch 0 | step 9260 |avg loss 8.439 |avg tokens 4790.800 |tokens/s 9402.877 |walltime 4692.387 |
Transformer | epoch 0 | step 9270 |avg loss 8.160 |avg tokens 4639.600 |tokens/s 8918.362 |walltime 4697.589 |
Transformer | epoch 0 | step 9280 |avg loss 8.537 |avg tokens 4674.900 |tokens/s 9027.540 |walltime 4702.768 |
Transformer | epoch 0 | step 9290 |avg loss 8.476 |avg tokens 3874.700 |tokens/s 8013.054 |walltime 4707.603 |
Transformer | epoch 0 | step 9300 |avg loss 8.422 |avg tokens 4565.700 |tokens/s 8786.410 |walltime 4712.799 |
Transformer | epoch 0 | step 9310 |avg loss 8.326 |avg tokens 4422.800 |tokens/s 8661.682 |walltime 4717.906 |
Transformer | epoch 0 | step 9320 |avg loss 8.597 |avg tokens 4645.400 |tokens/s 8902.810 |walltime 4723.123 |
Transformer | epoch 0 | step 9330 |avg loss 8.558 |avg tokens 4216.800 |tokens/s 8483.343 |walltime 4728.094 |
Transformer | epoch 0 | step 9340 |avg loss 8.652 |avg tokens 3889.600 |tokens/s 8056.345 |walltime 4732.922 |
Transformer | epoch 0 | step 9350 |avg loss 8.413 |avg tokens 4536.400 |tokens/s 9085.021 |walltime 4737.915 |
Transformer | epoch 0 | step 9360 |avg loss 8.289 |avg tokens 4628.000 |tokens/s 8984.774 |walltime 4743.066 |
Transformer | epoch 0 | step 9370 |avg loss 8.478 |avg tokens 4883.300 |tokens/s 9719.658 |walltime 4748.090 |
Transformer | epoch 0 | step 9380 |avg loss 8.582 |avg tokens 3769.600 |tokens/s 7786.459 |walltime 4752.932 |
Transformer | epoch 0 | step 9390 |avg loss 8.455 |avg tokens 4657.000 |tokens/s 9056.329 |walltime 4758.074 |
Transformer | epoch 0 | step 9400 |avg loss 8.480 |avg tokens 4698.400 |tokens/s 9112.524 |walltime 4763.230 |
Transformer | epoch 0 | step 9410 |avg loss 8.388 |avg tokens 4346.000 |tokens/s 8717.778 |walltime 4768.215 |
Transformer | epoch 0 | step 9420 |avg loss 8.235 |avg tokens 4664.800 |tokens/s 9150.848 |walltime 4773.313 |
Transformer | epoch 0 | step 9430 |avg loss 8.183 |avg tokens 4725.800 |tokens/s 9087.573 |walltime 4778.513 |
Transformer | epoch 0 | step 9440 |avg loss 8.414 |avg tokens 4245.000 |tokens/s 8440.527 |walltime 4783.542 |
Transformer | epoch 0 | step 9450 |avg loss 8.201 |avg tokens 4689.500 |tokens/s 9009.189 |walltime 4788.748 |
Transformer | epoch 0 | step 9460 |avg loss 8.533 |avg tokens 4163.300 |tokens/s 8436.539 |walltime 4793.683 |
Transformer | epoch 0 | step 9470 |avg loss 8.429 |avg tokens 4450.500 |tokens/s 8878.501 |walltime 4798.695 |
Transformer | epoch 0 | step 9480 |avg loss 8.466 |avg tokens 4099.600 |tokens/s 8511.926 |walltime 4803.512 |
Transformer | epoch 0 | step 9490 |avg loss 8.326 |avg tokens 4171.700 |tokens/s 8396.109 |walltime 4808.480 |
Transformer | epoch 0 | step 9500 |avg loss 8.339 |avg tokens 4590.600 |tokens/s 8806.189 |walltime 4813.693 |
Transformer | epoch 0 | step 9510 |avg loss 8.233 |avg tokens 4910.900 |tokens/s 9314.641 |walltime 4818.965 |
Transformer | epoch 0 | step 9520 |avg loss 8.310 |avg tokens 4896.900 |tokens/s 9355.479 |walltime 4824.200 |
Transformer | epoch 0 | step 9530 |avg loss 8.175 |avg tokens 4264.400 |tokens/s 8467.361 |walltime 4829.236 |
Transformer | epoch 0 | step 9540 |avg loss 8.142 |avg tokens 4464.500 |tokens/s 8757.232 |walltime 4834.334 |
Transformer | epoch 0 | step 9550 |avg loss 8.076 |avg tokens 4574.100 |tokens/s 8780.442 |walltime 4839.543 |
Transformer | epoch 0 | step 9560 |avg loss 8.522 |avg tokens 4440.800 |tokens/s 8728.740 |walltime 4844.631 |
Transformer | epoch 0 | step 9570 |avg loss 8.334 |avg tokens 4733.700 |tokens/s 9104.805 |walltime 4849.830 |
Transformer | epoch 0 | step 9580 |avg loss 8.203 |avg tokens 4775.300 |tokens/s 9066.131 |walltime 4855.097 |
Transformer | epoch 0 | step 9590 |avg loss 8.356 |avg tokens 4394.300 |tokens/s 8670.094 |walltime 4860.166 |
Transformer | epoch 0 | step 9600 |avg loss 8.362 |avg tokens 4241.000 |tokens/s 8525.130 |walltime 4865.140 |
Transformer | epoch 0 | step 9610 |avg loss 8.259 |avg tokens 4386.700 |tokens/s 8843.923 |walltime 4870.100 |
Transformer | epoch 0 | step 9620 |avg loss 8.306 |avg tokens 4877.700 |tokens/s 9421.665 |walltime 4875.278 |
Transformer | epoch 0 | step 9630 |avg loss 8.273 |avg tokens 4847.200 |tokens/s 9036.856 |walltime 4880.641 |
Transformer | epoch 0 | step 9640 |avg loss 8.030 |avg tokens 4846.900 |tokens/s 9435.692 |walltime 4885.778 |
Transformer | epoch 0 | step 9650 |avg loss 8.367 |avg tokens 4474.100 |tokens/s 9017.542 |walltime 4890.740 |
Transformer | epoch 0 | step 9660 |avg loss 8.225 |avg tokens 4238.100 |tokens/s 8576.231 |walltime 4895.681 |
Transformer | epoch 0 | step 9670 |avg loss 8.043 |avg tokens 4339.600 |tokens/s 8660.681 |walltime 4900.692 |
Transformer | epoch 0 | step 9680 |avg loss 8.337 |avg tokens 4600.600 |tokens/s 8863.354 |walltime 4905.883 |
Transformer | epoch 0 | step 9690 |avg loss 8.519 |avg tokens 4322.200 |tokens/s 9242.101 |walltime 4910.559 |
Transformer | epoch 0 | step 9700 |avg loss 8.334 |avg tokens 4614.400 |tokens/s 8976.598 |walltime 4915.700 |
Transformer | epoch 0 | step 9710 |avg loss 8.490 |avg tokens 4753.000 |tokens/s 9388.672 |walltime 4920.762 |
Transformer | epoch 0 | step 9720 |avg loss 8.340 |avg tokens 4421.600 |tokens/s 8810.223 |walltime 4925.781 |
Transformer | epoch 0 | step 9730 |avg loss 8.481 |avg tokens 4339.500 |tokens/s 9042.222 |walltime 4930.580 |
Transformer | epoch 0 | step 9740 |avg loss 8.329 |avg tokens 4454.100 |tokens/s 8747.589 |walltime 4935.672 |
Transformer | epoch 0 | step 9750 |avg loss 8.285 |avg tokens 4522.800 |tokens/s 8920.055 |walltime 4940.742 |
Transformer | epoch 0 | step 9760 |avg loss 8.460 |avg tokens 4627.900 |tokens/s 9345.169 |walltime 4945.694 |
Transformer | epoch 0 | step 9770 |avg loss 7.868 |avg tokens 4665.300 |tokens/s 9067.283 |walltime 4950.840 |
Transformer | epoch 0 | step 9780 |avg loss 8.475 |avg tokens 4509.900 |tokens/s 8813.860 |walltime 4955.957 |
Transformer | epoch 0 | step 9790 |avg loss 8.319 |avg tokens 4380.200 |tokens/s 8752.148 |walltime 4960.961 |
Transformer | epoch 0 | step 9800 |avg loss 8.479 |avg tokens 4310.900 |tokens/s 8882.007 |walltime 4965.815 |
Transformer | epoch 0 | step 9810 |avg loss 8.276 |avg tokens 4766.300 |tokens/s 9297.939 |walltime 4970.941 |
Transformer | epoch 0 | step 9820 |avg loss 8.254 |avg tokens 4710.000 |tokens/s 9003.760 |walltime 4976.172 |
Transformer | epoch 0 | step 9830 |avg loss 8.530 |avg tokens 4271.000 |tokens/s 8905.541 |walltime 4980.968 |
Transformer | epoch 0 | step 9840 |avg loss 8.455 |avg tokens 4867.100 |tokens/s 9416.433 |walltime 4986.137 |
Transformer | epoch 0 | step 9850 |avg loss 8.578 |avg tokens 4001.700 |tokens/s 8731.150 |walltime 4990.720 |
Transformer | epoch 0 | step 9860 |avg loss 8.451 |avg tokens 4672.600 |tokens/s 9321.536 |walltime 4995.733 |
Transformer | epoch 0 | step 9870 |avg loss 8.141 |avg tokens 4797.200 |tokens/s 9181.120 |walltime 5000.958 |
Transformer | epoch 0 | step 9880 |avg loss 8.509 |avg tokens 4411.900 |tokens/s 9102.910 |walltime 5005.804 |
Transformer | epoch 0 | step 9890 |avg loss 8.314 |avg tokens 4673.700 |tokens/s 9090.980 |walltime 5010.945 |
Transformer | epoch 0 | step 9900 |avg loss 8.309 |avg tokens 4716.700 |tokens/s 9281.638 |walltime 5016.027 |
Transformer | epoch 0 | step 9910 |avg loss 8.367 |avg tokens 4828.800 |tokens/s 9194.822 |walltime 5021.279 |
Transformer | epoch 0 | step 9920 |avg loss 8.357 |avg tokens 4502.400 |tokens/s 8771.923 |walltime 5026.412 |
Transformer | epoch 0 | step 9930 |avg loss 8.404 |avg tokens 4589.900 |tokens/s 9184.213 |walltime 5031.409 |
Transformer | epoch 0 | step 9940 |avg loss 8.201 |avg tokens 4376.900 |tokens/s 8827.538 |walltime 5036.367 |
Transformer | epoch 0 | step 9950 |avg loss 8.393 |avg tokens 4781.500 |tokens/s 9372.478 |walltime 5041.469 |
Transformer | epoch 0 | step 9960 |avg loss 8.340 |avg tokens 4720.100 |tokens/s 9086.756 |walltime 5046.664 |
Transformer | epoch 0 | step 9970 |avg loss 8.510 |avg tokens 4628.900 |tokens/s 9080.619 |walltime 5051.761 |
Transformer | epoch 0 | step 9980 |avg loss 8.012 |avg tokens 4792.400 |tokens/s 9332.499 |walltime 5056.896 |
Transformer | epoch 0 | step 9990 |avg loss 7.948 |avg tokens 4552.700 |tokens/s 8784.125 |walltime 5062.079 |
Transformer | epoch 0 | step 10000 |avg loss 8.322 |avg tokens 4465.500 |tokens/s 8861.144 |walltime 5067.119 |
Transformer | epoch 0 | step 10010 |avg loss 8.330 |avg tokens 4976.700 |tokens/s 9280.851 |walltime 5072.481 |
Transformer | epoch 0 | step 10020 |avg loss 8.258 |avg tokens 4674.100 |tokens/s 9004.612 |walltime 5077.672 |
Transformer | epoch 0 | step 10030 |avg loss 8.498 |avg tokens 4162.700 |tokens/s 8699.746 |walltime 5082.457 |
Transformer | epoch 0 | step 10040 |avg loss 7.999 |avg tokens 4833.700 |tokens/s 9180.000 |walltime 5087.722 |
Transformer | epoch 0 | step 10050 |avg loss 8.556 |avg tokens 4436.700 |tokens/s 9197.086 |walltime 5092.546 |
Transformer | epoch 0 | step 10060 |avg loss 8.322 |avg tokens 4812.300 |tokens/s 9312.673 |walltime 5097.714 |
Transformer | epoch 0 | step 10070 |avg loss 8.054 |avg tokens 4848.000 |tokens/s 9101.308 |walltime 5103.040 |
Transformer | epoch 0 | step 10080 |avg loss 8.184 |avg tokens 4390.100 |tokens/s 8913.492 |walltime 5107.966 |
Transformer | epoch 0 | step 10090 |avg loss 8.847 |avg tokens 3619.300 |tokens/s 8081.956 |walltime 5112.444 |
Transformer | epoch 0 | step 10100 |avg loss 8.381 |avg tokens 4332.600 |tokens/s 8844.378 |walltime 5117.342 |
Transformer | epoch 0 | step 10110 |avg loss 8.601 |avg tokens 4043.200 |tokens/s 8590.394 |walltime 5122.049 |
Transformer | epoch 0 | step 10120 |avg loss 8.469 |avg tokens 4457.500 |tokens/s 8864.489 |walltime 5127.078 |
Transformer | epoch 0 | step 10130 |avg loss 8.408 |avg tokens 4709.000 |tokens/s 8974.005 |walltime 5132.325 |
Transformer | epoch 0 | step 10140 |avg loss 8.130 |avg tokens 4418.400 |tokens/s 8693.781 |walltime 5137.407 |
Transformer | epoch 0 | step 10150 |avg loss 8.414 |avg tokens 4724.200 |tokens/s 9310.328 |walltime 5142.481 |
Transformer | epoch 0 | step 10160 |avg loss 8.436 |avg tokens 4321.800 |tokens/s 8562.947 |walltime 5147.529 |
Transformer | epoch 0 | step 10170 |avg loss 8.569 |avg tokens 4105.300 |tokens/s 8540.032 |walltime 5152.336 |
Transformer | epoch 0 | step 10180 |avg loss 8.275 |avg tokens 4577.600 |tokens/s 8813.379 |walltime 5157.530 |
Transformer | epoch 0 | step 10190 |avg loss 8.194 |avg tokens 4745.600 |tokens/s 9069.558 |walltime 5162.762 |
Transformer | epoch 0 | step 10200 |avg loss 8.399 |avg tokens 4619.900 |tokens/s 9179.638 |walltime 5167.795 |
Transformer | epoch 0 | step 10210 |avg loss 8.397 |avg tokens 4082.500 |tokens/s 8467.687 |walltime 5172.616 |
Transformer | epoch 0 | step 10220 |avg loss 8.175 |avg tokens 4845.600 |tokens/s 9144.203 |walltime 5177.915 |
Transformer | epoch 0 | step 10230 |avg loss 8.722 |avg tokens 4455.900 |tokens/s 9039.764 |walltime 5182.844 |
Transformer | epoch 0 | step 10240 |avg loss 8.480 |avg tokens 4325.300 |tokens/s 8891.196 |walltime 5187.709 |
Transformer | epoch 0 | step 10250 |avg loss 8.291 |avg tokens 4347.500 |tokens/s 8670.565 |walltime 5192.723 |
Transformer | epoch 0 | step 10260 |avg loss 8.526 |avg tokens 4220.500 |tokens/s 8906.756 |walltime 5197.462 |
Transformer | epoch 0 | step 10270 |avg loss 8.169 |avg tokens 4637.800 |tokens/s 8797.085 |walltime 5202.734 |
Transformer | epoch 0 | step 10280 |avg loss 8.332 |avg tokens 4380.200 |tokens/s 8825.988 |walltime 5207.697 |
Transformer | epoch 0 | step 10290 |avg loss 8.523 |avg tokens 4564.100 |tokens/s 9054.855 |walltime 5212.737 |
Transformer | epoch 0 | step 10300 |avg loss 8.052 |avg tokens 4845.600 |tokens/s 9145.635 |walltime 5218.035 |
Transformer | epoch 0 | step 10310 |avg loss 8.093 |avg tokens 4655.600 |tokens/s 8899.586 |walltime 5223.267 |
Transformer | epoch 0 | step 10320 |avg loss 8.465 |avg tokens 4871.800 |tokens/s 9653.717 |walltime 5228.313 |
Transformer | epoch 0 | step 10330 |avg loss 8.394 |avg tokens 4669.700 |tokens/s 9119.068 |walltime 5233.434 |
Transformer | epoch 0 | step 10340 |avg loss 8.277 |avg tokens 4009.200 |tokens/s 8092.249 |walltime 5238.388 |
Transformer | epoch 0 | step 10350 |avg loss 8.298 |avg tokens 4841.500 |tokens/s 9251.340 |walltime 5243.622 |
Transformer | epoch 0 | step 10360 |avg loss 8.203 |avg tokens 4895.900 |tokens/s 9265.921 |walltime 5248.905 |
Transformer | epoch 0 | step 10370 |avg loss 8.392 |avg tokens 4269.700 |tokens/s 8711.393 |walltime 5253.807 |
Transformer | epoch 0 | step 10380 |avg loss 8.659 |avg tokens 4456.000 |tokens/s 9213.584 |walltime 5258.643 |
Transformer | epoch 0 | step 10390 |avg loss 8.498 |avg tokens 4165.800 |tokens/s 8375.950 |walltime 5263.617 |
Transformer | epoch 0 | step 10400 |avg loss 8.317 |avg tokens 4443.100 |tokens/s 8808.302 |walltime 5268.661 |
Transformer | epoch 0 | step 10410 |avg loss 8.461 |avg tokens 4763.200 |tokens/s 9414.370 |walltime 5273.720 |
Transformer | epoch 0 | step 10420 |avg loss 8.212 |avg tokens 4777.900 |tokens/s 9203.093 |walltime 5278.912 |
Transformer | epoch 0 | step 10430 |avg loss 8.182 |avg tokens 4361.000 |tokens/s 8615.862 |walltime 5283.973 |
Transformer | epoch 0 | step 10440 |avg loss 8.596 |avg tokens 4274.900 |tokens/s 9205.235 |walltime 5288.617 |
Transformer | epoch 0 | step 10450 |avg loss 8.477 |avg tokens 4390.200 |tokens/s 9140.258 |walltime 5293.421 |
Transformer | epoch 0 | step 10460 |avg loss 8.363 |avg tokens 4791.600 |tokens/s 9291.065 |walltime 5298.578 |
Transformer | epoch 0 | step 10470 |avg loss 8.374 |avg tokens 4975.700 |tokens/s 9275.620 |walltime 5303.942 |
Transformer | epoch 0 | step 10480 |avg loss 8.348 |avg tokens 4391.000 |tokens/s 8784.636 |walltime 5308.941 |
Transformer | epoch 0 | step 10490 |avg loss 8.655 |avg tokens 4587.900 |tokens/s 9630.197 |walltime 5313.705 |
Transformer | epoch 0 | step 10500 |avg loss 8.188 |avg tokens 4443.400 |tokens/s 8677.853 |walltime 5318.825 |
Transformer | epoch 0 | step 10510 |avg loss 8.308 |avg tokens 4271.000 |tokens/s 8539.066 |walltime 5323.827 |
Transformer | epoch 0 | step 10520 |avg loss 8.440 |avg tokens 4381.500 |tokens/s 8650.658 |walltime 5328.892 |
Transformer | epoch 0 | step 10530 |avg loss 8.049 |avg tokens 4713.600 |tokens/s 8876.871 |walltime 5334.202 |
Transformer | epoch 0 | step 10540 |avg loss 8.404 |avg tokens 4410.900 |tokens/s 8948.043 |walltime 5339.131 |
Transformer | epoch 0 | step 10550 |avg loss 8.444 |avg tokens 4564.500 |tokens/s 9027.529 |walltime 5344.187 |
Transformer | epoch 0 | step 10560 |avg loss 8.661 |avg tokens 4595.600 |tokens/s 9156.649 |walltime 5349.206 |
Transformer | epoch 0 | step 10570 |avg loss 8.176 |avg tokens 4567.100 |tokens/s 9368.175 |walltime 5354.081 |
Transformer | epoch 0 | step 10580 |avg loss 8.322 |avg tokens 5002.300 |tokens/s 9418.613 |walltime 5359.392 |
Transformer | epoch 0 | step 10590 |avg loss 8.351 |avg tokens 4560.000 |tokens/s 9114.794 |walltime 5364.395 |
Transformer | epoch 0 | step 10600 |avg loss 8.507 |avg tokens 3996.600 |tokens/s 8440.416 |walltime 5369.130 |
Transformer | epoch 0 | step 10610 |avg loss 8.347 |avg tokens 4505.500 |tokens/s 8939.859 |walltime 5374.170 |
Transformer | epoch 0 | step 10620 |avg loss 8.394 |avg tokens 4790.300 |tokens/s 9309.668 |walltime 5379.316 |
Transformer | epoch 0 | step 10630 |avg loss 8.564 |avg tokens 4474.200 |tokens/s 8999.635 |walltime 5384.287 |
Transformer | epoch 0 | step 10640 |avg loss 8.664 |avg tokens 4486.000 |tokens/s 9146.861 |walltime 5389.192 |
Transformer | epoch 0 | step 10650 |avg loss 8.363 |avg tokens 4628.200 |tokens/s 8629.258 |walltime 5394.555 |
Transformer | epoch 0 | step 10660 |avg loss 8.205 |avg tokens 4440.900 |tokens/s 8853.411 |walltime 5399.571 |
Transformer | epoch 0 | step 10670 |avg loss 8.065 |avg tokens 4821.300 |tokens/s 9147.918 |walltime 5404.841 |
Transformer | epoch 0 | step 10680 |avg loss 8.405 |avg tokens 4553.900 |tokens/s 8787.314 |walltime 5410.024 |
Transformer | epoch 0 | step 10690 |avg loss 8.353 |avg tokens 4671.100 |tokens/s 8886.815 |walltime 5415.280 |
Transformer | epoch 0 | step 10700 |avg loss 8.321 |avg tokens 4130.900 |tokens/s 8777.163 |walltime 5419.986 |
Transformer | epoch 0 | step 10710 |avg loss 8.656 |avg tokens 4113.100 |tokens/s 8646.187 |walltime 5424.744 |
Transformer | epoch 0 | step 10720 |avg loss 8.568 |avg tokens 4595.700 |tokens/s 9276.733 |walltime 5429.698 |
Transformer | epoch 0 | step 10730 |avg loss 8.404 |avg tokens 4647.600 |tokens/s 9001.087 |walltime 5434.861 |
Transformer | epoch 0 | step 10740 |avg loss 8.274 |avg tokens 4548.000 |tokens/s 8843.192 |walltime 5440.004 |
Transformer | epoch 0 | step 10750 |avg loss 8.202 |avg tokens 4500.300 |tokens/s 8832.012 |walltime 5445.099 |
Transformer | epoch 0 | step 10760 |avg loss 8.039 |avg tokens 4619.300 |tokens/s 8814.824 |walltime 5450.340 |
Transformer | epoch 0 | step 10770 |avg loss 8.671 |avg tokens 4531.200 |tokens/s 9059.875 |walltime 5455.341 |
Transformer | epoch 0 | step 10780 |avg loss 8.581 |avg tokens 4396.600 |tokens/s 8734.585 |walltime 5460.375 |
Transformer | epoch 0 | step 10790 |avg loss 8.112 |avg tokens 4309.300 |tokens/s 8601.682 |walltime 5465.385 |
Transformer | epoch 0 | step 10800 |avg loss 8.400 |avg tokens 4734.800 |tokens/s 9263.972 |walltime 5470.496 |
Transformer | epoch 0 | step 10810 |avg loss 8.481 |avg tokens 4194.200 |tokens/s 8656.869 |walltime 5475.340 |
Transformer | epoch 0 | step 10820 |avg loss 8.142 |avg tokens 4426.800 |tokens/s 8862.139 |walltime 5480.336 |
Transformer | epoch 0 | step 10830 |avg loss 8.471 |avg tokens 4547.900 |tokens/s 9023.612 |walltime 5485.376 |
Transformer | epoch 0 | step 10840 |avg loss 8.312 |avg tokens 4768.000 |tokens/s 8968.327 |walltime 5490.692 |
Transformer | epoch 0 | step 10850 |avg loss 8.334 |avg tokens 4733.600 |tokens/s 9956.938 |walltime 5495.446 |
Transformer | epoch 0 | step 10860 |avg loss 8.360 |avg tokens 3946.100 |tokens/s 8024.515 |walltime 5500.364 |
Transformer | epoch 0 | step 10870 |avg loss 8.324 |avg tokens 4883.900 |tokens/s 9210.912 |walltime 5505.666 |
Transformer | epoch 0 | step 10880 |avg loss 8.480 |avg tokens 4399.100 |tokens/s 9041.476 |walltime 5510.532 |
Transformer | epoch 0 | step 10890 |avg loss 8.466 |avg tokens 4352.900 |tokens/s 8816.957 |walltime 5515.469 |
Transformer | epoch 0 | step 10900 |avg loss 8.799 |avg tokens 4621.200 |tokens/s 9055.238 |walltime 5520.572 |
Transformer | epoch 0 | step 10910 |avg loss 8.757 |avg tokens 4323.800 |tokens/s 9243.985 |walltime 5525.249 |
Transformer | epoch 0 | step 10920 |avg loss 8.477 |avg tokens 4428.200 |tokens/s 8046.177 |walltime 5530.753 |
Transformer | epoch 0 | step 10930 |avg loss 8.215 |avg tokens 4717.700 |tokens/s 9254.153 |walltime 5535.851 |
Transformer | epoch 0 | step 10940 |avg loss 8.315 |avg tokens 4358.300 |tokens/s 8796.399 |walltime 5540.805 |
Transformer | epoch 0 | step 10950 |avg loss 8.507 |avg tokens 4354.600 |tokens/s 8563.504 |walltime 5545.890 |
Transformer | epoch 0 | step 10960 |avg loss 8.409 |avg tokens 4630.300 |tokens/s 9080.322 |walltime 5550.990 |
Transformer | epoch 0 | step 10970 |avg loss 8.436 |avg tokens 4439.500 |tokens/s 8856.498 |walltime 5556.002 |
Transformer | epoch 0 | step 10980 |avg loss 8.483 |avg tokens 4342.300 |tokens/s 8661.450 |walltime 5561.016 |
Transformer | epoch 0 | step 10990 |avg loss 8.638 |avg tokens 4330.500 |tokens/s 8901.325 |walltime 5565.881 |
Transformer | epoch 0 | step 11000 |avg loss 8.355 |avg tokens 4611.400 |tokens/s 9165.340 |walltime 5570.912 |
Transformer | epoch 0 | step 11010 |avg loss 8.337 |avg tokens 4720.800 |tokens/s 9155.673 |walltime 5576.068 |
Transformer | epoch 0 | step 11020 |avg loss 8.282 |avg tokens 4350.200 |tokens/s 8582.981 |walltime 5581.137 |
Transformer | epoch 0 | step 11030 |avg loss 8.253 |avg tokens 4580.500 |tokens/s 8865.895 |walltime 5586.303 |
Transformer | epoch 0 | step 11040 |avg loss 8.353 |avg tokens 4142.600 |tokens/s 8336.965 |walltime 5591.272 |
Transformer | epoch 0 | step 11050 |avg loss 8.582 |avg tokens 4422.800 |tokens/s 9007.583 |walltime 5596.182 |
Transformer | epoch 0 | step 11060 |avg loss 8.883 |avg tokens 4627.000 |tokens/s 9499.673 |walltime 5601.053 |
Transformer | epoch 0 | step 11070 |avg loss 8.499 |avg tokens 4770.700 |tokens/s 9603.397 |walltime 5606.021 |
Transformer | epoch 0 | step 11080 |avg loss 8.444 |avg tokens 4659.700 |tokens/s 9029.374 |walltime 5611.181 |
Transformer | epoch 0 | step 11090 |avg loss 8.462 |avg tokens 3906.700 |tokens/s 7814.424 |walltime 5616.181 |
Transformer | epoch 0 | step 11100 |avg loss 8.591 |avg tokens 3946.700 |tokens/s 8414.236 |walltime 5620.871 |
Transformer | epoch 0 | step 11110 |avg loss 8.647 |avg tokens 4772.000 |tokens/s 9260.965 |walltime 5626.024 |
Transformer | epoch 0 | step 11120 |avg loss 8.298 |avg tokens 4643.700 |tokens/s 8997.233 |walltime 5631.185 |
Transformer | epoch 0 | step 11130 |avg loss 8.527 |avg tokens 4243.000 |tokens/s 8590.602 |walltime 5636.124 |
Transformer | epoch 0 | step 11140 |avg loss 8.454 |avg tokens 4572.500 |tokens/s 8912.862 |walltime 5641.254 |
Transformer | epoch 0 | step 11150 |avg loss 8.410 |avg tokens 4653.600 |tokens/s 9045.564 |walltime 5646.399 |
Transformer | epoch 0 | step 11160 |avg loss 8.328 |avg tokens 4906.800 |tokens/s 9209.270 |walltime 5651.727 |
Transformer | epoch 0 | step 11170 |avg loss 8.392 |avg tokens 4819.600 |tokens/s 9665.963 |walltime 5656.713 |
Transformer | epoch 0 | step 11180 |avg loss 8.297 |avg tokens 4745.100 |tokens/s 9011.283 |walltime 5661.979 |
Transformer | epoch 0 | step 11190 |avg loss 8.642 |avg tokens 4503.300 |tokens/s 8982.273 |walltime 5666.993 |
Transformer | epoch 0 | step 11200 |avg loss 8.609 |avg tokens 4298.700 |tokens/s 8676.261 |walltime 5671.947 |
Transformer | epoch 0 | step 11210 |avg loss 8.706 |avg tokens 4026.900 |tokens/s 8556.855 |walltime 5676.653 |
Transformer | epoch 0 | step 11220 |avg loss 8.404 |avg tokens 4750.800 |tokens/s 9488.708 |walltime 5681.660 |
Transformer | epoch 0 | step 11230 |avg loss 8.192 |avg tokens 4504.700 |tokens/s 8800.152 |walltime 5686.779 |
Transformer | epoch 0 | step 11240 |avg loss 8.341 |avg tokens 4768.300 |tokens/s 9068.773 |walltime 5692.037 |
Transformer | epoch 0 | step 11250 |avg loss 8.489 |avg tokens 4361.400 |tokens/s 8497.585 |walltime 5697.169 |
Transformer | epoch 0 | step 11260 |avg loss 8.239 |avg tokens 4532.600 |tokens/s 8769.153 |walltime 5702.338 |
Transformer | epoch 0 | step 11270 |avg loss 8.680 |avg tokens 4884.900 |tokens/s 9657.365 |walltime 5707.396 |
Transformer | epoch 0 | step 11280 |avg loss 8.335 |avg tokens 4552.300 |tokens/s 8799.894 |walltime 5712.570 |
Transformer | epoch 0 | step 11290 |avg loss 8.475 |avg tokens 4699.000 |tokens/s 9031.875 |walltime 5717.772 |
Transformer | epoch 0 | step 11300 |avg loss 8.419 |avg tokens 4578.400 |tokens/s 8971.255 |walltime 5722.876 |
Transformer | epoch 0 | step 11310 |avg loss 8.580 |avg tokens 4690.800 |tokens/s 9264.800 |walltime 5727.939 |
Transformer | epoch 0 | step 11320 |avg loss 8.265 |avg tokens 4518.400 |tokens/s 8805.316 |walltime 5733.070 |
Transformer | epoch 0 | step 11330 |avg loss 8.454 |avg tokens 4674.100 |tokens/s 9037.119 |walltime 5738.242 |
Transformer | epoch 0 | step 11340 |avg loss 8.263 |avg tokens 4529.800 |tokens/s 8813.999 |walltime 5743.382 |
Transformer | epoch 0 | step 11350 |avg loss 8.380 |avg tokens 4502.800 |tokens/s 8834.024 |walltime 5748.479 |
Transformer | epoch 0 | step 11360 |avg loss 8.634 |avg tokens 4589.200 |tokens/s 9262.295 |walltime 5753.433 |
Transformer | epoch 0 | step 11370 |avg loss 8.272 |avg tokens 4768.300 |tokens/s 9048.754 |walltime 5758.703 |
Transformer | epoch 0 | step 11380 |avg loss 8.323 |avg tokens 4748.200 |tokens/s 9034.957 |walltime 5763.958 |
Transformer | epoch 0 | step 11390 |avg loss 8.652 |avg tokens 4629.800 |tokens/s 9351.253 |walltime 5768.909 |
Transformer | epoch 0 | step 11400 |avg loss 8.396 |avg tokens 4573.500 |tokens/s 9286.742 |walltime 5773.834 |
Transformer | epoch 0 | step 11410 |avg loss 8.546 |avg tokens 4407.800 |tokens/s 8979.736 |walltime 5778.743 |
Transformer | epoch 0 | step 11420 |avg loss 8.355 |avg tokens 4619.400 |tokens/s 9026.654 |walltime 5783.860 |
Transformer | epoch 0 | step 11430 |avg loss 8.373 |avg tokens 4600.900 |tokens/s 9107.246 |walltime 5788.912 |
Transformer | epoch 0 | step 11440 |avg loss 8.369 |avg tokens 4122.500 |tokens/s 8518.738 |walltime 5793.751 |
Transformer | epoch 0 | step 11450 |avg loss 8.658 |avg tokens 4795.900 |tokens/s 9572.016 |walltime 5798.762 |
Transformer | epoch 0 | step 11460 |avg loss 8.692 |avg tokens 4248.200 |tokens/s 8778.507 |walltime 5803.601 |
Transformer | epoch 0 | step 11470 |avg loss 8.370 |avg tokens 4820.000 |tokens/s 9195.806 |walltime 5808.843 |
Transformer | epoch 0 | step 11480 |avg loss 8.202 |avg tokens 4606.400 |tokens/s 8982.701 |walltime 5813.971 |
Transformer | epoch 0 | step 11490 |avg loss 8.705 |avg tokens 4629.000 |tokens/s 9050.366 |walltime 5819.085 |
Transformer | epoch 0 | step 11500 |avg loss 8.448 |avg tokens 4640.400 |tokens/s 9158.626 |walltime 5824.152 |
Transformer | epoch 0 | step 11510 |avg loss 8.474 |avg tokens 4579.100 |tokens/s 9090.580 |walltime 5829.189 |
Transformer | epoch 0 | step 11520 |avg loss 8.585 |avg tokens 3906.200 |tokens/s 8638.863 |walltime 5833.711 |
Transformer | epoch 0 | step 11530 |avg loss 8.697 |avg tokens 4505.500 |tokens/s 9019.799 |walltime 5838.706 |
Transformer | epoch 0 | step 11540 |avg loss 8.814 |avg tokens 4330.800 |tokens/s 9124.261 |walltime 5843.453 |
Transformer | epoch 0 | step 11550 |avg loss 8.610 |avg tokens 4557.800 |tokens/s 9315.330 |walltime 5848.345 |
Transformer | epoch 0 | step 11560 |avg loss 8.499 |avg tokens 4662.800 |tokens/s 9014.964 |walltime 5853.518 |
Transformer | epoch 0 | step 11570 |avg loss 8.629 |avg tokens 4328.000 |tokens/s 8789.578 |walltime 5858.442 |
Transformer | epoch 0 | step 11580 |avg loss 8.586 |avg tokens 4221.900 |tokens/s 8972.053 |walltime 5863.147 |
Transformer | epoch 0 | step 11590 |avg loss 8.920 |avg tokens 3810.800 |tokens/s 8405.730 |walltime 5867.681 |
Transformer | epoch 0 | step 11600 |avg loss 8.457 |avg tokens 4540.200 |tokens/s 8935.847 |walltime 5872.762 |
Transformer | epoch 0 | step 11610 |avg loss 8.589 |avg tokens 4219.600 |tokens/s 8674.391 |walltime 5877.626 |
Transformer | epoch 0 | step 11620 |avg loss 8.279 |avg tokens 4497.300 |tokens/s 8786.070 |walltime 5882.745 |
Transformer | epoch 0 | step 11630 |avg loss 8.666 |avg tokens 4602.300 |tokens/s 9172.813 |walltime 5887.762 |
Transformer | epoch 0 | step 11640 |avg loss 8.440 |avg tokens 4527.800 |tokens/s 8890.558 |walltime 5892.855 |
Transformer | epoch 0 | step 11650 |avg loss 8.405 |avg tokens 4918.500 |tokens/s 9313.440 |walltime 5898.136 |
Transformer | epoch 0 | step 11660 |avg loss 8.471 |avg tokens 4834.500 |tokens/s 9324.660 |walltime 5903.321 |
Transformer | epoch 0 | step 11670 |avg loss 8.210 |avg tokens 4726.500 |tokens/s 8995.066 |walltime 5908.575 |
Transformer | epoch 0 | step 11680 |avg loss 8.821 |avg tokens 4541.000 |tokens/s 9324.997 |walltime 5913.445 |
Transformer | epoch 0 | step 11690 |avg loss 8.602 |avg tokens 3855.100 |tokens/s 7885.968 |walltime 5918.334 |
Transformer | epoch 0 | step 11700 |avg loss 8.562 |avg tokens 4750.300 |tokens/s 9209.961 |walltime 5923.491 |
Transformer | epoch 0 | step 11710 |avg loss 8.532 |avg tokens 4828.100 |tokens/s 9542.114 |walltime 5928.551 |
Transformer | epoch 0 | step 11720 |avg loss 8.291 |avg tokens 4664.200 |tokens/s 8897.727 |walltime 5933.793 |
Transformer | epoch 0 | step 11730 |avg loss 8.717 |avg tokens 4155.700 |tokens/s 8722.832 |walltime 5938.557 |
Transformer | epoch 0 | step 11740 |avg loss 8.380 |avg tokens 4346.500 |tokens/s 8598.618 |walltime 5943.612 |
Transformer | epoch 0 | step 11750 |avg loss 8.563 |avg tokens 4072.200 |tokens/s 8606.795 |walltime 5948.344 |
Transformer | epoch 0 | step 11760 |avg loss 8.625 |avg tokens 4446.600 |tokens/s 9136.562 |walltime 5953.210 |
Transformer | epoch 0 | step 11770 |avg loss 8.359 |avg tokens 4724.500 |tokens/s 9427.502 |walltime 5958.222 |
Transformer | epoch 0 | step 11780 |avg loss 8.442 |avg tokens 4499.900 |tokens/s 9205.536 |walltime 5963.110 |
Transformer | epoch 0 | step 11790 |avg loss 8.413 |avg tokens 4474.300 |tokens/s 8564.632 |walltime 5968.334 |
Transformer | epoch 0 | step 11800 |avg loss 8.235 |avg tokens 4977.600 |tokens/s 9443.316 |walltime 5973.605 |
Transformer | epoch 0 | step 11810 |avg loss 8.435 |avg tokens 4438.200 |tokens/s 8804.160 |walltime 5978.646 |
Transformer | epoch 0 | step 11820 |avg loss 8.343 |avg tokens 4310.200 |tokens/s 8471.634 |walltime 5983.734 |
Transformer | epoch 0 | step 11830 |avg loss 8.054 |avg tokens 4787.000 |tokens/s 8960.229 |walltime 5989.077 |
Transformer | epoch 0 | step 11840 |avg loss 8.332 |avg tokens 4412.900 |tokens/s 8996.088 |walltime 5993.982 |
Transformer | epoch 0 | step 11850 |avg loss 8.773 |avg tokens 4379.100 |tokens/s 8931.767 |walltime 5998.885 |
Transformer | epoch 0 | step 11860 |avg loss 8.243 |avg tokens 4243.400 |tokens/s 8911.163 |walltime 6003.647 |
Transformer | epoch 0 | step 11870 |avg loss 9.060 |avg tokens 4417.900 |tokens/s 9243.089 |walltime 6008.426 |
Transformer | epoch 0 | step 11880 |avg loss 8.467 |avg tokens 4563.400 |tokens/s 8954.253 |walltime 6013.523 |
Transformer | epoch 0 | step 11890 |avg loss 8.562 |avg tokens 4452.000 |tokens/s 9013.955 |walltime 6018.462 |
Transformer | epoch 0 | step 11900 |avg loss 8.540 |avg tokens 4612.500 |tokens/s 8999.633 |walltime 6023.587 |
Transformer | epoch 0 | step 11910 |avg loss 8.620 |avg tokens 4666.400 |tokens/s 9324.251 |walltime 6028.592 |
Transformer | epoch 0 | step 11920 |avg loss 8.476 |avg tokens 4679.300 |tokens/s 9169.227 |walltime 6033.695 |
Transformer | epoch 0 | step 11930 |avg loss 8.601 |avg tokens 4244.200 |tokens/s 8987.955 |walltime 6038.417 |
Transformer | epoch 0 | step 11940 |avg loss 8.612 |avg tokens 4541.000 |tokens/s 8964.356 |walltime 6043.483 |
Transformer | epoch 0 | step 11950 |avg loss 8.356 |avg tokens 4639.900 |tokens/s 8805.906 |walltime 6048.752 |
Transformer | epoch 0 | step 11960 |avg loss 8.333 |avg tokens 4543.700 |tokens/s 8914.682 |walltime 6053.849 |
Transformer | epoch 0 | step 11970 |avg loss 8.584 |avg tokens 3989.500 |tokens/s 8544.531 |walltime 6058.518 |
Transformer | epoch 0 | step 11980 |avg loss 8.537 |avg tokens 4768.600 |tokens/s 9267.186 |walltime 6063.663 |
Transformer | epoch 0 | step 11990 |avg loss 8.470 |avg tokens 4542.900 |tokens/s 8888.482 |walltime 6068.774 |
Transformer | epoch 0 | step 12000 |avg loss 8.437 |avg tokens 4267.200 |tokens/s 8843.427 |walltime 6073.600 |
Transformer | epoch 0 | step 12010 |avg loss 8.229 |avg tokens 4359.500 |tokens/s 8533.502 |walltime 6078.708 |
Transformer | epoch 0 | step 12020 |avg loss 8.212 |avg tokens 4481.400 |tokens/s 8744.720 |walltime 6083.833 |
Transformer | epoch 0 | step 12030 |avg loss 8.674 |avg tokens 3904.000 |tokens/s 8471.752 |walltime 6088.441 |
Transformer | epoch 0 | step 12040 |avg loss 8.532 |avg tokens 4195.200 |tokens/s 8639.703 |walltime 6093.297 |
Transformer | epoch 0 | step 12050 |avg loss 8.312 |avg tokens 4613.400 |tokens/s 9023.476 |walltime 6098.410 |
Transformer | epoch 0 | step 12060 |avg loss 8.175 |avg tokens 4670.800 |tokens/s 9591.055 |walltime 6103.280 |
Transformer | epoch 0 | step 12070 |avg loss 8.803 |avg tokens 4308.300 |tokens/s 8431.480 |walltime 6108.389 |
Transformer | epoch 0 | step 12080 |avg loss 8.776 |avg tokens 4009.200 |tokens/s 8457.263 |walltime 6113.130 |
Transformer | epoch 0 | step 12090 |avg loss 8.824 |avg tokens 4187.000 |tokens/s 8872.291 |walltime 6117.849 |
Transformer | epoch 0 | step 12100 |avg loss 8.427 |avg tokens 4662.500 |tokens/s 9121.954 |walltime 6122.960 |
Transformer | epoch 0 | step 12110 |avg loss 8.452 |avg tokens 4335.800 |tokens/s 8583.228 |walltime 6128.012 |
Transformer | epoch 0 | step 12120 |avg loss 8.471 |avg tokens 4848.500 |tokens/s 9441.639 |walltime 6133.147 |
Transformer | epoch 0 | step 12130 |avg loss 8.479 |avg tokens 4807.900 |tokens/s 9556.510 |walltime 6138.178 |
Transformer | epoch 0 | step 12140 |avg loss 8.348 |avg tokens 4347.400 |tokens/s 8570.389 |walltime 6143.251 |
Transformer | epoch 0 | step 12150 |avg loss 8.653 |avg tokens 4512.800 |tokens/s 9176.176 |walltime 6148.169 |
Transformer | epoch 0 | step 12160 |avg loss 8.432 |avg tokens 4698.600 |tokens/s 9534.872 |walltime 6153.096 |
Transformer | epoch 0 | step 12170 |avg loss 8.640 |avg tokens 4112.000 |tokens/s 8513.864 |walltime 6157.926 |
Transformer | epoch 0 | step 12180 |avg loss 8.508 |avg tokens 3654.800 |tokens/s 7859.339 |walltime 6162.577 |
Transformer | epoch 0 | step 12190 |avg loss 8.651 |avg tokens 4510.200 |tokens/s 9114.062 |walltime 6167.525 |
Transformer | epoch 0 | step 12200 |avg loss 8.701 |avg tokens 3772.300 |tokens/s 7944.757 |walltime 6172.273 |
Transformer | epoch 0 | step 12210 |avg loss 8.521 |avg tokens 4375.200 |tokens/s 9029.712 |walltime 6177.119 |
Transformer | epoch 0 | step 12220 |avg loss 8.429 |avg tokens 4536.800 |tokens/s 8757.881 |walltime 6182.299 |
Transformer | epoch 0 | step 12230 |avg loss 8.630 |avg tokens 4701.200 |tokens/s 9440.650 |walltime 6187.279 |
Transformer | epoch 0 | step 12240 |avg loss 8.457 |avg tokens 4663.200 |tokens/s 9086.644 |walltime 6192.411 |
Transformer | epoch 0 | step 12250 |avg loss 8.507 |avg tokens 4209.000 |tokens/s 8841.187 |walltime 6197.171 |
Transformer | epoch 0 | step 12260 |avg loss 8.375 |avg tokens 4991.700 |tokens/s 9438.775 |walltime 6202.460 |
Transformer | epoch 0 | step 12270 |avg loss 7.949 |avg tokens 4914.400 |tokens/s 9197.365 |walltime 6207.803 |
Transformer | epoch 0 | step 12280 |avg loss 8.427 |avg tokens 4726.300 |tokens/s 9121.694 |walltime 6212.984 |
Transformer | epoch 0 | step 12290 |avg loss 8.637 |avg tokens 4410.000 |tokens/s 9088.988 |walltime 6217.836 |
Transformer | epoch 0 | step 12300 |avg loss 8.479 |avg tokens 4080.900 |tokens/s 8514.807 |walltime 6222.629 |
Transformer | epoch 0 | step 12310 |avg loss 8.468 |avg tokens 4775.400 |tokens/s 9274.988 |walltime 6227.778 |
Transformer | epoch 0 | step 12320 |avg loss 8.712 |avg tokens 4331.800 |tokens/s 9158.342 |walltime 6232.508 |
Transformer | epoch 0 | step 12330 |avg loss 8.409 |avg tokens 4599.400 |tokens/s 8869.319 |walltime 6237.693 |
Transformer | epoch 0 | step 12340 |avg loss 8.591 |avg tokens 4558.900 |tokens/s 9058.870 |walltime 6242.726 |
Transformer | epoch 0 | step 12350 |avg loss 8.400 |avg tokens 4620.300 |tokens/s 8969.973 |walltime 6247.877 |
Transformer | epoch 0 | step 12360 |avg loss 8.073 |avg tokens 4742.500 |tokens/s 9341.919 |walltime 6252.953 |
Transformer | epoch 0 | step 12370 |avg loss 8.477 |avg tokens 4302.300 |tokens/s 8868.586 |walltime 6257.805 |
Transformer | epoch 0 | step 12380 |avg loss 8.279 |avg tokens 4400.800 |tokens/s 8633.401 |walltime 6262.902 |
Transformer | epoch 0 | step 12390 |avg loss 8.374 |avg tokens 4336.200 |tokens/s 8632.648 |walltime 6267.925 |
Transformer | epoch 0 | step 12400 |avg loss 8.667 |avg tokens 4409.700 |tokens/s 8566.309 |walltime 6273.073 |
Transformer | epoch 0 | step 12410 |avg loss 8.409 |avg tokens 4550.500 |tokens/s 8913.646 |walltime 6278.178 |
Transformer | epoch 0 | step 12420 |avg loss 8.284 |avg tokens 4883.500 |tokens/s 9375.211 |walltime 6283.387 |
Transformer | epoch 0 | step 12430 |avg loss 8.508 |avg tokens 4664.500 |tokens/s 9053.624 |walltime 6288.539 |
Transformer | epoch 0 | step 12440 |avg loss 8.607 |avg tokens 4328.300 |tokens/s 8871.443 |walltime 6293.418 |
Transformer | epoch 0 | step 12450 |avg loss 8.340 |avg tokens 4719.100 |tokens/s 9253.614 |walltime 6298.518 |
Transformer | epoch 0 | step 12460 |avg loss 8.408 |avg tokens 4097.900 |tokens/s 8426.036 |walltime 6303.381 |
Transformer | epoch 0 | step 12470 |avg loss 8.423 |avg tokens 4403.900 |tokens/s 8708.207 |walltime 6308.438 |
Transformer | epoch 0 | step 12480 |avg loss 8.438 |avg tokens 4509.700 |tokens/s 8577.741 |walltime 6313.696 |
Transformer | epoch 0 | step 12490 |avg loss 8.293 |avg tokens 4199.300 |tokens/s 8457.715 |walltime 6318.661 |
Transformer | epoch 0 | step 12500 |avg loss 8.324 |avg tokens 4255.300 |tokens/s 8571.021 |walltime 6323.625 |
Transformer | epoch 0 | step 12510 |avg loss 8.380 |avg tokens 3838.800 |tokens/s 8071.014 |walltime 6328.382 |
Transformer | epoch 0 | step 12520 |avg loss 8.485 |avg tokens 4270.800 |tokens/s 8587.304 |walltime 6333.355 |
Transformer | epoch 0 | step 12530 |avg loss 8.327 |avg tokens 4399.300 |tokens/s 8714.879 |walltime 6338.403 |
Transformer | epoch 0 | step 12540 |avg loss 8.664 |avg tokens 4179.600 |tokens/s 8716.423 |walltime 6343.198 |
Transformer | epoch 0 | step 12550 |avg loss 8.412 |avg tokens 4896.800 |tokens/s 9219.209 |walltime 6348.510 |
Transformer | epoch 0 | step 12560 |avg loss 8.116 |avg tokens 4295.500 |tokens/s 8606.063 |walltime 6353.501 |
Transformer | epoch 0 | step 12570 |avg loss 8.780 |avg tokens 4402.200 |tokens/s 9292.228 |walltime 6358.238 |
Transformer | epoch 0 | step 12580 |avg loss 8.410 |avg tokens 4762.800 |tokens/s 8965.922 |walltime 6363.551 |
Transformer | epoch 0 | step 12590 |avg loss 8.418 |avg tokens 4682.800 |tokens/s 9373.181 |walltime 6368.547 |
Transformer | epoch 0 | step 12600 |avg loss 8.400 |avg tokens 4764.800 |tokens/s 9066.910 |walltime 6373.802 |
Transformer | epoch 0 | step 12610 |avg loss 8.302 |avg tokens 4572.500 |tokens/s 9023.896 |walltime 6378.869 |
Transformer | epoch 0 | step 12620 |avg loss 8.436 |avg tokens 4518.400 |tokens/s 8936.909 |walltime 6383.925 |
Transformer | epoch 0 | step 12630 |avg loss 8.339 |avg tokens 4612.800 |tokens/s 8918.239 |walltime 6389.097 |
Transformer | epoch 0 | step 12640 |avg loss 8.220 |avg tokens 4755.700 |tokens/s 9087.586 |walltime 6394.330 |
Transformer | epoch 0 | step 12650 |avg loss 8.391 |avg tokens 4681.700 |tokens/s 9033.913 |walltime 6399.513 |
Transformer | epoch 0 | step 12660 |avg loss 8.530 |avg tokens 4348.200 |tokens/s 9021.461 |walltime 6404.332 |
Transformer | epoch 0 | step 12670 |avg loss 8.431 |avg tokens 4448.600 |tokens/s 8878.059 |walltime 6409.343 |
Transformer | epoch 0 | step 12680 |avg loss 8.513 |avg tokens 4784.300 |tokens/s 9089.501 |walltime 6414.607 |
Transformer | epoch 0 | step 12690 |avg loss 8.578 |avg tokens 3652.900 |tokens/s 7703.565 |walltime 6419.349 |
Transformer | epoch 0 | step 12700 |avg loss 8.414 |avg tokens 4738.200 |tokens/s 9630.554 |walltime 6424.269 |
Transformer | epoch 0 | step 12710 |avg loss 8.343 |avg tokens 4789.100 |tokens/s 9546.852 |walltime 6429.285 |
Transformer | epoch 0 | step 12720 |avg loss 8.588 |avg tokens 4432.500 |tokens/s 8776.109 |walltime 6434.336 |
Transformer | epoch 0 | step 12730 |avg loss 8.565 |avg tokens 4804.700 |tokens/s 9159.485 |walltime 6439.581 |
Transformer | epoch 0 | step 12740 |avg loss 8.767 |avg tokens 4467.000 |tokens/s 9340.641 |walltime 6444.364 |
Transformer | epoch 0 | step 12750 |avg loss 8.456 |avg tokens 4709.300 |tokens/s 9201.339 |walltime 6449.482 |
Transformer | epoch 0 | step 12760 |avg loss 8.675 |avg tokens 4511.100 |tokens/s 9413.958 |walltime 6454.274 |
Transformer | epoch 0 | step 12770 |avg loss 8.315 |avg tokens 4810.300 |tokens/s 8953.228 |walltime 6459.646 |
Transformer | epoch 0 | step 12780 |avg loss 8.349 |avg tokens 4607.700 |tokens/s 8986.046 |walltime 6464.774 |
Transformer | epoch 0 | step 12790 |avg loss 8.390 |avg tokens 4683.700 |tokens/s 9026.447 |walltime 6469.963 |
Transformer | epoch 0 | step 12800 |avg loss 8.432 |avg tokens 4866.400 |tokens/s 9318.895 |walltime 6475.185 |
Transformer | epoch 0 | step 12810 |avg loss 8.391 |avg tokens 4761.800 |tokens/s 9181.734 |walltime 6480.371 |
Transformer | epoch 0 | step 12820 |avg loss 8.394 |avg tokens 4244.200 |tokens/s 8494.145 |walltime 6485.368 |
Transformer | epoch 0 | step 12830 |avg loss 8.445 |avg tokens 4498.600 |tokens/s 8900.870 |walltime 6490.422 |
Transformer | epoch 0 | step 12840 |avg loss 8.503 |avg tokens 4525.400 |tokens/s 8922.014 |walltime 6495.494 |
Transformer | epoch 0 | step 12850 |avg loss 8.387 |avg tokens 4659.600 |tokens/s 8877.670 |walltime 6500.743 |
Transformer | epoch 0 | step 12860 |avg loss 8.500 |avg tokens 4546.800 |tokens/s 8793.316 |walltime 6505.913 |
Transformer | epoch 0 | step 12870 |avg loss 8.590 |avg tokens 4216.200 |tokens/s 8727.497 |walltime 6510.744 |
Transformer | epoch 0 | step 12880 |avg loss 8.448 |avg tokens 4532.800 |tokens/s 8760.473 |walltime 6515.918 |
Transformer | epoch 0 | step 12890 |avg loss 8.305 |avg tokens 4514.300 |tokens/s 8849.664 |walltime 6521.020 |
Transformer | epoch 0 | step 12900 |avg loss 8.327 |avg tokens 4762.000 |tokens/s 8941.331 |walltime 6526.345 |
Transformer | epoch 0 | step 12910 |avg loss 8.237 |avg tokens 4760.800 |tokens/s 9164.428 |walltime 6531.540 |
Transformer | epoch 0 | step 12920 |avg loss 8.687 |avg tokens 4062.300 |tokens/s 8739.038 |walltime 6536.189 |
Transformer | epoch 0 | step 12930 |avg loss 8.692 |avg tokens 4657.400 |tokens/s 9128.179 |walltime 6541.291 |
Transformer | epoch 0 | step 12940 |avg loss 8.461 |avg tokens 4633.600 |tokens/s 9021.357 |walltime 6546.427 |
Transformer | epoch 0 | step 12950 |avg loss 8.529 |avg tokens 4871.200 |tokens/s 9386.702 |walltime 6551.617 |
Transformer | epoch 0 | step 12960 |avg loss 8.522 |avg tokens 4586.400 |tokens/s 8806.750 |walltime 6556.824 |
Transformer | epoch 0 | step 12970 |avg loss 8.359 |avg tokens 4715.800 |tokens/s 9100.178 |walltime 6562.007 |
Transformer | epoch 0 | step 12980 |avg loss 8.520 |avg tokens 4449.400 |tokens/s 8817.995 |walltime 6567.052 |
Transformer | epoch 0 | step 12990 |avg loss 8.589 |avg tokens 4396.300 |tokens/s 8635.244 |walltime 6572.144 |
Transformer | epoch 0 | step 13000 |avg loss 8.483 |avg tokens 4827.600 |tokens/s 9502.549 |walltime 6577.224 |
Transformer | epoch 0 | step 13010 |avg loss 8.111 |avg tokens 4485.800 |tokens/s 8864.229 |walltime 6582.284 |
Transformer | epoch 0 | step 13020 |avg loss 8.381 |avg tokens 4319.900 |tokens/s 8504.296 |walltime 6587.364 |
Transformer | epoch 0 | step 13030 |avg loss 8.363 |avg tokens 4058.500 |tokens/s 8470.426 |walltime 6592.155 |
Transformer | epoch 0 | step 13040 |avg loss 8.618 |avg tokens 4791.500 |tokens/s 9392.803 |walltime 6597.257 |
Transformer | epoch 0 | step 13050 |avg loss 8.431 |avg tokens 4294.100 |tokens/s 8515.992 |walltime 6602.299 |
Transformer | epoch 0 | step 13060 |avg loss 8.629 |avg tokens 4597.800 |tokens/s 9110.897 |walltime 6607.346 |
Transformer | epoch 0 | step 13070 |avg loss 8.573 |avg tokens 4670.500 |tokens/s 9192.498 |walltime 6612.426 |
Transformer | epoch 0 | step 13080 |avg loss 8.568 |avg tokens 4119.900 |tokens/s 8352.903 |walltime 6617.359 |
Transformer | epoch 0 | step 13090 |avg loss 8.500 |avg tokens 4483.200 |tokens/s 9105.713 |walltime 6622.282 |
Transformer | epoch 0 | step 13100 |avg loss 8.337 |avg tokens 4271.900 |tokens/s 8626.651 |walltime 6627.234 |
Transformer | epoch 0 | step 13110 |avg loss 8.365 |avg tokens 4812.300 |tokens/s 9294.613 |walltime 6632.412 |
Transformer | epoch 0 | step 13120 |avg loss 8.775 |avg tokens 3831.600 |tokens/s 8566.836 |walltime 6636.884 |
Transformer | epoch 0 | step 13130 |avg loss 8.663 |avg tokens 4729.600 |tokens/s 9279.470 |walltime 6641.981 |
Transformer | epoch 0 | step 13140 |avg loss 8.539 |avg tokens 4187.400 |tokens/s 8545.755 |walltime 6646.881 |
Transformer | epoch 0 | step 13150 |avg loss 8.304 |avg tokens 4208.400 |tokens/s 8491.545 |walltime 6651.837 |
Transformer | epoch 0 | step 13160 |avg loss 8.376 |avg tokens 4578.100 |tokens/s 8874.930 |walltime 6656.996 |
Transformer | epoch 0 | step 13170 |avg loss 8.494 |avg tokens 4310.800 |tokens/s 8754.477 |walltime 6661.920 |
Transformer | epoch 0 | step 13180 |avg loss 8.563 |avg tokens 4549.600 |tokens/s 8973.065 |walltime 6666.990 |
Transformer | epoch 0 | step 13190 |avg loss 8.284 |avg tokens 4436.400 |tokens/s 8914.918 |walltime 6671.966 |
Transformer | epoch 0 | step 13200 |avg loss 8.806 |avg tokens 3979.200 |tokens/s 8402.842 |walltime 6676.702 |
Transformer | epoch 0 | step 13210 |avg loss 8.677 |avg tokens 4787.700 |tokens/s 9540.942 |walltime 6681.720 |
Transformer | epoch 0 | step 13220 |avg loss 8.355 |avg tokens 4473.300 |tokens/s 8768.361 |walltime 6686.822 |
Transformer | epoch 0 | step 13230 |avg loss 8.370 |avg tokens 4832.500 |tokens/s 9413.457 |walltime 6691.955 |
Transformer | epoch 0 | step 13240 |avg loss 8.578 |avg tokens 4653.200 |tokens/s 9213.620 |walltime 6697.006 |
Transformer | epoch 0 | step 13250 |avg loss 8.259 |avg tokens 4381.500 |tokens/s 8541.103 |walltime 6702.135 |
Transformer | epoch 0 | step 13260 |avg loss 8.229 |avg tokens 4992.000 |tokens/s 9326.435 |walltime 6707.488 |
Transformer | epoch 0 | step 13270 |avg loss 8.488 |avg tokens 4532.100 |tokens/s 9224.469 |walltime 6712.401 |
Transformer | epoch 0 | step 13280 |avg loss 8.466 |avg tokens 4657.300 |tokens/s 8912.030 |walltime 6717.627 |
Transformer | epoch 0 | step 13290 |avg loss 8.501 |avg tokens 4556.800 |tokens/s 8829.644 |walltime 6722.788 |
Transformer | epoch 0 | step 13300 |avg loss 8.336 |avg tokens 4834.700 |tokens/s 9131.401 |walltime 6728.082 |
Transformer | epoch 0 | step 13310 |avg loss 8.685 |avg tokens 4072.200 |tokens/s 8491.318 |walltime 6732.878 |
Transformer | epoch 0 | step 13320 |avg loss 8.378 |avg tokens 4315.600 |tokens/s 8607.287 |walltime 6737.892 |
Transformer | epoch 0 | step 13330 |avg loss 8.519 |avg tokens 4642.300 |tokens/s 9265.960 |walltime 6742.902 |
Transformer | epoch 0 | step 13340 |avg loss 8.508 |avg tokens 4308.000 |tokens/s 8593.159 |walltime 6747.915 |
Transformer | epoch 0 | step 13350 |avg loss 8.425 |avg tokens 4546.700 |tokens/s 9192.192 |walltime 6752.862 |
Transformer | epoch 0 | step 13360 |avg loss 8.522 |avg tokens 4709.400 |tokens/s 9309.511 |walltime 6757.920 |
Transformer | epoch 0 | step 13370 |avg loss 8.378 |avg tokens 4776.300 |tokens/s 9275.488 |walltime 6763.070 |
Transformer | epoch 0 | step 13380 |avg loss 8.754 |avg tokens 4482.500 |tokens/s 8687.648 |walltime 6768.229 |
Transformer | epoch 0 | step 13390 |avg loss 8.584 |avg tokens 4300.500 |tokens/s 8678.633 |walltime 6773.185 |
Transformer | epoch 0 | step 13400 |avg loss 8.654 |avg tokens 4778.200 |tokens/s 9566.831 |walltime 6778.179 |
Transformer | epoch 0 | step 13410 |avg loss 8.608 |avg tokens 4417.800 |tokens/s 8970.521 |walltime 6783.104 |
Transformer | epoch 0 | step 13420 |avg loss 8.297 |avg tokens 4591.200 |tokens/s 8869.254 |walltime 6788.280 |
Transformer | epoch 0 | step 13430 |avg loss 8.706 |avg tokens 3976.100 |tokens/s 7907.080 |walltime 6793.309 |
Transformer | epoch 0 | step 13440 |avg loss 8.385 |avg tokens 4646.800 |tokens/s 9158.486 |walltime 6798.383 |
Transformer | epoch 0 | step 13450 |avg loss 8.824 |avg tokens 4342.100 |tokens/s 9285.176 |walltime 6803.059 |
Transformer | epoch 0 | step 13460 |avg loss 8.305 |avg tokens 4710.900 |tokens/s 9027.117 |walltime 6808.278 |
Transformer | epoch 0 | step 13470 |avg loss 8.306 |avg tokens 4849.700 |tokens/s 9407.972 |walltime 6813.433 |
Transformer | epoch 0 | step 13480 |avg loss 8.479 |avg tokens 4687.300 |tokens/s 9222.715 |walltime 6818.515 |
Transformer | epoch 0 | step 13490 |avg loss 8.360 |avg tokens 4480.000 |tokens/s 8773.244 |walltime 6823.621 |
Transformer | epoch 0 | step 13500 |avg loss 8.567 |avg tokens 4168.500 |tokens/s 8654.169 |walltime 6828.438 |
Transformer | epoch 0 | step 13510 |avg loss 8.430 |avg tokens 4632.800 |tokens/s 9102.894 |walltime 6833.528 |
Transformer | epoch 0 | step 13520 |avg loss 8.478 |avg tokens 4336.300 |tokens/s 8823.938 |walltime 6838.442 |
Transformer | epoch 0 | step 13530 |avg loss 8.300 |avg tokens 4387.500 |tokens/s 8781.561 |walltime 6843.438 |
Transformer | epoch 0 | step 13540 |avg loss 8.834 |avg tokens 4262.500 |tokens/s 9124.860 |walltime 6848.109 |
Transformer | epoch 0 | step 13550 |avg loss 8.710 |avg tokens 3607.700 |tokens/s 7811.248 |walltime 6852.728 |
Transformer | epoch 0 | step 13560 |avg loss 8.441 |avg tokens 4685.100 |tokens/s 9022.854 |walltime 6857.920 |
Transformer | epoch 0 | step 13570 |avg loss 8.455 |avg tokens 4581.200 |tokens/s 8913.674 |walltime 6863.060 |
Transformer | epoch 0 | step 13580 |avg loss 8.450 |avg tokens 4563.100 |tokens/s 8932.160 |walltime 6868.169 |
Transformer | epoch 0 | step 13590 |avg loss 8.489 |avg tokens 4397.600 |tokens/s 8915.731 |walltime 6873.101 |
Transformer | epoch 0 | step 13600 |avg loss 8.714 |avg tokens 3968.700 |tokens/s 8596.987 |walltime 6877.717 |
Transformer | epoch 0 | step 13610 |avg loss 8.407 |avg tokens 4350.000 |tokens/s 8812.992 |walltime 6882.653 |
Transformer | epoch 0 | step 13620 |avg loss 8.429 |avg tokens 4590.300 |tokens/s 8978.519 |walltime 6887.766 |
Transformer | epoch 0 | step 13630 |avg loss 8.516 |avg tokens 4419.400 |tokens/s 8855.535 |walltime 6892.756 |
Transformer | epoch 0 | step 13640 |avg loss 8.556 |avg tokens 4622.800 |tokens/s 9324.982 |walltime 6897.714 |
Transformer | epoch 0 | step 13650 |avg loss 8.359 |avg tokens 4666.200 |tokens/s 8888.210 |walltime 6902.964 |
Transformer | epoch 0 | step 13660 |avg loss 8.266 |avg tokens 4416.900 |tokens/s 8958.706 |walltime 6907.894 |
Transformer | epoch 0 | step 13670 |avg loss 8.328 |avg tokens 4668.000 |tokens/s 9046.349 |walltime 6913.054 |
Transformer | epoch 0 | step 13680 |avg loss 8.644 |avg tokens 4594.700 |tokens/s 9210.627 |walltime 6918.043 |
Transformer | epoch 0 | step 13690 |avg loss 8.819 |avg tokens 4357.600 |tokens/s 8860.029 |walltime 6922.961 |
Transformer | epoch 0 | step 13700 |avg loss 8.263 |avg tokens 4835.800 |tokens/s 9152.275 |walltime 6928.245 |
Transformer | epoch 0 | step 13710 |avg loss 8.452 |avg tokens 4884.200 |tokens/s 9118.393 |walltime 6933.601 |
Transformer | epoch 0 | step 13720 |avg loss 8.395 |avg tokens 4631.200 |tokens/s 8945.718 |walltime 6938.778 |
Transformer | epoch 0 | step 13730 |avg loss 8.790 |avg tokens 4620.100 |tokens/s 9442.185 |walltime 6943.671 |
Transformer | epoch 0 | step 13740 |avg loss 8.625 |avg tokens 4798.000 |tokens/s 9306.824 |walltime 6948.826 |
Transformer | epoch 0 | step 13750 |avg loss 8.806 |avg tokens 4258.300 |tokens/s 8968.450 |walltime 6953.575 |
Transformer | epoch 0 | step 13760 |avg loss 8.329 |avg tokens 4431.500 |tokens/s 8832.511 |walltime 6958.592 |
Transformer | epoch 0 | step 13770 |avg loss 8.468 |avg tokens 4483.700 |tokens/s 8754.672 |walltime 6963.713 |
Transformer | epoch 0 | step 13780 |avg loss 8.324 |avg tokens 4733.800 |tokens/s 9011.150 |walltime 6968.967 |
Transformer | epoch 0 | step 13790 |avg loss 8.124 |avg tokens 4773.900 |tokens/s 9121.669 |walltime 6974.200 |
Transformer | epoch 0 | step 13800 |avg loss 8.285 |avg tokens 4649.800 |tokens/s 8848.865 |walltime 6979.455 |
Transformer | epoch 0 | step 13810 |avg loss 8.770 |avg tokens 4250.900 |tokens/s 8926.208 |walltime 6984.217 |
Transformer | epoch 0 | step 13820 |avg loss 8.854 |avg tokens 4518.900 |tokens/s 9373.156 |walltime 6989.038 |
Transformer | epoch 0 | step 13830 |avg loss 8.554 |avg tokens 4763.700 |tokens/s 9289.605 |walltime 6994.166 |
Transformer | epoch 0 | step 13840 |avg loss 8.554 |avg tokens 4453.500 |tokens/s 8717.283 |walltime 6999.275 |
Transformer | epoch 0 | step 13850 |avg loss 8.345 |avg tokens 4929.000 |tokens/s 9099.968 |walltime 7004.692 |
Transformer | epoch 0 | step 13860 |avg loss 8.479 |avg tokens 4743.500 |tokens/s 8921.535 |walltime 7010.008 |
Transformer | epoch 0 | step 13870 |avg loss 8.445 |avg tokens 4862.100 |tokens/s 9064.940 |walltime 7015.372 |
Transformer | epoch 0 | step 13880 |avg loss 8.498 |avg tokens 4073.200 |tokens/s 8492.804 |walltime 7020.168 |
Transformer | epoch 0 | step 13890 |avg loss 8.486 |avg tokens 4256.200 |tokens/s 8455.815 |walltime 7025.202 |
Transformer | epoch 0 | step 13900 |avg loss 8.525 |avg tokens 4504.700 |tokens/s 8891.103 |walltime 7030.268 |
Transformer | epoch 0 | step 13910 |avg loss 8.546 |avg tokens 4862.900 |tokens/s 9701.461 |walltime 7035.281 |
Transformer | epoch 0 | step 13920 |avg loss 8.314 |avg tokens 4477.000 |tokens/s 8815.663 |walltime 7040.359 |
Transformer | epoch 0 | step 13930 |avg loss 8.366 |avg tokens 4585.500 |tokens/s 9047.312 |walltime 7045.427 |
Transformer | epoch 0 | step 13940 |avg loss 8.284 |avg tokens 4721.200 |tokens/s 8961.114 |walltime 7050.696 |
Transformer | epoch 0 | step 13950 |avg loss 8.332 |avg tokens 4321.200 |tokens/s 8626.608 |walltime 7055.705 |
Transformer | epoch 0 | step 13960 |avg loss 8.425 |avg tokens 4461.500 |tokens/s 9051.812 |walltime 7060.634 |
Transformer | epoch 0 | step 13970 |avg loss 8.224 |avg tokens 4744.000 |tokens/s 8988.211 |walltime 7065.912 |
Transformer | epoch 0 | step 13980 |avg loss 8.290 |avg tokens 4888.100 |tokens/s 9180.283 |walltime 7071.237 |
Transformer | epoch 0 | step 13990 |avg loss 8.463 |avg tokens 4233.400 |tokens/s 8452.541 |walltime 7076.245 |
Transformer | epoch 0 | step 14000 |avg loss 8.424 |avg tokens 4190.200 |tokens/s 8420.029 |walltime 7081.222 |
Transformer | epoch 0 | step 14010 |avg loss 8.426 |avg tokens 4863.800 |tokens/s 9368.639 |walltime 7086.413 |
Transformer | epoch 0 | step 14020 |avg loss 8.638 |avg tokens 4451.500 |tokens/s 9183.654 |walltime 7091.260 |
Transformer | epoch 0 | step 14030 |avg loss 8.320 |avg tokens 4933.600 |tokens/s 9319.880 |walltime 7096.554 |
Transformer | epoch 0 | step 14040 |avg loss 8.402 |avg tokens 4477.900 |tokens/s 8825.625 |walltime 7101.628 |
Transformer | epoch 0 | step 14050 |avg loss 8.269 |avg tokens 4781.100 |tokens/s 9046.298 |walltime 7106.913 |
Transformer | epoch 0 | step 14060 |avg loss 8.433 |avg tokens 4725.600 |tokens/s 9193.794 |walltime 7112.053 |
Transformer | epoch 0 | step 14070 |avg loss 8.525 |avg tokens 4691.800 |tokens/s 9397.744 |walltime 7117.045 |
Transformer | epoch 0 | step 14080 |avg loss 8.182 |avg tokens 4675.900 |tokens/s 9011.157 |walltime 7122.234 |
Transformer | epoch 0 | step 14090 |avg loss 8.571 |avg tokens 4049.700 |tokens/s 8287.839 |walltime 7127.121 |
Transformer | epoch 0 | step 14100 |avg loss 8.876 |avg tokens 4112.900 |tokens/s 8882.671 |walltime 7131.751 |
Transformer | epoch 0 | step 14110 |avg loss 8.566 |avg tokens 4593.700 |tokens/s 9132.089 |walltime 7136.781 |
Transformer | epoch 0 | step 14120 |avg loss 8.644 |avg tokens 4485.800 |tokens/s 8747.406 |walltime 7141.909 |
Transformer | epoch 0 | step 14130 |avg loss 8.703 |avg tokens 4559.500 |tokens/s 9278.008 |walltime 7146.824 |
Transformer | epoch 0 | step 14140 |avg loss 8.223 |avg tokens 4659.200 |tokens/s 8835.671 |walltime 7152.097 |
Transformer | epoch 0 | step 14150 |avg loss 8.074 |avg tokens 4964.900 |tokens/s 9518.588 |walltime 7157.313 |
Transformer | epoch 0 | step 14160 |avg loss 8.208 |avg tokens 4171.900 |tokens/s 8340.136 |walltime 7162.315 |
Transformer | epoch 0 | step 14170 |avg loss 8.676 |avg tokens 4305.800 |tokens/s 8901.425 |walltime 7167.152 |
Transformer | epoch 0 | step 14180 |avg loss 8.359 |avg tokens 4644.100 |tokens/s 9123.776 |walltime 7172.242 |
Transformer | epoch 0 | step 14190 |avg loss 8.419 |avg tokens 4566.400 |tokens/s 8923.037 |walltime 7177.360 |
Transformer | epoch 0 | step 14200 |avg loss 8.510 |avg tokens 4142.600 |tokens/s 8294.974 |walltime 7182.354 |
Transformer | epoch 0 | step 14210 |avg loss 8.613 |avg tokens 4362.300 |tokens/s 9093.147 |walltime 7187.151 |
Transformer | epoch 0 | step 14220 |avg loss 8.584 |avg tokens 4100.100 |tokens/s 8351.104 |walltime 7192.061 |
Transformer | epoch 0 | step 14230 |avg loss 8.425 |avg tokens 4697.600 |tokens/s 9299.070 |walltime 7197.113 |
Transformer | epoch 0 | step 14240 |avg loss 8.260 |avg tokens 4424.600 |tokens/s 8706.923 |walltime 7202.194 |
Transformer | epoch 0 | step 14250 |avg loss 8.399 |avg tokens 4733.900 |tokens/s 8979.770 |walltime 7207.466 |
Transformer | epoch 0 | step 14260 |avg loss 8.260 |avg tokens 4628.800 |tokens/s 9010.299 |walltime 7212.603 |
Transformer | epoch 0 | step 14270 |avg loss 8.157 |avg tokens 4790.400 |tokens/s 9263.914 |walltime 7217.774 |
Transformer | epoch 0 | step 14280 |avg loss 8.443 |avg tokens 4243.500 |tokens/s 8548.668 |walltime 7222.738 |
Transformer | epoch 0 | step 14290 |avg loss 8.236 |avg tokens 4936.000 |tokens/s 9237.978 |walltime 7228.082 |
Transformer | epoch 0 | step 14300 |avg loss 8.454 |avg tokens 4504.500 |tokens/s 8750.391 |walltime 7233.229 |
Transformer | epoch 0 | step 14310 |avg loss 8.384 |avg tokens 4435.500 |tokens/s 8904.543 |walltime 7238.210 |
Transformer | epoch 0 | step 14320 |avg loss 8.426 |avg tokens 4907.500 |tokens/s 9138.886 |walltime 7243.580 |
Transformer | epoch 0 | step 14330 |avg loss 8.279 |avg tokens 4786.400 |tokens/s 9142.074 |walltime 7248.816 |
Transformer | epoch 0 | step 14340 |avg loss 8.385 |avg tokens 4509.600 |tokens/s 9058.119 |walltime 7253.794 |
Transformer | epoch 0 | step 14350 |avg loss 8.450 |avg tokens 4397.800 |tokens/s 9056.981 |walltime 7258.650 |
Transformer | epoch 0 | step 14360 |avg loss 8.364 |avg tokens 4636.700 |tokens/s 8980.357 |walltime 7263.813 |
Transformer | epoch 0 | step 14370 |avg loss 8.851 |avg tokens 4789.400 |tokens/s 9903.859 |walltime 7268.649 |
Transformer | epoch 0 | step 14380 |avg loss 8.471 |avg tokens 4741.000 |tokens/s 9198.862 |walltime 7273.803 |
Transformer | epoch 0 | step 14390 |avg loss 8.572 |avg tokens 4344.500 |tokens/s 8741.044 |walltime 7278.773 |
Transformer | epoch 0 | step 14400 |avg loss 8.435 |avg tokens 4273.300 |tokens/s 8595.171 |walltime 7283.745 |
Transformer | epoch 0 | step 14410 |avg loss 8.582 |avg tokens 3978.700 |tokens/s 8283.907 |walltime 7288.548 |
Transformer | epoch 0 | step 14420 |avg loss 8.441 |avg tokens 4690.700 |tokens/s 9233.733 |walltime 7293.628 |
Transformer | epoch 0 | step 14430 |avg loss 8.608 |avg tokens 4695.000 |tokens/s 9713.129 |walltime 7298.462 |
Transformer | epoch 0 | step 14440 |avg loss 8.599 |avg tokens 3559.200 |tokens/s 7812.683 |walltime 7303.017 |
Transformer | epoch 0 | step 14450 |avg loss 8.512 |avg tokens 4805.600 |tokens/s 9070.633 |walltime 7308.315 |
Transformer | epoch 0 | step 14460 |avg loss 8.640 |avg tokens 4652.300 |tokens/s 9405.526 |walltime 7313.262 |
Transformer | epoch 0 | step 14470 |avg loss 8.663 |avg tokens 4518.400 |tokens/s 9124.853 |walltime 7318.213 |
Transformer | epoch 0 | step 14480 |avg loss 8.520 |avg tokens 4549.600 |tokens/s 9027.337 |walltime 7323.253 |
Transformer | epoch 0 | step 14490 |avg loss 8.712 |avg tokens 4031.300 |tokens/s 8443.959 |walltime 7328.027 |
Transformer | epoch 0 | step 14500 |avg loss 8.630 |avg tokens 4489.900 |tokens/s 9059.637 |walltime 7332.983 |
Transformer | epoch 0 | step 14510 |avg loss 8.517 |avg tokens 4768.100 |tokens/s 9118.596 |walltime 7338.212 |
Transformer | epoch 0 | step 14520 |avg loss 8.797 |avg tokens 4473.900 |tokens/s 9443.327 |walltime 7342.950 |
Transformer | epoch 0 | step 14530 |avg loss 8.580 |avg tokens 4470.500 |tokens/s 8740.046 |walltime 7348.065 |
Transformer | epoch 0 | step 14540 |avg loss 8.470 |avg tokens 4442.000 |tokens/s 8793.257 |walltime 7353.117 |
Transformer | epoch 0 | step 14550 |avg loss 8.572 |avg tokens 4304.600 |tokens/s 8715.032 |walltime 7358.056 |
Transformer | epoch 0 | step 14560 |avg loss 8.580 |avg tokens 4463.500 |tokens/s 8912.130 |walltime 7363.064 |
Transformer | epoch 0 | step 14570 |avg loss 8.651 |avg tokens 4620.600 |tokens/s 9090.956 |walltime 7368.147 |
Transformer | epoch 0 | step 14580 |avg loss 8.461 |avg tokens 4748.400 |tokens/s 9222.307 |walltime 7373.296 |
Transformer | epoch 0 | step 14590 |avg loss 8.545 |avg tokens 4153.600 |tokens/s 8707.826 |walltime 7378.066 |
Transformer | epoch 0 | step 14600 |avg loss 8.602 |avg tokens 4630.700 |tokens/s 8840.383 |walltime 7383.304 |
Transformer | epoch 0 | step 14610 |avg loss 8.837 |avg tokens 4141.100 |tokens/s 8632.567 |walltime 7388.101 |
Transformer | epoch 0 | step 14620 |avg loss 8.587 |avg tokens 4907.400 |tokens/s 9574.720 |walltime 7393.226 |
Transformer | epoch 0 | step 14630 |avg loss 8.669 |avg tokens 4391.200 |tokens/s 8541.152 |walltime 7398.367 |
Transformer | epoch 0 | step 14640 |avg loss 8.402 |avg tokens 4649.500 |tokens/s 9039.002 |walltime 7403.511 |
Transformer | epoch 0 | step 14650 |avg loss 8.705 |avg tokens 4281.100 |tokens/s 8887.115 |walltime 7408.328 |
Transformer | epoch 0 | step 14660 |avg loss 8.619 |avg tokens 4459.600 |tokens/s 9124.476 |walltime 7413.216 |
Transformer | epoch 0 | step 14670 |avg loss 8.656 |avg tokens 4367.500 |tokens/s 9018.919 |walltime 7418.059 |
Transformer | epoch 0 | step 14680 |avg loss 8.494 |avg tokens 4331.600 |tokens/s 8670.032 |walltime 7423.055 |
Transformer | epoch 0 | step 14690 |avg loss 8.529 |avg tokens 4393.500 |tokens/s 8764.875 |walltime 7428.067 |
Transformer | epoch 0 | step 14700 |avg loss 8.526 |avg tokens 4359.100 |tokens/s 8907.493 |walltime 7432.961 |
Transformer | epoch 0 | step 14710 |avg loss 8.597 |avg tokens 4589.700 |tokens/s 9119.046 |walltime 7437.994 |
Transformer | epoch 0 | step 14720 |avg loss 8.644 |avg tokens 4428.000 |tokens/s 8649.318 |walltime 7443.114 |
Transformer | epoch 0 | step 14730 |avg loss 8.564 |avg tokens 4517.700 |tokens/s 9035.982 |walltime 7448.113 |
Transformer | epoch 0 | step 14740 |avg loss 8.577 |avg tokens 4786.500 |tokens/s 9294.647 |walltime 7453.263 |
Transformer | epoch 0 | step 14750 |avg loss 8.550 |avg tokens 4495.200 |tokens/s 9085.866 |walltime 7458.210 |
Transformer | epoch 0 | step 14760 |avg loss 8.370 |avg tokens 4681.700 |tokens/s 9431.509 |walltime 7463.174 |
Transformer | epoch 0 | step 14770 |avg loss 8.582 |avg tokens 4681.000 |tokens/s 9303.017 |walltime 7468.206 |
Transformer | epoch 0 | step 14780 |avg loss 8.731 |avg tokens 4117.200 |tokens/s 8649.386 |walltime 7472.966 |
Transformer | epoch 0 | step 14790 |avg loss 8.679 |avg tokens 3985.400 |tokens/s 8298.704 |walltime 7477.769 |
Transformer | epoch 0 | step 14800 |avg loss 8.547 |avg tokens 4890.900 |tokens/s 9624.430 |walltime 7482.850 |
Transformer | epoch 0 | step 14810 |avg loss 8.506 |avg tokens 4523.900 |tokens/s 9033.066 |walltime 7487.859 |
Transformer | epoch 0 | step 14820 |avg loss 8.414 |avg tokens 4404.000 |tokens/s 8648.667 |walltime 7492.951 |
Transformer | epoch 0 | step 14830 |avg loss 8.520 |avg tokens 4694.800 |tokens/s 9186.132 |walltime 7498.061 |
Transformer | epoch 0 | step 14840 |avg loss 8.266 |avg tokens 4570.000 |tokens/s 9110.895 |walltime 7503.077 |
Transformer | epoch 0 | step 14850 |avg loss 8.440 |avg tokens 4628.800 |tokens/s 8740.572 |walltime 7508.373 |
Transformer | epoch 0 | step 14860 |avg loss 8.268 |avg tokens 4560.000 |tokens/s 8877.613 |walltime 7513.510 |
Transformer | epoch 0 | step 14870 |avg loss 8.405 |avg tokens 4656.000 |tokens/s 8970.753 |walltime 7518.700 |
Transformer | epoch 0 | step 14880 |avg loss 8.300 |avg tokens 4490.600 |tokens/s 8768.885 |walltime 7523.821 |
Transformer | epoch 0 | step 14890 |avg loss 8.790 |avg tokens 4137.700 |tokens/s 8907.553 |walltime 7528.466 |
Transformer | epoch 0 | step 14900 |avg loss 8.478 |avg tokens 4939.200 |tokens/s 9690.586 |walltime 7533.563 |
Transformer | epoch 0 | step 14910 |avg loss 8.318 |avg tokens 4665.100 |tokens/s 8910.226 |walltime 7538.799 |
Transformer | epoch 0 | step 14920 |avg loss 8.587 |avg tokens 4317.100 |tokens/s 8556.876 |walltime 7543.844 |
Transformer | epoch 0 | step 14930 |avg loss 8.511 |avg tokens 4750.600 |tokens/s 9240.165 |walltime 7548.985 |
Transformer | epoch 0 | step 14940 |avg loss 8.438 |avg tokens 4550.100 |tokens/s 8824.425 |walltime 7554.141 |
Transformer | epoch 0 | step 14950 |avg loss 8.694 |avg tokens 4455.500 |tokens/s 9000.051 |walltime 7559.092 |
Transformer | epoch 0 | step 14960 |avg loss 8.817 |avg tokens 4373.200 |tokens/s 9190.660 |walltime 7563.850 |
Transformer | epoch 0 | step 14970 |avg loss 8.440 |avg tokens 4863.000 |tokens/s 9217.113 |walltime 7569.126 |
Transformer | epoch 0 | step 14980 |avg loss 8.477 |avg tokens 4631.300 |tokens/s 9033.245 |walltime 7574.253 |
Transformer | epoch 0 | step 14990 |avg loss 8.227 |avg tokens 4847.200 |tokens/s 9124.073 |walltime 7579.566 |
Transformer | epoch 0 | step 15000 |avg loss 8.458 |avg tokens 4246.200 |tokens/s 8618.731 |walltime 7584.492 |
Transformer | epoch 0 | step 15010 |avg loss 8.632 |avg tokens 4037.700 |tokens/s 8451.289 |walltime 7589.270 |
Transformer | epoch 0 | step 15020 |avg loss 8.313 |avg tokens 4879.100 |tokens/s 9189.515 |walltime 7594.580 |
Transformer | epoch 0 | step 15030 |avg loss 8.601 |avg tokens 4334.900 |tokens/s 8852.273 |walltime 7599.476 |
Transformer | epoch 0 | step 15040 |avg loss 8.251 |avg tokens 4773.600 |tokens/s 9406.869 |walltime 7604.551 |
Transformer | epoch 0 | step 15050 |avg loss 8.620 |avg tokens 4724.100 |tokens/s 9368.957 |walltime 7609.593 |
Transformer | epoch 0 | step 15060 |avg loss 8.432 |avg tokens 4574.700 |tokens/s 8995.688 |walltime 7614.679 |
Transformer | epoch 0 | step 15070 |avg loss 8.431 |avg tokens 3989.400 |tokens/s 8322.916 |walltime 7619.472 |
Transformer | epoch 0 | step 15080 |avg loss 8.379 |avg tokens 4440.500 |tokens/s 9254.596 |walltime 7624.270 |
Transformer | epoch 0 | step 15090 |avg loss 8.442 |avg tokens 4797.300 |tokens/s 9270.487 |walltime 7629.445 |
Transformer | epoch 0 | step 15100 |avg loss 8.418 |avg tokens 4801.600 |tokens/s 9301.072 |walltime 7634.607 |
Transformer | epoch 0 | step 15110 |avg loss 8.757 |avg tokens 4407.300 |tokens/s 9083.190 |walltime 7639.460 |
Transformer | epoch 0 | step 15120 |avg loss 8.497 |avg tokens 4652.000 |tokens/s 9115.239 |walltime 7644.563 |
Transformer | epoch 0 | step 15130 |avg loss 8.208 |avg tokens 4776.000 |tokens/s 9099.529 |walltime 7649.812 |
Transformer | epoch 0 | step 15140 |avg loss 8.603 |avg tokens 4246.700 |tokens/s 8870.924 |walltime 7654.599 |
Transformer | epoch 0 | step 15150 |avg loss 8.389 |avg tokens 4407.600 |tokens/s 8506.657 |walltime 7659.780 |
Transformer | epoch 0 | step 15160 |avg loss 8.450 |avg tokens 4630.400 |tokens/s 8927.106 |walltime 7664.967 |
Transformer | epoch 0 | step 15170 |avg loss 8.589 |avg tokens 4393.200 |tokens/s 8803.160 |walltime 7669.958 |
Transformer | epoch 0 | step 15180 |avg loss 8.548 |avg tokens 4612.300 |tokens/s 9024.008 |walltime 7675.069 |
Transformer | epoch 0 | step 15190 |avg loss 8.675 |avg tokens 4737.500 |tokens/s 9663.084 |walltime 7679.972 |
Transformer | epoch 0 | step 15200 |avg loss 8.396 |avg tokens 4569.800 |tokens/s 8853.705 |walltime 7685.133 |
Transformer | epoch 0 | step 15210 |avg loss 8.416 |avg tokens 4984.000 |tokens/s 9556.697 |walltime 7690.348 |
Transformer | epoch 0 | step 15220 |avg loss 8.503 |avg tokens 4530.600 |tokens/s 9042.167 |walltime 7695.359 |
Transformer | epoch 0 | step 15230 |avg loss 8.705 |avg tokens 2894.900 |tokens/s 6885.859 |walltime 7699.563 |
Transformer | epoch 0 | step 15240 |avg loss 8.358 |avg tokens 4064.900 |tokens/s 8310.179 |walltime 7704.454 |
Transformer | epoch 0 | step 15250 |avg loss 8.547 |avg tokens 4310.700 |tokens/s 8729.180 |walltime 7709.393 |
Transformer | epoch 0 | step 15260 |avg loss 8.478 |avg tokens 4663.200 |tokens/s 8913.055 |walltime 7714.624 |
Transformer | epoch 0 | step 15270 |avg loss 8.583 |avg tokens 4582.700 |tokens/s 9152.169 |walltime 7719.632 |
Transformer | epoch 0 | step 15280 |avg loss 8.552 |avg tokens 4307.400 |tokens/s 9105.775 |walltime 7724.362 |
Transformer | epoch 0 | step 15290 |avg loss 8.503 |avg tokens 4550.000 |tokens/s 8710.227 |walltime 7729.586 |
Transformer | epoch 0 | step 15300 |avg loss 8.857 |avg tokens 4632.100 |tokens/s 9508.596 |walltime 7734.457 |
Transformer | epoch 0 | step 15310 |avg loss 8.368 |avg tokens 4852.000 |tokens/s 9244.960 |walltime 7739.706 |
Transformer | epoch 0 | step 15320 |avg loss 8.482 |avg tokens 4638.600 |tokens/s 8989.093 |walltime 7744.866 |
Transformer | epoch 0 | step 15330 |avg loss 8.574 |avg tokens 4807.000 |tokens/s 9377.433 |walltime 7749.992 |
Transformer | epoch 0 | step 15340 |avg loss 8.465 |avg tokens 4563.700 |tokens/s 9014.345 |walltime 7755.055 |
Transformer | epoch 0 | step 15350 |avg loss 8.376 |avg tokens 4398.600 |tokens/s 8687.000 |walltime 7760.118 |
Transformer | epoch 0 | step 15360 |avg loss 8.570 |avg tokens 4867.500 |tokens/s 9585.038 |walltime 7765.196 |
Transformer | epoch 0 | step 15370 |avg loss 8.544 |avg tokens 4246.800 |tokens/s 8440.763 |walltime 7770.228 |
Transformer | epoch 0 | step 15380 |avg loss 8.553 |avg tokens 4377.300 |tokens/s 8797.282 |walltime 7775.203 |
Transformer | epoch 0 | step 15390 |avg loss 8.592 |avg tokens 4378.600 |tokens/s 8698.456 |walltime 7780.237 |
Transformer | epoch 0 | step 15400 |avg loss 8.584 |avg tokens 3998.200 |tokens/s 8343.075 |walltime 7785.029 |
Transformer | epoch 0 | step 15410 |avg loss 8.683 |avg tokens 4322.300 |tokens/s 8668.290 |walltime 7790.016 |
Transformer | epoch 0 | step 15420 |avg loss 8.448 |avg tokens 4712.000 |tokens/s 9184.527 |walltime 7795.146 |
Transformer | epoch 0 | step 15430 |avg loss 8.547 |avg tokens 4665.900 |tokens/s 9349.141 |walltime 7800.137 |
Transformer | epoch 0 | step 15440 |avg loss 8.490 |avg tokens 4556.600 |tokens/s 9046.235 |walltime 7805.174 |
Transformer | epoch 0 | step 15450 |avg loss 8.580 |avg tokens 4639.000 |tokens/s 9189.364 |walltime 7810.222 |
Transformer | epoch 0 | step 15460 |avg loss 8.327 |avg tokens 4563.700 |tokens/s 8764.997 |walltime 7815.429 |
Transformer | epoch 0 | step 15470 |avg loss 8.344 |avg tokens 4744.000 |tokens/s 9193.089 |walltime 7820.589 |
Transformer | epoch 0 | step 15480 |avg loss 8.599 |avg tokens 4801.300 |tokens/s 9431.019 |walltime 7825.680 |
Transformer | epoch 0 | step 15490 |avg loss 8.674 |avg tokens 4111.100 |tokens/s 8493.057 |walltime 7830.521 |
Transformer | epoch 0 | step 15500 |avg loss 8.476 |avg tokens 4413.800 |tokens/s 8856.888 |walltime 7835.504 |
Transformer | epoch 0 | step 15510 |avg loss 8.638 |avg tokens 3881.100 |tokens/s 8277.252 |walltime 7840.193 |
Transformer | epoch 0 | step 15520 |avg loss 8.649 |avg tokens 4735.100 |tokens/s 9447.109 |walltime 7845.205 |
Transformer | epoch 0 | step 15530 |avg loss 8.600 |avg tokens 4067.900 |tokens/s 8411.422 |walltime 7850.042 |
Transformer | epoch 0 | step 15540 |avg loss 8.478 |avg tokens 4897.600 |tokens/s 9374.993 |walltime 7855.266 |
Transformer | epoch 0 | step 15550 |avg loss 8.628 |avg tokens 4574.800 |tokens/s 9327.769 |walltime 7860.170 |
Transformer | epoch 0 | step 15560 |avg loss 8.533 |avg tokens 4689.800 |tokens/s 9017.511 |walltime 7865.371 |
Transformer | epoch 0 | step 15570 |avg loss 8.688 |avg tokens 4498.700 |tokens/s 8986.883 |walltime 7870.377 |
Transformer | epoch 0 | step 15580 |avg loss 8.619 |avg tokens 3921.700 |tokens/s 8124.414 |walltime 7875.204 |
Transformer | epoch 0 | step 15590 |avg loss 8.964 |avg tokens 4612.500 |tokens/s 9603.740 |walltime 7880.007 |
Transformer | epoch 0 | step 15600 |avg loss 8.576 |avg tokens 4582.800 |tokens/s 9086.673 |walltime 7885.050 |
Transformer | epoch 0 | step 15610 |avg loss 8.401 |avg tokens 4835.800 |tokens/s 9177.637 |walltime 7890.319 |
Transformer | epoch 0 | step 15620 |avg loss 8.312 |avg tokens 4486.600 |tokens/s 8852.102 |walltime 7895.388 |
Transformer | epoch 0 | step 15630 |avg loss 8.490 |avg tokens 4562.400 |tokens/s 8844.134 |walltime 7900.546 |
Transformer | epoch 0 | step 15640 |avg loss 8.568 |avg tokens 4657.400 |tokens/s 9112.476 |walltime 7905.657 |
Transformer | epoch 0 | step 15650 |avg loss 8.603 |avg tokens 4610.000 |tokens/s 8802.666 |walltime 7910.894 |
Transformer | epoch 0 | step 15660 |avg loss 8.470 |avg tokens 4366.600 |tokens/s 8693.622 |walltime 7915.917 |
Transformer | epoch 0 | step 15670 |avg loss 8.637 |avg tokens 4660.600 |tokens/s 9385.022 |walltime 7920.883 |
Transformer | epoch 0 | step 15680 |avg loss 8.415 |avg tokens 4645.300 |tokens/s 8770.207 |walltime 7926.180 |
Transformer | epoch 0 | step 15690 |avg loss 8.530 |avg tokens 4223.300 |tokens/s 8565.678 |walltime 7931.110 |
Transformer | epoch 0 | step 15700 |avg loss 8.500 |avg tokens 4584.600 |tokens/s 9479.715 |walltime 7935.946 |
Transformer | epoch 0 | step 15710 |avg loss 8.665 |avg tokens 4802.200 |tokens/s 9161.172 |walltime 7941.188 |
Transformer | epoch 0 | step 15720 |avg loss 8.495 |avg tokens 4883.300 |tokens/s 9199.452 |walltime 7946.497 |
Transformer | epoch 0 | step 15730 |avg loss 8.691 |avg tokens 4395.300 |tokens/s 8995.800 |walltime 7951.383 |
Transformer | epoch 0 | step 15740 |avg loss 8.231 |avg tokens 4818.600 |tokens/s 9194.449 |walltime 7956.623 |
Transformer | epoch 0 | step 15750 |avg loss 8.629 |avg tokens 4760.500 |tokens/s 9303.598 |walltime 7961.740 |
Transformer | epoch 0 | step 15760 |avg loss 8.677 |avg tokens 4121.400 |tokens/s 8396.976 |walltime 7966.648 |
Transformer | epoch 0 | step 15770 |avg loss 8.596 |avg tokens 4797.100 |tokens/s 9507.543 |walltime 7971.694 |
Transformer | epoch 0 | step 15780 |avg loss 8.353 |avg tokens 4738.600 |tokens/s 8994.225 |walltime 7976.962 |
Transformer | epoch 0 | step 15790 |avg loss 8.732 |avg tokens 4289.300 |tokens/s 8889.704 |walltime 7981.788 |
Transformer | epoch 0 | step 15800 |avg loss 8.808 |avg tokens 4395.200 |tokens/s 9278.066 |walltime 7986.525 |
Transformer | epoch 0 | step 15810 |avg loss 8.558 |avg tokens 4749.900 |tokens/s 9324.639 |walltime 7991.619 |
Transformer | epoch 0 | step 15820 |avg loss 8.614 |avg tokens 4663.400 |tokens/s 9343.760 |walltime 7996.610 |
Transformer | epoch 0 | step 15830 |avg loss 8.245 |avg tokens 4840.900 |tokens/s 9291.772 |walltime 8001.819 |
Transformer | epoch 0 | step 15840 |avg loss 8.511 |avg tokens 4474.100 |tokens/s 8868.456 |walltime 8006.864 |
Transformer | epoch 0 | step 15850 |avg loss 8.678 |avg tokens 4702.900 |tokens/s 9086.959 |walltime 8012.040 |
Transformer | epoch 0 | step 15860 |avg loss 8.877 |avg tokens 4082.700 |tokens/s 8784.629 |walltime 8016.687 |
Transformer | epoch 0 | step 15870 |avg loss 8.118 |avg tokens 4550.800 |tokens/s 8953.469 |walltime 8021.770 |
Transformer | epoch 0 | step 15880 |avg loss 8.557 |avg tokens 4373.000 |tokens/s 8738.092 |walltime 8026.775 |
Transformer | epoch 0 | step 15890 |avg loss 8.491 |avg tokens 4464.200 |tokens/s 8749.604 |walltime 8031.877 |
Transformer | epoch 0 | step 15900 |avg loss 8.279 |avg tokens 4450.100 |tokens/s 8707.749 |walltime 8036.987 |
Transformer | epoch 0 | step 15910 |avg loss 8.777 |avg tokens 4624.800 |tokens/s 9552.478 |walltime 8041.829 |
Transformer | epoch 0 | step 15920 |avg loss 8.611 |avg tokens 4876.000 |tokens/s 9342.091 |walltime 8047.048 |
Transformer | epoch 0 | step 15930 |avg loss 8.705 |avg tokens 4642.300 |tokens/s 9264.939 |walltime 8052.059 |
Transformer | epoch 0 | step 15940 |avg loss 8.729 |avg tokens 4797.600 |tokens/s 9725.093 |walltime 8056.992 |
Transformer | epoch 0 | step 15950 |avg loss 8.535 |avg tokens 4310.000 |tokens/s 8713.644 |walltime 8061.938 |
Transformer | epoch 0 | step 15960 |avg loss 8.339 |avg tokens 4732.800 |tokens/s 9243.927 |walltime 8067.058 |
Transformer | epoch 0 | step 15970 |avg loss 8.352 |avg tokens 4423.900 |tokens/s 8755.894 |walltime 8072.111 |
Transformer | epoch 0 | step 15980 |avg loss 8.709 |avg tokens 4613.000 |tokens/s 9512.316 |walltime 8076.960 |
Transformer | epoch 0 | step 15990 |avg loss 8.335 |avg tokens 4711.000 |tokens/s 9309.553 |walltime 8082.021 |
Transformer | epoch 0 | step 16000 |avg loss 8.263 |avg tokens 4673.900 |tokens/s 9161.850 |walltime 8087.122 |
Transformer | epoch 0 | step 16010 |avg loss 8.486 |avg tokens 4759.500 |tokens/s 8964.248 |walltime 8092.432 |
Transformer | epoch 0 | step 16020 |avg loss 8.256 |avg tokens 4666.200 |tokens/s 9006.981 |walltime 8097.612 |
Transformer | epoch 0 | step 16030 |avg loss 8.476 |avg tokens 4540.300 |tokens/s 9124.185 |walltime 8102.588 |
Transformer | epoch 0 | step 16040 |avg loss 8.521 |avg tokens 4872.700 |tokens/s 9449.831 |walltime 8107.745 |
Transformer | epoch 0 | step 16050 |avg loss 8.527 |avg tokens 4461.900 |tokens/s 9024.598 |walltime 8112.689 |
Transformer | epoch 0 | step 16060 |avg loss 8.643 |avg tokens 4407.700 |tokens/s 8991.836 |walltime 8117.591 |
Transformer | epoch 0 | step 16070 |avg loss 8.623 |avg tokens 3808.900 |tokens/s 8243.585 |walltime 8122.211 |
Transformer | epoch 0 | step 16080 |avg loss 9.040 |avg tokens 4129.400 |tokens/s 9063.503 |walltime 8126.767 |
Transformer | epoch 0 | step 16090 |avg loss 8.502 |avg tokens 4354.900 |tokens/s 8721.184 |walltime 8131.761 |
Transformer | epoch 0 | step 16100 |avg loss 8.433 |avg tokens 4537.700 |tokens/s 8780.049 |walltime 8136.929 |
Transformer | epoch 0 | step 16110 |avg loss 8.518 |avg tokens 4651.600 |tokens/s 9140.252 |walltime 8142.018 |
Transformer | epoch 0 | step 16120 |avg loss 8.418 |avg tokens 4422.300 |tokens/s 8952.836 |walltime 8146.958 |
Transformer | epoch 0 | step 16130 |avg loss 8.350 |avg tokens 4170.100 |tokens/s 8394.627 |walltime 8151.925 |
Transformer | epoch 0 | step 16140 |avg loss 8.469 |avg tokens 4750.900 |tokens/s 9554.637 |walltime 8156.898 |
Transformer | epoch 0 | step 16150 |avg loss 8.480 |avg tokens 4890.200 |tokens/s 9802.155 |walltime 8161.886 |
Transformer | epoch 0 | step 16160 |avg loss 8.444 |avg tokens 4200.400 |tokens/s 8412.159 |walltime 8166.880 |
Transformer | epoch 0 | step 16170 |avg loss 8.370 |avg tokens 4660.000 |tokens/s 8951.422 |walltime 8172.086 |
Transformer | epoch 0 | step 16180 |avg loss 8.499 |avg tokens 4450.900 |tokens/s 8847.251 |walltime 8177.116 |
Transformer | epoch 0 | step 16190 |avg loss 8.546 |avg tokens 4723.500 |tokens/s 9267.800 |walltime 8182.213 |
Transformer | epoch 0 | step 16200 |avg loss 8.424 |avg tokens 4472.200 |tokens/s 8859.723 |walltime 8187.261 |
Transformer | epoch 0 | step 16210 |avg loss 8.586 |avg tokens 4315.600 |tokens/s 8811.088 |walltime 8192.159 |
Transformer | epoch 0 | step 16220 |avg loss 8.626 |avg tokens 4254.800 |tokens/s 8808.575 |walltime 8196.989 |
Transformer | epoch 0 | step 16230 |avg loss 8.680 |avg tokens 3805.300 |tokens/s 8325.538 |walltime 8201.560 |
Transformer | epoch 0 | step 16240 |avg loss 8.720 |avg tokens 4480.900 |tokens/s 9070.426 |walltime 8206.500 |
Transformer | epoch 0 | step 16250 |avg loss 8.670 |avg tokens 4242.400 |tokens/s 8670.187 |walltime 8211.393 |
Transformer | epoch 0 | step 16260 |avg loss 8.413 |avg tokens 4310.800 |tokens/s 8587.272 |walltime 8216.413 |
Transformer | epoch 0 | step 16270 |avg loss 8.501 |avg tokens 4273.600 |tokens/s 8608.954 |walltime 8221.377 |
Transformer | epoch 0 | step 16280 |avg loss 8.676 |avg tokens 4316.000 |tokens/s 8867.287 |walltime 8226.244 |
Transformer | epoch 0 | step 16290 |avg loss 8.906 |avg tokens 4269.100 |tokens/s 9180.866 |walltime 8230.894 |
Transformer | epoch 0 | step 16300 |avg loss 8.637 |avg tokens 4833.700 |tokens/s 9476.470 |walltime 8235.995 |
Transformer | epoch 0 | step 16310 |avg loss 8.437 |avg tokens 4650.900 |tokens/s 8966.897 |walltime 8241.182 |
Transformer | epoch 0 | step 16320 |avg loss 8.700 |avg tokens 3866.000 |tokens/s 8450.490 |walltime 8245.757 |
Transformer | epoch 0 | step 16330 |avg loss 8.582 |avg tokens 4466.600 |tokens/s 9013.790 |walltime 8250.712 |
Transformer | epoch 0 | step 16340 |avg loss 8.378 |avg tokens 4556.400 |tokens/s 8694.364 |walltime 8255.953 |
Transformer | epoch 0 | step 16350 |avg loss 8.428 |avg tokens 4370.500 |tokens/s 8554.843 |walltime 8261.062 |
Transformer | epoch 0 | step 16360 |avg loss 8.550 |avg tokens 4304.600 |tokens/s 8947.911 |walltime 8265.872 |
Transformer | epoch 0 | step 16370 |avg loss 8.624 |avg tokens 4518.600 |tokens/s 9106.357 |walltime 8270.834 |
Transformer | epoch 0 | step 16380 |avg loss 8.326 |avg tokens 4908.800 |tokens/s 9483.051 |walltime 8276.011 |
Transformer | epoch 0 | step 16390 |avg loss 8.394 |avg tokens 4445.600 |tokens/s 8694.833 |walltime 8281.124 |
Transformer | epoch 0 | step 16400 |avg loss 8.444 |avg tokens 4639.300 |tokens/s 8954.943 |walltime 8286.304 |
Transformer | epoch 0 | step 16410 |avg loss 8.229 |avg tokens 4775.200 |tokens/s 9081.397 |walltime 8291.563 |
Transformer | epoch 0 | step 16420 |avg loss 8.240 |avg tokens 4597.000 |tokens/s 8944.876 |walltime 8296.702 |
Transformer | epoch 0 | step 16430 |avg loss 8.640 |avg tokens 4864.800 |tokens/s 9603.647 |walltime 8301.767 |
Transformer | epoch 0 | step 16440 |avg loss 8.548 |avg tokens 4195.900 |tokens/s 8344.979 |walltime 8306.795 |
Transformer | epoch 0 | step 16450 |avg loss 8.693 |avg tokens 4718.200 |tokens/s 9492.665 |walltime 8311.766 |
Transformer | epoch 0 | step 16460 |avg loss 8.553 |avg tokens 4366.400 |tokens/s 8895.744 |walltime 8316.674 |
Transformer | epoch 0 | step 16470 |avg loss 8.193 |avg tokens 4718.100 |tokens/s 9059.811 |walltime 8321.882 |
Transformer | epoch 0 | step 16480 |avg loss 8.142 |avg tokens 4673.100 |tokens/s 8922.076 |walltime 8327.120 |
Transformer | epoch 0 | step 16490 |avg loss 8.252 |avg tokens 4537.200 |tokens/s 8792.144 |walltime 8332.280 |
Transformer | epoch 0 | step 16500 |avg loss 8.561 |avg tokens 4680.800 |tokens/s 9249.870 |walltime 8337.341 |
Transformer | epoch 0 | step 16510 |avg loss 8.551 |avg tokens 4696.800 |tokens/s 9335.957 |walltime 8342.371 |
Transformer | epoch 0 | step 16520 |avg loss 8.361 |avg tokens 4785.600 |tokens/s 9085.184 |walltime 8347.639 |
Transformer | epoch 0 | step 16530 |avg loss 8.619 |avg tokens 4488.900 |tokens/s 8822.136 |walltime 8352.727 |
Transformer | epoch 0 | step 16540 |avg loss 8.409 |avg tokens 4619.300 |tokens/s 8948.812 |walltime 8357.889 |
Transformer | epoch 0 | step 16550 |avg loss 8.406 |avg tokens 4830.800 |tokens/s 9307.892 |walltime 8363.079 |
Transformer | epoch 0 | step 16560 |avg loss 8.462 |avg tokens 4612.500 |tokens/s 9191.677 |walltime 8368.097 |
Transformer | epoch 0 | step 16570 |avg loss 8.429 |avg tokens 4845.400 |tokens/s 9204.591 |walltime 8373.361 |
Transformer | epoch 0 | step 16580 |avg loss 8.392 |avg tokens 4567.000 |tokens/s 8733.385 |walltime 8378.591 |
Transformer | epoch 0 | step 16590 |avg loss 8.565 |avg tokens 4651.600 |tokens/s 9113.173 |walltime 8383.695 |
Transformer | epoch 0 | step 16600 |avg loss 8.428 |avg tokens 4526.400 |tokens/s 9067.349 |walltime 8388.687 |
Transformer | epoch 0 | step 16610 |avg loss 8.173 |avg tokens 4751.600 |tokens/s 9122.814 |walltime 8393.895 |
Transformer | epoch 0 | step 16620 |avg loss 8.560 |avg tokens 4612.100 |tokens/s 9287.531 |walltime 8398.861 |
Transformer | epoch 0 | step 16630 |avg loss 8.292 |avg tokens 4970.400 |tokens/s 9255.736 |walltime 8404.231 |
Transformer | epoch 0 | step 16640 |avg loss 8.526 |avg tokens 4002.100 |tokens/s 8261.443 |walltime 8409.076 |
Transformer | epoch 0 | step 16650 |avg loss 8.571 |avg tokens 4492.500 |tokens/s 8916.794 |walltime 8414.114 |
Transformer | epoch 0 | step 16660 |avg loss 8.118 |avg tokens 4399.900 |tokens/s 8760.292 |walltime 8419.137 |
Transformer | epoch 0 | step 16670 |avg loss 8.524 |avg tokens 4737.400 |tokens/s 9286.412 |walltime 8424.238 |
Transformer | epoch 0 | step 16680 |avg loss 8.389 |avg tokens 4963.300 |tokens/s 9297.062 |walltime 8429.577 |
Transformer | epoch 0 | step 16690 |avg loss 8.479 |avg tokens 4555.200 |tokens/s 8825.659 |walltime 8434.738 |
Transformer | epoch 0 | step 16700 |avg loss 8.426 |avg tokens 4387.500 |tokens/s 8779.175 |walltime 8439.735 |
Transformer | epoch 0 | step 16710 |avg loss 8.585 |avg tokens 4340.400 |tokens/s 8789.367 |walltime 8444.674 |
Transformer | epoch 0 | step 16720 |avg loss 8.362 |avg tokens 4762.000 |tokens/s 9116.990 |walltime 8449.897 |
Transformer | epoch 0 | step 16730 |avg loss 8.311 |avg tokens 4488.700 |tokens/s 8867.923 |walltime 8454.959 |
Transformer | epoch 0 | step 16740 |avg loss 8.134 |avg tokens 4752.800 |tokens/s 9166.424 |walltime 8460.144 |
Transformer | epoch 0 | step 16750 |avg loss 8.371 |avg tokens 4768.000 |tokens/s 9065.388 |walltime 8465.403 |
Transformer | epoch 0 | step 16760 |avg loss 8.478 |avg tokens 4500.000 |tokens/s 8746.565 |walltime 8470.548 |
Transformer | epoch 0 | step 16770 |avg loss 8.492 |avg tokens 4295.300 |tokens/s 8565.995 |walltime 8475.562 |
Transformer | epoch 0 | step 16780 |avg loss 8.767 |avg tokens 4659.200 |tokens/s 9581.350 |walltime 8480.425 |
Transformer | epoch 0 | step 16790 |avg loss 8.155 |avg tokens 4591.300 |tokens/s 8795.946 |walltime 8485.645 |
Transformer | epoch 0 | step 16800 |avg loss 8.524 |avg tokens 4405.900 |tokens/s 8736.689 |walltime 8490.688 |
Transformer | epoch 0 | step 16810 |avg loss 8.576 |avg tokens 4789.300 |tokens/s 9278.524 |walltime 8495.850 |
Transformer | epoch 0 | step 16820 |avg loss 8.416 |avg tokens 4324.100 |tokens/s 8731.842 |walltime 8500.802 |
Transformer | epoch 0 | step 16830 |avg loss 8.401 |avg tokens 4842.500 |tokens/s 9488.727 |walltime 8505.905 |
Transformer | epoch 0 | step 16840 |avg loss 8.521 |avg tokens 4958.500 |tokens/s 9457.794 |walltime 8511.148 |
Transformer | epoch 0 | step 16850 |avg loss 8.582 |avg tokens 4447.700 |tokens/s 8954.274 |walltime 8516.115 |
Transformer | epoch 0 | step 16860 |avg loss 8.720 |avg tokens 4481.300 |tokens/s 9149.614 |walltime 8521.013 |
Transformer | epoch 0 | step 16870 |avg loss 8.588 |avg tokens 4015.500 |tokens/s 8406.836 |walltime 8525.789 |
Transformer | epoch 0 | step 16880 |avg loss 8.553 |avg tokens 4505.700 |tokens/s 9033.294 |walltime 8530.777 |
Transformer | epoch 0 | step 16890 |avg loss 8.514 |avg tokens 4629.400 |tokens/s 9169.368 |walltime 8535.826 |
Transformer | epoch 0 | step 16900 |avg loss 8.332 |avg tokens 4918.400 |tokens/s 9246.182 |walltime 8541.146 |
Transformer | epoch 0 | step 16910 |avg loss 8.502 |avg tokens 4943.900 |tokens/s 9490.874 |walltime 8546.355 |
Transformer | epoch 0 | step 16920 |avg loss 8.537 |avg tokens 4720.800 |tokens/s 9232.506 |walltime 8551.468 |
Transformer | epoch 0 | step 16930 |avg loss 8.463 |avg tokens 4456.500 |tokens/s 9113.208 |walltime 8556.358 |
Transformer | epoch 0 | step 16940 |avg loss 8.515 |avg tokens 4448.600 |tokens/s 8831.461 |walltime 8561.395 |
Transformer | epoch 0 | step 16950 |avg loss 8.413 |avg tokens 4624.000 |tokens/s 8733.269 |walltime 8566.690 |
Transformer | epoch 0 | step 16960 |avg loss 8.528 |avg tokens 4623.900 |tokens/s 9066.370 |walltime 8571.790 |
Transformer | epoch 0 | step 16970 |avg loss 8.298 |avg tokens 4905.200 |tokens/s 9227.254 |walltime 8577.106 |
Transformer | epoch 0 | step 16980 |avg loss 8.490 |avg tokens 4673.200 |tokens/s 9233.061 |walltime 8582.167 |
Transformer | epoch 0 | step 16990 |avg loss 8.621 |avg tokens 4270.500 |tokens/s 8724.482 |walltime 8587.062 |
Transformer | epoch 0 | step 17000 |avg loss 8.564 |avg tokens 4662.300 |tokens/s 9083.326 |walltime 8592.195 |
Transformer | epoch 0 | step 17010 |avg loss 8.620 |avg tokens 4017.600 |tokens/s 8509.733 |walltime 8596.916 |
Transformer | epoch 0 | step 17020 |avg loss 8.519 |avg tokens 3739.400 |tokens/s 7884.059 |walltime 8601.659 |
Transformer | epoch 0 | step 17030 |avg loss 8.433 |avg tokens 4865.600 |tokens/s 9313.665 |walltime 8606.883 |
Transformer | epoch 0 | step 17040 |avg loss 8.378 |avg tokens 4206.800 |tokens/s 8500.376 |walltime 8611.832 |
Transformer | epoch 0 | step 17050 |avg loss 8.520 |avg tokens 4438.000 |tokens/s 8763.377 |walltime 8616.897 |
Transformer | epoch 0 | step 17060 |avg loss 8.524 |avg tokens 4522.200 |tokens/s 8847.152 |walltime 8622.008 |
Transformer | epoch 0 | step 17070 |avg loss 8.306 |avg tokens 4797.600 |tokens/s 9003.241 |walltime 8627.337 |
Transformer | epoch 0 | step 17080 |avg loss 8.892 |avg tokens 4540.600 |tokens/s 9058.885 |walltime 8632.349 |
Transformer | epoch 0 | step 17090 |avg loss 8.944 |avg tokens 4537.800 |tokens/s 9010.132 |walltime 8637.385 |
Transformer | epoch 0 | step 17100 |avg loss 8.616 |avg tokens 4453.000 |tokens/s 9045.810 |walltime 8642.308 |
Transformer | epoch 0 | step 17110 |avg loss 8.223 |avg tokens 4908.800 |tokens/s 9240.062 |walltime 8647.621 |
Transformer | epoch 0 | step 17120 |avg loss 8.369 |avg tokens 4516.600 |tokens/s 8746.877 |walltime 8652.784 |
Transformer | epoch 0 | step 17130 |avg loss 8.328 |avg tokens 4962.900 |tokens/s 9433.569 |walltime 8658.045 |
Transformer | epoch 0 | step 17140 |avg loss 8.556 |avg tokens 4374.200 |tokens/s 8807.975 |walltime 8663.011 |
Transformer | epoch 0 | step 17150 |avg loss 8.471 |avg tokens 4632.300 |tokens/s 9103.838 |walltime 8668.100 |
Transformer | epoch 0 | step 17160 |avg loss 8.448 |avg tokens 4482.400 |tokens/s 8800.671 |walltime 8673.193 |
Transformer | epoch 0 | step 17170 |avg loss 8.348 |avg tokens 4348.100 |tokens/s 8761.026 |walltime 8678.156 |
Transformer | epoch 0 | step 17180 |avg loss 8.633 |avg tokens 4226.900 |tokens/s 8540.963 |walltime 8683.105 |
Transformer | epoch 0 | step 17190 |avg loss 8.567 |avg tokens 4324.300 |tokens/s 8702.984 |walltime 8688.074 |
Transformer | epoch 0 | step 17200 |avg loss 8.597 |avg tokens 4332.500 |tokens/s 8711.432 |walltime 8693.047 |
Transformer | epoch 0 | step 17210 |avg loss 8.389 |avg tokens 4211.900 |tokens/s 8461.676 |walltime 8698.025 |
Transformer | epoch 0 | step 17220 |avg loss 8.586 |avg tokens 4671.400 |tokens/s 9277.071 |walltime 8703.060 |
Transformer | epoch 0 | step 17230 |avg loss 8.491 |avg tokens 4521.800 |tokens/s 8674.975 |walltime 8708.273 |
Transformer | epoch 0 | step 17240 |avg loss 8.415 |avg tokens 4597.200 |tokens/s 8968.755 |walltime 8713.398 |
Transformer | epoch 0 | step 17250 |avg loss 8.465 |avg tokens 4402.300 |tokens/s 8928.524 |walltime 8718.329 |
Transformer | epoch 0 | step 17260 |avg loss 8.316 |avg tokens 4385.800 |tokens/s 8672.700 |walltime 8723.386 |
Transformer | epoch 0 | step 17270 |avg loss 8.804 |avg tokens 3697.900 |tokens/s 7969.356 |walltime 8728.026 |
Transformer | epoch 0 | step 17280 |avg loss 8.554 |avg tokens 4600.800 |tokens/s 8802.213 |walltime 8733.253 |
Transformer | epoch 0 | step 17290 |avg loss 8.598 |avg tokens 4552.900 |tokens/s 9030.629 |walltime 8738.295 |
Transformer | epoch 0 | step 17300 |avg loss 8.490 |avg tokens 4789.600 |tokens/s 9213.763 |walltime 8743.493 |
Transformer | epoch 0 | step 17310 |avg loss 8.541 |avg tokens 4754.400 |tokens/s 9343.436 |walltime 8748.582 |
Transformer | epoch 0 | step 17320 |avg loss 8.551 |avg tokens 4726.800 |tokens/s 8981.247 |walltime 8753.844 |
Transformer | epoch 0 | step 17330 |avg loss 8.452 |avg tokens 4271.600 |tokens/s 8668.094 |walltime 8758.772 |
Transformer | epoch 0 | step 17340 |avg loss 8.382 |avg tokens 4947.200 |tokens/s 9438.558 |walltime 8764.014 |
Transformer | epoch 0 | step 17350 |avg loss 8.340 |avg tokens 4815.700 |tokens/s 9477.309 |walltime 8769.095 |
Transformer | epoch 0 | step 17360 |avg loss 8.297 |avg tokens 4241.300 |tokens/s 8419.789 |walltime 8774.133 |
Transformer | epoch 0 | step 17370 |avg loss 8.501 |avg tokens 4282.900 |tokens/s 8644.788 |walltime 8779.087 |
Transformer | epoch 0 | step 17380 |avg loss 8.401 |avg tokens 4740.100 |tokens/s 9164.780 |walltime 8784.259 |
Transformer | epoch 0 | step 17390 |avg loss 8.651 |avg tokens 4176.400 |tokens/s 8571.999 |walltime 8789.131 |
Transformer | epoch 0 | step 17400 |avg loss 8.388 |avg tokens 4649.900 |tokens/s 9159.428 |walltime 8794.208 |
Transformer | epoch 0 | step 17410 |avg loss 8.385 |avg tokens 4653.800 |tokens/s 8756.837 |walltime 8799.522 |
Transformer | epoch 0 | step 17420 |avg loss 8.636 |avg tokens 4075.300 |tokens/s 8612.307 |walltime 8804.254 |
Transformer | epoch 0 | step 17430 |avg loss 8.335 |avg tokens 4454.900 |tokens/s 8859.435 |walltime 8809.283 |
Transformer | epoch 0 | step 17440 |avg loss 8.680 |avg tokens 4315.300 |tokens/s 8807.894 |walltime 8814.182 |
Transformer | epoch 0 | step 17450 |avg loss 8.375 |avg tokens 4706.900 |tokens/s 9276.195 |walltime 8819.256 |
Transformer | epoch 0 | step 17460 |avg loss 8.441 |avg tokens 4658.800 |tokens/s 9387.792 |walltime 8824.219 |
Transformer | epoch 0 | step 17470 |avg loss 8.477 |avg tokens 4249.900 |tokens/s 8559.213 |walltime 8829.184 |
Transformer | epoch 0 | step 17480 |avg loss 8.409 |avg tokens 4737.600 |tokens/s 9039.000 |walltime 8834.425 |
Transformer | epoch 0 | step 17490 |avg loss 8.429 |avg tokens 4232.200 |tokens/s 8587.877 |walltime 8839.353 |
Transformer | epoch 0 | step 17500 |avg loss 8.654 |avg tokens 4627.000 |tokens/s 9334.625 |walltime 8844.310 |
Transformer | epoch 0 | step 17510 |avg loss 8.461 |avg tokens 4358.200 |tokens/s 8844.980 |walltime 8849.238 |
Transformer | epoch 0 | step 17520 |avg loss 8.636 |avg tokens 3584.100 |tokens/s 7766.712 |walltime 8853.852 |
Transformer | epoch 0 | step 17530 |avg loss 8.614 |avg tokens 4105.900 |tokens/s 8273.608 |walltime 8858.815 |
Transformer | epoch 0 | step 17540 |avg loss 8.570 |avg tokens 4188.100 |tokens/s 8609.300 |walltime 8863.680 |
Transformer | epoch 0 | step 17550 |avg loss 8.380 |avg tokens 4782.500 |tokens/s 9009.735 |walltime 8868.988 |
Transformer | epoch 0 | step 17560 |avg loss 8.468 |avg tokens 4695.000 |tokens/s 8976.717 |walltime 8874.218 |
Transformer | epoch 0 | step 17570 |avg loss 8.238 |avg tokens 4802.200 |tokens/s 9314.699 |walltime 8879.373 |
Transformer | epoch 0 | step 17580 |avg loss 8.341 |avg tokens 4208.800 |tokens/s 8524.340 |walltime 8884.311 |
Transformer | epoch 0 | step 17590 |avg loss 8.539 |avg tokens 4755.800 |tokens/s 9160.302 |walltime 8889.503 |
Transformer | epoch 0 | step 17600 |avg loss 8.384 |avg tokens 4314.900 |tokens/s 8779.097 |walltime 8894.417 |
Transformer | epoch 0 | step 17610 |avg loss 8.671 |avg tokens 4435.600 |tokens/s 9037.760 |walltime 8899.325 |
Transformer | epoch 0 | step 17620 |avg loss 8.544 |avg tokens 4471.800 |tokens/s 8948.408 |walltime 8904.323 |
Transformer | epoch 0 | step 17630 |avg loss 8.670 |avg tokens 4402.600 |tokens/s 9066.397 |walltime 8909.179 |
Transformer | epoch 0 | step 17640 |avg loss 8.486 |avg tokens 4146.500 |tokens/s 8401.634 |walltime 8914.114 |
Transformer | epoch 0 | step 17650 |avg loss 8.471 |avg tokens 4537.300 |tokens/s 9094.458 |walltime 8919.103 |
Transformer | epoch 0 | step 17660 |avg loss 8.689 |avg tokens 4470.900 |tokens/s 8892.456 |walltime 8924.131 |
Transformer | epoch 0 | step 17670 |avg loss 8.596 |avg tokens 4317.600 |tokens/s 8928.598 |walltime 8928.967 |
Transformer | epoch 0 | step 17680 |avg loss 8.555 |avg tokens 3821.300 |tokens/s 7952.329 |walltime 8933.772 |
Transformer | epoch 0 | step 17690 |avg loss 8.591 |avg tokens 4867.300 |tokens/s 9221.308 |walltime 8939.050 |
Transformer | epoch 0 | step 17700 |avg loss 8.472 |avg tokens 4578.400 |tokens/s 9419.866 |walltime 8943.910 |
Transformer | epoch 0 | step 17710 |avg loss 8.220 |avg tokens 4819.400 |tokens/s 9155.296 |walltime 8949.175 |
Transformer | epoch 0 | step 17720 |avg loss 8.612 |avg tokens 3828.800 |tokens/s 8268.878 |walltime 8953.805 |
Transformer | epoch 0 | step 17730 |avg loss 8.456 |avg tokens 4695.800 |tokens/s 9194.697 |walltime 8958.912 |
Transformer | epoch 0 | step 17740 |avg loss 8.569 |avg tokens 4631.000 |tokens/s 9364.335 |walltime 8963.857 |
Transformer | epoch 0 | step 17750 |avg loss 8.350 |avg tokens 4426.900 |tokens/s 8547.391 |walltime 8969.037 |
Transformer | epoch 0 | step 17760 |avg loss 8.419 |avg tokens 4472.500 |tokens/s 8819.284 |walltime 8974.108 |
Transformer | epoch 0 | step 17770 |avg loss 8.444 |avg tokens 4606.400 |tokens/s 9124.154 |walltime 8979.156 |
Transformer | epoch 0 | step 17780 |avg loss 8.625 |avg tokens 4904.900 |tokens/s 9383.352 |walltime 8984.384 |
Transformer | epoch 0 | step 17790 |avg loss 8.654 |avg tokens 4678.200 |tokens/s 9232.620 |walltime 8989.451 |
Transformer | epoch 0 | step 17800 |avg loss 8.567 |avg tokens 4240.800 |tokens/s 8697.822 |walltime 8994.326 |
Transformer | epoch 0 | step 17810 |avg loss 8.737 |avg tokens 4783.800 |tokens/s 9612.743 |walltime 8999.303 |
Transformer | epoch 0 | step 17820 |avg loss 8.406 |avg tokens 4173.500 |tokens/s 8468.349 |walltime 9004.231 |
Transformer | epoch 0 | step 17830 |avg loss 8.406 |avg tokens 4695.500 |tokens/s 9280.962 |walltime 9009.291 |
Transformer | epoch 0 | step 17840 |avg loss 8.490 |avg tokens 4649.600 |tokens/s 8996.170 |walltime 9014.459 |
Transformer | epoch 0 | step 17850 |avg loss 8.528 |avg tokens 4152.700 |tokens/s 8298.334 |walltime 9019.463 |
Transformer | epoch 0 | step 17860 |avg loss 8.203 |avg tokens 4795.000 |tokens/s 9145.784 |walltime 9024.706 |
Transformer | epoch 0 | step 17870 |avg loss 8.527 |avg tokens 4628.300 |tokens/s 9137.616 |walltime 9029.771 |
Transformer | epoch 0 | step 17880 |avg loss 8.562 |avg tokens 4716.500 |tokens/s 8994.131 |walltime 9035.015 |
Transformer | epoch 0 | step 17890 |avg loss 8.497 |avg tokens 4494.400 |tokens/s 8789.095 |walltime 9040.129 |
Transformer | epoch 0 | step 17900 |avg loss 8.371 |avg tokens 4261.700 |tokens/s 8900.872 |walltime 9044.917 |
Transformer | epoch 0 | step 17910 |avg loss 8.676 |avg tokens 4337.400 |tokens/s 8409.621 |walltime 9050.074 |
Transformer | epoch 0 | step 17920 |avg loss 8.524 |avg tokens 4700.000 |tokens/s 8947.822 |walltime 9055.327 |
Transformer | epoch 0 | step 17930 |avg loss 8.516 |avg tokens 4130.400 |tokens/s 8474.366 |walltime 9060.201 |
Transformer | epoch 0 | step 17940 |avg loss 8.576 |avg tokens 4872.200 |tokens/s 9347.672 |walltime 9065.413 |
Transformer | epoch 0 | step 17950 |avg loss 8.744 |avg tokens 4528.300 |tokens/s 8852.076 |walltime 9070.529 |
Transformer | epoch 0 | step 17960 |avg loss 8.660 |avg tokens 4629.900 |tokens/s 9182.578 |walltime 9075.571 |
Transformer | epoch 0 | step 17970 |avg loss 8.350 |avg tokens 4907.200 |tokens/s 9366.093 |walltime 9080.810 |
Transformer | epoch 0 | step 17980 |avg loss 8.447 |avg tokens 4356.000 |tokens/s 8840.619 |walltime 9085.737 |
Transformer | epoch 0 | step 17990 |avg loss 8.373 |avg tokens 4868.500 |tokens/s 9072.212 |walltime 9091.104 |
Transformer | epoch 0 | step 18000 |avg loss 8.454 |avg tokens 4205.800 |tokens/s 8405.740 |walltime 9096.107 |
Transformer | epoch 0 | step 18010 |avg loss 8.503 |avg tokens 4778.400 |tokens/s 9145.133 |walltime 9101.332 |
Transformer | epoch 0 | step 18020 |avg loss 8.265 |avg tokens 4692.200 |tokens/s 8892.954 |walltime 9106.609 |
Transformer | epoch 0 | step 18030 |avg loss 8.461 |avg tokens 4200.500 |tokens/s 8518.996 |walltime 9111.540 |
Transformer | epoch 0 | step 18040 |avg loss 8.222 |avg tokens 4128.000 |tokens/s 8706.196 |walltime 9116.281 |
Transformer | epoch 0 | step 18050 |avg loss 8.527 |avg tokens 4628.700 |tokens/s 8882.962 |walltime 9121.492 |
Transformer | epoch 0 | step 18060 |avg loss 8.296 |avg tokens 4324.800 |tokens/s 8600.161 |walltime 9126.520 |
Transformer | epoch 0 | step 18070 |avg loss 8.450 |avg tokens 4626.800 |tokens/s 9125.894 |walltime 9131.590 |
Transformer | epoch 0 | step 18080 |avg loss 8.358 |avg tokens 4370.400 |tokens/s 8814.077 |walltime 9136.549 |
Transformer | epoch 0 | step 18090 |avg loss 8.418 |avg tokens 4665.300 |tokens/s 9111.796 |walltime 9141.669 |
Transformer | epoch 0 | step 18100 |avg loss 8.499 |avg tokens 4238.600 |tokens/s 8444.622 |walltime 9146.688 |
Transformer | epoch 0 | step 18110 |avg loss 8.310 |avg tokens 4897.800 |tokens/s 9090.830 |walltime 9152.076 |
Transformer | epoch 0 | step 18120 |avg loss 8.506 |avg tokens 4537.900 |tokens/s 9040.260 |walltime 9157.096 |
Transformer | epoch 0 | step 18130 |avg loss 8.460 |avg tokens 4733.800 |tokens/s 9240.451 |walltime 9162.218 |
Transformer | epoch 0 | step 18140 |avg loss 8.775 |avg tokens 4680.800 |tokens/s 9339.983 |walltime 9167.230 |
Transformer | epoch 0 | step 18150 |avg loss 8.440 |avg tokens 4535.700 |tokens/s 8825.896 |walltime 9172.369 |
Transformer | epoch 0 | step 18160 |avg loss 8.683 |avg tokens 4442.000 |tokens/s 9458.563 |walltime 9177.065 |
Transformer | epoch 0 | step 18170 |avg loss 8.316 |avg tokens 4773.300 |tokens/s 9054.421 |walltime 9182.337 |
Transformer | epoch 0 | step 18180 |avg loss 8.423 |avg tokens 4330.200 |tokens/s 8733.896 |walltime 9187.295 |
Transformer | epoch 0 | step 18190 |avg loss 8.583 |avg tokens 3922.200 |tokens/s 8120.581 |walltime 9192.125 |
Transformer | epoch 0 | step 18200 |avg loss 8.251 |avg tokens 4903.900 |tokens/s 9239.428 |walltime 9197.433 |
Transformer | epoch 0 | step 18210 |avg loss 8.426 |avg tokens 4641.100 |tokens/s 9256.949 |walltime 9202.446 |
Transformer | epoch 0 | step 18220 |avg loss 8.473 |avg tokens 4680.700 |tokens/s 9072.510 |walltime 9207.606 |
Transformer | epoch 0 | step 18230 |avg loss 8.436 |avg tokens 4558.800 |tokens/s 8968.914 |walltime 9212.688 |
Transformer | epoch 0 | step 18240 |avg loss 8.362 |avg tokens 4491.800 |tokens/s 8797.519 |walltime 9217.794 |
Transformer | epoch 0 | step 18250 |avg loss 8.690 |avg tokens 4609.300 |tokens/s 9266.273 |walltime 9222.768 |
Transformer | epoch 0 | step 18260 |avg loss 8.337 |avg tokens 4566.000 |tokens/s 8885.220 |walltime 9227.907 |
Transformer | epoch 0 | step 18270 |avg loss 8.791 |avg tokens 4728.200 |tokens/s 9365.150 |walltime 9232.956 |
Transformer | epoch 0 | step 18280 |avg loss 8.535 |avg tokens 3785.400 |tokens/s 8257.730 |walltime 9237.540 |
Transformer | epoch 0 | step 18290 |avg loss 8.603 |avg tokens 3787.700 |tokens/s 8049.840 |walltime 9242.245 |
Transformer | epoch 0 | step 18300 |avg loss 8.504 |avg tokens 4831.600 |tokens/s 9095.000 |walltime 9247.558 |
Transformer | epoch 0 | step 18310 |avg loss 8.555 |avg tokens 4188.200 |tokens/s 8473.261 |walltime 9252.501 |
Transformer | epoch 0 | step 18320 |avg loss 8.696 |avg tokens 4474.400 |tokens/s 9247.344 |walltime 9257.339 |
Transformer | epoch 0 | step 18330 |avg loss 8.575 |avg tokens 4974.800 |tokens/s 9648.553 |walltime 9262.495 |
Transformer | epoch 0 | step 18340 |avg loss 8.773 |avg tokens 4773.700 |tokens/s 9369.264 |walltime 9267.590 |
Transformer | epoch 0 | step 18350 |avg loss 8.689 |avg tokens 3895.000 |tokens/s 8415.402 |walltime 9272.219 |
Transformer | epoch 0 | step 18360 |avg loss 8.388 |avg tokens 4609.300 |tokens/s 8843.959 |walltime 9277.431 |
Transformer | epoch 0 | step 18370 |avg loss 8.451 |avg tokens 4611.800 |tokens/s 8953.131 |walltime 9282.582 |
Transformer | epoch 0 | step 18380 |avg loss 8.418 |avg tokens 4741.400 |tokens/s 9200.264 |walltime 9287.735 |
Transformer | epoch 0 | step 18390 |avg loss 8.419 |avg tokens 4591.500 |tokens/s 8938.119 |walltime 9292.872 |
Transformer | epoch 0 | step 18400 |avg loss 8.666 |avg tokens 4426.500 |tokens/s 8929.439 |walltime 9297.829 |
Transformer | epoch 0 | step 18410 |avg loss 8.691 |avg tokens 3799.400 |tokens/s 7671.750 |walltime 9302.782 |
Transformer | epoch 0 | step 18420 |avg loss 8.362 |avg tokens 4884.800 |tokens/s 9169.409 |walltime 9308.109 |
Transformer | epoch 0 | step 18430 |avg loss 8.269 |avg tokens 4818.600 |tokens/s 9688.850 |walltime 9313.082 |
Transformer | epoch 0 | step 18440 |avg loss 8.535 |avg tokens 4834.400 |tokens/s 9164.140 |walltime 9318.358 |
Transformer | epoch 0 | step 18450 |avg loss 8.737 |avg tokens 4567.200 |tokens/s 9618.600 |walltime 9323.106 |
Transformer | epoch 0 | step 18460 |avg loss 8.405 |avg tokens 4593.400 |tokens/s 8716.005 |walltime 9328.376 |
Transformer | epoch 0 | step 18470 |avg loss 8.198 |avg tokens 4574.000 |tokens/s 8992.468 |walltime 9333.463 |
Transformer | epoch 0 | step 18480 |avg loss 8.514 |avg tokens 3988.300 |tokens/s 8286.525 |walltime 9338.276 |
Transformer | epoch 0 | step 18490 |avg loss 8.473 |avg tokens 4681.800 |tokens/s 9955.300 |walltime 9342.978 |
Transformer | epoch 0 | step 18500 |avg loss 8.600 |avg tokens 4483.300 |tokens/s 8827.266 |walltime 9348.057 |
Transformer | epoch 0 | step 18510 |avg loss 8.546 |avg tokens 4153.100 |tokens/s 8578.563 |walltime 9352.899 |
Transformer | epoch 0 | step 18520 |avg loss 8.251 |avg tokens 4564.800 |tokens/s 8996.258 |walltime 9357.973 |
Transformer | epoch 0 | step 18530 |avg loss 8.511 |avg tokens 4684.700 |tokens/s 9074.914 |walltime 9363.135 |
Transformer | epoch 0 | step 18540 |avg loss 8.502 |avg tokens 4632.000 |tokens/s 8834.020 |walltime 9368.378 |
Transformer | epoch 0 | step 18550 |avg loss 8.623 |avg tokens 4396.200 |tokens/s 8805.452 |walltime 9373.371 |
Transformer | epoch 0 | step 18560 |avg loss 8.596 |avg tokens 4510.600 |tokens/s 9059.831 |walltime 9378.350 |
Transformer | epoch 0 | step 18570 |avg loss 8.359 |avg tokens 4786.400 |tokens/s 9041.907 |walltime 9383.643 |
Transformer | epoch 0 | step 18580 |avg loss 8.594 |avg tokens 4183.700 |tokens/s 8556.873 |walltime 9388.532 |
Transformer | epoch 0 | step 18590 |avg loss 8.576 |avg tokens 4548.700 |tokens/s 9174.148 |walltime 9393.491 |
Transformer | epoch 0 | step 18600 |avg loss 8.473 |avg tokens 4326.400 |tokens/s 8749.678 |walltime 9398.435 |
Transformer | epoch 0 | step 18610 |avg loss 8.598 |avg tokens 4652.200 |tokens/s 9670.895 |walltime 9403.246 |
Transformer | epoch 0 | step 18620 |avg loss 8.353 |avg tokens 4497.200 |tokens/s 8883.908 |walltime 9408.308 |
Transformer | epoch 0 | step 18630 |avg loss 8.497 |avg tokens 4698.400 |tokens/s 9178.550 |walltime 9413.427 |
Transformer | epoch 0 | step 18640 |avg loss 8.497 |avg tokens 4806.400 |tokens/s 9383.469 |walltime 9418.549 |
Transformer | epoch 0 | step 18650 |avg loss 8.403 |avg tokens 4897.900 |tokens/s 9341.050 |walltime 9423.793 |
Transformer | epoch 0 | step 18660 |avg loss 8.432 |avg tokens 4409.100 |tokens/s 8903.384 |walltime 9428.745 |
Transformer | epoch 0 | step 18670 |avg loss 8.448 |avg tokens 4409.600 |tokens/s 8801.322 |walltime 9433.755 |
Transformer | epoch 0 | step 18680 |avg loss 8.416 |avg tokens 4461.300 |tokens/s 8981.173 |walltime 9438.722 |
Transformer | epoch 0 | step 18690 |avg loss 8.277 |avg tokens 4477.200 |tokens/s 8747.623 |walltime 9443.840 |
Transformer | epoch 0 | step 18700 |avg loss 8.607 |avg tokens 4871.900 |tokens/s 9381.276 |walltime 9449.034 |
Transformer | epoch 0 | step 18710 |avg loss 8.309 |avg tokens 4661.700 |tokens/s 9026.750 |walltime 9454.198 |
Transformer | epoch 0 | step 18720 |avg loss 8.395 |avg tokens 4843.200 |tokens/s 9071.591 |walltime 9459.537 |
Transformer | epoch 0 | step 18730 |avg loss 8.375 |avg tokens 4498.100 |tokens/s 8774.173 |walltime 9464.663 |
Transformer | epoch 0 | step 18740 |avg loss 8.545 |avg tokens 4632.400 |tokens/s 9177.101 |walltime 9469.711 |
Transformer | epoch 0 | step 18750 |avg loss 8.523 |avg tokens 4181.300 |tokens/s 8200.405 |walltime 9474.810 |
Transformer | epoch 0 | step 18760 |avg loss 8.536 |avg tokens 4286.500 |tokens/s 8556.854 |walltime 9479.819 |
Transformer | epoch 0 | step 18770 |avg loss 8.695 |avg tokens 4445.600 |tokens/s 9043.222 |walltime 9484.735 |
Transformer | epoch 0 | step 18780 |avg loss 8.747 |avg tokens 4451.800 |tokens/s 8974.849 |walltime 9489.696 |
Transformer | epoch 0 | step 18790 |avg loss 8.431 |avg tokens 4902.100 |tokens/s 9432.667 |walltime 9494.893 |
Transformer | epoch 0 | step 18800 |avg loss 8.349 |avg tokens 4692.100 |tokens/s 8950.413 |walltime 9500.135 |
Transformer | epoch 0 | step 18810 |avg loss 8.678 |avg tokens 4530.800 |tokens/s 9366.029 |walltime 9504.973 |
Transformer | epoch 0 | step 18820 |avg loss 8.331 |avg tokens 4553.600 |tokens/s 8858.771 |walltime 9510.113 |
Transformer | epoch 0 | step 18830 |avg loss 8.318 |avg tokens 4672.100 |tokens/s 9285.510 |walltime 9515.144 |
Transformer | epoch 0 | step 18840 |avg loss 8.675 |avg tokens 4250.700 |tokens/s 8715.443 |walltime 9520.022 |
Transformer | epoch 0 | step 18850 |avg loss 8.599 |avg tokens 3953.800 |tokens/s 8378.116 |walltime 9524.741 |
Transformer | epoch 0 | step 18860 |avg loss 8.557 |avg tokens 4858.700 |tokens/s 9284.638 |walltime 9529.974 |
Transformer | epoch 0 | step 18870 |avg loss 8.173 |avg tokens 4634.400 |tokens/s 8828.795 |walltime 9535.223 |
Transformer | epoch 0 | step 18880 |avg loss 8.420 |avg tokens 4387.600 |tokens/s 8704.823 |walltime 9540.263 |
Transformer | epoch 0 | step 18890 |avg loss 8.189 |avg tokens 4632.800 |tokens/s 8914.863 |walltime 9545.460 |
Transformer | epoch 0 | step 18900 |avg loss 8.401 |avg tokens 4865.000 |tokens/s 9217.089 |walltime 9550.738 |
Transformer | epoch 0 | step 18910 |avg loss 8.518 |avg tokens 4914.700 |tokens/s 9601.710 |walltime 9555.857 |
Transformer | epoch 0 | step 18920 |avg loss 8.222 |avg tokens 4825.400 |tokens/s 9174.461 |walltime 9561.117 |
Transformer | epoch 0 | step 18930 |avg loss 8.464 |avg tokens 4554.700 |tokens/s 8867.901 |walltime 9566.253 |
Transformer | epoch 0 | step 18940 |avg loss 8.390 |avg tokens 4226.800 |tokens/s 8502.925 |walltime 9571.224 |
Transformer | epoch 0 | step 18950 |avg loss 8.405 |avg tokens 4410.300 |tokens/s 8692.060 |walltime 9576.298 |
Transformer | epoch 0 | step 18960 |avg loss 8.492 |avg tokens 4243.000 |tokens/s 8557.908 |walltime 9581.256 |
Transformer | epoch 0 | step 18970 |avg loss 8.670 |avg tokens 4696.800 |tokens/s 9437.070 |walltime 9586.233 |
Transformer | epoch 0 | step 18980 |avg loss 8.664 |avg tokens 4437.700 |tokens/s 8872.567 |walltime 9591.234 |
Transformer | epoch 0 | step 18990 |avg loss 8.438 |avg tokens 4535.800 |tokens/s 9105.884 |walltime 9596.215 |
Transformer | epoch 0 | step 19000 |avg loss 8.377 |avg tokens 4545.500 |tokens/s 9117.011 |walltime 9601.201 |
Transformer | epoch 0 | step 19010 |avg loss 8.355 |avg tokens 4507.400 |tokens/s 8927.728 |walltime 9606.250 |
Transformer | epoch 0 | step 19020 |avg loss 8.468 |avg tokens 3558.900 |tokens/s 7689.850 |walltime 9610.878 |
Transformer | epoch 0 | step 19030 |avg loss 8.385 |avg tokens 4865.300 |tokens/s 9289.788 |walltime 9616.115 |
Transformer | epoch 0 | step 19040 |avg loss 8.514 |avg tokens 4590.200 |tokens/s 8986.808 |walltime 9621.223 |
Transformer | epoch 0 | step 19050 |avg loss 8.558 |avg tokens 4203.000 |tokens/s 8690.638 |walltime 9626.059 |
Transformer | epoch 0 | step 19060 |avg loss 8.383 |avg tokens 4656.500 |tokens/s 8974.634 |walltime 9631.248 |
Transformer | epoch 0 | step 19070 |avg loss 8.569 |avg tokens 4572.000 |tokens/s 9124.958 |walltime 9636.258 |
Transformer | epoch 0 | step 19080 |avg loss 8.548 |avg tokens 4668.600 |tokens/s 9295.540 |walltime 9641.281 |
Transformer | epoch 0 | step 19090 |avg loss 8.215 |avg tokens 4398.500 |tokens/s 8670.589 |walltime 9646.353 |
Transformer | epoch 0 | step 19100 |avg loss 8.273 |avg tokens 4492.800 |tokens/s 8895.771 |walltime 9651.404 |
Transformer | epoch 0 | step 19110 |avg loss 8.583 |avg tokens 4103.300 |tokens/s 8437.581 |walltime 9656.267 |
Transformer | epoch 0 | step 19120 |avg loss 8.481 |avg tokens 4166.600 |tokens/s 8656.584 |walltime 9661.080 |
Transformer | epoch 0 | step 19130 |avg loss 8.825 |avg tokens 3990.200 |tokens/s 8625.874 |walltime 9665.706 |
Transformer | epoch 0 | step 19140 |avg loss 8.488 |avg tokens 4597.600 |tokens/s 9039.761 |walltime 9670.792 |
Transformer | epoch 0 | step 19150 |avg loss 8.319 |avg tokens 4672.800 |tokens/s 9073.440 |walltime 9675.942 |
Transformer | epoch 0 | step 19160 |avg loss 8.458 |avg tokens 4503.300 |tokens/s 9103.736 |walltime 9680.889 |
Transformer | epoch 0 | step 19170 |avg loss 8.279 |avg tokens 4641.400 |tokens/s 8919.551 |walltime 9686.092 |
Transformer | epoch 0 | step 19180 |avg loss 8.374 |avg tokens 4832.800 |tokens/s 9051.793 |walltime 9691.431 |
Transformer | epoch 0 | step 19190 |avg loss 8.480 |avg tokens 4386.800 |tokens/s 8681.086 |walltime 9696.485 |
Transformer | epoch 0 | step 19200 |avg loss 8.642 |avg tokens 4477.000 |tokens/s 8705.964 |walltime 9701.627 |
Transformer | epoch 0 | step 19210 |avg loss 8.351 |avg tokens 4795.600 |tokens/s 9045.906 |walltime 9706.929 |
Transformer | epoch 0 | step 19220 |avg loss 8.402 |avg tokens 4088.200 |tokens/s 8536.748 |walltime 9711.718 |
Transformer | epoch 0 | step 19230 |avg loss 9.028 |avg tokens 4675.400 |tokens/s 10186.157 |walltime 9716.307 |
Transformer | epoch 0 | step 19240 |avg loss 8.569 |avg tokens 4552.300 |tokens/s 8752.227 |walltime 9721.509 |
Transformer | epoch 0 | step 19250 |avg loss 8.500 |avg tokens 4555.600 |tokens/s 8972.171 |walltime 9726.586 |
Transformer | epoch 0 | step 19260 |avg loss 8.811 |avg tokens 4266.300 |tokens/s 8980.710 |walltime 9731.337 |
Transformer | epoch 0 | step 19270 |avg loss 8.460 |avg tokens 4483.300 |tokens/s 8840.223 |walltime 9736.408 |
Transformer | epoch 0 | step 19280 |avg loss 8.409 |avg tokens 4503.700 |tokens/s 8937.508 |walltime 9741.447 |
Transformer | epoch 0 | step 19290 |avg loss 8.551 |avg tokens 4260.400 |tokens/s 8795.936 |walltime 9746.291 |
Transformer | epoch 0 | step 19300 |avg loss 8.605 |avg tokens 4457.800 |tokens/s 9070.082 |walltime 9751.206 |
Transformer | epoch 0 | step 19310 |avg loss 8.484 |avg tokens 4616.600 |tokens/s 8897.935 |walltime 9756.394 |
Transformer | epoch 0 | step 19320 |avg loss 8.443 |avg tokens 4627.800 |tokens/s 9099.699 |walltime 9761.480 |
Transformer | epoch 0 | step 19330 |avg loss 8.687 |avg tokens 4296.500 |tokens/s 8337.784 |walltime 9766.633 |
Transformer | epoch 0 | step 19340 |avg loss 8.558 |avg tokens 4593.200 |tokens/s 9009.968 |walltime 9771.731 |
Transformer | epoch 0 | step 19350 |avg loss 8.378 |avg tokens 4914.000 |tokens/s 9582.918 |walltime 9776.859 |
Transformer | epoch 0 | step 19360 |avg loss 8.478 |avg tokens 4553.600 |tokens/s 8860.556 |walltime 9781.998 |
Transformer | epoch 0 | step 19370 |avg loss 8.453 |avg tokens 4742.700 |tokens/s 9066.333 |walltime 9787.229 |
Transformer | epoch 0 | step 19380 |avg loss 8.445 |avg tokens 4660.100 |tokens/s 9321.197 |walltime 9792.228 |
Transformer | epoch 0 | step 19390 |avg loss 8.342 |avg tokens 4478.400 |tokens/s 8863.906 |walltime 9797.281 |
Transformer | epoch 0 | step 19400 |avg loss 8.176 |avg tokens 4757.600 |tokens/s 8922.204 |walltime 9802.613 |
Transformer | epoch 0 | step 19410 |avg loss 8.369 |avg tokens 4899.400 |tokens/s 9413.997 |walltime 9807.818 |
Transformer | epoch 0 | step 19420 |avg loss 8.673 |avg tokens 4472.700 |tokens/s 9036.870 |walltime 9812.767 |
Transformer | epoch 0 | step 19430 |avg loss 8.423 |avg tokens 4820.000 |tokens/s 9199.494 |walltime 9818.006 |
Transformer | epoch 0 | step 19440 |avg loss 8.604 |avg tokens 4786.300 |tokens/s 9422.811 |walltime 9823.086 |
Transformer | epoch 0 | step 19450 |avg loss 8.350 |avg tokens 4783.500 |tokens/s 9118.518 |walltime 9828.332 |
Transformer | epoch 0 | step 19460 |avg loss 8.317 |avg tokens 4928.300 |tokens/s 9427.694 |walltime 9833.559 |
Transformer | epoch 0 | step 19470 |avg loss 8.452 |avg tokens 4497.600 |tokens/s 8768.777 |walltime 9838.688 |
Transformer | epoch 0 | step 19480 |avg loss 8.266 |avg tokens 4877.600 |tokens/s 9256.570 |walltime 9843.958 |
Transformer | epoch 0 | step 19490 |avg loss 8.355 |avg tokens 4918.300 |tokens/s 9280.671 |walltime 9849.257 |
Transformer | epoch 0 | step 19500 |avg loss 8.559 |avg tokens 4574.300 |tokens/s 9068.127 |walltime 9854.302 |
Transformer | epoch 0 | step 19510 |avg loss 8.447 |avg tokens 4421.200 |tokens/s 8620.819 |walltime 9859.430 |
Transformer | epoch 0 | step 19520 |avg loss 8.107 |avg tokens 4646.200 |tokens/s 9028.344 |walltime 9864.576 |
Transformer | epoch 0 | step 19530 |avg loss 8.479 |avg tokens 4532.700 |tokens/s 9051.481 |walltime 9869.584 |
Transformer | epoch 0 | step 19540 |avg loss 8.559 |avg tokens 4278.000 |tokens/s 8622.879 |walltime 9874.545 |
Transformer | epoch 0 | step 19550 |avg loss 8.827 |avg tokens 4645.500 |tokens/s 9760.827 |walltime 9879.305 |
Transformer | epoch 0 | step 19560 |avg loss 8.491 |avg tokens 4561.700 |tokens/s 8637.692 |walltime 9884.586 |
Transformer | epoch 0 | step 19570 |avg loss 8.499 |avg tokens 4383.500 |tokens/s 8924.595 |walltime 9889.498 |
Transformer | epoch 0 | step 19580 |avg loss 8.598 |avg tokens 4475.500 |tokens/s 8908.779 |walltime 9894.521 |
Transformer | epoch 0 | step 19590 |avg loss 8.477 |avg tokens 4529.500 |tokens/s 8871.263 |walltime 9899.627 |
Transformer | epoch 0 | step 19600 |avg loss 8.731 |avg tokens 3882.500 |tokens/s 8309.446 |walltime 9904.299 |
Transformer | epoch 0 | step 19610 |avg loss 8.922 |avg tokens 4780.400 |tokens/s 9605.523 |walltime 9909.276 |
Transformer | epoch 0 | step 19620 |avg loss 8.360 |avg tokens 4797.600 |tokens/s 8932.875 |walltime 9914.647 |
Transformer | epoch 0 | step 19630 |avg loss 8.202 |avg tokens 4640.400 |tokens/s 9098.068 |walltime 9919.747 |
Transformer | epoch 0 | step 19640 |avg loss 8.530 |avg tokens 4805.100 |tokens/s 8788.292 |walltime 9925.215 |
Transformer | epoch 0 | step 19650 |avg loss 8.696 |avg tokens 4404.100 |tokens/s 9159.409 |walltime 9930.023 |
Transformer | epoch 0 | step 19660 |avg loss 8.546 |avg tokens 4115.100 |tokens/s 8349.180 |walltime 9934.952 |
Transformer | epoch 0 | step 19670 |avg loss 8.515 |avg tokens 4244.700 |tokens/s 8659.314 |walltime 9939.854 |
Transformer | epoch 0 | step 19680 |avg loss 8.699 |avg tokens 4279.200 |tokens/s 8769.661 |walltime 9944.733 |
Transformer | epoch 0 | step 19690 |avg loss 8.493 |avg tokens 4299.800 |tokens/s 8715.234 |walltime 9949.667 |
Transformer | epoch 0 | step 19700 |avg loss 8.597 |avg tokens 4546.700 |tokens/s 9129.249 |walltime 9954.647 |
Transformer | epoch 0 | step 19710 |avg loss 8.435 |avg tokens 4880.000 |tokens/s 9253.614 |walltime 9959.921 |
Transformer | epoch 0 | step 19720 |avg loss 8.632 |avg tokens 4632.700 |tokens/s 9336.496 |walltime 9964.883 |
Transformer | epoch 0 | step 19730 |avg loss 8.543 |avg tokens 4875.700 |tokens/s 9482.357 |walltime 9970.025 |
Transformer | epoch 0 | step 19740 |avg loss 8.482 |avg tokens 4664.700 |tokens/s 9000.925 |walltime 9975.207 |
Transformer | epoch 0 | step 19750 |avg loss 8.480 |avg tokens 4276.400 |tokens/s 8719.991 |walltime 9980.111 |
Transformer | epoch 0 | step 19760 |avg loss 8.332 |avg tokens 4673.600 |tokens/s 8830.871 |walltime 9985.404 |
Transformer | epoch 0 | step 19770 |avg loss 8.351 |avg tokens 4580.200 |tokens/s 8816.249 |walltime 9990.599 |
Transformer | epoch 0 | step 19780 |avg loss 8.494 |avg tokens 4587.100 |tokens/s 9107.831 |walltime 9995.635 |
Transformer | epoch 0 | step 19790 |avg loss 8.677 |avg tokens 4355.300 |tokens/s 9067.829 |walltime 10000.438 |
Transformer | epoch 0 | step 19800 |avg loss 8.651 |avg tokens 3935.200 |tokens/s 8902.703 |walltime 10004.859 |
Transformer | epoch 0 | step 19810 |avg loss 8.522 |avg tokens 4760.700 |tokens/s 9316.046 |walltime 10009.969 |
Transformer | epoch 0 | step 19820 |avg loss 8.394 |avg tokens 4439.300 |tokens/s 8781.725 |walltime 10015.024 |
Transformer | epoch 0 | step 19830 |avg loss 8.672 |avg tokens 4884.700 |tokens/s 9791.083 |walltime 10020.013 |
Transformer | epoch 0 | step 19840 |avg loss 8.449 |avg tokens 4767.900 |tokens/s 9030.994 |walltime 10025.292 |
Transformer | epoch 0 | step 19850 |avg loss 8.568 |avg tokens 4743.300 |tokens/s 9188.755 |walltime 10030.455 |
Transformer | epoch 0 | step 19860 |avg loss 8.717 |avg tokens 4619.300 |tokens/s 9423.520 |walltime 10035.356 |
Transformer | epoch 0 | step 19870 |avg loss 8.581 |avg tokens 4681.900 |tokens/s 8995.881 |walltime 10040.561 |
Transformer | epoch 0 | step 19880 |avg loss 8.520 |avg tokens 4430.700 |tokens/s 8710.941 |walltime 10045.647 |
Transformer | epoch 0 | step 19890 |avg loss 8.563 |avg tokens 4707.700 |tokens/s 9303.462 |walltime 10050.707 |
Transformer | epoch 0 | step 19900 |avg loss 8.351 |avg tokens 3806.900 |tokens/s 7709.641 |walltime 10055.645 |
Transformer | epoch 0 | step 19910 |avg loss 8.746 |avg tokens 4544.700 |tokens/s 9215.519 |walltime 10060.577 |
Transformer | epoch 0 | step 19920 |avg loss 8.260 |avg tokens 4624.100 |tokens/s 8942.144 |walltime 10065.748 |
Transformer | epoch 0 | step 19930 |avg loss 8.608 |avg tokens 3942.100 |tokens/s 8174.691 |walltime 10070.570 |
Transformer | epoch 0 | step 19940 |avg loss 8.621 |avg tokens 3854.300 |tokens/s 8249.435 |walltime 10075.243 |
Transformer | epoch 0 | step 19950 |avg loss 8.497 |avg tokens 4841.600 |tokens/s 9447.641 |walltime 10080.367 |
Transformer | epoch 0 | step 19960 |avg loss 8.759 |avg tokens 4405.800 |tokens/s 8934.721 |walltime 10085.298 |
Transformer | epoch 0 | step 19970 |avg loss 8.464 |avg tokens 4744.500 |tokens/s 9120.695 |walltime 10090.500 |
Transformer | epoch 0 | step 19980 |avg loss 8.794 |avg tokens 4144.000 |tokens/s 8461.576 |walltime 10095.398 |
Transformer | epoch 0 | step 19990 |avg loss 8.837 |avg tokens 4400.300 |tokens/s 9228.078 |walltime 10100.166 |
Transformer | epoch 0 | step 20000 |avg loss 8.532 |avg tokens 4732.600 |tokens/s 9274.248 |walltime 10105.269 |
Transformer | epoch 0 | step 20010 |avg loss 8.608 |avg tokens 4268.400 |tokens/s 8792.420 |walltime 10110.124 |
Transformer | epoch 0 | step 20020 |avg loss 8.445 |avg tokens 4445.400 |tokens/s 8779.334 |walltime 10115.187 |
Transformer | epoch 0 | step 20030 |avg loss 8.682 |avg tokens 4124.300 |tokens/s 8710.558 |walltime 10119.922 |
Transformer | epoch 0 | step 20040 |avg loss 8.410 |avg tokens 4746.400 |tokens/s 9460.837 |walltime 10124.939 |
Transformer | epoch 0 | step 20050 |avg loss 8.546 |avg tokens 4840.500 |tokens/s 9073.516 |walltime 10130.274 |
Transformer | epoch 0 | step 20060 |avg loss 8.506 |avg tokens 4450.400 |tokens/s 9121.830 |walltime 10135.152 |
Transformer | epoch 0 | step 20070 |avg loss 8.449 |avg tokens 4483.100 |tokens/s 8719.840 |walltime 10140.294 |
Transformer | epoch 0 | step 20080 |avg loss 8.631 |avg tokens 4191.000 |tokens/s 8267.046 |walltime 10145.363 |
Transformer | epoch 0 | step 20090 |avg loss 8.526 |avg tokens 4596.000 |tokens/s 9051.303 |walltime 10150.441 |
Transformer | epoch 0 | step 20100 |avg loss 8.850 |avg tokens 4166.100 |tokens/s 8614.697 |walltime 10155.277 |
Transformer | epoch 0 | step 20110 |avg loss 8.533 |avg tokens 4171.300 |tokens/s 8610.816 |walltime 10160.121 |
Transformer | epoch 0 | step 20120 |avg loss 8.465 |avg tokens 4435.400 |tokens/s 9116.860 |walltime 10164.986 |
Transformer | epoch 0 | step 20130 |avg loss 8.513 |avg tokens 4636.400 |tokens/s 9419.537 |walltime 10169.908 |
Transformer | epoch 0 | step 20140 |avg loss 8.421 |avg tokens 4628.000 |tokens/s 8724.119 |walltime 10175.213 |
Transformer | epoch 0 | step 20150 |avg loss 8.488 |avg tokens 4967.200 |tokens/s 9517.837 |walltime 10180.432 |
Transformer | epoch 0 | step 20160 |avg loss 8.850 |avg tokens 4617.500 |tokens/s 9426.346 |walltime 10185.331 |
Transformer | epoch 0 | step 20170 |avg loss 8.436 |avg tokens 4509.300 |tokens/s 8772.222 |walltime 10190.471 |
Transformer | epoch 0 | step 20180 |avg loss 8.454 |avg tokens 4684.200 |tokens/s 9336.682 |walltime 10195.488 |
Transformer | epoch 0 | step 20190 |avg loss 8.378 |avg tokens 4696.200 |tokens/s 9049.563 |walltime 10200.677 |
Transformer | epoch 0 | step 20200 |avg loss 8.368 |avg tokens 4803.400 |tokens/s 9606.114 |walltime 10205.678 |
Transformer | epoch 0 | step 20210 |avg loss 8.476 |avg tokens 4791.500 |tokens/s 9361.542 |walltime 10210.796 |
Transformer | epoch 0 | step 20220 |avg loss 8.515 |avg tokens 4479.900 |tokens/s 8933.254 |walltime 10215.811 |
Transformer | epoch 0 | step 20230 |avg loss 8.576 |avg tokens 4240.900 |tokens/s 8554.860 |walltime 10220.768 |
Transformer | epoch 0 | step 20240 |avg loss 8.496 |avg tokens 4201.500 |tokens/s 8482.922 |walltime 10225.721 |
Transformer | epoch 0 | step 20250 |avg loss 8.514 |avg tokens 4363.000 |tokens/s 8709.107 |walltime 10230.731 |
Transformer | epoch 0 | step 20260 |avg loss 8.586 |avg tokens 4420.300 |tokens/s 8813.280 |walltime 10235.746 |
Transformer | epoch 0 | step 20270 |avg loss 8.763 |avg tokens 4332.700 |tokens/s 9316.054 |walltime 10240.397 |
Transformer | epoch 0 | step 20280 |avg loss 8.571 |avg tokens 4762.700 |tokens/s 9098.045 |walltime 10245.632 |
Transformer | epoch 0 | step 20290 |avg loss 8.794 |avg tokens 4332.500 |tokens/s 8990.831 |walltime 10250.451 |
Transformer | epoch 0 | step 20300 |avg loss 8.613 |avg tokens 4903.000 |tokens/s 9212.722 |walltime 10255.773 |
Transformer | epoch 0 | step 20310 |avg loss 8.536 |avg tokens 4441.000 |tokens/s 8871.741 |walltime 10260.779 |
Transformer | epoch 0 | step 20320 |avg loss 8.737 |avg tokens 4347.400 |tokens/s 9010.416 |walltime 10265.603 |
Transformer | epoch 0 | step 20330 |avg loss 8.605 |avg tokens 4632.300 |tokens/s 8955.712 |walltime 10270.776 |
Transformer | epoch 0 | step 20340 |avg loss 8.536 |avg tokens 4748.000 |tokens/s 9233.454 |walltime 10275.918 |
Transformer | epoch 0 | step 20350 |avg loss 8.191 |avg tokens 4544.200 |tokens/s 8869.322 |walltime 10281.042 |
Transformer | epoch 0 | step 20360 |avg loss 8.451 |avg tokens 4694.300 |tokens/s 9158.005 |walltime 10286.167 |
Transformer | epoch 0 | step 20370 |avg loss 8.352 |avg tokens 4444.800 |tokens/s 8790.660 |walltime 10291.224 |
Transformer | epoch 0 | step 20380 |avg loss 8.537 |avg tokens 4437.400 |tokens/s 8825.715 |walltime 10296.252 |
Transformer | epoch 0 | step 20390 |avg loss 8.555 |avg tokens 4624.800 |tokens/s 9158.538 |walltime 10301.301 |
Transformer | epoch 0 | step 20400 |avg loss 8.410 |avg tokens 4379.900 |tokens/s 8672.743 |walltime 10306.351 |
Transformer | epoch 0 | step 20410 |avg loss 8.812 |avg tokens 4646.100 |tokens/s 9294.694 |walltime 10311.350 |
Transformer | epoch 0 | step 20420 |avg loss 8.820 |avg tokens 4567.200 |tokens/s 9306.028 |walltime 10316.258 |
Transformer | epoch 0 | step 20430 |avg loss 8.706 |avg tokens 4530.700 |tokens/s 9218.166 |walltime 10321.173 |
Transformer | epoch 0 | step 20440 |avg loss 8.773 |avg tokens 4503.300 |tokens/s 9193.599 |walltime 10326.071 |
Transformer | epoch 0 | step 20450 |avg loss 8.525 |avg tokens 4594.700 |tokens/s 8995.570 |walltime 10331.179 |
Transformer | epoch 0 | step 20460 |avg loss 8.611 |avg tokens 4638.200 |tokens/s 9030.121 |walltime 10336.315 |
Transformer | epoch 0 | step 20470 |avg loss 8.492 |avg tokens 4563.200 |tokens/s 8908.417 |walltime 10341.438 |
Transformer | epoch 0 | step 20480 |avg loss 8.344 |avg tokens 4642.200 |tokens/s 8947.746 |walltime 10346.626 |
Transformer | epoch 0 | step 20490 |avg loss 8.691 |avg tokens 4536.300 |tokens/s 9433.164 |walltime 10351.435 |
Transformer | epoch 0 | step 20500 |avg loss 8.600 |avg tokens 4179.300 |tokens/s 8797.591 |walltime 10356.185 |
Transformer | epoch 0 | step 20510 |avg loss 8.705 |avg tokens 4402.700 |tokens/s 8902.031 |walltime 10361.131 |
Transformer | epoch 0 | step 20520 |avg loss 8.564 |avg tokens 4864.800 |tokens/s 9344.363 |walltime 10366.337 |
Transformer | epoch 0 | step 20530 |avg loss 8.336 |avg tokens 4728.300 |tokens/s 9057.119 |walltime 10371.558 |
Transformer | epoch 0 | step 20540 |avg loss 8.636 |avg tokens 4690.700 |tokens/s 9213.229 |walltime 10376.649 |
Transformer | epoch 0 | step 20550 |avg loss 8.595 |avg tokens 4183.400 |tokens/s 8375.364 |walltime 10381.644 |
Transformer | epoch 0 | step 20560 |avg loss 8.445 |avg tokens 4394.500 |tokens/s 8861.231 |walltime 10386.603 |
Transformer | epoch 0 | step 20570 |avg loss 8.716 |avg tokens 4246.300 |tokens/s 8755.847 |walltime 10391.453 |
Transformer | epoch 0 | step 20580 |avg loss 8.481 |avg tokens 4237.600 |tokens/s 8735.729 |walltime 10396.304 |
Transformer | epoch 0 | step 20590 |avg loss 8.691 |avg tokens 4765.700 |tokens/s 9493.129 |walltime 10401.324 |
Transformer | epoch 0 | step 20600 |avg loss 8.397 |avg tokens 4916.800 |tokens/s 9502.861 |walltime 10406.498 |
Transformer | epoch 0 | step 20610 |avg loss 8.428 |avg tokens 4380.200 |tokens/s 8634.471 |walltime 10411.571 |
Transformer | epoch 0 | step 20620 |avg loss 8.358 |avg tokens 4725.300 |tokens/s 9088.494 |walltime 10416.770 |
Transformer | epoch 0 | step 20630 |avg loss 8.074 |avg tokens 4795.200 |tokens/s 8972.677 |walltime 10422.114 |
Transformer | epoch 0 | step 20640 |avg loss 8.452 |avg tokens 4243.300 |tokens/s 8539.166 |walltime 10427.083 |
Transformer | epoch 0 | step 20650 |avg loss 8.777 |avg tokens 4546.900 |tokens/s 9285.041 |walltime 10431.980 |
Transformer | epoch 0 | step 20660 |avg loss 8.449 |avg tokens 4740.000 |tokens/s 9108.192 |walltime 10437.184 |
Transformer | epoch 0 | step 20670 |avg loss 8.676 |avg tokens 4238.500 |tokens/s 8766.056 |walltime 10442.020 |
Transformer | epoch 0 | step 20680 |avg loss 8.330 |avg tokens 4612.300 |tokens/s 8965.341 |walltime 10447.164 |
Transformer | epoch 0 | step 20690 |avg loss 8.733 |avg tokens 4386.200 |tokens/s 8696.930 |walltime 10452.208 |
Transformer | epoch 0 | step 20700 |avg loss 8.557 |avg tokens 4446.300 |tokens/s 8841.250 |walltime 10457.237 |
Transformer | epoch 0 | step 20710 |avg loss 8.358 |avg tokens 4753.600 |tokens/s 9198.114 |walltime 10462.405 |
Transformer | epoch 0 | step 20720 |avg loss 8.518 |avg tokens 4921.700 |tokens/s 9479.203 |walltime 10467.597 |
Transformer | epoch 0 | step 20730 |avg loss 8.848 |avg tokens 3966.000 |tokens/s 8218.651 |walltime 10472.422 |
Transformer | epoch 0 | step 20740 |avg loss 8.523 |avg tokens 4825.900 |tokens/s 9312.573 |walltime 10477.605 |
Transformer | epoch 0 | step 20750 |avg loss 8.641 |avg tokens 4424.200 |tokens/s 8878.016 |walltime 10482.588 |
Transformer | epoch 0 | step 20760 |avg loss 8.946 |avg tokens 4586.400 |tokens/s 9706.529 |walltime 10487.313 |
Transformer | epoch 0 | step 20770 |avg loss 8.480 |avg tokens 4596.000 |tokens/s 9003.519 |walltime 10492.418 |
Transformer | epoch 0 | step 20780 |avg loss 8.084 |avg tokens 4193.900 |tokens/s 8580.935 |walltime 10497.305 |
Transformer | epoch 0 | step 20790 |avg loss 8.557 |avg tokens 4250.900 |tokens/s 8525.987 |walltime 10502.291 |
Transformer | epoch 0 | step 20800 |avg loss 8.500 |avg tokens 4716.300 |tokens/s 9324.207 |walltime 10507.349 |
Transformer | epoch 0 | step 20810 |avg loss 8.647 |avg tokens 4496.500 |tokens/s 8624.335 |walltime 10512.563 |
Transformer | epoch 0 | step 20820 |avg loss 8.632 |avg tokens 4701.200 |tokens/s 9542.832 |walltime 10517.489 |
Transformer | epoch 0 | step 20830 |avg loss 8.357 |avg tokens 4507.300 |tokens/s 8654.261 |walltime 10522.697 |
Transformer | epoch 0 | step 20840 |avg loss 8.563 |avg tokens 4590.400 |tokens/s 8974.581 |walltime 10527.812 |
Transformer | epoch 0 | step 20850 |avg loss 8.647 |avg tokens 4351.100 |tokens/s 8829.469 |walltime 10532.740 |
Transformer | epoch 0 | step 20860 |avg loss 8.514 |avg tokens 4841.700 |tokens/s 9318.536 |walltime 10537.936 |
Transformer | epoch 0 | step 20870 |avg loss 8.844 |avg tokens 4400.100 |tokens/s 9172.520 |walltime 10542.733 |
Transformer | epoch 0 | step 20880 |avg loss 8.734 |avg tokens 4445.600 |tokens/s 9270.018 |walltime 10547.529 |
Transformer | epoch 0 | step 20890 |avg loss 8.826 |avg tokens 4491.000 |tokens/s 9327.896 |walltime 10552.343 |
Transformer | epoch 0 | step 20900 |avg loss 8.451 |avg tokens 4585.800 |tokens/s 8846.084 |walltime 10557.527 |
Transformer | epoch 0 | step 20910 |avg loss 8.579 |avg tokens 4326.100 |tokens/s 8779.347 |walltime 10562.455 |
Transformer | epoch 0 | step 20920 |avg loss 8.560 |avg tokens 4793.800 |tokens/s 9353.634 |walltime 10567.580 |
Transformer | epoch 0 | step 20930 |avg loss 8.554 |avg tokens 4540.300 |tokens/s 8811.661 |walltime 10572.733 |
Transformer | epoch 0 | step 20940 |avg loss 8.512 |avg tokens 3943.100 |tokens/s 8259.832 |walltime 10577.506 |
Transformer | epoch 0 | step 20950 |avg loss 8.565 |avg tokens 4313.600 |tokens/s 8870.264 |walltime 10582.369 |
Transformer | epoch 0 | step 20960 |avg loss 8.535 |avg tokens 4535.100 |tokens/s 8846.121 |walltime 10587.496 |
Transformer | epoch 0 | step 20970 |avg loss 8.187 |avg tokens 4158.600 |tokens/s 8315.610 |walltime 10592.497 |
Transformer | epoch 0 | step 20980 |avg loss 8.499 |avg tokens 4660.800 |tokens/s 9172.683 |walltime 10597.578 |
Transformer | epoch 0 | step 20990 |avg loss 8.381 |avg tokens 4502.500 |tokens/s 8731.092 |walltime 10602.735 |
Transformer | epoch 0 | step 21000 |avg loss 8.665 |avg tokens 4497.100 |tokens/s 9174.564 |walltime 10607.637 |
Transformer | epoch 0 | step 21010 |avg loss 8.384 |avg tokens 4875.300 |tokens/s 9299.790 |walltime 10612.879 |
Transformer | epoch 0 | step 21020 |avg loss 8.440 |avg tokens 4562.700 |tokens/s 9038.380 |walltime 10617.927 |
Transformer | epoch 0 | step 21030 |avg loss 8.418 |avg tokens 3955.400 |tokens/s 8196.062 |walltime 10622.753 |
Transformer | epoch 0 | step 21040 |avg loss 8.465 |avg tokens 4574.700 |tokens/s 8833.023 |walltime 10627.932 |
Transformer | epoch 0 | step 21050 |avg loss 8.726 |avg tokens 3944.700 |tokens/s 8286.199 |walltime 10632.693 |
Transformer | epoch 0 | step 21060 |avg loss 8.573 |avg tokens 4425.500 |tokens/s 8897.603 |walltime 10637.667 |
Transformer | epoch 0 | step 21070 |avg loss 8.454 |avg tokens 4588.700 |tokens/s 9061.176 |walltime 10642.731 |
Transformer | epoch 0 | step 21080 |avg loss 8.634 |avg tokens 4097.600 |tokens/s 8470.785 |walltime 10647.568 |
Transformer | epoch 0 | step 21090 |avg loss 8.213 |avg tokens 4569.200 |tokens/s 8824.585 |walltime 10652.746 |
Transformer | epoch 0 | step 21100 |avg loss 8.552 |avg tokens 4377.500 |tokens/s 8792.405 |walltime 10657.725 |
Transformer | epoch 0 | step 21110 |avg loss 8.623 |avg tokens 3891.200 |tokens/s 7886.954 |walltime 10662.658 |
Transformer | epoch 0 | step 21120 |avg loss 8.596 |avg tokens 4835.200 |tokens/s 9452.216 |walltime 10667.774 |
Transformer | epoch 0 | step 21130 |avg loss 8.343 |avg tokens 4618.300 |tokens/s 9136.534 |walltime 10672.829 |
Transformer | epoch 0 | step 21140 |avg loss 8.495 |avg tokens 4538.100 |tokens/s 8846.253 |walltime 10677.959 |
Transformer | epoch 0 | step 21150 |avg loss 8.599 |avg tokens 4049.900 |tokens/s 8322.454 |walltime 10682.825 |
Transformer | epoch 0 | step 21160 |avg loss 8.434 |avg tokens 4792.800 |tokens/s 9316.399 |walltime 10687.969 |
Transformer | epoch 0 | step 21170 |avg loss 8.648 |avg tokens 3905.200 |tokens/s 8485.682 |walltime 10692.571 |
Transformer | epoch 0 | step 21180 |avg loss 8.442 |avg tokens 4676.000 |tokens/s 9189.991 |walltime 10697.660 |
Transformer | epoch 0 | step 21190 |avg loss 8.476 |avg tokens 4634.900 |tokens/s 9193.558 |walltime 10702.701 |
Transformer | epoch 0 | step 21200 |avg loss 8.461 |avg tokens 4813.900 |tokens/s 9060.905 |walltime 10708.014 |
Transformer | epoch 0 | step 21210 |avg loss 8.548 |avg tokens 4646.400 |tokens/s 9423.200 |walltime 10712.945 |
Transformer | epoch 0 | step 21220 |avg loss 8.619 |avg tokens 4425.500 |tokens/s 8800.477 |walltime 10717.973 |
Transformer | epoch 0 | step 21230 |avg loss 8.570 |avg tokens 4552.000 |tokens/s 9060.682 |walltime 10722.997 |
Transformer | epoch 0 | step 21240 |avg loss 8.566 |avg tokens 4652.800 |tokens/s 8737.703 |walltime 10728.322 |
Transformer | epoch 0 | step 21250 |avg loss 8.532 |avg tokens 4675.100 |tokens/s 9167.889 |walltime 10733.422 |
Transformer | epoch 0 | step 21260 |avg loss 8.417 |avg tokens 4725.000 |tokens/s 9306.892 |walltime 10738.499 |
Transformer | epoch 0 | step 21270 |avg loss 8.453 |avg tokens 4605.200 |tokens/s 9214.019 |walltime 10743.497 |
Transformer | epoch 0 | step 21280 |avg loss 8.450 |avg tokens 4828.000 |tokens/s 9035.196 |walltime 10748.840 |
Transformer | epoch 0 | step 21290 |avg loss 8.683 |avg tokens 4481.100 |tokens/s 9141.679 |walltime 10753.742 |
Transformer | epoch 0 | step 21300 |avg loss 8.369 |avg tokens 4566.700 |tokens/s 9010.816 |walltime 10758.810 |
Transformer | epoch 0 | step 21310 |avg loss 8.368 |avg tokens 4321.900 |tokens/s 8648.462 |walltime 10763.807 |
Transformer | epoch 0 | step 21320 |avg loss 8.392 |avg tokens 4749.400 |tokens/s 9149.008 |walltime 10768.998 |
Transformer | epoch 0 | step 21330 |avg loss 8.632 |avg tokens 4284.700 |tokens/s 8941.025 |walltime 10773.791 |
Transformer | epoch 0 | step 21340 |avg loss 8.644 |avg tokens 4709.900 |tokens/s 9571.842 |walltime 10778.711 |
Transformer | epoch 0 | step 21350 |avg loss 8.267 |avg tokens 4501.200 |tokens/s 8695.459 |walltime 10783.888 |
Transformer | epoch 0 | step 21360 |avg loss 8.628 |avg tokens 4930.600 |tokens/s 9721.733 |walltime 10788.959 |
Transformer | epoch 0 | step 21370 |avg loss 8.463 |avg tokens 4820.800 |tokens/s 9201.923 |walltime 10794.198 |
Transformer | epoch 0 | step 21380 |avg loss 8.525 |avg tokens 4653.300 |tokens/s 8979.733 |walltime 10799.380 |
Transformer | epoch 0 | step 21390 |avg loss 8.450 |avg tokens 4453.700 |tokens/s 8788.993 |walltime 10804.448 |
Transformer | epoch 0 | step 21400 |avg loss 8.589 |avg tokens 4450.000 |tokens/s 8776.553 |walltime 10809.518 |
Transformer | epoch 0 | step 21410 |avg loss 8.435 |avg tokens 4061.600 |tokens/s 8441.789 |walltime 10814.329 |
Transformer | epoch 0 | step 21420 |avg loss 8.591 |avg tokens 4492.600 |tokens/s 8909.972 |walltime 10819.372 |
Transformer | epoch 0 | step 21430 |avg loss 8.421 |avg tokens 4995.500 |tokens/s 9810.476 |walltime 10824.464 |
Transformer | epoch 0 | step 21440 |avg loss 8.531 |avg tokens 4536.900 |tokens/s 9058.856 |walltime 10829.472 |
Transformer | epoch 0 | step 21450 |avg loss 8.568 |avg tokens 4615.500 |tokens/s 9009.012 |walltime 10834.595 |
Transformer | epoch 0 | step 21460 |avg loss 8.425 |avg tokens 4802.400 |tokens/s 9054.489 |walltime 10839.899 |
Transformer | epoch 0 | step 21470 |avg loss 8.612 |avg tokens 3805.600 |tokens/s 8185.412 |walltime 10844.548 |
Transformer | epoch 0 | step 21480 |avg loss 8.657 |avg tokens 4630.500 |tokens/s 9227.732 |walltime 10849.566 |
Transformer | epoch 0 | step 21490 |avg loss 8.340 |avg tokens 4516.800 |tokens/s 8817.641 |walltime 10854.689 |
Transformer | epoch 0 | step 21500 |avg loss 8.574 |avg tokens 4766.800 |tokens/s 9538.564 |walltime 10859.686 |
Transformer | epoch 0 | step 21510 |avg loss 8.581 |avg tokens 4280.200 |tokens/s 8618.661 |walltime 10864.652 |
Transformer | epoch 0 | step 21520 |avg loss 8.651 |avg tokens 4332.800 |tokens/s 8893.153 |walltime 10869.524 |
Transformer | epoch 0 | step 21530 |avg loss 8.452 |avg tokens 4390.400 |tokens/s 8691.015 |walltime 10874.576 |
Transformer | epoch 0 | step 21540 |avg loss 8.732 |avg tokens 4544.700 |tokens/s 9043.465 |walltime 10879.601 |
Transformer | epoch 0 | step 21550 |avg loss 8.600 |avg tokens 4544.400 |tokens/s 8736.470 |walltime 10884.803 |
Transformer | epoch 0 | step 21560 |avg loss 8.733 |avg tokens 3895.000 |tokens/s 8385.921 |walltime 10889.448 |
Transformer | epoch 0 | step 21570 |avg loss 8.668 |avg tokens 4636.300 |tokens/s 9479.929 |walltime 10894.338 |
Transformer | epoch 0 | step 21580 |avg loss 8.519 |avg tokens 4835.300 |tokens/s 9178.574 |walltime 10899.606 |
Transformer | epoch 0 | step 21590 |avg loss 8.534 |avg tokens 4506.100 |tokens/s 9103.813 |walltime 10904.556 |
Transformer | epoch 0 | step 21600 |avg loss 8.713 |avg tokens 4368.000 |tokens/s 9125.203 |walltime 10909.343 |
Transformer | epoch 0 | step 21610 |avg loss 8.263 |avg tokens 4716.100 |tokens/s 9048.460 |walltime 10914.555 |
Transformer | epoch 0 | step 21620 |avg loss 8.678 |avg tokens 4131.800 |tokens/s 8672.978 |walltime 10919.319 |
Transformer | epoch 0 | step 21630 |avg loss 8.683 |avg tokens 4795.100 |tokens/s 9640.784 |walltime 10924.293 |
Transformer | epoch 0 | step 21640 |avg loss 8.387 |avg tokens 4504.000 |tokens/s 8924.457 |walltime 10929.339 |
Transformer | epoch 0 | step 21650 |avg loss 8.784 |avg tokens 4479.400 |tokens/s 9470.657 |walltime 10934.069 |
Transformer | epoch 0 | step 21660 |avg loss 8.546 |avg tokens 4490.100 |tokens/s 9047.174 |walltime 10939.032 |
Transformer | epoch 0 | step 21670 |avg loss 8.603 |avg tokens 4657.600 |tokens/s 9318.492 |walltime 10944.030 |
Transformer | epoch 0 | step 21680 |avg loss 8.685 |avg tokens 4551.900 |tokens/s 8718.835 |walltime 10949.251 |
Transformer | epoch 0 | step 21690 |avg loss 8.879 |avg tokens 4443.900 |tokens/s 9268.518 |walltime 10954.046 |
Transformer | epoch 0 | step 21700 |avg loss 8.651 |avg tokens 4140.200 |tokens/s 8919.301 |walltime 10958.688 |
Transformer | epoch 0 | step 21710 |avg loss 8.408 |avg tokens 4733.200 |tokens/s 8948.360 |walltime 10963.977 |
Transformer | epoch 0 | step 21720 |avg loss 8.510 |avg tokens 4259.500 |tokens/s 8434.512 |walltime 10969.027 |
Transformer | epoch 0 | step 21730 |avg loss 8.630 |avg tokens 4598.400 |tokens/s 9402.911 |walltime 10973.918 |
Transformer | epoch 0 | step 21740 |avg loss 8.227 |avg tokens 4717.600 |tokens/s 8976.661 |walltime 10979.173 |
Transformer | epoch 0 | step 21750 |avg loss 8.474 |avg tokens 4493.700 |tokens/s 8896.947 |walltime 10984.224 |
Transformer | epoch 0 | step 21760 |avg loss 8.656 |avg tokens 4733.300 |tokens/s 9352.534 |walltime 10989.285 |
Transformer | epoch 0 | step 21770 |avg loss 8.448 |avg tokens 4822.600 |tokens/s 9241.069 |walltime 10994.504 |
Transformer | epoch 0 | step 21780 |avg loss 8.619 |avg tokens 4611.400 |tokens/s 9094.608 |walltime 10999.574 |
Transformer | epoch 0 | step 21790 |avg loss 8.790 |avg tokens 3999.700 |tokens/s 8528.858 |walltime 11004.264 |
Transformer | epoch 0 | step 21800 |avg loss 8.573 |avg tokens 4585.600 |tokens/s 8980.088 |walltime 11009.370 |
Transformer | epoch 0 | step 21810 |avg loss 8.607 |avg tokens 4285.400 |tokens/s 8515.969 |walltime 11014.402 |
Transformer | epoch 0 | step 21820 |avg loss 8.476 |avg tokens 4799.900 |tokens/s 9202.690 |walltime 11019.618 |
Transformer | epoch 0 | step 21830 |avg loss 8.410 |avg tokens 4430.300 |tokens/s 8660.586 |walltime 11024.734 |
Transformer | epoch 0 | step 21840 |avg loss 8.375 |avg tokens 4307.900 |tokens/s 8882.606 |walltime 11029.583 |
Transformer | epoch 0 | step 21850 |avg loss 8.335 |avg tokens 4685.200 |tokens/s 9195.486 |walltime 11034.678 |
Transformer | epoch 0 | step 21860 |avg loss 8.813 |avg tokens 4621.300 |tokens/s 9485.984 |walltime 11039.550 |
Transformer | epoch 0 | step 21870 |avg loss 8.487 |avg tokens 4523.900 |tokens/s 9077.506 |walltime 11044.534 |
Transformer | epoch 0 | step 21880 |avg loss 8.484 |avg tokens 4644.700 |tokens/s 9063.834 |walltime 11049.658 |
Transformer | epoch 0 | step 21890 |avg loss 8.527 |avg tokens 4639.400 |tokens/s 9098.258 |walltime 11054.757 |
Transformer | epoch 0 | step 21900 |avg loss 8.382 |avg tokens 4838.800 |tokens/s 9534.992 |walltime 11059.832 |
Transformer | epoch 0 | step 21910 |avg loss 8.492 |avg tokens 4922.600 |tokens/s 9220.678 |walltime 11065.171 |
Transformer | epoch 0 | step 21920 |avg loss 8.555 |avg tokens 4565.000 |tokens/s 8867.527 |walltime 11070.319 |
Transformer | epoch 0 | step 21930 |avg loss 8.413 |avg tokens 4458.000 |tokens/s 8660.474 |walltime 11075.466 |
Transformer | epoch 0 | step 21940 |avg loss 8.824 |avg tokens 4566.000 |tokens/s 9121.439 |walltime 11080.472 |
Transformer | epoch 0 | step 21950 |avg loss 8.678 |avg tokens 4620.300 |tokens/s 9233.884 |walltime 11085.476 |
Transformer | epoch 0 | step 21960 |avg loss 8.565 |avg tokens 4629.900 |tokens/s 8946.434 |walltime 11090.651 |
Transformer | epoch 0 | step 21970 |avg loss 8.674 |avg tokens 4217.300 |tokens/s 8646.575 |walltime 11095.528 |
Transformer | epoch 0 | step 21980 |avg loss 8.526 |avg tokens 4556.900 |tokens/s 9246.997 |walltime 11100.456 |
Transformer | epoch 0 | step 21990 |avg loss 8.610 |avg tokens 4088.100 |tokens/s 8521.630 |walltime 11105.254 |
Transformer | epoch 0 | step 22000 |avg loss 8.727 |avg tokens 3770.900 |tokens/s 8151.651 |walltime 11109.880 |
Transformer | epoch 0 | step 22010 |avg loss 8.421 |avg tokens 4520.400 |tokens/s 8843.856 |walltime 11114.991 |
Transformer | epoch 0 | step 22020 |avg loss 8.735 |avg tokens 4509.800 |tokens/s 9121.049 |walltime 11119.935 |
Transformer | epoch 0 | step 22030 |avg loss 8.438 |avg tokens 4565.300 |tokens/s 9107.726 |walltime 11124.948 |
Transformer | epoch 0 | step 22040 |avg loss 8.469 |avg tokens 4396.300 |tokens/s 8795.532 |walltime 11129.946 |
Transformer | epoch 0 | step 22050 |avg loss 8.708 |avg tokens 4388.300 |tokens/s 9014.043 |walltime 11134.815 |
Transformer | epoch 0 | step 22060 |avg loss 8.616 |avg tokens 4648.100 |tokens/s 8925.146 |walltime 11140.022 |
Transformer | epoch 0 | step 22070 |avg loss 8.769 |avg tokens 4181.800 |tokens/s 9098.157 |walltime 11144.619 |
Transformer | epoch 0 | step 22080 |avg loss 8.588 |avg tokens 4613.000 |tokens/s 8990.835 |walltime 11149.750 |
Transformer | epoch 0 | step 22090 |avg loss 8.490 |avg tokens 4862.400 |tokens/s 9369.314 |walltime 11154.939 |
Transformer | epoch 0 | step 22100 |avg loss 8.859 |avg tokens 3975.800 |tokens/s 8298.498 |walltime 11159.730 |
Transformer | epoch 0 | step 22110 |avg loss 8.682 |avg tokens 4836.700 |tokens/s 9514.843 |walltime 11164.814 |
Transformer | epoch 0 | step 22120 |avg loss 8.400 |avg tokens 4487.100 |tokens/s 8863.882 |walltime 11169.876 |
Transformer | epoch 0 | step 22130 |avg loss 8.485 |avg tokens 4658.800 |tokens/s 9109.559 |walltime 11174.990 |
Transformer | epoch 0 | step 22140 |avg loss 8.672 |avg tokens 3337.000 |tokens/s 7576.948 |walltime 11179.394 |
Transformer | epoch 0 | step 22150 |avg loss 8.407 |avg tokens 4616.800 |tokens/s 9262.482 |walltime 11184.379 |
Transformer | epoch 0 | step 22160 |avg loss 8.574 |avg tokens 4635.600 |tokens/s 9103.315 |walltime 11189.471 |
Transformer | epoch 0 | step 22170 |avg loss 8.456 |avg tokens 4508.800 |tokens/s 8709.611 |walltime 11194.648 |
Transformer | epoch 0 | step 22180 |avg loss 8.618 |avg tokens 4708.900 |tokens/s 9279.624 |walltime 11199.722 |
Transformer | epoch 0 | step 22190 |avg loss 8.627 |avg tokens 4555.600 |tokens/s 9438.869 |walltime 11204.548 |
Transformer | epoch 0 | step 22200 |avg loss 8.507 |avg tokens 4426.300 |tokens/s 8769.804 |walltime 11209.596 |
Transformer | epoch 0 | step 22210 |avg loss 8.703 |avg tokens 4405.800 |tokens/s 8960.843 |walltime 11214.512 |
Transformer | epoch 0 | step 22220 |avg loss 8.456 |avg tokens 4657.000 |tokens/s 9047.472 |walltime 11219.660 |
Transformer | epoch 0 | step 22230 |avg loss 8.740 |avg tokens 4663.500 |tokens/s 9284.800 |walltime 11224.682 |
Transformer | epoch 0 | step 22240 |avg loss 8.492 |avg tokens 4646.100 |tokens/s 8910.280 |walltime 11229.897 |
Transformer | epoch 0 | step 22250 |avg loss 8.452 |avg tokens 4763.100 |tokens/s 9193.956 |walltime 11235.077 |
Transformer | epoch 0 | step 22260 |avg loss 8.319 |avg tokens 4217.100 |tokens/s 8343.852 |walltime 11240.132 |
Transformer | epoch 0 | step 22270 |avg loss 8.408 |avg tokens 4687.200 |tokens/s 9165.148 |walltime 11245.246 |
Transformer | epoch 0 | step 22280 |avg loss 8.272 |avg tokens 4373.600 |tokens/s 8629.327 |walltime 11250.314 |
Transformer | epoch 0 | step 22290 |avg loss 8.758 |avg tokens 4772.300 |tokens/s 9625.423 |walltime 11255.272 |
Transformer | epoch 0 | step 22300 |avg loss 8.714 |avg tokens 4768.300 |tokens/s 9760.069 |walltime 11260.158 |
Transformer | epoch 0 | step 22310 |avg loss 8.422 |avg tokens 4574.000 |tokens/s 8965.093 |walltime 11265.260 |
Transformer | epoch 0 | step 22320 |avg loss 8.407 |avg tokens 4695.200 |tokens/s 8995.001 |walltime 11270.479 |
Transformer | epoch 0 | step 22330 |avg loss 8.337 |avg tokens 4865.600 |tokens/s 9437.996 |walltime 11275.635 |
Transformer | epoch 0 | step 22340 |avg loss 8.599 |avg tokens 4933.400 |tokens/s 9930.370 |walltime 11280.603 |
Transformer | epoch 0 | step 22350 |avg loss 8.630 |avg tokens 4590.000 |tokens/s 9109.362 |walltime 11285.642 |
Transformer | epoch 0 | step 22360 |avg loss 8.497 |avg tokens 4579.400 |tokens/s 9284.110 |walltime 11290.574 |
Transformer | epoch 0 | step 22370 |avg loss 8.409 |avg tokens 4234.400 |tokens/s 8464.146 |walltime 11295.577 |
Transformer | epoch 0 | step 22380 |avg loss 8.381 |avg tokens 4890.400 |tokens/s 9329.112 |walltime 11300.819 |
Transformer | epoch 0 | step 22390 |avg loss 8.519 |avg tokens 4750.800 |tokens/s 9289.508 |walltime 11305.933 |
Transformer | epoch 0 | step 22400 |avg loss 8.506 |avg tokens 4497.400 |tokens/s 9040.452 |walltime 11310.908 |
Transformer | epoch 0 | step 22410 |avg loss 8.559 |avg tokens 4437.100 |tokens/s 8767.281 |walltime 11315.969 |
Transformer | epoch 0 | step 22420 |avg loss 8.735 |avg tokens 4370.800 |tokens/s 8835.467 |walltime 11320.916 |
Transformer | epoch 0 | step 22430 |avg loss 8.529 |avg tokens 4532.300 |tokens/s 8696.386 |walltime 11326.127 |
Transformer | epoch 0 | step 22440 |avg loss 8.667 |avg tokens 3830.400 |tokens/s 8070.384 |walltime 11330.874 |
Transformer | epoch 0 | step 22450 |avg loss 8.687 |avg tokens 4079.700 |tokens/s 8229.667 |walltime 11335.831 |
Transformer | epoch 0 | step 22460 |avg loss 8.571 |avg tokens 4654.400 |tokens/s 9143.905 |walltime 11340.921 |
Transformer | epoch 0 | step 22470 |avg loss 8.482 |avg tokens 4244.800 |tokens/s 8498.717 |walltime 11345.916 |
Transformer | epoch 0 | step 22480 |avg loss 8.651 |avg tokens 4592.400 |tokens/s 9350.659 |walltime 11350.827 |
Transformer | epoch 0 | step 22490 |avg loss 8.337 |avg tokens 4653.600 |tokens/s 9143.763 |walltime 11355.916 |
Transformer | epoch 0 | step 22500 |avg loss 8.432 |avg tokens 4796.800 |tokens/s 9068.857 |walltime 11361.206 |
Transformer | epoch 0 | step 22510 |avg loss 8.406 |avg tokens 4654.300 |tokens/s 9084.209 |walltime 11366.329 |
Transformer | epoch 0 | step 22520 |avg loss 8.430 |avg tokens 4915.500 |tokens/s 9375.049 |walltime 11371.572 |
Transformer | epoch 0 | step 22530 |avg loss 8.667 |avg tokens 4495.900 |tokens/s 8999.199 |walltime 11376.568 |
Transformer | epoch 0 | step 22540 |avg loss 8.671 |avg tokens 4323.500 |tokens/s 8831.473 |walltime 11381.464 |
Transformer | epoch 0 | step 22550 |avg loss 8.416 |avg tokens 4557.100 |tokens/s 8809.372 |walltime 11386.637 |
Transformer | epoch 0 | step 22560 |avg loss 8.792 |avg tokens 3786.300 |tokens/s 8239.040 |walltime 11391.232 |
Transformer | epoch 0 | step 22570 |avg loss 8.793 |avg tokens 4057.000 |tokens/s 8677.279 |walltime 11395.908 |
Transformer | epoch 0 | step 22580 |avg loss 8.539 |avg tokens 4655.300 |tokens/s 8800.726 |walltime 11401.198 |
Transformer | epoch 0 | step 22590 |avg loss 8.713 |avg tokens 4337.200 |tokens/s 8979.265 |walltime 11406.028 |
Transformer | epoch 0 | step 22600 |avg loss 8.572 |avg tokens 4697.900 |tokens/s 9064.497 |walltime 11411.211 |
Transformer | epoch 0 | step 22610 |avg loss 8.756 |avg tokens 4759.400 |tokens/s 9427.634 |walltime 11416.259 |
Transformer | epoch 0 | step 22620 |avg loss 8.623 |avg tokens 4244.500 |tokens/s 8682.064 |walltime 11421.148 |
Transformer | epoch 0 | step 22630 |avg loss 8.735 |avg tokens 4580.700 |tokens/s 9120.450 |walltime 11426.170 |
Transformer | epoch 0 | step 22640 |avg loss 8.427 |avg tokens 4712.000 |tokens/s 9191.980 |walltime 11431.296 |
Transformer | epoch 0 | step 22650 |avg loss 8.477 |avg tokens 4740.600 |tokens/s 9096.989 |walltime 11436.508 |
Transformer | epoch 0 | step 22660 |avg loss 8.583 |avg tokens 3819.600 |tokens/s 8124.001 |walltime 11441.209 |
Transformer | epoch 0 | step 22670 |avg loss 8.802 |avg tokens 4516.400 |tokens/s 9071.433 |walltime 11446.188 |
Transformer | epoch 0 | step 22680 |avg loss 8.445 |avg tokens 4925.900 |tokens/s 9690.377 |walltime 11451.271 |
Transformer | epoch 0 | step 22690 |avg loss 8.550 |avg tokens 4271.500 |tokens/s 8524.483 |walltime 11456.282 |
Transformer | epoch 0 | step 22700 |avg loss 8.667 |avg tokens 4671.100 |tokens/s 9153.682 |walltime 11461.385 |
Transformer | epoch 0 | step 22710 |avg loss 8.273 |avg tokens 4901.600 |tokens/s 9168.459 |walltime 11466.731 |
Transformer | epoch 0 | step 22720 |avg loss 8.300 |avg tokens 4153.300 |tokens/s 8544.218 |walltime 11471.592 |
Transformer | epoch 0 | step 22730 |avg loss 8.366 |avg tokens 4589.300 |tokens/s 8915.021 |walltime 11476.740 |
Transformer | epoch 0 | step 22740 |avg loss 8.630 |avg tokens 4317.500 |tokens/s 8688.261 |walltime 11481.709 |
Transformer | epoch 0 | step 22750 |avg loss 8.656 |avg tokens 4283.200 |tokens/s 8397.391 |walltime 11486.810 |
Transformer | epoch 0 | step 22760 |avg loss 8.379 |avg tokens 4851.200 |tokens/s 9125.761 |walltime 11492.126 |
Transformer | epoch 0 | step 22770 |avg loss 8.554 |avg tokens 4566.300 |tokens/s 9093.073 |walltime 11497.148 |
Transformer | epoch 0 | step 22780 |avg loss 8.363 |avg tokens 4556.000 |tokens/s 8877.057 |walltime 11502.280 |
Transformer | epoch 0 | step 22790 |avg loss 8.641 |avg tokens 4336.300 |tokens/s 9102.109 |walltime 11507.044 |
Transformer | epoch 0 | step 22800 |avg loss 8.709 |avg tokens 4287.500 |tokens/s 8870.268 |walltime 11511.878 |
Transformer | epoch 0 | step 22810 |avg loss 8.436 |avg tokens 4431.700 |tokens/s 8963.420 |walltime 11516.822 |
Transformer | epoch 0 | step 22820 |avg loss 8.238 |avg tokens 4925.600 |tokens/s 9443.507 |walltime 11522.038 |
Transformer | epoch 0 | step 22830 |avg loss 8.494 |avg tokens 4109.300 |tokens/s 8471.313 |walltime 11526.889 |
Transformer | epoch 0 | step 22840 |avg loss 8.551 |avg tokens 4541.900 |tokens/s 8942.641 |walltime 11531.967 |
Transformer | epoch 0 | step 22850 |avg loss 8.352 |avg tokens 4847.200 |tokens/s 9307.929 |walltime 11537.175 |
Transformer | epoch 0 | step 22860 |avg loss 8.598 |avg tokens 4887.900 |tokens/s 9708.525 |walltime 11542.210 |
Transformer | epoch 0 | step 22870 |avg loss 8.391 |avg tokens 4887.300 |tokens/s 9285.468 |walltime 11547.473 |
Transformer | epoch 0 | step 22880 |avg loss 8.423 |avg tokens 4400.100 |tokens/s 8831.476 |walltime 11552.455 |
Transformer | epoch 0 | step 22890 |avg loss 8.560 |avg tokens 4180.800 |tokens/s 8595.971 |walltime 11557.319 |
Transformer | epoch 0 | step 22900 |avg loss 8.328 |avg tokens 4490.500 |tokens/s 8885.509 |walltime 11562.373 |
Transformer | epoch 0 | step 22910 |avg loss 8.340 |avg tokens 4359.000 |tokens/s 8824.729 |walltime 11567.312 |
Transformer | epoch 0 | step 22920 |avg loss 8.383 |avg tokens 4698.100 |tokens/s 9186.865 |walltime 11572.426 |
Transformer | epoch 0 | step 22930 |avg loss 8.398 |avg tokens 4818.900 |tokens/s 9127.171 |walltime 11577.706 |
Transformer | epoch 0 | step 22940 |avg loss 8.561 |avg tokens 4847.700 |tokens/s 9461.244 |walltime 11582.830 |
Transformer | epoch 0 | step 22950 |avg loss 8.403 |avg tokens 4820.300 |tokens/s 9157.487 |walltime 11588.094 |
Transformer | epoch 0 | step 22960 |avg loss 8.666 |avg tokens 4381.700 |tokens/s 9051.712 |walltime 11592.934 |
Transformer | epoch 0 | step 22970 |avg loss 8.573 |avg tokens 4347.400 |tokens/s 8684.218 |walltime 11597.940 |
Transformer | epoch 0 | step 22980 |avg loss 8.578 |avg tokens 4248.400 |tokens/s 8525.845 |walltime 11602.923 |
Transformer | epoch 0 | step 22990 |avg loss 8.385 |avg tokens 4505.500 |tokens/s 9137.634 |walltime 11607.854 |
Transformer | epoch 0 | step 23000 |avg loss 8.685 |avg tokens 4311.300 |tokens/s 9081.350 |walltime 11612.601 |
Transformer | epoch 0 | step 23010 |avg loss 8.415 |avg tokens 4285.600 |tokens/s 8730.603 |walltime 11617.510 |
Transformer | epoch 0 | step 23020 |avg loss 8.763 |avg tokens 4184.000 |tokens/s 8732.958 |walltime 11622.301 |
Transformer | epoch 0 | step 23030 |avg loss 8.674 |avg tokens 4025.800 |tokens/s 8361.605 |walltime 11627.116 |
Transformer | epoch 0 | step 23040 |avg loss 8.693 |avg tokens 4336.300 |tokens/s 8824.306 |walltime 11632.030 |
Transformer | epoch 0 | step 23050 |avg loss 8.513 |avg tokens 4623.500 |tokens/s 9132.751 |walltime 11637.092 |
Transformer | epoch 0 | step 23060 |avg loss 8.415 |avg tokens 4705.300 |tokens/s 9026.528 |walltime 11642.305 |
Transformer | epoch 0 | step 23070 |avg loss 8.430 |avg tokens 4551.700 |tokens/s 8795.892 |walltime 11647.480 |
Transformer | epoch 0 | step 23080 |avg loss 8.427 |avg tokens 4768.700 |tokens/s 9128.916 |walltime 11652.704 |
Transformer | epoch 0 | step 23090 |avg loss 8.361 |avg tokens 4312.900 |tokens/s 8606.031 |walltime 11657.715 |
Transformer | epoch 0 | step 23100 |avg loss 8.094 |avg tokens 4707.300 |tokens/s 9010.858 |walltime 11662.939 |
Transformer | epoch 0 | step 23110 |avg loss 8.408 |avg tokens 4498.300 |tokens/s 8847.822 |walltime 11668.023 |
Transformer | epoch 0 | step 23120 |avg loss 8.457 |avg tokens 4648.400 |tokens/s 9077.697 |walltime 11673.144 |
Transformer | epoch 0 | step 23130 |avg loss 8.553 |avg tokens 4616.500 |tokens/s 8817.449 |walltime 11678.380 |
Transformer | epoch 0 | step 23140 |avg loss 8.658 |avg tokens 4856.200 |tokens/s 9646.107 |walltime 11683.414 |
Transformer | epoch 0 | step 23150 |avg loss 8.523 |avg tokens 4152.800 |tokens/s 8607.430 |walltime 11688.239 |
Transformer | epoch 0 | step 23160 |avg loss 8.680 |avg tokens 4484.100 |tokens/s 8858.878 |walltime 11693.300 |
Transformer | epoch 0 | step 23170 |avg loss 8.295 |avg tokens 4418.100 |tokens/s 8605.999 |walltime 11698.434 |
Transformer | epoch 0 | step 23180 |avg loss 8.503 |avg tokens 4790.400 |tokens/s 9232.981 |walltime 11703.623 |
Transformer | epoch 0 | step 23190 |avg loss 8.423 |avg tokens 4638.500 |tokens/s 8767.134 |walltime 11708.913 |
Transformer | epoch 0 | step 23200 |avg loss 8.722 |avg tokens 4336.400 |tokens/s 8867.322 |walltime 11713.804 |
Transformer | epoch 0 | step 23210 |avg loss 8.609 |avg tokens 4776.300 |tokens/s 9285.976 |walltime 11718.947 |
Transformer | epoch 0 | step 23220 |avg loss 8.641 |avg tokens 4165.200 |tokens/s 8567.705 |walltime 11723.809 |
Transformer | epoch 0 | step 23230 |avg loss 8.393 |avg tokens 4203.700 |tokens/s 8659.828 |walltime 11728.663 |
Transformer | epoch 0 | step 23240 |avg loss 8.633 |avg tokens 4403.200 |tokens/s 8693.412 |walltime 11733.728 |
Transformer | epoch 0 | step 23250 |avg loss 8.547 |avg tokens 3910.800 |tokens/s 8275.232 |walltime 11738.454 |
Transformer | epoch 0 | step 23260 |avg loss 8.739 |avg tokens 4121.300 |tokens/s 8864.644 |walltime 11743.103 |
Transformer | epoch 0 | step 23270 |avg loss 8.476 |avg tokens 4755.200 |tokens/s 9009.514 |walltime 11748.381 |
Transformer | epoch 0 | step 23280 |avg loss 8.674 |avg tokens 4435.300 |tokens/s 8685.968 |walltime 11753.487 |
Transformer | epoch 0 | step 23290 |avg loss 8.439 |avg tokens 4555.100 |tokens/s 8970.913 |walltime 11758.565 |
Transformer | epoch 0 | step 23300 |avg loss 8.453 |avg tokens 4508.800 |tokens/s 8883.811 |walltime 11763.640 |
Transformer | epoch 0 | step 23310 |avg loss 8.558 |avg tokens 4574.600 |tokens/s 9460.753 |walltime 11768.476 |
Transformer | epoch 0 | step 23320 |avg loss 8.417 |avg tokens 4694.700 |tokens/s 8900.962 |walltime 11773.750 |
Transformer | epoch 0 | step 23330 |avg loss 8.661 |avg tokens 4394.800 |tokens/s 8684.411 |walltime 11778.811 |
Transformer | epoch 0 | step 23340 |avg loss 8.557 |avg tokens 4970.200 |tokens/s 9386.074 |walltime 11784.106 |
Transformer | epoch 0 | step 23350 |avg loss 8.429 |avg tokens 4766.400 |tokens/s 9106.423 |walltime 11789.340 |
Transformer | epoch 0 | step 23360 |avg loss 8.475 |avg tokens 4831.200 |tokens/s 9178.830 |walltime 11794.603 |
Transformer | epoch 0 | step 23370 |avg loss 8.575 |avg tokens 4948.300 |tokens/s 9756.211 |walltime 11799.675 |
Transformer | epoch 0 | step 23380 |avg loss 8.620 |avg tokens 4474.600 |tokens/s 9181.810 |walltime 11804.549 |
Transformer | epoch 0 | step 23390 |avg loss 8.661 |avg tokens 4126.800 |tokens/s 8689.995 |walltime 11809.298 |
Transformer | epoch 0 | step 23400 |avg loss 8.461 |avg tokens 4533.400 |tokens/s 8743.868 |walltime 11814.482 |
Transformer | epoch 0 | step 23410 |avg loss 8.513 |avg tokens 4467.900 |tokens/s 8721.205 |walltime 11819.605 |
Transformer | epoch 0 | step 23420 |avg loss 8.339 |avg tokens 4707.000 |tokens/s 8967.203 |walltime 11824.854 |
Transformer | epoch 0 | step 23430 |avg loss 8.659 |avg tokens 4928.900 |tokens/s 9509.578 |walltime 11830.037 |
Transformer | epoch 0 | step 23440 |avg loss 8.167 |avg tokens 3999.500 |tokens/s 8656.084 |walltime 11834.658 |
Transformer | epoch 0 | step 23450 |avg loss 8.449 |avg tokens 4054.900 |tokens/s 8376.680 |walltime 11839.499 |
Transformer | epoch 0 | step 23460 |avg loss 8.413 |avg tokens 4422.000 |tokens/s 8870.593 |walltime 11844.484 |
Transformer | epoch 0 | step 23470 |avg loss 8.484 |avg tokens 4404.800 |tokens/s 8761.247 |walltime 11849.511 |
Transformer | epoch 0 | step 23480 |avg loss 8.516 |avg tokens 4660.400 |tokens/s 9073.216 |walltime 11854.648 |
Transformer | epoch 0 | step 23490 |avg loss 8.520 |avg tokens 4931.600 |tokens/s 9520.274 |walltime 11859.828 |
Transformer | epoch 0 | step 23500 |avg loss 8.439 |avg tokens 4398.900 |tokens/s 8618.955 |walltime 11864.932 |
Transformer | epoch 0 | step 23510 |avg loss 8.221 |avg tokens 4567.900 |tokens/s 8658.264 |walltime 11870.207 |
Transformer | epoch 0 | step 23520 |avg loss 8.616 |avg tokens 4264.100 |tokens/s 8862.534 |walltime 11875.019 |
Transformer | epoch 0 | step 23530 |avg loss 8.578 |avg tokens 4446.000 |tokens/s 8838.256 |walltime 11880.049 |
Transformer | epoch 0 | step 23540 |avg loss 8.790 |avg tokens 4702.000 |tokens/s 9323.216 |walltime 11885.092 |
Transformer | epoch 0 | step 23550 |avg loss 8.436 |avg tokens 4753.900 |tokens/s 9122.290 |walltime 11890.304 |
Transformer | epoch 0 | step 23560 |avg loss 8.497 |avg tokens 4466.600 |tokens/s 8796.436 |walltime 11895.381 |
Transformer | epoch 0 | step 23570 |avg loss 8.613 |avg tokens 4142.600 |tokens/s 8517.513 |walltime 11900.245 |
Transformer | epoch 0 | step 23580 |avg loss 8.780 |avg tokens 4578.800 |tokens/s 9008.624 |walltime 11905.328 |
Transformer | epoch 0 | step 23590 |avg loss 8.664 |avg tokens 4593.700 |tokens/s 9074.790 |walltime 11910.390 |
Transformer | epoch 0 | step 23600 |avg loss 8.574 |avg tokens 4675.700 |tokens/s 8697.069 |walltime 11915.766 |
Transformer | epoch 0 | step 23610 |avg loss 8.558 |avg tokens 4590.000 |tokens/s 9100.734 |walltime 11920.810 |
Transformer | epoch 0 | step 23620 |avg loss 8.201 |avg tokens 4106.900 |tokens/s 8359.926 |walltime 11925.722 |
Transformer | epoch 0 | step 23630 |avg loss 8.326 |avg tokens 4730.800 |tokens/s 8989.871 |walltime 11930.985 |
Transformer | epoch 0 | step 23640 |avg loss 8.646 |avg tokens 4034.800 |tokens/s 8173.852 |walltime 11935.921 |
Transformer | epoch 0 | step 23650 |avg loss 8.571 |avg tokens 4812.900 |tokens/s 9066.456 |walltime 11941.229 |
Transformer | epoch 0 | step 23660 |avg loss 8.420 |avg tokens 4796.000 |tokens/s 9331.226 |walltime 11946.369 |
Transformer | epoch 0 | step 23670 |avg loss 8.441 |avg tokens 4988.000 |tokens/s 9472.149 |walltime 11951.635 |
Transformer | epoch 0 | step 23680 |avg loss 8.698 |avg tokens 4420.100 |tokens/s 8760.452 |walltime 11956.680 |
Transformer | epoch 0 | step 23690 |avg loss 8.424 |avg tokens 4718.100 |tokens/s 9181.773 |walltime 11961.819 |
Transformer | epoch 0 | step 23700 |avg loss 8.593 |avg tokens 4220.200 |tokens/s 8694.988 |walltime 11966.673 |
Transformer | epoch 0 | step 23710 |avg loss 8.387 |avg tokens 4405.500 |tokens/s 8536.011 |walltime 11971.834 |
Transformer | epoch 0 | step 23720 |avg loss 8.444 |avg tokens 4832.600 |tokens/s 9380.559 |walltime 11976.985 |
Transformer | epoch 0 | step 23730 |avg loss 8.434 |avg tokens 4923.900 |tokens/s 9237.250 |walltime 11982.316 |
Transformer | epoch 0 | step 23740 |avg loss 8.572 |avg tokens 4630.800 |tokens/s 8994.613 |walltime 11987.464 |
Transformer | epoch 0 | step 23750 |avg loss 8.437 |avg tokens 4435.700 |tokens/s 8808.786 |walltime 11992.500 |
Transformer | epoch 0 | step 23760 |avg loss 8.805 |avg tokens 4303.400 |tokens/s 9059.220 |walltime 11997.250 |
Transformer | epoch 0 | step 23770 |avg loss 8.545 |avg tokens 4915.600 |tokens/s 9522.545 |walltime 12002.412 |
Transformer | epoch 0 | step 23780 |avg loss 8.482 |avg tokens 4739.300 |tokens/s 9106.045 |walltime 12007.617 |
Transformer | epoch 0 | step 23790 |avg loss 8.702 |avg tokens 4237.200 |tokens/s 8522.568 |walltime 12012.589 |
Transformer | epoch 0 | step 23800 |avg loss 8.626 |avg tokens 4592.500 |tokens/s 9075.226 |walltime 12017.649 |
Transformer | epoch 0 | step 23810 |avg loss 8.542 |avg tokens 4113.400 |tokens/s 8437.296 |walltime 12022.524 |
Transformer | epoch 0 | step 23820 |avg loss 8.589 |avg tokens 4189.700 |tokens/s 8690.657 |walltime 12027.345 |
Transformer | epoch 0 | step 23830 |avg loss 8.881 |avg tokens 3828.200 |tokens/s 8660.357 |walltime 12031.766 |
Transformer | epoch 0 | step 23840 |avg loss 8.848 |avg tokens 4560.800 |tokens/s 9221.990 |walltime 12036.711 |
Transformer | epoch 0 | step 23850 |avg loss 8.635 |avg tokens 4630.500 |tokens/s 9128.628 |walltime 12041.784 |
Transformer | epoch 0 | step 23860 |avg loss 8.472 |avg tokens 4922.600 |tokens/s 9315.810 |walltime 12047.068 |
Transformer | epoch 0 | step 23870 |avg loss 8.350 |avg tokens 4789.600 |tokens/s 9236.481 |walltime 12052.253 |
Transformer | epoch 0 | step 23880 |avg loss 8.437 |avg tokens 4608.400 |tokens/s 9079.160 |walltime 12057.329 |
Transformer | epoch 0 | step 23890 |avg loss 8.267 |avg tokens 4340.500 |tokens/s 8654.125 |walltime 12062.345 |
Transformer | epoch 0 | step 23900 |avg loss 8.757 |avg tokens 4097.100 |tokens/s 8380.742 |walltime 12067.233 |
Transformer | epoch 0 | step 23910 |avg loss 8.640 |avg tokens 4427.100 |tokens/s 8954.318 |walltime 12072.177 |
Transformer | epoch 0 | step 23920 |avg loss 8.691 |avg tokens 4467.100 |tokens/s 9155.830 |walltime 12077.056 |
Transformer | epoch 0 | step 23930 |avg loss 8.470 |avg tokens 4654.100 |tokens/s 8868.776 |walltime 12082.304 |
Transformer | epoch 0 | step 23940 |avg loss 9.026 |avg tokens 3670.900 |tokens/s 8350.910 |walltime 12086.700 |
Transformer | epoch 0 | step 23950 |avg loss 8.370 |avg tokens 4466.500 |tokens/s 8680.692 |walltime 12091.845 |
Transformer | epoch 0 | step 23960 |avg loss 8.629 |avg tokens 4608.400 |tokens/s 9094.395 |walltime 12096.913 |
Transformer | epoch 0 | step 23970 |avg loss 8.338 |avg tokens 3823.500 |tokens/s 7934.353 |walltime 12101.732 |
Transformer | epoch 0 | step 23980 |avg loss 8.638 |avg tokens 4299.400 |tokens/s 8751.058 |walltime 12106.645 |
Transformer | epoch 0 | step 23990 |avg loss 8.607 |avg tokens 4550.800 |tokens/s 9095.396 |walltime 12111.648 |
Transformer | epoch 0 | step 24000 |avg loss 8.738 |avg tokens 4470.700 |tokens/s 9091.611 |walltime 12116.565 |
Transformer | epoch 0 | step 24010 |avg loss 8.513 |avg tokens 4544.400 |tokens/s 8901.790 |walltime 12121.670 |
Transformer | epoch 0 | step 24020 |avg loss 8.310 |avg tokens 4230.400 |tokens/s 8657.607 |walltime 12126.557 |
Transformer | epoch 0 | step 24030 |avg loss 8.423 |avg tokens 4884.800 |tokens/s 9095.455 |walltime 12131.927 |
Transformer | epoch 0 | step 24040 |avg loss 8.670 |avg tokens 4625.700 |tokens/s 9325.479 |walltime 12136.888 |
Transformer | epoch 0 | step 24050 |avg loss 8.488 |avg tokens 4369.500 |tokens/s 8789.689 |walltime 12141.859 |
Transformer | epoch 0 | step 24060 |avg loss 8.354 |avg tokens 4814.100 |tokens/s 9299.357 |walltime 12147.036 |
Transformer | epoch 0 | step 24070 |avg loss 8.427 |avg tokens 4556.900 |tokens/s 8966.862 |walltime 12152.118 |
Transformer | epoch 0 | step 24080 |avg loss 8.643 |avg tokens 4312.400 |tokens/s 8538.515 |walltime 12157.168 |
Transformer | epoch 0 | step 24090 |avg loss 8.541 |avg tokens 4645.300 |tokens/s 9051.816 |walltime 12162.300 |
Transformer | epoch 0 | step 24100 |avg loss 8.570 |avg tokens 4602.600 |tokens/s 9238.530 |walltime 12167.282 |
Transformer | epoch 0 | step 24110 |avg loss 8.532 |avg tokens 4426.700 |tokens/s 8107.140 |walltime 12172.742 |
Transformer | epoch 0 | step 24120 |avg loss 8.332 |avg tokens 4714.900 |tokens/s 9260.089 |walltime 12177.834 |
Transformer | epoch 0 | step 24130 |avg loss 8.737 |avg tokens 4632.400 |tokens/s 9331.548 |walltime 12182.798 |
Transformer | epoch 0 | step 24140 |avg loss 8.565 |avg tokens 4149.700 |tokens/s 8511.526 |walltime 12187.673 |
Transformer | epoch 0 | step 24150 |avg loss 8.239 |avg tokens 4505.500 |tokens/s 8789.714 |walltime 12192.799 |
Transformer | epoch 0 | step 24160 |avg loss 8.437 |avg tokens 4662.400 |tokens/s 9212.858 |walltime 12197.860 |
Transformer | epoch 0 | step 24170 |avg loss 8.481 |avg tokens 4424.100 |tokens/s 9154.971 |walltime 12202.693 |
Transformer | epoch 0 | step 24180 |avg loss 8.793 |avg tokens 4028.900 |tokens/s 8769.663 |walltime 12207.287 |
Transformer | epoch 0 | step 24190 |avg loss 8.803 |avg tokens 4380.600 |tokens/s 8611.761 |walltime 12212.373 |
Transformer | epoch 0 | step 24200 |avg loss 8.258 |avg tokens 4539.200 |tokens/s 8809.030 |walltime 12217.526 |
Transformer | epoch 0 | step 24210 |avg loss 8.391 |avg tokens 4745.400 |tokens/s 8997.002 |walltime 12222.801 |
Transformer | epoch 0 | step 24220 |avg loss 8.521 |avg tokens 4635.500 |tokens/s 9306.908 |walltime 12227.781 |
Transformer | epoch 0 | step 24230 |avg loss 8.300 |avg tokens 4801.000 |tokens/s 9114.949 |walltime 12233.049 |
Transformer | epoch 0 | step 24240 |avg loss 8.300 |avg tokens 4771.800 |tokens/s 9215.185 |walltime 12238.227 |
Transformer | epoch 0 | step 24250 |avg loss 8.792 |avg tokens 4426.400 |tokens/s 8828.580 |walltime 12243.241 |
Transformer | epoch 0 | step 24260 |avg loss 8.479 |avg tokens 4613.500 |tokens/s 9297.548 |walltime 12248.203 |
Transformer | epoch 0 | step 24270 |avg loss 8.683 |avg tokens 4594.000 |tokens/s 8918.901 |walltime 12253.354 |
Transformer | epoch 0 | step 24280 |avg loss 8.678 |avg tokens 4240.500 |tokens/s 8739.532 |walltime 12258.206 |
Transformer | epoch 0 | step 24290 |avg loss 8.319 |avg tokens 4393.600 |tokens/s 8561.734 |walltime 12263.337 |
Transformer | epoch 0 | step 24300 |avg loss 8.463 |avg tokens 4059.700 |tokens/s 8263.592 |walltime 12268.250 |
Transformer | epoch 0 | step 24310 |avg loss 8.646 |avg tokens 4268.100 |tokens/s 8898.691 |walltime 12273.046 |
Transformer | epoch 0 | step 24320 |avg loss 8.274 |avg tokens 4854.400 |tokens/s 9503.314 |walltime 12278.154 |
Transformer | epoch 0 | step 24330 |avg loss 8.367 |avg tokens 4623.000 |tokens/s 9133.924 |walltime 12283.216 |
Transformer | epoch 0 | step 24340 |avg loss 8.398 |avg tokens 4057.600 |tokens/s 8245.341 |walltime 12288.137 |
Transformer | epoch 0 | step 24350 |avg loss 8.669 |avg tokens 4735.500 |tokens/s 9696.498 |walltime 12293.021 |
Transformer | epoch 0 | step 24360 |avg loss 8.323 |avg tokens 4776.900 |tokens/s 9013.009 |walltime 12298.321 |
Transformer | epoch 0 | step 24370 |avg loss 8.595 |avg tokens 4128.000 |tokens/s 8773.346 |walltime 12303.026 |
Transformer | epoch 0 | step 24380 |avg loss 8.403 |avg tokens 4683.100 |tokens/s 9161.213 |walltime 12308.138 |
Transformer | epoch 0 | step 24390 |avg loss 8.186 |avg tokens 4669.900 |tokens/s 9084.221 |walltime 12313.278 |
Transformer | epoch 0 | step 24400 |avg loss 8.446 |avg tokens 4408.900 |tokens/s 8719.980 |walltime 12318.334 |
Transformer | epoch 0 | step 24410 |avg loss 8.383 |avg tokens 4542.000 |tokens/s 8890.636 |walltime 12323.443 |
Transformer | epoch 0 | step 24420 |avg loss 8.443 |avg tokens 4522.400 |tokens/s 8709.700 |walltime 12328.636 |
Transformer | epoch 0 | step 24430 |avg loss 8.699 |avg tokens 4861.500 |tokens/s 9727.794 |walltime 12333.633 |
Transformer | epoch 0 | step 24440 |avg loss 8.485 |avg tokens 4205.400 |tokens/s 8804.211 |walltime 12338.410 |
Transformer | epoch 0 | step 24450 |avg loss 8.609 |avg tokens 3792.000 |tokens/s 8110.544 |walltime 12343.085 |
Transformer | epoch 0 | step 24460 |avg loss 8.356 |avg tokens 4863.400 |tokens/s 9376.826 |walltime 12348.272 |
Transformer | epoch 0 | step 24470 |avg loss 8.456 |avg tokens 4356.000 |tokens/s 8612.884 |walltime 12353.329 |
Transformer | epoch 0 | step 24480 |avg loss 8.304 |avg tokens 4812.800 |tokens/s 9243.373 |walltime 12358.536 |
Transformer | epoch 0 | step 24490 |avg loss 8.722 |avg tokens 4370.800 |tokens/s 8923.787 |walltime 12363.434 |
Transformer | epoch 0 | step 24500 |avg loss 8.505 |avg tokens 4727.900 |tokens/s 9340.573 |walltime 12368.496 |
Transformer | epoch 0 | step 24510 |avg loss 8.466 |avg tokens 4805.300 |tokens/s 9371.684 |walltime 12373.623 |
Transformer | epoch 0 | step 24520 |avg loss 8.593 |avg tokens 4404.000 |tokens/s 8881.408 |walltime 12378.582 |
Transformer | epoch 0 | step 24530 |avg loss 8.669 |avg tokens 4546.100 |tokens/s 9191.457 |walltime 12383.528 |
Transformer | epoch 0 | step 24540 |avg loss 8.702 |avg tokens 4323.300 |tokens/s 8450.845 |walltime 12388.644 |
Transformer | epoch 0 | step 24550 |avg loss 8.340 |avg tokens 4640.800 |tokens/s 8964.947 |walltime 12393.820 |
Transformer | epoch 0 | step 24560 |avg loss 8.542 |avg tokens 4436.300 |tokens/s 8890.646 |walltime 12398.810 |
Transformer | epoch 0 | step 24570 |avg loss 8.498 |avg tokens 4861.500 |tokens/s 9253.939 |walltime 12404.064 |
Transformer | epoch 0 | step 24580 |avg loss 8.472 |avg tokens 4500.300 |tokens/s 8887.077 |walltime 12409.127 |
Transformer | epoch 0 | step 24590 |avg loss 8.439 |avg tokens 4549.400 |tokens/s 9095.546 |walltime 12414.129 |
Transformer | epoch 0 | step 24600 |avg loss 8.769 |avg tokens 4211.700 |tokens/s 8944.361 |walltime 12418.838 |
Transformer | epoch 0 | step 24610 |avg loss 8.408 |avg tokens 4508.100 |tokens/s 9144.542 |walltime 12423.768 |
Transformer | epoch 0 | step 24620 |avg loss 8.308 |avg tokens 4876.700 |tokens/s 9261.331 |walltime 12429.033 |
Transformer | epoch 0 | step 24630 |avg loss 8.228 |avg tokens 4522.600 |tokens/s 8967.413 |walltime 12434.077 |
Transformer | epoch 0 | step 24640 |avg loss 8.421 |avg tokens 4410.200 |tokens/s 8662.129 |walltime 12439.168 |
Transformer | epoch 0 | step 24650 |avg loss 8.504 |avg tokens 3819.700 |tokens/s 8068.846 |walltime 12443.902 |
Transformer | epoch 0 | step 24660 |avg loss 8.203 |avg tokens 4449.000 |tokens/s 8697.190 |walltime 12449.018 |
Transformer | epoch 0 | step 24670 |avg loss 8.443 |avg tokens 4304.300 |tokens/s 8644.819 |walltime 12453.997 |
Transformer | epoch 0 | step 24680 |avg loss 8.603 |avg tokens 4591.400 |tokens/s 9142.955 |walltime 12459.018 |
Transformer | epoch 0 | step 24690 |avg loss 8.465 |avg tokens 4438.100 |tokens/s 8809.248 |walltime 12464.056 |
Transformer | epoch 0 | step 24700 |avg loss 8.682 |avg tokens 4458.900 |tokens/s 8996.007 |walltime 12469.013 |
Transformer | epoch 0 | step 24710 |avg loss 8.473 |avg tokens 4735.700 |tokens/s 9079.696 |walltime 12474.229 |
Transformer | epoch 0 | step 24720 |avg loss 8.584 |avg tokens 4403.300 |tokens/s 8761.895 |walltime 12479.254 |
Transformer | epoch 0 | step 24730 |avg loss 8.511 |avg tokens 4461.600 |tokens/s 8757.644 |walltime 12484.349 |
Transformer | epoch 0 | step 24740 |avg loss 8.548 |avg tokens 4570.600 |tokens/s 8876.376 |walltime 12489.498 |
Transformer | epoch 0 | step 24750 |avg loss 8.405 |avg tokens 4620.600 |tokens/s 9097.711 |walltime 12494.577 |
Transformer | epoch 0 | step 24760 |avg loss 8.463 |avg tokens 4796.700 |tokens/s 9269.648 |walltime 12499.751 |
Transformer | epoch 0 | step 24770 |avg loss 8.355 |avg tokens 4979.600 |tokens/s 9401.625 |walltime 12505.048 |
Transformer | epoch 0 | step 24780 |avg loss 8.430 |avg tokens 4803.700 |tokens/s 9061.196 |walltime 12510.349 |
Transformer | epoch 0 | step 24790 |avg loss 8.756 |avg tokens 4295.700 |tokens/s 8680.863 |walltime 12515.298 |
Transformer | epoch 0 | step 24800 |avg loss 8.658 |avg tokens 4229.000 |tokens/s 8486.077 |walltime 12520.281 |
Transformer | epoch 0 | step 24810 |avg loss 8.394 |avg tokens 4481.600 |tokens/s 8919.284 |walltime 12525.306 |
Transformer | epoch 0 | step 24820 |avg loss 8.483 |avg tokens 4530.400 |tokens/s 8876.390 |walltime 12530.410 |
Transformer | epoch 0 | step 24830 |avg loss 8.544 |avg tokens 4421.900 |tokens/s 8886.328 |walltime 12535.386 |
Transformer | epoch 0 | step 24840 |avg loss 8.562 |avg tokens 4439.200 |tokens/s 8917.523 |walltime 12540.364 |
Transformer | epoch 0 | step 24850 |avg loss 8.683 |avg tokens 4680.900 |tokens/s 9365.788 |walltime 12545.362 |
Transformer | epoch 0 | step 24860 |avg loss 8.460 |avg tokens 4464.500 |tokens/s 8875.024 |walltime 12550.392 |
Transformer | epoch 0 | step 24870 |avg loss 8.560 |avg tokens 4709.700 |tokens/s 9444.805 |walltime 12555.379 |
Transformer | epoch 0 | step 24880 |avg loss 8.434 |avg tokens 4634.200 |tokens/s 9078.706 |walltime 12560.483 |
Transformer | epoch 0 | step 24890 |avg loss 8.135 |avg tokens 4775.900 |tokens/s 9114.451 |walltime 12565.723 |
Transformer | epoch 0 | step 24900 |avg loss 8.489 |avg tokens 4684.500 |tokens/s 8898.700 |walltime 12570.987 |
Transformer | epoch 0 | step 24910 |avg loss 8.462 |avg tokens 4804.900 |tokens/s 9028.154 |walltime 12576.309 |
Transformer | epoch 0 | step 24920 |avg loss 8.404 |avg tokens 4672.200 |tokens/s 9129.278 |walltime 12581.427 |
Transformer | epoch 0 | step 24930 |avg loss 8.624 |avg tokens 4401.100 |tokens/s 8522.399 |walltime 12586.591 |
Transformer | epoch 0 | step 24940 |avg loss 8.518 |avg tokens 4753.800 |tokens/s 9347.413 |walltime 12591.677 |
Transformer | epoch 0 | step 24950 |avg loss 8.440 |avg tokens 4439.200 |tokens/s 8955.359 |walltime 12596.634 |
Transformer | epoch 0 | step 24960 |avg loss 8.406 |avg tokens 4265.200 |tokens/s 8587.493 |walltime 12601.601 |
Transformer | epoch 0 | step 24970 |avg loss 8.534 |avg tokens 4837.100 |tokens/s 9313.648 |walltime 12606.794 |
Transformer | epoch 0 | step 24980 |avg loss 8.787 |avg tokens 4843.100 |tokens/s 10139.188 |walltime 12611.571 |
Transformer | epoch 0 | step 24990 |avg loss 8.518 |avg tokens 4830.900 |tokens/s 9386.514 |walltime 12616.718 |
Transformer | epoch 0 | step 25000 |avg loss 8.437 |avg tokens 4557.700 |tokens/s 9014.602 |walltime 12621.774 |
Transformer | epoch 0 | step 25010 |avg loss 8.457 |avg tokens 4517.600 |tokens/s 9006.544 |walltime 12626.790 |
Transformer | epoch 0 | step 25020 |avg loss 8.330 |avg tokens 4844.400 |tokens/s 9264.124 |walltime 12632.019 |
Transformer | epoch 0 | step 25030 |avg loss 8.588 |avg tokens 4524.400 |tokens/s 8872.063 |walltime 12637.118 |
Transformer | epoch 0 | step 25040 |avg loss 8.689 |avg tokens 4007.300 |tokens/s 8348.215 |walltime 12641.919 |
Transformer | epoch 0 | step 25050 |avg loss 8.626 |avg tokens 4662.400 |tokens/s 9140.508 |walltime 12647.019 |
Transformer | epoch 0 | step 25060 |avg loss 8.454 |avg tokens 4164.000 |tokens/s 8725.194 |walltime 12651.792 |
Transformer | epoch 0 | step 25070 |avg loss 8.465 |avg tokens 4739.300 |tokens/s 9387.009 |walltime 12656.841 |
Transformer | epoch 0 | step 25080 |avg loss 8.862 |avg tokens 4430.200 |tokens/s 9712.055 |walltime 12661.402 |
Transformer | epoch 0 | step 25090 |avg loss 8.743 |avg tokens 3667.600 |tokens/s 7978.476 |walltime 12665.999 |
Transformer | epoch 0 | step 25100 |avg loss 8.413 |avg tokens 4125.400 |tokens/s 8361.053 |walltime 12670.933 |
Transformer | epoch 0 | step 25110 |avg loss 8.510 |avg tokens 4558.200 |tokens/s 8981.993 |walltime 12676.008 |
Transformer | epoch 0 | step 25120 |avg loss 8.558 |avg tokens 4436.000 |tokens/s 8916.499 |walltime 12680.983 |
Transformer | epoch 0 | step 25130 |avg loss 8.958 |avg tokens 4930.400 |tokens/s 10231.884 |walltime 12685.802 |
Transformer | epoch 0 | step 25140 |avg loss 8.586 |avg tokens 4526.900 |tokens/s 8861.950 |walltime 12690.910 |
Transformer | epoch 0 | step 25150 |avg loss 8.663 |avg tokens 3991.300 |tokens/s 8378.957 |walltime 12695.673 |
Transformer | epoch 0 | step 25160 |avg loss 8.502 |avg tokens 4279.500 |tokens/s 8621.811 |walltime 12700.637 |
Transformer | epoch 0 | step 25170 |avg loss 8.631 |avg tokens 4781.500 |tokens/s 9574.079 |walltime 12705.631 |
Transformer | epoch 0 | step 25180 |avg loss 8.431 |avg tokens 4757.600 |tokens/s 9148.841 |walltime 12710.831 |
Transformer | epoch 0 | step 25190 |avg loss 8.882 |avg tokens 4627.800 |tokens/s 9815.063 |walltime 12715.546 |
Transformer | epoch 0 | step 25200 |avg loss 8.366 |avg tokens 4258.400 |tokens/s 8445.349 |walltime 12720.589 |
Transformer | epoch 0 | step 25210 |avg loss 8.266 |avg tokens 4871.200 |tokens/s 9472.929 |walltime 12725.731 |
Transformer | epoch 0 | step 25220 |avg loss 8.639 |avg tokens 4300.300 |tokens/s 8614.307 |walltime 12730.723 |
Transformer | epoch 0 | step 25230 |avg loss 8.860 |avg tokens 4319.100 |tokens/s 8979.893 |walltime 12735.533 |
Transformer | epoch 0 | step 25240 |avg loss 8.433 |avg tokens 4603.600 |tokens/s 8839.143 |walltime 12740.741 |
Transformer | epoch 0 | step 25250 |avg loss 8.452 |avg tokens 4915.200 |tokens/s 9244.117 |walltime 12746.058 |
Transformer | epoch 0 | step 25260 |avg loss 8.414 |avg tokens 4599.200 |tokens/s 8792.217 |walltime 12751.289 |
Transformer | epoch 0 | step 25270 |avg loss 8.719 |avg tokens 4558.900 |tokens/s 9373.206 |walltime 12756.153 |
Transformer | epoch 0 | step 25280 |avg loss 8.572 |avg tokens 4131.000 |tokens/s 8464.015 |walltime 12761.033 |
Transformer | epoch 0 | step 25290 |avg loss 8.641 |avg tokens 4359.200 |tokens/s 8711.386 |walltime 12766.037 |
Transformer | epoch 0 | step 25300 |avg loss 8.513 |avg tokens 4624.100 |tokens/s 9179.147 |walltime 12771.075 |
Transformer | epoch 0 | step 25310 |avg loss 8.599 |avg tokens 4722.100 |tokens/s 9224.305 |walltime 12776.194 |
Transformer | epoch 0 | step 25320 |avg loss 8.407 |avg tokens 4533.400 |tokens/s 8734.907 |walltime 12781.384 |
Transformer | epoch 0 | step 25330 |avg loss 8.671 |avg tokens 4585.700 |tokens/s 8838.214 |walltime 12786.573 |
Transformer | epoch 0 | step 25340 |avg loss 8.793 |avg tokens 3884.900 |tokens/s 8406.041 |walltime 12791.194 |
Transformer | epoch 0 | step 25350 |avg loss 8.603 |avg tokens 4475.800 |tokens/s 9243.240 |walltime 12796.037 |
Transformer | epoch 0 | step 25360 |avg loss 8.360 |avg tokens 4821.500 |tokens/s 9001.564 |walltime 12801.393 |
Transformer | epoch 0 | step 25370 |avg loss 8.491 |avg tokens 4203.800 |tokens/s 8450.378 |walltime 12806.368 |
Transformer | epoch 0 | step 25380 |avg loss 8.487 |avg tokens 4555.700 |tokens/s 8852.332 |walltime 12811.514 |
Transformer | epoch 0 | step 25390 |avg loss 8.684 |avg tokens 4758.400 |tokens/s 9624.052 |walltime 12816.458 |
Transformer | epoch 0 | step 25400 |avg loss 8.342 |avg tokens 4794.600 |tokens/s 9418.072 |walltime 12821.549 |
Transformer | epoch 0 | step 25410 |avg loss 8.807 |avg tokens 4508.600 |tokens/s 9402.536 |walltime 12826.344 |
Transformer | epoch 0 | step 25420 |avg loss 8.510 |avg tokens 4711.300 |tokens/s 8897.277 |walltime 12831.639 |
Transformer | epoch 0 | step 25430 |avg loss 8.691 |avg tokens 4128.700 |tokens/s 8472.603 |walltime 12836.512 |
Transformer | epoch 0 | step 25440 |avg loss 8.727 |avg tokens 4371.500 |tokens/s 8989.105 |walltime 12841.375 |
Transformer | epoch 0 | step 25450 |avg loss 8.493 |avg tokens 4219.100 |tokens/s 8465.600 |walltime 12846.359 |
Transformer | epoch 0 | step 25460 |avg loss 8.610 |avg tokens 4242.300 |tokens/s 8738.467 |walltime 12851.214 |
Transformer | epoch 0 | step 25470 |avg loss 8.435 |avg tokens 4502.000 |tokens/s 8978.114 |walltime 12856.228 |
Transformer | epoch 0 | step 25480 |avg loss 8.345 |avg tokens 4385.300 |tokens/s 8723.891 |walltime 12861.255 |
Transformer | epoch 0 | step 25490 |avg loss 8.462 |avg tokens 4399.300 |tokens/s 8918.599 |walltime 12866.188 |
Transformer | epoch 0 | step 25500 |avg loss 8.556 |avg tokens 4671.300 |tokens/s 9197.752 |walltime 12871.267 |
Transformer | epoch 0 | step 25510 |avg loss 8.782 |avg tokens 4085.100 |tokens/s 8750.796 |walltime 12875.935 |
Transformer | epoch 0 | step 25520 |avg loss 8.418 |avg tokens 4458.400 |tokens/s 8864.547 |walltime 12880.964 |
Transformer | epoch 0 | step 25530 |avg loss 8.387 |avg tokens 4484.200 |tokens/s 8694.092 |walltime 12886.122 |
Transformer | epoch 0 | step 25540 |avg loss 8.577 |avg tokens 4480.200 |tokens/s 8907.432 |walltime 12891.152 |
Transformer | epoch 0 | step 25550 |avg loss 8.203 |avg tokens 4742.000 |tokens/s 9123.243 |walltime 12896.350 |
Transformer | epoch 0 | step 25560 |avg loss 8.317 |avg tokens 4231.800 |tokens/s 8713.713 |walltime 12901.206 |
Transformer | epoch 0 | step 25570 |avg loss 8.415 |avg tokens 4499.400 |tokens/s 8879.222 |walltime 12906.273 |
Transformer | epoch 0 | step 25580 |avg loss 8.648 |avg tokens 4590.500 |tokens/s 9112.041 |walltime 12911.311 |
Transformer | epoch 0 | step 25590 |avg loss 8.427 |avg tokens 4464.500 |tokens/s 9107.389 |walltime 12916.213 |
Transformer | epoch 0 | step 25600 |avg loss 8.829 |avg tokens 4132.000 |tokens/s 8517.133 |walltime 12921.065 |
Transformer | epoch 0 | step 25610 |avg loss 8.370 |avg tokens 4424.700 |tokens/s 8750.190 |walltime 12926.121 |
Transformer | epoch 0 | step 25620 |avg loss 8.157 |avg tokens 4524.000 |tokens/s 8727.160 |walltime 12931.305 |
Transformer | epoch 0 | step 25630 |avg loss 8.694 |avg tokens 4175.200 |tokens/s 8490.806 |walltime 12936.223 |
Transformer | epoch 0 | step 25640 |avg loss 8.369 |avg tokens 4570.500 |tokens/s 8948.805 |walltime 12941.330 |
Transformer | epoch 0 | step 25650 |avg loss 8.774 |avg tokens 4093.700 |tokens/s 8572.295 |walltime 12946.105 |
Transformer | epoch 0 | step 25660 |avg loss 8.470 |avg tokens 4639.200 |tokens/s 9045.861 |walltime 12951.234 |
Transformer | epoch 0 | step 25670 |avg loss 8.510 |avg tokens 4726.600 |tokens/s 9424.555 |walltime 12956.249 |
Transformer | epoch 0 | step 25680 |avg loss 8.643 |avg tokens 4250.500 |tokens/s 8673.456 |walltime 12961.150 |
Transformer | epoch 0 | step 25690 |avg loss 8.447 |avg tokens 4597.300 |tokens/s 9088.809 |walltime 12966.208 |
Transformer | epoch 0 | step 25700 |avg loss 8.499 |avg tokens 4627.900 |tokens/s 9122.360 |walltime 12971.281 |
Transformer | epoch 0 | step 25710 |avg loss 8.468 |avg tokens 4888.200 |tokens/s 9334.041 |walltime 12976.518 |
Transformer | epoch 0 | step 25720 |avg loss 8.556 |avg tokens 4208.100 |tokens/s 8585.457 |walltime 12981.420 |
Transformer | epoch 0 | step 25730 |avg loss 8.391 |avg tokens 4768.500 |tokens/s 9374.945 |walltime 12986.506 |
Transformer | epoch 0 | step 25740 |avg loss 8.552 |avg tokens 4719.300 |tokens/s 9048.645 |walltime 12991.721 |
Transformer | epoch 0 | step 25750 |avg loss 8.494 |avg tokens 4229.000 |tokens/s 8665.985 |walltime 12996.601 |
Transformer | epoch 0 | step 25760 |avg loss 8.641 |avg tokens 4810.200 |tokens/s 9305.825 |walltime 13001.770 |
Transformer | epoch 0 | step 25770 |avg loss 8.683 |avg tokens 4566.000 |tokens/s 9106.841 |walltime 13006.784 |
Transformer | epoch 0 | step 25780 |avg loss 8.458 |avg tokens 4337.500 |tokens/s 8645.158 |walltime 13011.802 |
Transformer | epoch 0 | step 25790 |avg loss 8.351 |avg tokens 4782.500 |tokens/s 9085.909 |walltime 13017.065 |
Transformer | epoch 0 | step 25800 |avg loss 8.424 |avg tokens 4582.100 |tokens/s 8910.142 |walltime 13022.208 |
Transformer | epoch 0 | step 25810 |avg loss 8.599 |avg tokens 4549.400 |tokens/s 8894.491 |walltime 13027.323 |
Transformer | epoch 0 | step 25820 |avg loss 8.716 |avg tokens 4246.200 |tokens/s 8817.621 |walltime 13032.138 |
Transformer | epoch 0 | step 25830 |avg loss 8.532 |avg tokens 4847.800 |tokens/s 9273.232 |walltime 13037.366 |
Transformer | epoch 0 | step 25840 |avg loss 8.416 |avg tokens 4341.200 |tokens/s 8544.711 |walltime 13042.447 |
Transformer | epoch 0 | step 25850 |avg loss 8.623 |avg tokens 4386.300 |tokens/s 8781.713 |walltime 13047.441 |
Transformer | epoch 0 | step 25860 |avg loss 8.358 |avg tokens 4413.100 |tokens/s 8777.459 |walltime 13052.469 |
Transformer | epoch 0 | step 25870 |avg loss 8.334 |avg tokens 4831.600 |tokens/s 9036.069 |walltime 13057.816 |
Transformer | epoch 0 | step 25880 |avg loss 8.280 |avg tokens 4746.400 |tokens/s 9065.354 |walltime 13063.052 |
Transformer | epoch 0 | step 25890 |avg loss 8.516 |avg tokens 4425.600 |tokens/s 9064.189 |walltime 13067.934 |
Transformer | epoch 0 | step 25900 |avg loss 8.581 |avg tokens 4755.200 |tokens/s 9120.453 |walltime 13073.148 |
Transformer | epoch 0 | step 25910 |avg loss 8.656 |avg tokens 4807.900 |tokens/s 9586.407 |walltime 13078.163 |
Transformer | epoch 0 | step 25920 |avg loss 8.881 |avg tokens 4087.900 |tokens/s 8651.550 |walltime 13082.889 |
Transformer | epoch 0 | step 25930 |avg loss 8.569 |avg tokens 4707.900 |tokens/s 9012.858 |walltime 13088.112 |
Transformer | epoch 0 | step 25940 |avg loss 8.430 |avg tokens 4730.400 |tokens/s 9166.405 |walltime 13093.273 |
Transformer | epoch 0 | step 25950 |avg loss 8.561 |avg tokens 4736.800 |tokens/s 8870.431 |walltime 13098.613 |
Transformer | epoch 0 | step 25960 |avg loss 8.504 |avg tokens 4511.400 |tokens/s 8849.870 |walltime 13103.710 |
Transformer | epoch 0 | step 25970 |avg loss 8.373 |avg tokens 4806.900 |tokens/s 8931.826 |walltime 13109.092 |
Transformer | epoch 0 | step 25980 |avg loss 8.694 |avg tokens 4140.600 |tokens/s 8421.182 |walltime 13114.009 |
Transformer | epoch 0 | step 25990 |avg loss 8.496 |avg tokens 4459.300 |tokens/s 8840.650 |walltime 13119.053 |
Transformer | epoch 0 | step 26000 |avg loss 8.479 |avg tokens 4599.600 |tokens/s 9028.402 |walltime 13124.148 |
Transformer | epoch 0 | step 26010 |avg loss 8.519 |avg tokens 4850.500 |tokens/s 9042.773 |walltime 13129.512 |
Transformer | epoch 0 | step 26020 |avg loss 8.676 |avg tokens 4189.800 |tokens/s 8579.240 |walltime 13134.395 |
Transformer | epoch 0 | step 26030 |avg loss 8.884 |avg tokens 4051.600 |tokens/s 8569.847 |walltime 13139.123 |
Transformer | epoch 0 | step 26040 |avg loss 8.211 |avg tokens 4989.000 |tokens/s 9497.906 |walltime 13144.376 |
Transformer | epoch 0 | step 26050 |avg loss 8.367 |avg tokens 4871.900 |tokens/s 9201.815 |walltime 13149.670 |
Transformer | epoch 0 | step 26060 |avg loss 8.712 |avg tokens 4266.500 |tokens/s 9108.200 |walltime 13154.355 |
Transformer | epoch 0 | step 26070 |avg loss 8.429 |avg tokens 4708.000 |tokens/s 9064.136 |walltime 13159.549 |
Transformer | epoch 0 | step 26080 |avg loss 8.480 |avg tokens 4741.600 |tokens/s 9272.817 |walltime 13164.662 |
Transformer | epoch 0 | step 26090 |avg loss 8.585 |avg tokens 4528.000 |tokens/s 8717.297 |walltime 13169.856 |
Transformer | epoch 0 | step 26100 |avg loss 8.368 |avg tokens 4451.400 |tokens/s 8773.152 |walltime 13174.930 |
Transformer | epoch 0 | step 26110 |avg loss 8.432 |avg tokens 4534.500 |tokens/s 8782.808 |walltime 13180.093 |
Transformer | epoch 0 | step 26120 |avg loss 8.648 |avg tokens 4276.800 |tokens/s 8716.809 |walltime 13185.000 |
Transformer | epoch 0 | step 26130 |avg loss 8.687 |avg tokens 4219.600 |tokens/s 8362.518 |walltime 13190.045 |
Transformer | epoch 0 | step 26140 |avg loss 8.687 |avg tokens 4341.000 |tokens/s 8859.516 |walltime 13194.945 |
Transformer | epoch 0 | step 26150 |avg loss 8.349 |avg tokens 4816.800 |tokens/s 9274.729 |walltime 13200.139 |
Transformer | epoch 0 | step 26160 |avg loss 8.786 |avg tokens 3671.700 |tokens/s 7986.583 |walltime 13204.736 |
Transformer | epoch 0 | step 26170 |avg loss 8.602 |avg tokens 4586.900 |tokens/s 8995.901 |walltime 13209.835 |
Transformer | epoch 0 | step 26180 |avg loss 8.657 |avg tokens 4664.800 |tokens/s 9131.256 |walltime 13214.944 |
Transformer | epoch 0 | step 26190 |avg loss 8.638 |avg tokens 4907.000 |tokens/s 9379.803 |walltime 13220.175 |
Transformer | epoch 0 | step 26200 |avg loss 8.538 |avg tokens 4731.400 |tokens/s 9047.841 |walltime 13225.404 |
Transformer | epoch 0 | step 26210 |avg loss 8.770 |avg tokens 4360.400 |tokens/s 9037.428 |walltime 13230.229 |
Transformer | epoch 0 | step 26220 |avg loss 8.479 |avg tokens 4918.000 |tokens/s 9567.490 |walltime 13235.369 |
Transformer | epoch 0 | step 26230 |avg loss 8.603 |avg tokens 4410.800 |tokens/s 8639.047 |walltime 13240.475 |
Transformer | epoch 0 | step 26240 |avg loss 8.629 |avg tokens 4329.100 |tokens/s 8707.840 |walltime 13245.447 |
Transformer | epoch 0 | step 26250 |avg loss 8.457 |avg tokens 4522.500 |tokens/s 9160.750 |walltime 13250.383 |
Transformer | epoch 0 | step 26260 |avg loss 8.617 |avg tokens 4718.000 |tokens/s 9286.605 |walltime 13255.464 |
Transformer | epoch 0 | step 26270 |avg loss 8.688 |avg tokens 4531.900 |tokens/s 8926.908 |walltime 13260.541 |
Transformer | epoch 0 | step 26280 |avg loss 8.691 |avg tokens 4402.400 |tokens/s 8802.647 |walltime 13265.542 |
Transformer | epoch 0 | step 26290 |avg loss 8.528 |avg tokens 4158.200 |tokens/s 8375.085 |walltime 13270.507 |
Transformer | epoch 0 | step 26300 |avg loss 8.428 |avg tokens 4351.600 |tokens/s 8686.519 |walltime 13275.516 |
Transformer | epoch 0 | step 26310 |avg loss 8.513 |avg tokens 3990.600 |tokens/s 8221.304 |walltime 13280.370 |
Transformer | epoch 0 | step 26320 |avg loss 8.308 |avg tokens 4457.800 |tokens/s 8650.032 |walltime 13285.524 |
Transformer | epoch 0 | step 26330 |avg loss 8.864 |avg tokens 4446.200 |tokens/s 9444.723 |walltime 13290.231 |
Transformer | epoch 0 | step 26340 |avg loss 8.536 |avg tokens 4076.900 |tokens/s 8272.561 |walltime 13295.160 |
Transformer | epoch 0 | step 26350 |avg loss 8.505 |avg tokens 4033.100 |tokens/s 8532.462 |walltime 13299.886 |
Transformer | epoch 0 | step 26360 |avg loss 8.566 |avg tokens 4530.200 |tokens/s 9113.845 |walltime 13304.857 |
Transformer | epoch 0 | step 26370 |avg loss 8.425 |avg tokens 4505.700 |tokens/s 8756.471 |walltime 13310.003 |
Transformer | epoch 0 | step 26380 |avg loss 8.289 |avg tokens 4653.200 |tokens/s 9217.519 |walltime 13315.051 |
Transformer | epoch 0 | step 26390 |avg loss 8.488 |avg tokens 4769.600 |tokens/s 9329.458 |walltime 13320.163 |
Transformer | epoch 0 | step 26400 |avg loss 8.276 |avg tokens 4913.800 |tokens/s 9492.859 |walltime 13325.340 |
Transformer | epoch 0 | step 26410 |avg loss 8.640 |avg tokens 4739.900 |tokens/s 9353.444 |walltime 13330.407 |
Transformer | epoch 0 | step 26420 |avg loss 8.459 |avg tokens 4647.200 |tokens/s 9043.810 |walltime 13335.546 |
Transformer | epoch 0 | step 26430 |avg loss 8.503 |avg tokens 4738.100 |tokens/s 9196.663 |walltime 13340.698 |
Transformer | epoch 0 | step 26440 |avg loss 8.658 |avg tokens 3938.900 |tokens/s 8316.226 |walltime 13345.434 |
Transformer | epoch 0 | step 26450 |avg loss 8.621 |avg tokens 4513.800 |tokens/s 8882.841 |walltime 13350.516 |
Transformer | epoch 0 | step 26460 |avg loss 8.533 |avg tokens 4540.800 |tokens/s 9086.866 |walltime 13355.513 |
Transformer | epoch 0 | step 26470 |avg loss 8.650 |avg tokens 4359.700 |tokens/s 8758.255 |walltime 13360.491 |
Transformer | epoch 0 | step 26480 |avg loss 8.801 |avg tokens 4282.400 |tokens/s 8685.347 |walltime 13365.421 |
Transformer | epoch 0 | step 26490 |avg loss 8.768 |avg tokens 4437.200 |tokens/s 8932.619 |walltime 13370.389 |
Transformer | epoch 0 | step 26500 |avg loss 8.231 |avg tokens 4216.100 |tokens/s 8649.774 |walltime 13375.263 |
Transformer | epoch 0 | step 26510 |avg loss 8.717 |avg tokens 4856.600 |tokens/s 9586.056 |walltime 13380.329 |
Transformer | epoch 0 | step 26520 |avg loss 8.483 |avg tokens 4762.100 |tokens/s 8950.591 |walltime 13385.650 |
Transformer | epoch 0 | step 26530 |avg loss 8.467 |avg tokens 4611.500 |tokens/s 8755.995 |walltime 13390.916 |
Transformer | epoch 0 | step 26540 |avg loss 8.656 |avg tokens 4225.100 |tokens/s 8381.792 |walltime 13395.957 |
Transformer | epoch 0 | step 26550 |avg loss 8.450 |avg tokens 4588.100 |tokens/s 8951.344 |walltime 13401.083 |
Transformer | epoch 0 | step 26560 |avg loss 8.431 |avg tokens 4867.600 |tokens/s 9157.826 |walltime 13406.398 |
Transformer | epoch 0 | step 26570 |avg loss 8.401 |avg tokens 4787.300 |tokens/s 9197.382 |walltime 13411.603 |
Transformer | epoch 0 | step 26580 |avg loss 8.642 |avg tokens 4223.900 |tokens/s 8576.084 |walltime 13416.528 |
Transformer | epoch 0 | step 26590 |avg loss 8.476 |avg tokens 4395.100 |tokens/s 8643.659 |walltime 13421.613 |
Transformer | epoch 0 | step 26600 |avg loss 8.465 |avg tokens 4143.900 |tokens/s 8523.988 |walltime 13426.474 |
Transformer | epoch 0 | step 26610 |avg loss 8.588 |avg tokens 4473.100 |tokens/s 8929.596 |walltime 13431.484 |
Transformer | epoch 0 | step 26620 |avg loss 8.580 |avg tokens 4394.200 |tokens/s 8875.122 |walltime 13436.435 |
Transformer | epoch 0 | step 26630 |avg loss 8.435 |avg tokens 4652.800 |tokens/s 9248.472 |walltime 13441.466 |
Transformer | epoch 0 | step 26640 |avg loss 8.377 |avg tokens 4757.500 |tokens/s 9040.754 |walltime 13446.728 |
Transformer | epoch 0 | step 26650 |avg loss 8.320 |avg tokens 4658.000 |tokens/s 8887.659 |walltime 13451.969 |
Transformer | epoch 0 | step 26660 |avg loss 8.393 |avg tokens 4617.600 |tokens/s 8742.041 |walltime 13457.251 |
Transformer | epoch 0 | step 26670 |avg loss 8.772 |avg tokens 4303.200 |tokens/s 8683.184 |walltime 13462.207 |
Transformer | epoch 0 | step 26680 |avg loss 8.581 |avg tokens 4713.900 |tokens/s 9052.095 |walltime 13467.414 |
Transformer | epoch 0 | step 26690 |avg loss 8.433 |avg tokens 4397.300 |tokens/s 8714.086 |walltime 13472.461 |
Transformer | epoch 0 | step 26700 |avg loss 7.905 |avg tokens 4907.000 |tokens/s 9264.270 |walltime 13477.757 |
Transformer | epoch 0 | step 26710 |avg loss 8.587 |avg tokens 4505.800 |tokens/s 8778.147 |walltime 13482.890 |
Transformer | epoch 0 | step 26720 |avg loss 8.497 |avg tokens 4441.900 |tokens/s 8782.324 |walltime 13487.948 |
Transformer | epoch 0 | step 26730 |avg loss 8.326 |avg tokens 4504.800 |tokens/s 8876.579 |walltime 13493.023 |
Transformer | epoch 0 | step 26740 |avg loss 8.565 |avg tokens 3977.500 |tokens/s 8515.251 |walltime 13497.694 |
Transformer | epoch 0 | step 26750 |avg loss 8.373 |avg tokens 4642.100 |tokens/s 8914.430 |walltime 13502.901 |
Transformer | epoch 0 | step 26760 |avg loss 8.510 |avg tokens 4687.900 |tokens/s 9113.958 |walltime 13508.045 |
Transformer | epoch 0 | step 26770 |avg loss 8.499 |avg tokens 4620.000 |tokens/s 9074.157 |walltime 13513.136 |
Transformer | epoch 0 | step 26780 |avg loss 8.408 |avg tokens 4715.700 |tokens/s 9232.341 |walltime 13518.244 |
Transformer | epoch 0 | step 26790 |avg loss 8.582 |avg tokens 4011.000 |tokens/s 8451.384 |walltime 13522.990 |
Transformer | epoch 0 | step 26800 |avg loss 8.674 |avg tokens 4593.700 |tokens/s 9324.955 |walltime 13527.916 |
Transformer | epoch 0 | step 26810 |avg loss 8.468 |avg tokens 4484.500 |tokens/s 9015.041 |walltime 13532.891 |
Transformer | epoch 0 | step 26820 |avg loss 8.464 |avg tokens 4719.000 |tokens/s 9342.116 |walltime 13537.942 |
Transformer | epoch 0 | step 26830 |avg loss 8.251 |avg tokens 4729.800 |tokens/s 9252.162 |walltime 13543.054 |
Transformer | epoch 0 | step 26840 |avg loss 8.422 |avg tokens 4717.800 |tokens/s 9201.136 |walltime 13548.182 |
Transformer | epoch 0 | step 26850 |avg loss 8.328 |avg tokens 4516.800 |tokens/s 8873.211 |walltime 13553.272 |
Transformer | epoch 0 | step 26860 |avg loss 8.715 |avg tokens 4374.900 |tokens/s 8858.707 |walltime 13558.211 |
Transformer | epoch 0 | step 26870 |avg loss 8.532 |avg tokens 4313.000 |tokens/s 8663.589 |walltime 13563.189 |
Transformer | epoch 0 | step 26880 |avg loss 8.434 |avg tokens 4460.200 |tokens/s 8837.242 |walltime 13568.236 |
Transformer | epoch 0 | step 26890 |avg loss 8.488 |avg tokens 3869.400 |tokens/s 7878.940 |walltime 13573.147 |
Transformer | epoch 0 | step 26900 |avg loss 8.275 |avg tokens 4551.200 |tokens/s 8941.587 |walltime 13578.237 |
Transformer | epoch 0 | step 26910 |avg loss 8.585 |avg tokens 4485.800 |tokens/s 8947.373 |walltime 13583.251 |
Transformer | epoch 0 | step 26920 |avg loss 8.300 |avg tokens 4706.400 |tokens/s 8998.109 |walltime 13588.481 |
Transformer | epoch 0 | step 26930 |avg loss 8.568 |avg tokens 4433.600 |tokens/s 9107.573 |walltime 13593.349 |
Transformer | epoch 0 | step 26940 |avg loss 8.428 |avg tokens 4423.300 |tokens/s 8750.122 |walltime 13598.404 |
Transformer | epoch 0 | step 26950 |avg loss 8.786 |avg tokens 4724.500 |tokens/s 9621.214 |walltime 13603.315 |
Transformer | epoch 0 | step 26960 |avg loss 8.534 |avg tokens 4587.900 |tokens/s 9100.119 |walltime 13608.356 |
Transformer | epoch 0 | step 26970 |avg loss 8.616 |avg tokens 4494.600 |tokens/s 9192.638 |walltime 13613.246 |
Transformer | epoch 0 | step 26980 |avg loss 8.772 |avg tokens 4429.500 |tokens/s 9160.743 |walltime 13618.081 |
Transformer | epoch 0 | step 26990 |avg loss 8.397 |avg tokens 4724.800 |tokens/s 8995.936 |walltime 13623.333 |
Transformer | epoch 0 | step 27000 |avg loss 8.353 |avg tokens 4836.800 |tokens/s 9199.603 |walltime 13628.591 |
Transformer | epoch 0 | step 27010 |avg loss 8.508 |avg tokens 4327.900 |tokens/s 8507.980 |walltime 13633.678 |
Transformer | epoch 0 | step 27020 |avg loss 8.412 |avg tokens 4744.800 |tokens/s 9042.300 |walltime 13638.925 |
Transformer | epoch 0 | step 27030 |avg loss 8.547 |avg tokens 4543.600 |tokens/s 9027.544 |walltime 13643.958 |
Transformer | epoch 0 | step 27040 |avg loss 8.513 |avg tokens 4656.900 |tokens/s 8965.025 |walltime 13649.152 |
Transformer | epoch 0 | step 27050 |avg loss 8.543 |avg tokens 3857.300 |tokens/s 8546.962 |walltime 13653.666 |
Transformer | epoch 0 | step 27060 |avg loss 8.490 |avg tokens 4541.800 |tokens/s 8862.977 |walltime 13658.790 |
Transformer | epoch 0 | step 27070 |avg loss 8.444 |avg tokens 4758.700 |tokens/s 9178.961 |walltime 13663.974 |
Transformer | epoch 0 | step 27080 |avg loss 8.385 |avg tokens 4898.400 |tokens/s 9208.748 |walltime 13669.294 |
Transformer | epoch 0 | step 27090 |avg loss 8.549 |avg tokens 4250.800 |tokens/s 8644.643 |walltime 13674.211 |
Transformer | epoch 0 | step 27100 |avg loss 8.659 |avg tokens 4029.600 |tokens/s 8256.099 |walltime 13679.092 |
Transformer | epoch 0 | step 27110 |avg loss 8.629 |avg tokens 4701.700 |tokens/s 9083.874 |walltime 13684.268 |
Transformer | epoch 0 | step 27120 |avg loss 8.321 |avg tokens 4356.500 |tokens/s 8578.840 |walltime 13689.346 |
Transformer | epoch 0 | step 27130 |avg loss 8.358 |avg tokens 4620.200 |tokens/s 8904.185 |walltime 13694.535 |
Transformer | epoch 0 | step 27140 |avg loss 8.210 |avg tokens 4341.600 |tokens/s 8703.321 |walltime 13699.523 |
Transformer | epoch 0 | step 27150 |avg loss 8.521 |avg tokens 4321.100 |tokens/s 8540.893 |walltime 13704.582 |
Transformer | epoch 0 | step 27160 |avg loss 8.758 |avg tokens 4508.600 |tokens/s 8973.651 |walltime 13709.607 |
Transformer | epoch 0 | step 27170 |avg loss 8.651 |avg tokens 4182.200 |tokens/s 8908.590 |walltime 13714.301 |
Transformer | epoch 0 | step 27180 |avg loss 8.861 |avg tokens 4239.800 |tokens/s 9074.380 |walltime 13718.973 |
Transformer | epoch 0 | step 27190 |avg loss 8.751 |avg tokens 3651.800 |tokens/s 7784.622 |walltime 13723.664 |
Transformer | epoch 0 | step 27200 |avg loss 8.386 |avg tokens 4903.200 |tokens/s 9347.971 |walltime 13728.910 |
Transformer | epoch 0 | step 27210 |avg loss 8.446 |avg tokens 4642.400 |tokens/s 9226.435 |walltime 13733.941 |
Transformer | epoch 0 | step 27220 |avg loss 8.458 |avg tokens 4737.000 |tokens/s 9110.272 |walltime 13739.141 |
Transformer | epoch 0 | step 27230 |avg loss 8.669 |avg tokens 4271.000 |tokens/s 8754.140 |walltime 13744.020 |
Transformer | epoch 0 | step 27240 |avg loss 8.711 |avg tokens 4224.600 |tokens/s 8736.777 |walltime 13748.855 |
Transformer | epoch 0 | step 27250 |avg loss 8.557 |avg tokens 4577.600 |tokens/s 8991.498 |walltime 13753.946 |
Transformer | epoch 0 | step 27260 |avg loss 8.477 |avg tokens 4402.800 |tokens/s 9148.292 |walltime 13758.759 |
Transformer | epoch 0 | step 27270 |avg loss 8.636 |avg tokens 4498.500 |tokens/s 8960.216 |walltime 13763.779 |
Transformer | epoch 0 | step 27280 |avg loss 8.492 |avg tokens 4734.700 |tokens/s 9102.643 |walltime 13768.981 |
Transformer | epoch 0 | step 27290 |avg loss 8.502 |avg tokens 4658.700 |tokens/s 9105.996 |walltime 13774.097 |
Transformer | epoch 0 | step 27300 |avg loss 8.485 |avg tokens 4834.600 |tokens/s 9497.300 |walltime 13779.188 |
Transformer | epoch 0 | step 27310 |avg loss 8.804 |avg tokens 3953.300 |tokens/s 8626.592 |walltime 13783.770 |
Transformer | epoch 0 | step 27320 |avg loss 8.568 |avg tokens 4355.500 |tokens/s 8673.077 |walltime 13788.792 |
Transformer | epoch 0 | step 27330 |avg loss 8.523 |avg tokens 4838.800 |tokens/s 9422.257 |walltime 13793.928 |
Transformer | epoch 0 | step 27340 |avg loss 8.633 |avg tokens 4852.600 |tokens/s 9601.564 |walltime 13798.982 |
Transformer | epoch 0 | step 27350 |avg loss 8.592 |avg tokens 4809.600 |tokens/s 9147.603 |walltime 13804.239 |
Transformer | epoch 0 | step 27360 |avg loss 8.585 |avg tokens 4688.500 |tokens/s 9084.612 |walltime 13809.400 |
Transformer | epoch 0 | step 27370 |avg loss 8.512 |avg tokens 4259.800 |tokens/s 8686.165 |walltime 13814.304 |
Transformer | epoch 0 | step 27380 |avg loss 8.493 |avg tokens 4494.000 |tokens/s 8818.270 |walltime 13819.401 |
Transformer | epoch 0 | step 27390 |avg loss 8.348 |avg tokens 4917.600 |tokens/s 9366.250 |walltime 13824.651 |
Transformer | epoch 0 | step 27400 |avg loss 8.507 |avg tokens 4746.000 |tokens/s 9071.642 |walltime 13829.883 |
Transformer | epoch 0 | step 27410 |avg loss 8.656 |avg tokens 4829.700 |tokens/s 9565.125 |walltime 13834.932 |
Transformer | epoch 0 | step 27420 |avg loss 8.478 |avg tokens 4732.600 |tokens/s 9172.934 |walltime 13840.091 |
Transformer | epoch 0 | step 27430 |avg loss 8.713 |avg tokens 4595.100 |tokens/s 9313.948 |walltime 13845.025 |
Transformer | epoch 0 | step 27440 |avg loss 8.358 |avg tokens 4804.800 |tokens/s 9107.116 |walltime 13850.301 |
Transformer | epoch 0 | step 27450 |avg loss 8.562 |avg tokens 4592.100 |tokens/s 9005.178 |walltime 13855.400 |
Transformer | epoch 0 | step 27460 |avg loss 8.488 |avg tokens 4504.700 |tokens/s 8929.515 |walltime 13860.445 |
Transformer | epoch 0 | step 27470 |avg loss 8.599 |avg tokens 4706.800 |tokens/s 9327.386 |walltime 13865.491 |
Transformer | epoch 0 | step 27480 |avg loss 8.335 |avg tokens 4749.600 |tokens/s 9001.188 |walltime 13870.768 |
Transformer | epoch 0 | step 27490 |avg loss 8.316 |avg tokens 4524.800 |tokens/s 8896.696 |walltime 13875.854 |
Transformer | epoch 0 | step 27500 |avg loss 8.483 |avg tokens 3964.600 |tokens/s 8240.037 |walltime 13880.665 |
Transformer | epoch 0 | step 27510 |avg loss 8.444 |avg tokens 4723.400 |tokens/s 8988.949 |walltime 13885.920 |
Transformer | epoch 0 | step 27520 |avg loss 8.400 |avg tokens 4515.800 |tokens/s 8823.227 |walltime 13891.038 |
Transformer | epoch 0 | step 27530 |avg loss 8.570 |avg tokens 4725.300 |tokens/s 9012.300 |walltime 13896.281 |
Transformer | epoch 0 | step 27540 |avg loss 8.555 |avg tokens 4221.400 |tokens/s 8720.101 |walltime 13901.122 |
Transformer | epoch 0 | step 27550 |avg loss 8.650 |avg tokens 3991.200 |tokens/s 8386.497 |walltime 13905.881 |
Transformer | epoch 0 | step 27560 |avg loss 8.429 |avg tokens 4912.700 |tokens/s 9456.924 |walltime 13911.076 |
Transformer | epoch 0 | step 27570 |avg loss 8.465 |avg tokens 4263.600 |tokens/s 8343.668 |walltime 13916.186 |
Transformer | epoch 0 | step 27580 |avg loss 8.463 |avg tokens 4426.500 |tokens/s 8610.919 |walltime 13921.326 |
Transformer | epoch 0 | step 27590 |avg loss 8.324 |avg tokens 4603.200 |tokens/s 8949.319 |walltime 13926.470 |
Transformer | epoch 0 | step 27600 |avg loss 8.536 |avg tokens 4546.100 |tokens/s 8802.943 |walltime 13931.634 |
Transformer | epoch 0 | step 27610 |avg loss 8.215 |avg tokens 4965.700 |tokens/s 9369.065 |walltime 13936.934 |
Transformer | epoch 0 | step 27620 |avg loss 8.411 |avg tokens 4818.200 |tokens/s 9131.650 |walltime 13942.211 |
Transformer | epoch 0 | step 27630 |avg loss 8.557 |avg tokens 4487.900 |tokens/s 8960.787 |walltime 13947.219 |
Transformer | epoch 0 | step 27640 |avg loss 8.544 |avg tokens 4806.100 |tokens/s 9183.817 |walltime 13952.452 |
Transformer | epoch 0 | step 27650 |avg loss 8.298 |avg tokens 4782.000 |tokens/s 9173.384 |walltime 13957.665 |
Transformer | epoch 0 | step 27660 |avg loss 8.598 |avg tokens 4627.100 |tokens/s 9184.768 |walltime 13962.703 |
Transformer | epoch 0 | step 27670 |avg loss 8.601 |avg tokens 4930.300 |tokens/s 9777.365 |walltime 13967.746 |
Transformer | epoch 0 | step 27680 |avg loss 8.521 |avg tokens 4390.100 |tokens/s 8606.644 |walltime 13972.847 |
Transformer | epoch 0 | step 27690 |avg loss 8.372 |avg tokens 4929.700 |tokens/s 9252.785 |walltime 13978.174 |
Transformer | epoch 0 | step 27700 |avg loss 8.542 |avg tokens 4740.000 |tokens/s 9434.787 |walltime 13983.198 |
Transformer | epoch 0 | step 27710 |avg loss 8.249 |avg tokens 4806.900 |tokens/s 9067.629 |walltime 13988.499 |
Transformer | epoch 0 | step 27720 |avg loss 8.840 |avg tokens 4865.700 |tokens/s 10221.964 |walltime 13993.260 |
Transformer | epoch 0 | step 27730 |avg loss 8.669 |avg tokens 3989.800 |tokens/s 8416.979 |walltime 13998.000 |
Transformer | epoch 0 | step 27740 |avg loss 8.355 |avg tokens 4530.400 |tokens/s 8858.868 |walltime 14003.114 |
Transformer | epoch 0 | step 27750 |avg loss 8.485 |avg tokens 4472.700 |tokens/s 9027.296 |walltime 14008.068 |
Transformer | epoch 0 | step 27760 |avg loss 8.620 |avg tokens 4635.400 |tokens/s 9199.796 |walltime 14013.107 |
Transformer | epoch 0 | step 27770 |avg loss 8.598 |avg tokens 4674.700 |tokens/s 9157.555 |walltime 14018.212 |
Transformer | epoch 0 | step 27780 |avg loss 8.416 |avg tokens 4436.800 |tokens/s 8707.599 |walltime 14023.307 |
Transformer | epoch 0 | step 27790 |avg loss 8.627 |avg tokens 4200.300 |tokens/s 8675.136 |walltime 14028.149 |
Transformer | epoch 0 | step 27800 |avg loss 8.589 |avg tokens 4765.700 |tokens/s 9352.016 |walltime 14033.245 |
Transformer | epoch 0 | step 27810 |avg loss 8.708 |avg tokens 4422.100 |tokens/s 8868.437 |walltime 14038.231 |
Transformer | epoch 0 | step 27820 |avg loss 8.685 |avg tokens 3981.500 |tokens/s 8370.610 |walltime 14042.988 |
Transformer | epoch 0 | step 27830 |avg loss 8.580 |avg tokens 4938.000 |tokens/s 9543.656 |walltime 14048.162 |
Transformer | epoch 0 | step 27840 |avg loss 8.574 |avg tokens 4490.200 |tokens/s 8804.084 |walltime 14053.262 |
Transformer | epoch 0 | step 27850 |avg loss 8.519 |avg tokens 4706.700 |tokens/s 9199.474 |walltime 14058.378 |
Transformer | epoch 0 | step 27860 |avg loss 8.780 |avg tokens 4793.500 |tokens/s 9257.602 |walltime 14063.556 |
Transformer | epoch 0 | step 27870 |avg loss 8.348 |avg tokens 4711.300 |tokens/s 8948.097 |walltime 14068.821 |
Transformer | epoch 0 | step 27880 |avg loss 8.605 |avg tokens 4883.000 |tokens/s 9769.221 |walltime 14073.819 |
Transformer | epoch 0 | step 27890 |avg loss 8.498 |avg tokens 4503.800 |tokens/s 9041.782 |walltime 14078.801 |
Transformer | epoch 0 | step 27900 |avg loss 8.483 |avg tokens 4442.300 |tokens/s 8902.758 |walltime 14083.790 |
Transformer | epoch 0 | step 27910 |avg loss 8.826 |avg tokens 4427.300 |tokens/s 9539.729 |walltime 14088.431 |
Transformer | epoch 0 | step 27920 |avg loss 8.573 |avg tokens 4730.900 |tokens/s 8990.739 |walltime 14093.693 |
Transformer | epoch 0 | step 27930 |avg loss 8.595 |avg tokens 4514.800 |tokens/s 9043.963 |walltime 14098.685 |
Transformer | epoch 0 | step 27940 |avg loss 8.507 |avg tokens 4760.600 |tokens/s 9067.535 |walltime 14103.936 |
Transformer | epoch 0 | step 27950 |avg loss 8.549 |avg tokens 4689.300 |tokens/s 9132.991 |walltime 14109.070 |
Transformer | epoch 0 | step 27960 |avg loss 8.401 |avg tokens 4443.000 |tokens/s 8914.380 |walltime 14114.054 |
Transformer | epoch 0 | step 27970 |avg loss 8.759 |avg tokens 4552.400 |tokens/s 9223.088 |walltime 14118.990 |
Transformer | epoch 0 | step 27980 |avg loss 8.462 |avg tokens 4290.600 |tokens/s 8819.803 |walltime 14123.855 |
Transformer | epoch 0 | step 27990 |avg loss 8.503 |avg tokens 4457.900 |tokens/s 8908.358 |walltime 14128.859 |
Transformer | epoch 0 | step 28000 |avg loss 8.572 |avg tokens 4868.900 |tokens/s 9543.318 |walltime 14133.961 |
Transformer | epoch 0 | step 28010 |avg loss 8.687 |avg tokens 4567.700 |tokens/s 8924.650 |walltime 14139.079 |
Transformer | epoch 0 | step 28020 |avg loss 8.593 |avg tokens 4543.900 |tokens/s 9010.590 |walltime 14144.122 |
Transformer | epoch 0 | step 28030 |avg loss 8.501 |avg tokens 4574.200 |tokens/s 8845.708 |walltime 14149.293 |
Transformer | epoch 0 | step 28040 |avg loss 8.597 |avg tokens 4263.900 |tokens/s 8681.220 |walltime 14154.204 |
Transformer | epoch 0 | step 28050 |avg loss 8.762 |avg tokens 4128.200 |tokens/s 8535.196 |walltime 14159.041 |
Transformer | epoch 0 | step 28060 |avg loss 8.578 |avg tokens 4059.200 |tokens/s 8475.912 |walltime 14163.830 |
Transformer | epoch 0 | step 28070 |avg loss 8.571 |avg tokens 4195.400 |tokens/s 8423.036 |walltime 14168.811 |
Transformer | epoch 0 | step 28080 |avg loss 8.557 |avg tokens 4345.300 |tokens/s 9056.632 |walltime 14173.609 |
Transformer | epoch 0 | step 28090 |avg loss 8.429 |avg tokens 4719.400 |tokens/s 9288.823 |walltime 14178.690 |
Transformer | epoch 0 | step 28100 |avg loss 8.535 |avg tokens 4711.000 |tokens/s 9067.870 |walltime 14183.885 |
Transformer | epoch 0 | step 28110 |avg loss 8.727 |avg tokens 4224.100 |tokens/s 9122.616 |walltime 14188.515 |
Transformer | epoch 0 | step 28120 |avg loss 8.828 |avg tokens 4530.100 |tokens/s 9348.360 |walltime 14193.361 |
Transformer | epoch 0 | step 28130 |avg loss 8.683 |avg tokens 4391.900 |tokens/s 8607.427 |walltime 14198.464 |
Transformer | epoch 0 | step 28140 |avg loss 8.599 |avg tokens 4812.600 |tokens/s 9475.268 |walltime 14203.543 |
Transformer | epoch 0 | step 28150 |avg loss 8.488 |avg tokens 4538.300 |tokens/s 8800.529 |walltime 14208.700 |
Transformer | epoch 0 | step 28160 |avg loss 8.476 |avg tokens 4474.600 |tokens/s 8954.292 |walltime 14213.697 |
Transformer | epoch 0 | step 28170 |avg loss 8.552 |avg tokens 4367.400 |tokens/s 8872.565 |walltime 14218.619 |
Transformer | epoch 0 | step 28180 |avg loss 8.801 |avg tokens 4644.000 |tokens/s 9519.712 |walltime 14223.497 |
Transformer | epoch 0 | step 28190 |avg loss 8.667 |avg tokens 4658.200 |tokens/s 9255.636 |walltime 14228.530 |
Transformer | epoch 0 | step 28200 |avg loss 8.495 |avg tokens 4601.700 |tokens/s 8881.108 |walltime 14233.712 |
Transformer | epoch 0 | step 28210 |avg loss 8.729 |avg tokens 4873.200 |tokens/s 9529.620 |walltime 14238.826 |
Transformer | epoch 0 | step 28220 |avg loss 8.596 |avg tokens 4781.100 |tokens/s 9341.751 |walltime 14243.944 |
Transformer | epoch 0 | step 28230 |avg loss 8.637 |avg tokens 3927.600 |tokens/s 8383.774 |walltime 14248.628 |
Transformer | epoch 0 | step 28240 |avg loss 8.693 |avg tokens 3893.900 |tokens/s 8241.271 |walltime 14253.353 |
Transformer | epoch 0 | step 28250 |avg loss 8.916 |avg tokens 3799.300 |tokens/s 7858.525 |walltime 14258.188 |
Transformer | epoch 0 | step 28260 |avg loss 8.655 |avg tokens 3983.000 |tokens/s 8345.649 |walltime 14262.960 |
Transformer | epoch 0 | step 28270 |avg loss 8.590 |avg tokens 4657.800 |tokens/s 9070.101 |walltime 14268.096 |
Transformer | epoch 0 | step 28280 |avg loss 8.433 |avg tokens 4419.800 |tokens/s 8790.138 |walltime 14273.124 |
Transformer | epoch 0 | step 28290 |avg loss 8.632 |avg tokens 4235.800 |tokens/s 8511.147 |walltime 14278.101 |
Transformer | epoch 0 | step 28300 |avg loss 8.487 |avg tokens 4557.800 |tokens/s 8752.394 |walltime 14283.308 |
Transformer | epoch 0 | step 28310 |avg loss 8.405 |avg tokens 4821.900 |tokens/s 9343.422 |walltime 14288.469 |
Transformer | epoch 0 | step 28320 |avg loss 8.531 |avg tokens 4688.500 |tokens/s 8921.754 |walltime 14293.724 |
Transformer | epoch 0 | step 28330 |avg loss 8.596 |avg tokens 3996.900 |tokens/s 8631.665 |walltime 14298.354 |
Transformer | epoch 0 | step 28340 |avg loss 8.543 |avg tokens 4372.400 |tokens/s 8741.371 |walltime 14303.356 |
Transformer | epoch 0 | step 28350 |avg loss 8.336 |avg tokens 4047.600 |tokens/s 8377.388 |walltime 14308.188 |
Transformer | epoch 0 | step 28360 |avg loss 8.731 |avg tokens 4573.800 |tokens/s 9254.064 |walltime 14313.130 |
Transformer | epoch 0 | step 28370 |avg loss 8.396 |avg tokens 4646.700 |tokens/s 8894.233 |walltime 14318.355 |
Transformer | epoch 0 | step 28380 |avg loss 8.638 |avg tokens 4399.900 |tokens/s 8861.972 |walltime 14323.320 |
Transformer | epoch 0 | step 28390 |avg loss 8.620 |avg tokens 4363.200 |tokens/s 8871.526 |walltime 14328.238 |
Transformer | epoch 0 | step 28400 |avg loss 8.605 |avg tokens 4263.800 |tokens/s 8694.661 |walltime 14333.142 |
Transformer | epoch 0 | step 28410 |avg loss 8.569 |avg tokens 4719.400 |tokens/s 9076.674 |walltime 14338.341 |
Transformer | epoch 0 | step 28420 |avg loss 8.509 |avg tokens 4183.600 |tokens/s 8598.942 |walltime 14343.207 |
Transformer | epoch 0 | step 28430 |avg loss 8.730 |avg tokens 4271.100 |tokens/s 9197.768 |walltime 14347.850 |
Transformer | epoch 0 | step 28440 |avg loss 8.539 |avg tokens 4674.800 |tokens/s 9000.202 |walltime 14353.044 |
Transformer | epoch 0 | step 28450 |avg loss 8.599 |avg tokens 4264.800 |tokens/s 8794.629 |walltime 14357.894 |
Transformer | epoch 0 | step 28460 |avg loss 8.762 |avg tokens 4585.100 |tokens/s 9351.391 |walltime 14362.797 |
Transformer | epoch 0 | step 28470 |avg loss 8.648 |avg tokens 3901.300 |tokens/s 8422.057 |walltime 14367.429 |
Transformer | epoch 0 | step 28480 |avg loss 8.674 |avg tokens 4642.600 |tokens/s 9330.967 |walltime 14372.405 |
Transformer | epoch 0 | step 28490 |avg loss 8.371 |avg tokens 4587.600 |tokens/s 9152.878 |walltime 14377.417 |
Transformer | epoch 0 | step 28500 |avg loss 8.525 |avg tokens 4609.800 |tokens/s 8806.269 |walltime 14382.651 |
Transformer | epoch 0 | step 28510 |avg loss 8.649 |avg tokens 4620.000 |tokens/s 9153.992 |walltime 14387.698 |
Transformer | epoch 0 | step 28520 |avg loss 8.443 |avg tokens 4538.800 |tokens/s 8963.316 |walltime 14392.762 |
Transformer | epoch 0 | step 28530 |avg loss 8.807 |avg tokens 4673.600 |tokens/s 9527.693 |walltime 14397.668 |
Transformer | epoch 0 | step 28540 |avg loss 8.469 |avg tokens 4483.500 |tokens/s 8767.672 |walltime 14402.781 |
Transformer | epoch 0 | step 28550 |avg loss 8.536 |avg tokens 4532.600 |tokens/s 8964.314 |walltime 14407.837 |
Transformer | epoch 0 | step 28560 |avg loss 8.142 |avg tokens 4655.700 |tokens/s 8827.684 |walltime 14413.111 |
Transformer | epoch 0 | step 28570 |avg loss 8.602 |avg tokens 4018.200 |tokens/s 8209.732 |walltime 14418.006 |
Transformer | epoch 0 | step 28580 |avg loss 8.816 |avg tokens 4177.700 |tokens/s 8738.377 |walltime 14422.787 |
Transformer | epoch 0 | step 28590 |avg loss 8.806 |avg tokens 3916.900 |tokens/s 8310.923 |walltime 14427.500 |
Transformer | epoch 0 | step 28600 |avg loss 8.688 |avg tokens 4889.200 |tokens/s 9554.538 |walltime 14432.617 |
Transformer | epoch 0 | step 28610 |avg loss 8.520 |avg tokens 4666.700 |tokens/s 9252.067 |walltime 14437.661 |
Transformer | epoch 0 | step 28620 |avg loss 8.827 |avg tokens 4046.500 |tokens/s 8639.433 |walltime 14442.345 |
Transformer | epoch 0 | step 28630 |avg loss 8.674 |avg tokens 4551.200 |tokens/s 9128.139 |walltime 14447.330 |
Transformer | epoch 0 | step 28640 |avg loss 8.326 |avg tokens 4863.200 |tokens/s 9055.671 |walltime 14452.701 |
Transformer | epoch 0 | step 28650 |avg loss 8.815 |avg tokens 4513.900 |tokens/s 9049.910 |walltime 14457.689 |
Transformer | epoch 0 | step 28660 |avg loss 8.499 |avg tokens 4805.000 |tokens/s 8938.621 |walltime 14463.064 |
Transformer | epoch 0 | step 28670 |avg loss 8.904 |avg tokens 3893.600 |tokens/s 8153.844 |walltime 14467.839 |
Transformer | epoch 0 | step 28680 |avg loss 8.656 |avg tokens 4460.400 |tokens/s 8856.137 |walltime 14472.876 |
Transformer | epoch 0 | step 28690 |avg loss 8.635 |avg tokens 4760.600 |tokens/s 9153.917 |walltime 14478.076 |
Transformer | epoch 0 | step 28700 |avg loss 8.551 |avg tokens 4822.100 |tokens/s 9547.307 |walltime 14483.127 |
Transformer | epoch 0 | step 28710 |avg loss 8.482 |avg tokens 4520.200 |tokens/s 8907.023 |walltime 14488.202 |
Transformer | epoch 0 | step 28720 |avg loss 8.614 |avg tokens 4722.800 |tokens/s 9454.405 |walltime 14493.197 |
Transformer | epoch 0 | step 28730 |avg loss 8.766 |avg tokens 4709.400 |tokens/s 9333.449 |walltime 14498.243 |
Transformer | epoch 0 | step 28740 |avg loss 8.647 |avg tokens 4149.700 |tokens/s 8314.167 |walltime 14503.234 |
Transformer | epoch 0 | step 28750 |avg loss 8.860 |avg tokens 4351.600 |tokens/s 9287.815 |walltime 14507.920 |
Transformer | epoch 0 | step 28760 |avg loss 8.398 |avg tokens 4407.600 |tokens/s 8652.332 |walltime 14513.014 |
Transformer | epoch 0 | step 28770 |avg loss 8.539 |avg tokens 4573.600 |tokens/s 8979.756 |walltime 14518.107 |
Transformer | epoch 0 | step 28780 |avg loss 8.637 |avg tokens 4540.500 |tokens/s 8974.185 |walltime 14523.166 |
Transformer | epoch 0 | step 28790 |avg loss 8.582 |avg tokens 4637.400 |tokens/s 8988.493 |walltime 14528.326 |
Transformer | epoch 0 | step 28800 |avg loss 8.465 |avg tokens 4650.400 |tokens/s 8954.075 |walltime 14533.519 |
Transformer | epoch 0 | step 28810 |avg loss 8.416 |avg tokens 4799.400 |tokens/s 9051.125 |walltime 14538.822 |
Transformer | epoch 0 | step 28820 |avg loss 8.633 |avg tokens 4959.300 |tokens/s 9557.934 |walltime 14544.011 |
Transformer | epoch 0 | step 28830 |avg loss 8.551 |avg tokens 4317.800 |tokens/s 8565.571 |walltime 14549.051 |
Transformer | epoch 0 | step 28840 |avg loss 8.418 |avg tokens 4807.800 |tokens/s 9195.900 |walltime 14554.280 |
Transformer | epoch 0 | step 28850 |avg loss 8.730 |avg tokens 4532.600 |tokens/s 9058.729 |walltime 14559.283 |
Transformer | epoch 0 | step 28860 |avg loss 8.875 |avg tokens 4268.100 |tokens/s 9003.849 |walltime 14564.024 |
Transformer | epoch 0 | step 28870 |avg loss 8.588 |avg tokens 4583.000 |tokens/s 8775.412 |walltime 14569.246 |
Transformer | epoch 0 | step 28880 |avg loss 8.762 |avg tokens 4143.200 |tokens/s 8311.610 |walltime 14574.231 |
Transformer | epoch 0 | step 28890 |avg loss 8.495 |avg tokens 4740.900 |tokens/s 9184.979 |walltime 14579.392 |
Transformer | epoch 0 | step 28900 |avg loss 8.562 |avg tokens 4752.100 |tokens/s 9019.607 |walltime 14584.661 |
Transformer | epoch 0 | step 28910 |avg loss 8.484 |avg tokens 4328.800 |tokens/s 8695.697 |walltime 14589.639 |
Transformer | epoch 0 | step 28920 |avg loss 8.713 |avg tokens 4631.800 |tokens/s 9134.330 |walltime 14594.710 |
Transformer | epoch 0 | step 28930 |avg loss 8.682 |avg tokens 4771.700 |tokens/s 9261.222 |walltime 14599.862 |
Transformer | epoch 0 | step 28940 |avg loss 8.454 |avg tokens 4566.500 |tokens/s 8759.449 |walltime 14605.076 |
Transformer | epoch 0 | step 28950 |avg loss 8.710 |avg tokens 4882.600 |tokens/s 9510.437 |walltime 14610.209 |
Transformer | epoch 0 | step 28960 |avg loss 8.388 |avg tokens 4677.100 |tokens/s 9313.045 |walltime 14615.232 |
Transformer | epoch 0 | step 28970 |avg loss 8.607 |avg tokens 4523.200 |tokens/s 8851.536 |walltime 14620.342 |
Transformer | epoch 0 | step 28980 |avg loss 8.617 |avg tokens 4747.900 |tokens/s 9331.063 |walltime 14625.430 |
Transformer | epoch 0 | step 28990 |avg loss 8.626 |avg tokens 4712.800 |tokens/s 9596.997 |walltime 14630.341 |
Transformer | epoch 0 | step 29000 |avg loss 8.555 |avg tokens 4557.100 |tokens/s 8834.883 |walltime 14635.499 |
Transformer | epoch 0 | step 29010 |avg loss 8.652 |avg tokens 4377.900 |tokens/s 8933.295 |walltime 14640.399 |
Transformer | epoch 0 | step 29020 |avg loss 8.466 |avg tokens 4427.300 |tokens/s 8692.242 |walltime 14645.493 |
Transformer | epoch 0 | step 29030 |avg loss 8.668 |avg tokens 4923.200 |tokens/s 9625.964 |walltime 14650.607 |
Transformer | epoch 0 | step 29040 |avg loss 8.487 |avg tokens 4773.200 |tokens/s 9328.183 |walltime 14655.724 |
Transformer | epoch 0 | step 29050 |avg loss 8.713 |avg tokens 4723.600 |tokens/s 9243.719 |walltime 14660.834 |
Transformer | epoch 0 | step 29060 |avg loss 8.499 |avg tokens 4456.300 |tokens/s 8711.419 |walltime 14665.950 |
Transformer | epoch 0 | step 29070 |avg loss 8.437 |avg tokens 4331.000 |tokens/s 8657.938 |walltime 14670.952 |
Transformer | epoch 0 | step 29080 |avg loss 8.588 |avg tokens 4221.600 |tokens/s 8414.804 |walltime 14675.969 |
Transformer | epoch 0 | step 29090 |avg loss 8.517 |avg tokens 4717.600 |tokens/s 8877.479 |walltime 14681.283 |
Transformer | epoch 0 | step 29100 |avg loss 8.558 |avg tokens 4376.700 |tokens/s 8711.628 |walltime 14686.307 |
Transformer | epoch 0 | step 29110 |avg loss 8.624 |avg tokens 4649.700 |tokens/s 9110.031 |walltime 14691.411 |
Transformer | epoch 0 | step 29120 |avg loss 8.647 |avg tokens 4610.600 |tokens/s 9257.334 |walltime 14696.392 |
Transformer | epoch 0 | step 29130 |avg loss 8.842 |avg tokens 4714.700 |tokens/s 9890.553 |walltime 14701.158 |
Transformer | epoch 0 | step 29140 |avg loss 8.696 |avg tokens 4613.500 |tokens/s 9066.745 |walltime 14706.247 |
Transformer | epoch 0 | step 29150 |avg loss 8.822 |avg tokens 4192.600 |tokens/s 9136.555 |walltime 14710.836 |
Transformer | epoch 0 | step 29160 |avg loss 8.387 |avg tokens 4444.500 |tokens/s 8840.606 |walltime 14715.863 |
Transformer | epoch 0 | step 29170 |avg loss 8.436 |avg tokens 4629.000 |tokens/s 8832.278 |walltime 14721.104 |
Transformer | epoch 0 | step 29180 |avg loss 8.582 |avg tokens 4206.900 |tokens/s 8806.703 |walltime 14725.881 |
Transformer | epoch 0 | step 29190 |avg loss 8.710 |avg tokens 4455.200 |tokens/s 8983.620 |walltime 14730.840 |
Transformer | epoch 0 | step 29200 |avg loss 8.673 |avg tokens 4730.100 |tokens/s 9247.738 |walltime 14735.955 |
Transformer | epoch 0 | step 29210 |avg loss 8.520 |avg tokens 4720.000 |tokens/s 9236.711 |walltime 14741.065 |
Transformer | epoch 0 | step 29220 |avg loss 8.664 |avg tokens 4709.300 |tokens/s 9211.822 |walltime 14746.177 |
Transformer | epoch 0 | step 29230 |avg loss 8.477 |avg tokens 4490.800 |tokens/s 9000.535 |walltime 14751.167 |
Transformer | epoch 0 | step 29240 |avg loss 8.663 |avg tokens 4389.900 |tokens/s 9243.647 |walltime 14755.916 |
Transformer | epoch 0 | step 29250 |avg loss 8.543 |avg tokens 4619.500 |tokens/s 8674.225 |walltime 14761.241 |
Transformer | epoch 0 | step 29260 |avg loss 8.784 |avg tokens 4257.100 |tokens/s 8833.573 |walltime 14766.061 |
Transformer | epoch 0 | step 29270 |avg loss 8.552 |avg tokens 4556.800 |tokens/s 8852.074 |walltime 14771.208 |
Transformer | epoch 0 | step 29280 |avg loss 8.754 |avg tokens 4375.900 |tokens/s 8849.354 |walltime 14776.153 |
Transformer | epoch 0 | step 29290 |avg loss 8.587 |avg tokens 4246.900 |tokens/s 8655.379 |walltime 14781.060 |
Transformer | epoch 0 | step 29300 |avg loss 8.566 |avg tokens 4630.300 |tokens/s 9040.547 |walltime 14786.182 |
Transformer | epoch 0 | step 29310 |avg loss 8.428 |avg tokens 4346.000 |tokens/s 8682.860 |walltime 14791.187 |
Transformer | epoch 0 | step 29320 |avg loss 8.495 |avg tokens 4540.000 |tokens/s 8728.874 |walltime 14796.388 |
Transformer | epoch 0 | step 29330 |avg loss 8.501 |avg tokens 4435.800 |tokens/s 8690.290 |walltime 14801.492 |
Transformer | epoch 0 | step 29340 |avg loss 8.649 |avg tokens 4648.200 |tokens/s 9478.386 |walltime 14806.396 |
Transformer | epoch 0 | step 29350 |avg loss 8.404 |avg tokens 4699.200 |tokens/s 8944.020 |walltime 14811.650 |
Transformer | epoch 0 | step 29360 |avg loss 8.622 |avg tokens 4632.200 |tokens/s 8951.176 |walltime 14816.825 |
Transformer | epoch 0 | step 29370 |avg loss 8.585 |avg tokens 4415.800 |tokens/s 8857.636 |walltime 14821.811 |
Transformer | epoch 0 | step 29380 |avg loss 8.599 |avg tokens 4591.600 |tokens/s 9233.421 |walltime 14826.784 |
Transformer | epoch 0 | step 29390 |avg loss 8.511 |avg tokens 4142.800 |tokens/s 8620.351 |walltime 14831.589 |
Transformer | epoch 0 | step 29400 |avg loss 8.627 |avg tokens 4325.500 |tokens/s 8699.482 |walltime 14836.561 |
Transformer | epoch 0 | step 29410 |avg loss 8.431 |avg tokens 4635.200 |tokens/s 9014.432 |walltime 14841.703 |
Transformer | epoch 0 | step 29420 |avg loss 8.536 |avg tokens 4735.700 |tokens/s 9155.932 |walltime 14846.876 |
Transformer | epoch 0 | step 29430 |avg loss 8.243 |avg tokens 4867.200 |tokens/s 9192.025 |walltime 14852.171 |
Transformer | epoch 0 | step 29440 |avg loss 8.532 |avg tokens 4705.500 |tokens/s 9433.509 |walltime 14857.159 |
Transformer | epoch 0 | step 29450 |avg loss 8.423 |avg tokens 4452.600 |tokens/s 8721.658 |walltime 14862.264 |
Transformer | epoch 0 | step 29460 |avg loss 8.419 |avg tokens 4603.600 |tokens/s 9234.121 |walltime 14867.250 |
Transformer | epoch 0 | step 29470 |avg loss 8.340 |avg tokens 4662.400 |tokens/s 9352.207 |walltime 14872.235 |
Transformer | epoch 0 | step 29480 |avg loss 8.714 |avg tokens 4810.200 |tokens/s 9579.148 |walltime 14877.256 |
Transformer | epoch 0 | step 29490 |avg loss 8.475 |avg tokens 4814.900 |tokens/s 9073.278 |walltime 14882.563 |
Transformer | epoch 0 | step 29500 |avg loss 8.618 |avg tokens 4685.000 |tokens/s 9170.504 |walltime 14887.672 |
Transformer | epoch 0 | step 29510 |avg loss 8.428 |avg tokens 4580.700 |tokens/s 8887.409 |walltime 14892.826 |
Transformer | epoch 0 | step 29520 |avg loss 8.533 |avg tokens 4792.600 |tokens/s 9094.768 |walltime 14898.096 |
Transformer | epoch 0 | step 29530 |avg loss 8.617 |avg tokens 4459.500 |tokens/s 9339.493 |walltime 14902.871 |
Transformer | epoch 0 | step 29540 |avg loss 8.539 |avg tokens 3982.400 |tokens/s 8205.109 |walltime 14907.724 |
Transformer | epoch 0 | step 29550 |avg loss 8.673 |avg tokens 4573.000 |tokens/s 9183.119 |walltime 14912.704 |
Transformer | epoch 0 | step 29560 |avg loss 8.601 |avg tokens 4593.200 |tokens/s 9085.704 |walltime 14917.759 |
Transformer | epoch 0 | step 29570 |avg loss 8.414 |avg tokens 4752.700 |tokens/s 9120.925 |walltime 14922.970 |
Transformer | epoch 0 | step 29580 |avg loss 8.586 |avg tokens 4618.100 |tokens/s 9278.290 |walltime 14927.947 |
Transformer | epoch 0 | step 29590 |avg loss 8.525 |avg tokens 4711.800 |tokens/s 8880.390 |walltime 14933.253 |
Transformer | epoch 0 | step 29600 |avg loss 8.412 |avg tokens 4452.900 |tokens/s 8824.380 |walltime 14938.299 |
Transformer | epoch 0 | step 29610 |avg loss 8.825 |avg tokens 4524.600 |tokens/s 8980.524 |walltime 14943.338 |
Transformer | epoch 0 | step 29620 |avg loss 8.628 |avg tokens 4541.100 |tokens/s 9114.510 |walltime 14948.320 |
Transformer | epoch 0 | step 29630 |avg loss 8.624 |avg tokens 4064.000 |tokens/s 8279.293 |walltime 14953.229 |
Transformer | epoch 0 | step 29640 |avg loss 8.645 |avg tokens 4230.000 |tokens/s 8720.871 |walltime 14958.079 |
Transformer | epoch 0 | step 29650 |avg loss 8.604 |avg tokens 4724.000 |tokens/s 9482.052 |walltime 14963.061 |
Transformer | epoch 0 | step 29660 |avg loss 8.350 |avg tokens 4374.400 |tokens/s 8659.500 |walltime 14968.113 |
Transformer | epoch 0 | step 29670 |avg loss 8.691 |avg tokens 4153.400 |tokens/s 8893.339 |walltime 14972.783 |
Transformer | epoch 0 | step 29680 |avg loss 9.024 |avg tokens 4234.600 |tokens/s 8805.176 |walltime 14977.592 |
Transformer | epoch 0 | step 29690 |avg loss 8.508 |avg tokens 4113.900 |tokens/s 8402.431 |walltime 14982.488 |
Transformer | epoch 0 | step 29700 |avg loss 8.596 |avg tokens 4836.700 |tokens/s 9163.686 |walltime 14987.766 |
Transformer | epoch 0 | step 29710 |avg loss 8.733 |avg tokens 4756.200 |tokens/s 9514.901 |walltime 14992.765 |
Transformer | epoch 0 | step 29720 |avg loss 8.662 |avg tokens 3896.300 |tokens/s 8100.492 |walltime 14997.575 |
Transformer | epoch 0 | step 29730 |avg loss 8.788 |avg tokens 4776.700 |tokens/s 9432.939 |walltime 15002.639 |
Transformer | epoch 0 | step 29740 |avg loss 8.470 |avg tokens 4856.300 |tokens/s 9253.503 |walltime 15007.887 |
Transformer | epoch 0 | step 29750 |avg loss 8.385 |avg tokens 4848.800 |tokens/s 9263.637 |walltime 15013.121 |
Transformer | epoch 0 | step 29760 |avg loss 8.802 |avg tokens 4346.600 |tokens/s 8874.898 |walltime 15018.019 |
Transformer | epoch 0 | step 29770 |avg loss 8.842 |avg tokens 4587.700 |tokens/s 9522.415 |walltime 15022.836 |
Transformer | epoch 0 | step 29780 |avg loss 8.580 |avg tokens 4500.200 |tokens/s 8791.264 |walltime 15027.955 |
Transformer | epoch 0 | step 29790 |avg loss 8.824 |avg tokens 4390.100 |tokens/s 9369.983 |walltime 15032.641 |
Transformer | epoch 0 | step 29800 |avg loss 8.389 |avg tokens 4533.600 |tokens/s 8987.847 |walltime 15037.685 |
Transformer | epoch 0 | step 29810 |avg loss 8.591 |avg tokens 4423.000 |tokens/s 8827.094 |walltime 15042.696 |
Transformer | epoch 0 | step 29820 |avg loss 8.529 |avg tokens 4803.200 |tokens/s 9093.516 |walltime 15047.978 |
Transformer | epoch 0 | step 29830 |avg loss 8.519 |avg tokens 4735.800 |tokens/s 9192.084 |walltime 15053.130 |
Transformer | epoch 0 | step 29840 |avg loss 8.421 |avg tokens 4856.200 |tokens/s 9132.444 |walltime 15058.447 |
Transformer | epoch 0 | step 29850 |avg loss 8.276 |avg tokens 4551.800 |tokens/s 8810.213 |walltime 15063.614 |
Transformer | epoch 0 | step 29860 |avg loss 8.677 |avg tokens 4172.400 |tokens/s 8537.060 |walltime 15068.501 |
Transformer | epoch 0 | step 29870 |avg loss 8.437 |avg tokens 4603.100 |tokens/s 8931.598 |walltime 15073.655 |
Transformer | epoch 0 | step 29880 |avg loss 8.539 |avg tokens 4733.600 |tokens/s 9183.917 |walltime 15078.809 |
Transformer | epoch 0 | step 29890 |avg loss 8.757 |avg tokens 4883.900 |tokens/s 9609.773 |walltime 15083.891 |
Transformer | epoch 0 | step 29900 |avg loss 8.475 |avg tokens 4628.000 |tokens/s 9123.081 |walltime 15088.964 |
Transformer | epoch 0 | step 29910 |avg loss 8.673 |avg tokens 4698.400 |tokens/s 9017.838 |walltime 15094.174 |
Transformer | epoch 0 | step 29920 |avg loss 8.657 |avg tokens 4349.800 |tokens/s 9038.796 |walltime 15098.987 |
Transformer | epoch 0 | step 29930 |avg loss 8.436 |avg tokens 4812.700 |tokens/s 9167.183 |walltime 15104.236 |
Transformer | epoch 0 | step 29940 |avg loss 8.331 |avg tokens 4369.500 |tokens/s 8524.812 |walltime 15109.362 |
Transformer | epoch 0 | step 29950 |avg loss 8.562 |avg tokens 4512.600 |tokens/s 8930.217 |walltime 15114.415 |
Transformer | epoch 0 | step 29960 |avg loss 8.690 |avg tokens 4456.600 |tokens/s 8981.960 |walltime 15119.377 |
Transformer | epoch 0 | step 29970 |avg loss 8.341 |avg tokens 4649.800 |tokens/s 9030.832 |walltime 15124.526 |
Transformer | epoch 0 | step 29980 |avg loss 8.742 |avg tokens 4283.100 |tokens/s 9071.661 |walltime 15129.247 |
Transformer | epoch 0 | step 29990 |avg loss 8.571 |avg tokens 4596.800 |tokens/s 8792.729 |walltime 15134.475 |
Transformer | epoch 0 | step 30000 |avg loss 8.943 |avg tokens 4297.700 |tokens/s 9341.330 |walltime 15139.076 |
Transformer | epoch 0 | step 30010 |avg loss 8.586 |avg tokens 4403.600 |tokens/s 8644.567 |walltime 15144.170 |
Transformer | epoch 0 | step 30020 |avg loss 8.722 |avg tokens 4488.100 |tokens/s 8992.898 |walltime 15149.161 |
Transformer | epoch 0 | step 30030 |avg loss 8.408 |avg tokens 4661.300 |tokens/s 9056.230 |walltime 15154.308 |
Transformer | epoch 0 | step 30040 |avg loss 9.007 |avg tokens 4725.100 |tokens/s 9809.953 |walltime 15159.124 |
Transformer | epoch 0 | step 30050 |avg loss 8.708 |avg tokens 4201.100 |tokens/s 8576.578 |walltime 15164.023 |
Transformer | epoch 0 | step 30060 |avg loss 8.494 |avg tokens 4724.100 |tokens/s 8978.786 |walltime 15169.284 |
Transformer | epoch 0 | step 30070 |avg loss 8.654 |avg tokens 3884.800 |tokens/s 8192.671 |walltime 15174.026 |
Transformer | epoch 0 | step 30080 |avg loss 8.861 |avg tokens 4686.700 |tokens/s 9534.003 |walltime 15178.942 |
Transformer | epoch 0 | step 30090 |avg loss 8.693 |avg tokens 4622.200 |tokens/s 9601.105 |walltime 15183.756 |
Transformer | epoch 0 | step 30100 |avg loss 8.513 |avg tokens 4597.200 |tokens/s 8940.321 |walltime 15188.898 |
Transformer | epoch 0 | step 30110 |avg loss 8.695 |avg tokens 4070.800 |tokens/s 8663.446 |walltime 15193.597 |
Transformer | epoch 0 | step 30120 |avg loss 8.475 |avg tokens 4649.800 |tokens/s 9229.206 |walltime 15198.635 |
Transformer | epoch 0 | step 30130 |avg loss 8.640 |avg tokens 4657.800 |tokens/s 9042.331 |walltime 15203.786 |
Transformer | epoch 0 | step 30140 |avg loss 8.248 |avg tokens 4446.900 |tokens/s 8858.762 |walltime 15208.806 |
Transformer | epoch 0 | step 30150 |avg loss 8.363 |avg tokens 4903.200 |tokens/s 9516.650 |walltime 15213.958 |
Transformer | epoch 0 | step 30160 |avg loss 8.609 |avg tokens 4763.900 |tokens/s 9166.025 |walltime 15219.156 |
Transformer | epoch 0 | step 30170 |avg loss 8.626 |avg tokens 4563.300 |tokens/s 9081.645 |walltime 15224.180 |
Transformer | epoch 0 | step 30180 |avg loss 8.351 |avg tokens 4733.100 |tokens/s 9089.982 |walltime 15229.387 |
Transformer | epoch 0 | step 30190 |avg loss 8.488 |avg tokens 4346.200 |tokens/s 8469.741 |walltime 15234.519 |
Transformer | epoch 0 | step 30200 |avg loss 8.695 |avg tokens 4313.600 |tokens/s 8776.636 |walltime 15239.434 |
Transformer | epoch 0 | step 30210 |avg loss 8.601 |avg tokens 4048.700 |tokens/s 8251.520 |walltime 15244.340 |
Transformer | epoch 0 | step 30220 |avg loss 8.351 |avg tokens 4744.800 |tokens/s 9027.559 |walltime 15249.596 |
Transformer | epoch 0 | step 30230 |avg loss 8.549 |avg tokens 4746.200 |tokens/s 9066.119 |walltime 15254.831 |
Transformer | epoch 0 | step 30240 |avg loss 8.430 |avg tokens 4622.400 |tokens/s 8887.549 |walltime 15260.032 |
Transformer | epoch 0 | step 30250 |avg loss 8.525 |avg tokens 4152.400 |tokens/s 8537.335 |walltime 15264.896 |
Transformer | epoch 0 | step 30260 |avg loss 8.187 |avg tokens 4775.200 |tokens/s 9044.580 |walltime 15270.176 |
Transformer | epoch 0 | step 30270 |avg loss 8.522 |avg tokens 4298.800 |tokens/s 8602.570 |walltime 15275.173 |
Transformer | epoch 0 | step 30280 |avg loss 8.581 |avg tokens 4340.900 |tokens/s 8814.256 |walltime 15280.098 |
Transformer | epoch 0 | step 30290 |avg loss 8.516 |avg tokens 4432.600 |tokens/s 8727.100 |walltime 15285.177 |
Transformer | epoch 0 | step 30300 |avg loss 8.557 |avg tokens 4814.800 |tokens/s 9634.852 |walltime 15290.174 |
Transformer | epoch 0 | step 30310 |avg loss 8.278 |avg tokens 4672.400 |tokens/s 9260.932 |walltime 15295.219 |
Transformer | epoch 0 | step 30320 |avg loss 8.804 |avg tokens 4628.300 |tokens/s 9360.739 |walltime 15300.164 |
Transformer | epoch 0 | step 30330 |avg loss 8.727 |avg tokens 4736.500 |tokens/s 9398.078 |walltime 15305.204 |
Transformer | epoch 0 | step 30340 |avg loss 8.565 |avg tokens 4770.400 |tokens/s 9390.330 |walltime 15310.284 |
Transformer | epoch 0 | step 30350 |avg loss 8.337 |avg tokens 4849.800 |tokens/s 9303.495 |walltime 15315.497 |
Transformer | epoch 0 | step 30360 |avg loss 8.518 |avg tokens 4239.700 |tokens/s 8864.452 |walltime 15320.279 |
Transformer | epoch 0 | step 30370 |avg loss 8.528 |avg tokens 4460.300 |tokens/s 8772.980 |walltime 15325.363 |
Transformer | epoch 0 | step 30380 |avg loss 8.537 |avg tokens 4665.100 |tokens/s 9291.563 |walltime 15330.384 |
Transformer | epoch 0 | step 30390 |avg loss 8.664 |avg tokens 4567.000 |tokens/s 9295.682 |walltime 15335.297 |
Transformer | epoch 0 | step 30400 |avg loss 8.504 |avg tokens 4778.000 |tokens/s 9390.751 |walltime 15340.385 |
Transformer | epoch 0 | step 30410 |avg loss 8.718 |avg tokens 4675.400 |tokens/s 9357.383 |walltime 15345.382 |
Transformer | epoch 0 | step 30420 |avg loss 8.570 |avg tokens 4400.200 |tokens/s 8899.912 |walltime 15350.326 |
Transformer | epoch 0 | step 30430 |avg loss 8.614 |avg tokens 4388.100 |tokens/s 8456.710 |walltime 15355.515 |
Transformer | epoch 0 | step 30440 |avg loss 8.505 |avg tokens 4530.400 |tokens/s 8900.017 |walltime 15360.605 |
Transformer | epoch 0 | step 30450 |avg loss 8.732 |avg tokens 4820.100 |tokens/s 9788.723 |walltime 15365.529 |
Transformer | epoch 0 | step 30460 |avg loss 8.627 |avg tokens 4328.100 |tokens/s 8564.150 |walltime 15370.583 |
Transformer | epoch 0 | step 30470 |avg loss 8.380 |avg tokens 4811.700 |tokens/s 9194.165 |walltime 15375.816 |
Transformer | epoch 0 | step 30480 |avg loss 8.659 |avg tokens 4138.700 |tokens/s 8654.188 |walltime 15380.599 |
Transformer | epoch 0 | step 30490 |avg loss 8.743 |avg tokens 4374.500 |tokens/s 8923.325 |walltime 15385.501 |
Transformer | epoch 0 | step 30500 |avg loss 8.548 |avg tokens 4746.400 |tokens/s 9196.971 |walltime 15390.662 |
Transformer | epoch 0 | step 30510 |avg loss 8.353 |avg tokens 4970.400 |tokens/s 9227.447 |walltime 15396.048 |
Transformer | epoch 0 | step 30520 |avg loss 8.494 |avg tokens 4940.700 |tokens/s 9475.513 |walltime 15401.263 |
Transformer | epoch 0 | step 30530 |avg loss 8.770 |avg tokens 4075.200 |tokens/s 8402.962 |walltime 15406.112 |
Transformer | epoch 0 | step 30540 |avg loss 8.289 |avg tokens 4301.300 |tokens/s 8552.839 |walltime 15411.141 |
Transformer | epoch 0 | step 30550 |avg loss 8.614 |avg tokens 4645.500 |tokens/s 9132.991 |walltime 15416.228 |
Transformer | epoch 0 | step 30560 |avg loss 8.485 |avg tokens 4559.100 |tokens/s 9287.624 |walltime 15421.137 |
Transformer | epoch 0 | step 30570 |avg loss 8.701 |avg tokens 4352.100 |tokens/s 8761.991 |walltime 15426.104 |
Transformer | epoch 0 | step 30580 |avg loss 8.474 |avg tokens 4706.400 |tokens/s 9053.505 |walltime 15431.302 |
Transformer | epoch 0 | step 30590 |avg loss 8.496 |avg tokens 3970.000 |tokens/s 8321.244 |walltime 15436.073 |
Transformer | epoch 0 | step 30600 |avg loss 8.538 |avg tokens 4052.500 |tokens/s 8169.634 |walltime 15441.034 |
Transformer | epoch 0 | step 30610 |avg loss 8.723 |avg tokens 4094.200 |tokens/s 8735.806 |walltime 15445.720 |
Transformer | epoch 0 | step 30620 |avg loss 8.742 |avg tokens 4345.800 |tokens/s 8785.609 |walltime 15450.667 |
Transformer | epoch 0 | step 30630 |avg loss 8.784 |avg tokens 4714.500 |tokens/s 9518.372 |walltime 15455.620 |
Transformer | epoch 0 | step 30640 |avg loss 8.316 |avg tokens 4635.200 |tokens/s 9120.647 |walltime 15460.702 |
Transformer | epoch 0 | step 30650 |avg loss 8.601 |avg tokens 4840.500 |tokens/s 9077.900 |walltime 15466.034 |
Transformer | epoch 0 | step 30660 |avg loss 8.668 |avg tokens 4244.400 |tokens/s 8617.145 |walltime 15470.960 |
Transformer | epoch 0 | step 30670 |avg loss 8.717 |avg tokens 4469.400 |tokens/s 9147.792 |walltime 15475.845 |
Transformer | epoch 0 | step 30680 |avg loss 8.553 |avg tokens 4526.200 |tokens/s 8838.693 |walltime 15480.966 |
Transformer | epoch 0 | step 30690 |avg loss 8.462 |avg tokens 4316.200 |tokens/s 8533.442 |walltime 15486.024 |
Transformer | epoch 0 | step 30700 |avg loss 8.640 |avg tokens 4419.200 |tokens/s 8752.938 |walltime 15491.073 |
Transformer | epoch 0 | step 30710 |avg loss 8.931 |avg tokens 4346.700 |tokens/s 9272.613 |walltime 15495.761 |
Transformer | epoch 0 | step 30720 |avg loss 8.637 |avg tokens 4170.500 |tokens/s 8274.601 |walltime 15500.801 |
Transformer | epoch 0 | step 30730 |avg loss 8.506 |avg tokens 4090.600 |tokens/s 8525.641 |walltime 15505.599 |
Transformer | epoch 0 | step 30740 |avg loss 8.708 |avg tokens 4807.700 |tokens/s 9532.811 |walltime 15510.642 |
Transformer | epoch 0 | step 30750 |avg loss 8.723 |avg tokens 4152.000 |tokens/s 8660.023 |walltime 15515.437 |
Transformer | epoch 0 | step 30760 |avg loss 8.629 |avg tokens 4089.600 |tokens/s 8298.123 |walltime 15520.365 |
Transformer | epoch 0 | step 30770 |avg loss 8.388 |avg tokens 4983.200 |tokens/s 9144.142 |walltime 15525.815 |
Transformer | epoch 0 | step 30780 |avg loss 8.339 |avg tokens 4831.400 |tokens/s 9274.901 |walltime 15531.024 |
Transformer | epoch 0 | step 30790 |avg loss 8.663 |avg tokens 4396.200 |tokens/s 9011.475 |walltime 15535.902 |
Transformer | epoch 0 | step 30800 |avg loss 8.547 |avg tokens 4478.200 |tokens/s 8759.657 |walltime 15541.015 |
Transformer | epoch 0 | step 30810 |avg loss 8.441 |avg tokens 4150.600 |tokens/s 8430.036 |walltime 15545.938 |
Transformer | epoch 0 | step 30820 |avg loss 8.263 |avg tokens 4687.600 |tokens/s 9072.733 |walltime 15551.105 |
Transformer | epoch 0 | step 30830 |avg loss 8.647 |avg tokens 4306.700 |tokens/s 8454.562 |walltime 15556.199 |
Transformer | epoch 0 | step 30840 |avg loss 8.478 |avg tokens 4649.800 |tokens/s 8792.455 |walltime 15561.487 |
Transformer | epoch 0 | step 30850 |avg loss 8.523 |avg tokens 4397.000 |tokens/s 8703.926 |walltime 15566.539 |
Transformer | epoch 0 | step 30860 |avg loss 8.500 |avg tokens 4922.900 |tokens/s 9608.036 |walltime 15571.663 |
Transformer | epoch 0 | step 30870 |avg loss 8.401 |avg tokens 4726.500 |tokens/s 8827.548 |walltime 15577.017 |
Transformer | epoch 0 | step 30880 |avg loss 8.226 |avg tokens 4740.400 |tokens/s 9175.801 |walltime 15582.183 |
Transformer | epoch 0 | step 30890 |avg loss 8.531 |avg tokens 4180.900 |tokens/s 8430.539 |walltime 15587.142 |
Transformer | epoch 0 | step 30900 |avg loss 8.647 |avg tokens 4683.500 |tokens/s 9445.952 |walltime 15592.101 |
Transformer | epoch 0 | step 30910 |avg loss 8.621 |avg tokens 4518.200 |tokens/s 8999.296 |walltime 15597.121 |
Transformer | epoch 0 | step 30920 |avg loss 8.520 |avg tokens 4710.200 |tokens/s 9031.961 |walltime 15602.336 |
Transformer | epoch 0 | step 30930 |avg loss 8.352 |avg tokens 4741.800 |tokens/s 9074.401 |walltime 15607.562 |
Transformer | epoch 0 | step 30940 |avg loss 8.364 |avg tokens 4518.800 |tokens/s 8723.543 |walltime 15612.742 |
Transformer | epoch 0 | step 30950 |avg loss 8.415 |avg tokens 4801.600 |tokens/s 9089.963 |walltime 15618.024 |
Transformer | epoch 0 | step 30960 |avg loss 8.484 |avg tokens 4640.500 |tokens/s 9048.483 |walltime 15623.152 |
Transformer | epoch 0 | step 30970 |avg loss 8.587 |avg tokens 4759.100 |tokens/s 9209.553 |walltime 15628.320 |
Transformer | epoch 0 | step 30980 |avg loss 8.854 |avg tokens 3754.900 |tokens/s 8249.318 |walltime 15632.872 |
Transformer | epoch 0 | step 30990 |avg loss 8.582 |avg tokens 4757.400 |tokens/s 9381.435 |walltime 15637.943 |
Transformer | epoch 0 | step 31000 |avg loss 8.609 |avg tokens 4737.200 |tokens/s 9378.986 |walltime 15642.994 |
Transformer | epoch 0 | step 31010 |avg loss 8.236 |avg tokens 4667.500 |tokens/s 9095.611 |walltime 15648.125 |
Transformer | epoch 0 | step 31020 |avg loss 8.518 |avg tokens 4582.100 |tokens/s 8993.079 |walltime 15653.221 |
Transformer | epoch 0 | step 31030 |avg loss 8.542 |avg tokens 4656.100 |tokens/s 9238.628 |walltime 15658.260 |
Transformer | epoch 0 | step 31040 |avg loss 8.289 |avg tokens 4684.800 |tokens/s 9101.720 |walltime 15663.408 |
Transformer | epoch 0 | step 31050 |avg loss 8.487 |avg tokens 4470.100 |tokens/s 8876.084 |walltime 15668.444 |
Transformer | epoch 0 | step 31060 |avg loss 8.491 |avg tokens 4774.600 |tokens/s 9794.971 |walltime 15673.318 |
Transformer | epoch 0 | step 31070 |avg loss 8.388 |avg tokens 4450.600 |tokens/s 8778.515 |walltime 15678.388 |
Transformer | epoch 0 | step 31080 |avg loss 8.779 |avg tokens 4411.300 |tokens/s 9254.473 |walltime 15683.155 |
Transformer | epoch 0 | step 31090 |avg loss 8.407 |avg tokens 4702.400 |tokens/s 8872.983 |walltime 15688.454 |
Transformer | epoch 0 | step 31100 |avg loss 8.220 |avg tokens 4411.500 |tokens/s 8796.162 |walltime 15693.470 |
Transformer | epoch 0 | step 31110 |avg loss 8.729 |avg tokens 4470.500 |tokens/s 9072.737 |walltime 15698.397 |
Transformer | epoch 0 | step 31120 |avg loss 8.568 |avg tokens 4876.200 |tokens/s 9383.682 |walltime 15703.594 |
Transformer | epoch 0 | step 31130 |avg loss 8.675 |avg tokens 4145.400 |tokens/s 8640.533 |walltime 15708.391 |
Transformer | epoch 0 | step 31140 |avg loss 8.405 |avg tokens 4435.400 |tokens/s 8715.949 |walltime 15713.480 |
Transformer | epoch 0 | step 31150 |avg loss 8.394 |avg tokens 4749.600 |tokens/s 8924.969 |walltime 15718.802 |
Transformer | epoch 0 | step 31160 |avg loss 8.334 |avg tokens 4374.900 |tokens/s 8758.383 |walltime 15723.797 |
Transformer | epoch 0 | step 31170 |avg loss 8.224 |avg tokens 4348.800 |tokens/s 8618.966 |walltime 15728.842 |
Transformer | epoch 0 | step 31180 |avg loss 8.558 |avg tokens 4757.700 |tokens/s 9311.608 |walltime 15733.952 |
Transformer | epoch 0 | step 31190 |avg loss 8.646 |avg tokens 4637.800 |tokens/s 9366.563 |walltime 15738.903 |
Transformer | epoch 0 | step 31200 |avg loss 8.810 |avg tokens 4266.600 |tokens/s 8444.300 |walltime 15743.956 |
Transformer | epoch 0 | step 31210 |avg loss 8.670 |avg tokens 4462.400 |tokens/s 8818.592 |walltime 15749.016 |
Transformer | epoch 0 | step 31220 |avg loss 8.501 |avg tokens 4759.500 |tokens/s 9140.489 |walltime 15754.223 |
Transformer | epoch 0 | step 31230 |avg loss 8.493 |avg tokens 4484.700 |tokens/s 8897.391 |walltime 15759.264 |
Transformer | epoch 0 | step 31240 |avg loss 8.448 |avg tokens 4776.000 |tokens/s 9229.595 |walltime 15764.438 |
Transformer | epoch 0 | step 31250 |avg loss 8.469 |avg tokens 4780.800 |tokens/s 9209.768 |walltime 15769.629 |
Transformer | epoch 0 | step 31260 |avg loss 8.568 |avg tokens 4218.800 |tokens/s 8381.407 |walltime 15774.663 |
Transformer | epoch 0 | step 31270 |avg loss 8.613 |avg tokens 4297.300 |tokens/s 8793.943 |walltime 15779.550 |
Transformer | epoch 0 | step 31280 |avg loss 8.566 |avg tokens 4646.700 |tokens/s 8968.270 |walltime 15784.731 |
Transformer | epoch 0 | step 31290 |avg loss 8.468 |avg tokens 4589.600 |tokens/s 8909.873 |walltime 15789.882 |
Transformer | epoch 0 | step 31300 |avg loss 8.698 |avg tokens 4459.000 |tokens/s 8841.318 |walltime 15794.925 |
Transformer | epoch 0 | step 31310 |avg loss 8.516 |avg tokens 4796.500 |tokens/s 9089.238 |walltime 15800.202 |
Transformer | epoch 0 | step 31320 |avg loss 8.460 |avg tokens 4823.300 |tokens/s 9080.129 |walltime 15805.514 |
Transformer | epoch 0 | step 31330 |avg loss 8.609 |avg tokens 4161.700 |tokens/s 8747.641 |walltime 15810.272 |
Transformer | epoch 0 | step 31340 |avg loss 8.374 |avg tokens 4619.400 |tokens/s 8851.706 |walltime 15815.491 |
Transformer | epoch 0 | step 31350 |avg loss 8.655 |avg tokens 4284.100 |tokens/s 8662.012 |walltime 15820.436 |
Transformer | epoch 0 | step 31360 |avg loss 8.593 |avg tokens 4272.200 |tokens/s 8560.387 |walltime 15825.427 |
Transformer | epoch 0 | step 31370 |avg loss 8.706 |avg tokens 4303.000 |tokens/s 8810.224 |walltime 15830.311 |
Transformer | epoch 0 | step 31380 |avg loss 8.506 |avg tokens 4485.600 |tokens/s 8696.138 |walltime 15835.469 |
Transformer | epoch 0 | step 31390 |avg loss 8.452 |avg tokens 4600.800 |tokens/s 9082.180 |walltime 15840.535 |
Transformer | epoch 0 | step 31400 |avg loss 8.488 |avg tokens 4280.100 |tokens/s 8551.359 |walltime 15845.540 |
Transformer | epoch 0 | step 31410 |avg loss 8.251 |avg tokens 4780.500 |tokens/s 9041.368 |walltime 15850.828 |
Transformer | epoch 0 | step 31420 |avg loss 8.448 |avg tokens 4155.000 |tokens/s 8317.141 |walltime 15855.823 |
Transformer | epoch 0 | step 31430 |avg loss 8.636 |avg tokens 3986.700 |tokens/s 8310.114 |walltime 15860.621 |
Transformer | epoch 0 | step 31440 |avg loss 8.756 |avg tokens 4487.100 |tokens/s 8830.534 |walltime 15865.702 |
Transformer | epoch 0 | step 31450 |avg loss 8.353 |avg tokens 4595.100 |tokens/s 8709.073 |walltime 15870.978 |
Transformer | epoch 0 | step 31460 |avg loss 8.493 |avg tokens 4754.400 |tokens/s 9166.863 |walltime 15876.165 |
Transformer | epoch 0 | step 31470 |avg loss 8.607 |avg tokens 4802.100 |tokens/s 9173.968 |walltime 15881.399 |
Transformer | epoch 0 | step 31480 |avg loss 8.364 |avg tokens 4997.600 |tokens/s 9467.948 |walltime 15886.678 |
Epoch time: 15881.264474153519
Transformer | epoch 0 | step 31487 |avg loss 8.418 |avg tokens 4388.143 |tokens/s 7441.169 |walltime 15890.806 |
Validation loss on subset valid: 8.601112504522968
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [124].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [122].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [124], which does not match the required output shape [122].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [122], which does not match the required output shape [68].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [68], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [103].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [103], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [130].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [130].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [130], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [91].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [91].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [91], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [84].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [84].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [84], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [79].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [79].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [79], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [62].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [62], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [63].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [63].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [63], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [58].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [58].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [58].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [58].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [58], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [51].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [51], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [48].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [48], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
| Translated 3000 sentences (76680 tokens) in 68.5s (43.81 sentences/s, 1119.73 tokens/s)
| Eval completed in: 80.27s | UNCASED BLEU 0.34
Transformer | epoch 1 | step 31497 |avg loss 8.285 |avg tokens 5237.000 |tokens/s 8912.848 |walltime 15993.409 |
Transformer | epoch 1 | step 31507 |avg loss 8.748 |avg tokens 4675.900 |tokens/s 9399.344 |walltime 15998.384 |
Transformer | epoch 1 | step 31517 |avg loss 8.276 |avg tokens 4426.700 |tokens/s 8951.910 |walltime 16003.329 |
Transformer | epoch 1 | step 31527 |avg loss 8.582 |avg tokens 4692.400 |tokens/s 9389.239 |walltime 16008.326 |
Transformer | epoch 1 | step 31537 |avg loss 8.699 |avg tokens 4671.300 |tokens/s 9179.792 |walltime 16013.415 |
Transformer | epoch 1 | step 31547 |avg loss 8.655 |avg tokens 4175.200 |tokens/s 8499.561 |walltime 16018.327 |
Transformer | epoch 1 | step 31557 |avg loss 8.591 |avg tokens 4382.700 |tokens/s 8765.021 |walltime 16023.328 |
Transformer | epoch 1 | step 31567 |avg loss 8.593 |avg tokens 4388.800 |tokens/s 9047.547 |walltime 16028.178 |
Transformer | epoch 1 | step 31577 |avg loss 8.512 |avg tokens 4685.200 |tokens/s 9449.443 |walltime 16033.136 |
Transformer | epoch 1 | step 31587 |avg loss 8.042 |avg tokens 4409.400 |tokens/s 8763.344 |walltime 16038.168 |
Transformer | epoch 1 | step 31597 |avg loss 8.592 |avg tokens 4285.500 |tokens/s 8693.025 |walltime 16043.098 |
Transformer | epoch 1 | step 31607 |avg loss 8.812 |avg tokens 4572.900 |tokens/s 9491.464 |walltime 16047.916 |
Transformer | epoch 1 | step 31617 |avg loss 8.436 |avg tokens 4798.500 |tokens/s 9179.598 |walltime 16053.143 |
Transformer | epoch 1 | step 31627 |avg loss 8.449 |avg tokens 4723.200 |tokens/s 9153.312 |walltime 16058.303 |
Transformer | epoch 1 | step 31637 |avg loss 8.486 |avg tokens 4296.600 |tokens/s 8742.143 |walltime 16063.218 |
Transformer | epoch 1 | step 31647 |avg loss 8.625 |avg tokens 3845.900 |tokens/s 8061.678 |walltime 16067.989 |
Transformer | epoch 1 | step 31657 |avg loss 8.622 |avg tokens 3877.600 |tokens/s 8305.568 |walltime 16072.657 |
Transformer | epoch 1 | step 31667 |avg loss 8.509 |avg tokens 4548.100 |tokens/s 9276.187 |walltime 16077.560 |
Transformer | epoch 1 | step 31677 |avg loss 8.361 |avg tokens 4738.200 |tokens/s 9059.200 |walltime 16082.791 |
Transformer | epoch 1 | step 31687 |avg loss 8.510 |avg tokens 4394.600 |tokens/s 8860.945 |walltime 16087.750 |
Transformer | epoch 1 | step 31697 |avg loss 8.735 |avg tokens 4316.100 |tokens/s 8867.820 |walltime 16092.617 |
Transformer | epoch 1 | step 31707 |avg loss 8.466 |avg tokens 4497.900 |tokens/s 8847.206 |walltime 16097.701 |
Transformer | epoch 1 | step 31717 |avg loss 8.426 |avg tokens 4736.000 |tokens/s 9178.338 |walltime 16102.861 |
Transformer | epoch 1 | step 31727 |avg loss 8.217 |avg tokens 4536.600 |tokens/s 8887.358 |walltime 16107.966 |
Transformer | epoch 1 | step 31737 |avg loss 8.548 |avg tokens 4995.200 |tokens/s 9521.095 |walltime 16113.212 |
Transformer | epoch 1 | step 31747 |avg loss 8.680 |avg tokens 4605.400 |tokens/s 9186.704 |walltime 16118.225 |
Transformer | epoch 1 | step 31757 |avg loss 8.510 |avg tokens 4692.300 |tokens/s 9000.627 |walltime 16123.439 |
Transformer | epoch 1 | step 31767 |avg loss 8.641 |avg tokens 4928.600 |tokens/s 9666.314 |walltime 16128.537 |
Transformer | epoch 1 | step 31777 |avg loss 8.436 |avg tokens 4523.000 |tokens/s 8853.255 |walltime 16133.646 |
Transformer | epoch 1 | step 31787 |avg loss 8.177 |avg tokens 4203.300 |tokens/s 8561.645 |walltime 16138.556 |
Transformer | epoch 1 | step 31797 |avg loss 8.744 |avg tokens 4816.500 |tokens/s 9620.575 |walltime 16143.562 |
Transformer | epoch 1 | step 31807 |avg loss 8.287 |avg tokens 4418.000 |tokens/s 8605.536 |walltime 16148.696 |
Transformer | epoch 1 | step 31817 |avg loss 8.470 |avg tokens 4626.200 |tokens/s 9011.597 |walltime 16153.830 |
Transformer | epoch 1 | step 31827 |avg loss 8.478 |avg tokens 4670.500 |tokens/s 9132.108 |walltime 16158.944 |
Transformer | epoch 1 | step 31837 |avg loss 8.565 |avg tokens 4732.300 |tokens/s 9486.844 |walltime 16163.932 |
Transformer | epoch 1 | step 31847 |avg loss 8.483 |avg tokens 4692.900 |tokens/s 9001.494 |walltime 16169.146 |
Transformer | epoch 1 | step 31857 |avg loss 8.459 |avg tokens 4497.000 |tokens/s 8926.806 |walltime 16174.184 |
Transformer | epoch 1 | step 31867 |avg loss 8.544 |avg tokens 4513.600 |tokens/s 8836.362 |walltime 16179.292 |
Transformer | epoch 1 | step 31877 |avg loss 8.439 |avg tokens 4305.000 |tokens/s 8512.373 |walltime 16184.349 |
Transformer | epoch 1 | step 31887 |avg loss 8.785 |avg tokens 4503.700 |tokens/s 9323.120 |walltime 16189.180 |
Transformer | epoch 1 | step 31897 |avg loss 8.641 |avg tokens 4626.100 |tokens/s 9078.903 |walltime 16194.275 |
Transformer | epoch 1 | step 31907 |avg loss 8.530 |avg tokens 4597.900 |tokens/s 9002.428 |walltime 16199.382 |
Transformer | epoch 1 | step 31917 |avg loss 8.500 |avg tokens 4683.200 |tokens/s 9038.049 |walltime 16204.564 |
Transformer | epoch 1 | step 31927 |avg loss 8.498 |avg tokens 4599.000 |tokens/s 9073.452 |walltime 16209.633 |
Transformer | epoch 1 | step 31937 |avg loss 8.372 |avg tokens 4667.100 |tokens/s 9079.695 |walltime 16214.773 |
Transformer | epoch 1 | step 31947 |avg loss 8.547 |avg tokens 4630.500 |tokens/s 9023.702 |walltime 16219.904 |
Transformer | epoch 1 | step 31957 |avg loss 8.511 |avg tokens 4937.200 |tokens/s 9605.391 |walltime 16225.044 |
Transformer | epoch 1 | step 31967 |avg loss 8.544 |avg tokens 4260.600 |tokens/s 8571.420 |walltime 16230.015 |
Transformer | epoch 1 | step 31977 |avg loss 8.621 |avg tokens 4359.700 |tokens/s 8780.927 |walltime 16234.980 |
Transformer | epoch 1 | step 31987 |avg loss 8.638 |avg tokens 4534.000 |tokens/s 8945.914 |walltime 16240.048 |
Transformer | epoch 1 | step 31997 |avg loss 8.567 |avg tokens 4764.400 |tokens/s 9224.863 |walltime 16245.213 |
Transformer | epoch 1 | step 32007 |avg loss 8.667 |avg tokens 4599.700 |tokens/s 9014.562 |walltime 16250.316 |
Transformer | epoch 1 | step 32017 |avg loss 8.539 |avg tokens 4528.200 |tokens/s 9161.285 |walltime 16255.258 |
Transformer | epoch 1 | step 32027 |avg loss 8.558 |avg tokens 4363.000 |tokens/s 8704.025 |walltime 16260.271 |
Transformer | epoch 1 | step 32037 |avg loss 8.400 |avg tokens 4043.800 |tokens/s 8160.232 |walltime 16265.226 |
Transformer | epoch 1 | step 32047 |avg loss 8.676 |avg tokens 4460.200 |tokens/s 8998.253 |walltime 16270.183 |
Transformer | epoch 1 | step 32057 |avg loss 8.616 |avg tokens 4567.500 |tokens/s 9100.908 |walltime 16275.202 |
Transformer | epoch 1 | step 32067 |avg loss 8.652 |avg tokens 4405.400 |tokens/s 8527.926 |walltime 16280.368 |
Transformer | epoch 1 | step 32077 |avg loss 8.550 |avg tokens 4100.800 |tokens/s 8343.033 |walltime 16285.283 |
Transformer | epoch 1 | step 32087 |avg loss 8.802 |avg tokens 4035.600 |tokens/s 8510.508 |walltime 16290.025 |
Transformer | epoch 1 | step 32097 |avg loss 8.717 |avg tokens 4133.600 |tokens/s 8451.776 |walltime 16294.916 |
Transformer | epoch 1 | step 32107 |avg loss 8.480 |avg tokens 3888.800 |tokens/s 8094.623 |walltime 16299.720 |
Transformer | epoch 1 | step 32117 |avg loss 8.367 |avg tokens 4427.000 |tokens/s 8770.611 |walltime 16304.767 |
Transformer | epoch 1 | step 32127 |avg loss 8.512 |avg tokens 4620.800 |tokens/s 8784.968 |walltime 16310.027 |
Transformer | epoch 1 | step 32137 |avg loss 8.820 |avg tokens 4481.900 |tokens/s 9047.783 |walltime 16314.981 |
Transformer | epoch 1 | step 32147 |avg loss 8.182 |avg tokens 4477.800 |tokens/s 8578.559 |walltime 16320.201 |
Transformer | epoch 1 | step 32157 |avg loss 8.623 |avg tokens 4566.800 |tokens/s 9119.184 |walltime 16325.209 |
Transformer | epoch 1 | step 32167 |avg loss 8.460 |avg tokens 4936.000 |tokens/s 9416.271 |walltime 16330.451 |
Transformer | epoch 1 | step 32177 |avg loss 8.721 |avg tokens 4193.600 |tokens/s 8649.724 |walltime 16335.299 |
Transformer | epoch 1 | step 32187 |avg loss 8.319 |avg tokens 4753.600 |tokens/s 9151.794 |walltime 16340.493 |
Transformer | epoch 1 | step 32197 |avg loss 8.281 |avg tokens 4899.200 |tokens/s 9141.155 |walltime 16345.853 |
Transformer | epoch 1 | step 32207 |avg loss 8.549 |avg tokens 4284.100 |tokens/s 8894.311 |walltime 16350.669 |
Transformer | epoch 1 | step 32217 |avg loss 8.331 |avg tokens 4577.100 |tokens/s 8833.055 |walltime 16355.851 |
Transformer | epoch 1 | step 32227 |avg loss 8.499 |avg tokens 4288.500 |tokens/s 8595.134 |walltime 16360.840 |
Transformer | epoch 1 | step 32237 |avg loss 8.461 |avg tokens 4608.400 |tokens/s 9088.533 |walltime 16365.911 |
Transformer | epoch 1 | step 32247 |avg loss 8.531 |avg tokens 4608.400 |tokens/s 9172.866 |walltime 16370.935 |
Transformer | epoch 1 | step 32257 |avg loss 8.667 |avg tokens 4797.400 |tokens/s 9524.606 |walltime 16375.972 |
Transformer | epoch 1 | step 32267 |avg loss 8.512 |avg tokens 4049.500 |tokens/s 8434.432 |walltime 16380.773 |
Transformer | epoch 1 | step 32277 |avg loss 8.462 |avg tokens 4382.300 |tokens/s 8668.596 |walltime 16385.828 |
Transformer | epoch 1 | step 32287 |avg loss 8.540 |avg tokens 4699.200 |tokens/s 9357.307 |walltime 16390.850 |
Transformer | epoch 1 | step 32297 |avg loss 8.405 |avg tokens 4717.000 |tokens/s 9072.764 |walltime 16396.049 |
Transformer | epoch 1 | step 32307 |avg loss 8.519 |avg tokens 4605.300 |tokens/s 8951.512 |walltime 16401.194 |
Transformer | epoch 1 | step 32317 |avg loss 8.523 |avg tokens 4499.900 |tokens/s 8808.995 |walltime 16406.302 |
Transformer | epoch 1 | step 32327 |avg loss 8.600 |avg tokens 4683.300 |tokens/s 9029.158 |walltime 16411.489 |
Transformer | epoch 1 | step 32337 |avg loss 8.424 |avg tokens 4804.400 |tokens/s 9135.474 |walltime 16416.748 |
Transformer | epoch 1 | step 32347 |avg loss 8.411 |avg tokens 4631.200 |tokens/s 8907.354 |walltime 16421.948 |
Transformer | epoch 1 | step 32357 |avg loss 8.559 |avg tokens 4216.300 |tokens/s 8778.565 |walltime 16426.751 |
Transformer | epoch 1 | step 32367 |avg loss 8.563 |avg tokens 4311.800 |tokens/s 8513.731 |walltime 16431.815 |
Transformer | epoch 1 | step 32377 |avg loss 8.610 |avg tokens 4684.500 |tokens/s 9389.754 |walltime 16436.804 |
Transformer | epoch 1 | step 32387 |avg loss 8.408 |avg tokens 4351.500 |tokens/s 8645.228 |walltime 16441.838 |
Transformer | epoch 1 | step 32397 |avg loss 8.595 |avg tokens 4304.900 |tokens/s 8866.171 |walltime 16446.693 |
Transformer | epoch 1 | step 32407 |avg loss 8.780 |avg tokens 4308.800 |tokens/s 8946.647 |walltime 16451.509 |
Transformer | epoch 1 | step 32417 |avg loss 8.380 |avg tokens 4843.200 |tokens/s 9189.247 |walltime 16456.780 |
Transformer | epoch 1 | step 32427 |avg loss 8.451 |avg tokens 4515.900 |tokens/s 8781.782 |walltime 16461.922 |
Transformer | epoch 1 | step 32437 |avg loss 8.072 |avg tokens 4324.700 |tokens/s 8557.696 |walltime 16466.976 |
Transformer | epoch 1 | step 32447 |avg loss 8.168 |avg tokens 4566.100 |tokens/s 8830.064 |walltime 16472.147 |
Transformer | epoch 1 | step 32457 |avg loss 8.676 |avg tokens 4488.100 |tokens/s 9116.006 |walltime 16477.070 |
Transformer | epoch 1 | step 32467 |avg loss 8.365 |avg tokens 4425.100 |tokens/s 8760.841 |walltime 16482.121 |
Transformer | epoch 1 | step 32477 |avg loss 8.601 |avg tokens 4378.300 |tokens/s 8950.471 |walltime 16487.013 |
Transformer | epoch 1 | step 32487 |avg loss 8.557 |avg tokens 4812.500 |tokens/s 9121.876 |walltime 16492.288 |
Transformer | epoch 1 | step 32497 |avg loss 8.398 |avg tokens 4723.600 |tokens/s 8999.947 |walltime 16497.537 |
Transformer | epoch 1 | step 32507 |avg loss 8.665 |avg tokens 4505.600 |tokens/s 9435.733 |walltime 16502.312 |
Transformer | epoch 1 | step 32517 |avg loss 8.592 |avg tokens 4149.700 |tokens/s 8305.693 |walltime 16507.308 |
Transformer | epoch 1 | step 32527 |avg loss 8.719 |avg tokens 3907.200 |tokens/s 8107.014 |walltime 16512.128 |
Transformer | epoch 1 | step 32537 |avg loss 8.609 |avg tokens 4151.100 |tokens/s 8542.451 |walltime 16516.987 |
Transformer | epoch 1 | step 32547 |avg loss 8.535 |avg tokens 4467.200 |tokens/s 8892.581 |walltime 16522.011 |
Transformer | epoch 1 | step 32557 |avg loss 8.648 |avg tokens 4285.500 |tokens/s 8820.999 |walltime 16526.869 |
Transformer | epoch 1 | step 32567 |avg loss 8.437 |avg tokens 4269.100 |tokens/s 8609.744 |walltime 16531.827 |
Transformer | epoch 1 | step 32577 |avg loss 8.588 |avg tokens 4643.800 |tokens/s 9124.294 |walltime 16536.917 |
Transformer | epoch 1 | step 32587 |avg loss 8.418 |avg tokens 4474.900 |tokens/s 8690.261 |walltime 16542.066 |
Transformer | epoch 1 | step 32597 |avg loss 8.423 |avg tokens 4444.300 |tokens/s 8809.921 |walltime 16547.111 |
Transformer | epoch 1 | step 32607 |avg loss 8.538 |avg tokens 4703.200 |tokens/s 9191.492 |walltime 16552.228 |
Transformer | epoch 1 | step 32617 |avg loss 8.472 |avg tokens 4414.700 |tokens/s 8618.969 |walltime 16557.350 |
Transformer | epoch 1 | step 32627 |avg loss 8.594 |avg tokens 4498.000 |tokens/s 8858.273 |walltime 16562.428 |
Transformer | epoch 1 | step 32637 |avg loss 8.470 |avg tokens 4683.900 |tokens/s 8917.499 |walltime 16567.680 |
Transformer | epoch 1 | step 32647 |avg loss 8.498 |avg tokens 4467.500 |tokens/s 8571.569 |walltime 16572.892 |
Transformer | epoch 1 | step 32657 |avg loss 8.385 |avg tokens 4365.200 |tokens/s 8606.403 |walltime 16577.964 |
Transformer | epoch 1 | step 32667 |avg loss 8.586 |avg tokens 4669.200 |tokens/s 8849.460 |walltime 16583.240 |
Transformer | epoch 1 | step 32677 |avg loss 8.642 |avg tokens 4246.700 |tokens/s 8697.293 |walltime 16588.123 |
Transformer | epoch 1 | step 32687 |avg loss 8.504 |avg tokens 4539.600 |tokens/s 9186.051 |walltime 16593.065 |
Transformer | epoch 1 | step 32697 |avg loss 8.508 |avg tokens 4716.300 |tokens/s 9205.368 |walltime 16598.188 |
Transformer | epoch 1 | step 32707 |avg loss 8.612 |avg tokens 4445.600 |tokens/s 9147.048 |walltime 16603.049 |
Transformer | epoch 1 | step 32717 |avg loss 8.747 |avg tokens 4852.700 |tokens/s 9566.907 |walltime 16608.121 |
Transformer | epoch 1 | step 32727 |avg loss 8.564 |avg tokens 4899.400 |tokens/s 9347.622 |walltime 16613.362 |
Transformer | epoch 1 | step 32737 |avg loss 8.603 |avg tokens 4555.300 |tokens/s 8851.220 |walltime 16618.509 |
Transformer | epoch 1 | step 32747 |avg loss 8.634 |avg tokens 4632.500 |tokens/s 9264.419 |walltime 16623.509 |
Transformer | epoch 1 | step 32757 |avg loss 8.629 |avg tokens 4411.900 |tokens/s 8841.483 |walltime 16628.499 |
Transformer | epoch 1 | step 32767 |avg loss 8.473 |avg tokens 4770.000 |tokens/s 9379.298 |walltime 16633.585 |
Transformer | epoch 1 | step 32777 |avg loss 8.538 |avg tokens 4527.900 |tokens/s 8800.656 |walltime 16638.730 |
Transformer | epoch 1 | step 32787 |avg loss 8.425 |avg tokens 4658.500 |tokens/s 9025.134 |walltime 16643.891 |
Transformer | epoch 1 | step 32797 |avg loss 8.556 |avg tokens 4582.500 |tokens/s 9093.404 |walltime 16648.931 |
Transformer | epoch 1 | step 32807 |avg loss 8.509 |avg tokens 4683.500 |tokens/s 9202.489 |walltime 16654.020 |
Transformer | epoch 1 | step 32817 |avg loss 8.704 |avg tokens 4318.400 |tokens/s 8802.693 |walltime 16658.926 |
Transformer | epoch 1 | step 32827 |avg loss 8.533 |avg tokens 4546.500 |tokens/s 9072.249 |walltime 16663.937 |
Transformer | epoch 1 | step 32837 |avg loss 8.811 |avg tokens 4424.100 |tokens/s 8908.347 |walltime 16668.904 |
Transformer | epoch 1 | step 32847 |avg loss 8.639 |avg tokens 4294.000 |tokens/s 8408.142 |walltime 16674.011 |
Transformer | epoch 1 | step 32857 |avg loss 8.495 |avg tokens 4881.500 |tokens/s 9174.849 |walltime 16679.331 |
Transformer | epoch 1 | step 32867 |avg loss 8.466 |avg tokens 4325.500 |tokens/s 8619.533 |walltime 16684.349 |
Transformer | epoch 1 | step 32877 |avg loss 8.473 |avg tokens 4464.800 |tokens/s 8839.983 |walltime 16689.400 |
Transformer | epoch 1 | step 32887 |avg loss 8.621 |avg tokens 4723.100 |tokens/s 9042.063 |walltime 16694.624 |
Transformer | epoch 1 | step 32897 |avg loss 8.789 |avg tokens 3870.600 |tokens/s 8310.263 |walltime 16699.281 |
Transformer | epoch 1 | step 32907 |avg loss 8.737 |avg tokens 4667.800 |tokens/s 9526.780 |walltime 16704.181 |
Transformer | epoch 1 | step 32917 |avg loss 8.558 |avg tokens 4722.500 |tokens/s 9199.477 |walltime 16709.314 |
Transformer | epoch 1 | step 32927 |avg loss 8.582 |avg tokens 4144.500 |tokens/s 8429.894 |walltime 16714.231 |
Transformer | epoch 1 | step 32937 |avg loss 8.606 |avg tokens 4700.200 |tokens/s 9164.875 |walltime 16719.359 |
Transformer | epoch 1 | step 32947 |avg loss 8.755 |avg tokens 4744.600 |tokens/s 9616.269 |walltime 16724.293 |
Transformer | epoch 1 | step 32957 |avg loss 8.525 |avg tokens 4821.400 |tokens/s 9206.472 |walltime 16729.530 |
Transformer | epoch 1 | step 32967 |avg loss 8.567 |avg tokens 4820.000 |tokens/s 9243.332 |walltime 16734.745 |
Transformer | epoch 1 | step 32977 |avg loss 8.847 |avg tokens 4395.900 |tokens/s 8490.772 |walltime 16739.922 |
Transformer | epoch 1 | step 32987 |avg loss 8.481 |avg tokens 4417.400 |tokens/s 8815.037 |walltime 16744.933 |
Transformer | epoch 1 | step 32997 |avg loss 8.612 |avg tokens 4666.900 |tokens/s 9090.338 |walltime 16750.067 |
Transformer | epoch 1 | step 33007 |avg loss 8.560 |avg tokens 4824.800 |tokens/s 9176.240 |walltime 16755.325 |
Transformer | epoch 1 | step 33017 |avg loss 8.777 |avg tokens 4463.900 |tokens/s 9055.012 |walltime 16760.255 |
Transformer | epoch 1 | step 33027 |avg loss 8.653 |avg tokens 4884.300 |tokens/s 9495.577 |walltime 16765.399 |
Transformer | epoch 1 | step 33037 |avg loss 8.397 |avg tokens 4918.300 |tokens/s 9464.182 |walltime 16770.595 |
Transformer | epoch 1 | step 33047 |avg loss 8.699 |avg tokens 4452.600 |tokens/s 8990.162 |walltime 16775.548 |
Transformer | epoch 1 | step 33057 |avg loss 8.589 |avg tokens 4903.600 |tokens/s 9750.893 |walltime 16780.577 |
Transformer | epoch 1 | step 33067 |avg loss 8.714 |avg tokens 4277.500 |tokens/s 8782.455 |walltime 16785.447 |
Transformer | epoch 1 | step 33077 |avg loss 8.339 |avg tokens 4833.600 |tokens/s 9230.181 |walltime 16790.684 |
Transformer | epoch 1 | step 33087 |avg loss 8.359 |avg tokens 4727.300 |tokens/s 9245.202 |walltime 16795.797 |
Transformer | epoch 1 | step 33097 |avg loss 8.557 |avg tokens 4082.700 |tokens/s 8322.559 |walltime 16800.703 |
Transformer | epoch 1 | step 33107 |avg loss 8.787 |avg tokens 4728.800 |tokens/s 9361.894 |walltime 16805.754 |
Transformer | epoch 1 | step 33117 |avg loss 8.876 |avg tokens 4349.100 |tokens/s 8934.499 |walltime 16810.622 |
Transformer | epoch 1 | step 33127 |avg loss 8.463 |avg tokens 4536.700 |tokens/s 8955.071 |walltime 16815.688 |
Transformer | epoch 1 | step 33137 |avg loss 8.721 |avg tokens 4415.800 |tokens/s 8916.031 |walltime 16820.641 |
Transformer | epoch 1 | step 33147 |avg loss 8.610 |avg tokens 4370.400 |tokens/s 8634.584 |walltime 16825.702 |
Transformer | epoch 1 | step 33157 |avg loss 8.310 |avg tokens 4740.000 |tokens/s 9235.600 |walltime 16830.834 |
Transformer | epoch 1 | step 33167 |avg loss 8.494 |avg tokens 4299.200 |tokens/s 8447.256 |walltime 16835.924 |
Transformer | epoch 1 | step 33177 |avg loss 8.597 |avg tokens 4423.200 |tokens/s 8959.832 |walltime 16840.861 |
Transformer | epoch 1 | step 33187 |avg loss 8.656 |avg tokens 4794.500 |tokens/s 9655.635 |walltime 16845.826 |
Transformer | epoch 1 | step 33197 |avg loss 8.458 |avg tokens 4544.600 |tokens/s 9203.844 |walltime 16850.764 |
Transformer | epoch 1 | step 33207 |avg loss 8.566 |avg tokens 4692.400 |tokens/s 9292.365 |walltime 16855.814 |
Transformer | epoch 1 | step 33217 |avg loss 8.419 |avg tokens 4837.300 |tokens/s 9147.791 |walltime 16861.102 |
Transformer | epoch 1 | step 33227 |avg loss 8.643 |avg tokens 4598.600 |tokens/s 9190.055 |walltime 16866.105 |
Transformer | epoch 1 | step 33237 |avg loss 8.710 |avg tokens 4519.400 |tokens/s 9017.771 |walltime 16871.117 |
Transformer | epoch 1 | step 33247 |avg loss 8.766 |avg tokens 4260.900 |tokens/s 8505.387 |walltime 16876.127 |
Transformer | epoch 1 | step 33257 |avg loss 8.336 |avg tokens 4811.000 |tokens/s 9021.885 |walltime 16881.459 |
Transformer | epoch 1 | step 33267 |avg loss 8.633 |avg tokens 4308.800 |tokens/s 8888.711 |walltime 16886.307 |
Transformer | epoch 1 | step 33277 |avg loss 8.557 |avg tokens 4545.900 |tokens/s 8928.011 |walltime 16891.399 |
Transformer | epoch 1 | step 33287 |avg loss 8.286 |avg tokens 4896.000 |tokens/s 9120.145 |walltime 16896.767 |
Transformer | epoch 1 | step 33297 |avg loss 8.482 |avg tokens 4589.600 |tokens/s 8821.850 |walltime 16901.969 |
Transformer | epoch 1 | step 33307 |avg loss 8.844 |avg tokens 4402.500 |tokens/s 9123.687 |walltime 16906.795 |
Transformer | epoch 1 | step 33317 |avg loss 8.477 |avg tokens 4658.100 |tokens/s 8853.986 |walltime 16912.056 |
Transformer | epoch 1 | step 33327 |avg loss 8.576 |avg tokens 4575.900 |tokens/s 8858.208 |walltime 16917.222 |
Transformer | epoch 1 | step 33337 |avg loss 8.500 |avg tokens 4393.000 |tokens/s 8847.216 |walltime 16922.187 |
Transformer | epoch 1 | step 33347 |avg loss 8.615 |avg tokens 4868.800 |tokens/s 9328.425 |walltime 16927.406 |
Transformer | epoch 1 | step 33357 |avg loss 8.529 |avg tokens 4653.700 |tokens/s 9154.954 |walltime 16932.490 |
Transformer | epoch 1 | step 33367 |avg loss 8.412 |avg tokens 4528.300 |tokens/s 8904.947 |walltime 16937.575 |
Transformer | epoch 1 | step 33377 |avg loss 8.473 |avg tokens 4939.100 |tokens/s 9116.302 |walltime 16942.993 |
Transformer | epoch 1 | step 33387 |avg loss 8.591 |avg tokens 4467.700 |tokens/s 8644.327 |walltime 16948.161 |
Transformer | epoch 1 | step 33397 |avg loss 8.659 |avg tokens 4545.500 |tokens/s 8868.365 |walltime 16953.286 |
Transformer | epoch 1 | step 33407 |avg loss 8.403 |avg tokens 4798.300 |tokens/s 9107.388 |walltime 16958.555 |
Transformer | epoch 1 | step 33417 |avg loss 8.793 |avg tokens 4320.100 |tokens/s 9144.479 |walltime 16963.279 |
Transformer | epoch 1 | step 33427 |avg loss 8.438 |avg tokens 4349.600 |tokens/s 8640.797 |walltime 16968.313 |
Transformer | epoch 1 | step 33437 |avg loss 8.535 |avg tokens 4217.100 |tokens/s 8580.946 |walltime 16973.228 |
Transformer | epoch 1 | step 33447 |avg loss 8.644 |avg tokens 4507.100 |tokens/s 8996.780 |walltime 16978.237 |
Transformer | epoch 1 | step 33457 |avg loss 8.386 |avg tokens 4352.200 |tokens/s 8751.808 |walltime 16983.210 |
Transformer | epoch 1 | step 33467 |avg loss 8.548 |avg tokens 4285.400 |tokens/s 8513.415 |walltime 16988.244 |
Transformer | epoch 1 | step 33477 |avg loss 8.759 |avg tokens 4162.700 |tokens/s 8901.477 |walltime 16992.920 |
Transformer | epoch 1 | step 33487 |avg loss 8.786 |avg tokens 4913.000 |tokens/s 10035.317 |walltime 16997.816 |
Transformer | epoch 1 | step 33497 |avg loss 8.484 |avg tokens 4525.300 |tokens/s 9165.406 |walltime 17002.753 |
Transformer | epoch 1 | step 33507 |avg loss 8.613 |avg tokens 4303.900 |tokens/s 8602.988 |walltime 17007.756 |
Transformer | epoch 1 | step 33517 |avg loss 8.345 |avg tokens 4852.100 |tokens/s 9342.112 |walltime 17012.950 |
Transformer | epoch 1 | step 33527 |avg loss 8.763 |avg tokens 4435.200 |tokens/s 9035.664 |walltime 17017.859 |
Transformer | epoch 1 | step 33537 |avg loss 8.546 |avg tokens 4420.400 |tokens/s 8651.220 |walltime 17022.968 |
Transformer | epoch 1 | step 33547 |avg loss 8.572 |avg tokens 4654.200 |tokens/s 9210.156 |walltime 17028.022 |
Transformer | epoch 1 | step 33557 |avg loss 8.721 |avg tokens 4060.100 |tokens/s 8309.377 |walltime 17032.908 |
Transformer | epoch 1 | step 33567 |avg loss 8.510 |avg tokens 4511.200 |tokens/s 8855.642 |walltime 17038.002 |
Transformer | epoch 1 | step 33577 |avg loss 8.503 |avg tokens 4412.400 |tokens/s 8822.607 |walltime 17043.003 |
Transformer | epoch 1 | step 33587 |avg loss 8.519 |avg tokens 4304.500 |tokens/s 8556.980 |walltime 17048.033 |
Transformer | epoch 1 | step 33597 |avg loss 8.712 |avg tokens 4350.800 |tokens/s 8866.062 |walltime 17052.941 |
Transformer | epoch 1 | step 33607 |avg loss 8.532 |avg tokens 4565.300 |tokens/s 8905.385 |walltime 17058.067 |
Transformer | epoch 1 | step 33617 |avg loss 8.517 |avg tokens 4710.100 |tokens/s 9225.768 |walltime 17063.173 |
Transformer | epoch 1 | step 33627 |avg loss 8.580 |avg tokens 4790.100 |tokens/s 9521.880 |walltime 17068.203 |
Transformer | epoch 1 | step 33637 |avg loss 8.690 |avg tokens 4515.100 |tokens/s 8998.245 |walltime 17073.221 |
Transformer | epoch 1 | step 33647 |avg loss 8.489 |avg tokens 4687.900 |tokens/s 8967.837 |walltime 17078.448 |
Transformer | epoch 1 | step 33657 |avg loss 8.627 |avg tokens 4154.300 |tokens/s 8539.341 |walltime 17083.313 |
Transformer | epoch 1 | step 33667 |avg loss 8.293 |avg tokens 4875.300 |tokens/s 9846.082 |walltime 17088.265 |
Transformer | epoch 1 | step 33677 |avg loss 8.582 |avg tokens 4674.100 |tokens/s 9056.275 |walltime 17093.426 |
Transformer | epoch 1 | step 33687 |avg loss 8.646 |avg tokens 4591.300 |tokens/s 8954.800 |walltime 17098.553 |
Transformer | epoch 1 | step 33697 |avg loss 8.482 |avg tokens 4873.600 |tokens/s 9282.365 |walltime 17103.804 |
Transformer | epoch 1 | step 33707 |avg loss 8.664 |avg tokens 4138.500 |tokens/s 8420.383 |walltime 17108.718 |
Transformer | epoch 1 | step 33717 |avg loss 8.770 |avg tokens 4461.000 |tokens/s 9174.268 |walltime 17113.581 |
Transformer | epoch 1 | step 33727 |avg loss 8.451 |avg tokens 4887.100 |tokens/s 9086.314 |walltime 17118.960 |
Transformer | epoch 1 | step 33737 |avg loss 8.513 |avg tokens 4365.600 |tokens/s 8910.087 |walltime 17123.859 |
Transformer | epoch 1 | step 33747 |avg loss 8.612 |avg tokens 4708.500 |tokens/s 9053.610 |walltime 17129.060 |
Transformer | epoch 1 | step 33757 |avg loss 8.585 |avg tokens 4460.700 |tokens/s 9061.265 |walltime 17133.983 |
Transformer | epoch 1 | step 33767 |avg loss 8.822 |avg tokens 3582.300 |tokens/s 7807.500 |walltime 17138.571 |
Transformer | epoch 1 | step 33777 |avg loss 8.498 |avg tokens 4424.300 |tokens/s 8771.079 |walltime 17143.615 |
Transformer | epoch 1 | step 33787 |avg loss 8.502 |avg tokens 4508.700 |tokens/s 9020.115 |walltime 17148.614 |
Transformer | epoch 1 | step 33797 |avg loss 8.684 |avg tokens 4790.200 |tokens/s 9038.537 |walltime 17153.913 |
Transformer | epoch 1 | step 33807 |avg loss 8.556 |avg tokens 4802.500 |tokens/s 9220.638 |walltime 17159.122 |
Transformer | epoch 1 | step 33817 |avg loss 8.680 |avg tokens 4500.500 |tokens/s 8852.085 |walltime 17164.206 |
Transformer | epoch 1 | step 33827 |avg loss 8.626 |avg tokens 4602.500 |tokens/s 9021.954 |walltime 17169.307 |
Transformer | epoch 1 | step 33837 |avg loss 8.485 |avg tokens 4118.700 |tokens/s 8370.967 |walltime 17174.228 |
Transformer | epoch 1 | step 33847 |avg loss 8.578 |avg tokens 4752.000 |tokens/s 9270.104 |walltime 17179.354 |
Transformer | epoch 1 | step 33857 |avg loss 8.363 |avg tokens 4811.900 |tokens/s 9187.763 |walltime 17184.591 |
Transformer | epoch 1 | step 33867 |avg loss 8.810 |avg tokens 4400.700 |tokens/s 9362.970 |walltime 17189.291 |
Transformer | epoch 1 | step 33877 |avg loss 8.642 |avg tokens 4388.100 |tokens/s 8825.248 |walltime 17194.263 |
Transformer | epoch 1 | step 33887 |avg loss 8.755 |avg tokens 3590.000 |tokens/s 7775.684 |walltime 17198.880 |
Transformer | epoch 1 | step 33897 |avg loss 8.502 |avg tokens 4582.000 |tokens/s 8793.074 |walltime 17204.091 |
Transformer | epoch 1 | step 33907 |avg loss 8.799 |avg tokens 4274.000 |tokens/s 8972.141 |walltime 17208.855 |
Transformer | epoch 1 | step 33917 |avg loss 8.495 |avg tokens 4514.400 |tokens/s 8841.545 |walltime 17213.961 |
Transformer | epoch 1 | step 33927 |avg loss 8.583 |avg tokens 4604.300 |tokens/s 9003.929 |walltime 17219.074 |
Transformer | epoch 1 | step 33937 |avg loss 8.929 |avg tokens 4007.400 |tokens/s 8746.788 |walltime 17223.656 |
Transformer | epoch 1 | step 33947 |avg loss 8.766 |avg tokens 3782.200 |tokens/s 8080.414 |walltime 17228.337 |
Transformer | epoch 1 | step 33957 |avg loss 8.419 |avg tokens 4307.400 |tokens/s 8797.641 |walltime 17233.233 |
Transformer | epoch 1 | step 33967 |avg loss 8.481 |avg tokens 4786.300 |tokens/s 9228.080 |walltime 17238.420 |
Transformer | epoch 1 | step 33977 |avg loss 8.778 |avg tokens 4401.300 |tokens/s 9149.035 |walltime 17243.230 |
Transformer | epoch 1 | step 33987 |avg loss 8.492 |avg tokens 4768.700 |tokens/s 9218.603 |walltime 17248.403 |
Transformer | epoch 1 | step 33997 |avg loss 8.686 |avg tokens 4734.200 |tokens/s 9346.053 |walltime 17253.469 |
Transformer | epoch 1 | step 34007 |avg loss 8.637 |avg tokens 4719.700 |tokens/s 9145.798 |walltime 17258.629 |
Transformer | epoch 1 | step 34017 |avg loss 8.265 |avg tokens 4592.700 |tokens/s 8936.671 |walltime 17263.768 |
Transformer | epoch 1 | step 34027 |avg loss 8.650 |avg tokens 4411.100 |tokens/s 8967.706 |walltime 17268.687 |
Transformer | epoch 1 | step 34037 |avg loss 8.080 |avg tokens 4095.700 |tokens/s 8558.632 |walltime 17273.473 |
Transformer | epoch 1 | step 34047 |avg loss 9.171 |avg tokens 4144.300 |tokens/s 9326.626 |walltime 17277.916 |
Transformer | epoch 1 | step 34057 |avg loss 8.889 |avg tokens 4720.200 |tokens/s 9472.930 |walltime 17282.899 |
Transformer | epoch 1 | step 34067 |avg loss 8.748 |avg tokens 4085.200 |tokens/s 8540.338 |walltime 17287.682 |
Transformer | epoch 1 | step 34077 |avg loss 8.546 |avg tokens 4341.300 |tokens/s 8674.543 |walltime 17292.687 |
Transformer | epoch 1 | step 34087 |avg loss 8.747 |avg tokens 4296.300 |tokens/s 9041.717 |walltime 17297.439 |
Transformer | epoch 1 | step 34097 |avg loss 8.781 |avg tokens 4822.300 |tokens/s 9781.571 |walltime 17302.369 |
Transformer | epoch 1 | step 34107 |avg loss 8.600 |avg tokens 4609.100 |tokens/s 9218.179 |walltime 17307.369 |
Transformer | epoch 1 | step 34117 |avg loss 8.414 |avg tokens 4968.000 |tokens/s 9310.276 |walltime 17312.705 |
Transformer | epoch 1 | step 34127 |avg loss 8.604 |avg tokens 4121.400 |tokens/s 8685.237 |walltime 17317.450 |
Transformer | epoch 1 | step 34137 |avg loss 8.540 |avg tokens 4638.700 |tokens/s 8773.057 |walltime 17322.737 |
Transformer | epoch 1 | step 34147 |avg loss 8.464 |avg tokens 4884.400 |tokens/s 9349.116 |walltime 17327.962 |
Transformer | epoch 1 | step 34157 |avg loss 8.509 |avg tokens 4114.300 |tokens/s 8352.608 |walltime 17332.888 |
Transformer | epoch 1 | step 34167 |avg loss 8.697 |avg tokens 4632.100 |tokens/s 9179.764 |walltime 17337.934 |
Transformer | epoch 1 | step 34177 |avg loss 8.622 |avg tokens 4731.500 |tokens/s 9208.799 |walltime 17343.072 |
Transformer | epoch 1 | step 34187 |avg loss 8.478 |avg tokens 4732.800 |tokens/s 9086.983 |walltime 17348.280 |
Transformer | epoch 1 | step 34197 |avg loss 8.748 |avg tokens 3842.900 |tokens/s 8104.188 |walltime 17353.022 |
Transformer | epoch 1 | step 34207 |avg loss 8.532 |avg tokens 4796.800 |tokens/s 9313.000 |walltime 17358.173 |
Transformer | epoch 1 | step 34217 |avg loss 8.962 |avg tokens 4218.200 |tokens/s 9083.834 |walltime 17362.816 |
Transformer | epoch 1 | step 34227 |avg loss 8.740 |avg tokens 4650.000 |tokens/s 9351.037 |walltime 17367.789 |
Transformer | epoch 1 | step 34237 |avg loss 8.196 |avg tokens 4264.700 |tokens/s 8520.273 |walltime 17372.794 |
Transformer | epoch 1 | step 34247 |avg loss 8.551 |avg tokens 4576.600 |tokens/s 9063.402 |walltime 17377.844 |
Transformer | epoch 1 | step 34257 |avg loss 8.588 |avg tokens 4752.100 |tokens/s 9372.209 |walltime 17382.914 |
Transformer | epoch 1 | step 34267 |avg loss 8.670 |avg tokens 4375.700 |tokens/s 8736.648 |walltime 17387.923 |
Transformer | epoch 1 | step 34277 |avg loss 8.794 |avg tokens 4244.100 |tokens/s 8921.825 |walltime 17392.680 |
Transformer | epoch 1 | step 34287 |avg loss 8.843 |avg tokens 4288.700 |tokens/s 8665.952 |walltime 17397.629 |
Transformer | epoch 1 | step 34297 |avg loss 8.559 |avg tokens 4131.200 |tokens/s 8490.543 |walltime 17402.494 |
Transformer | epoch 1 | step 34307 |avg loss 8.635 |avg tokens 4745.500 |tokens/s 9564.111 |walltime 17407.456 |
Transformer | epoch 1 | step 34317 |avg loss 8.533 |avg tokens 4797.700 |tokens/s 9058.327 |walltime 17412.752 |
Transformer | epoch 1 | step 34327 |avg loss 8.496 |avg tokens 4663.600 |tokens/s 8976.302 |walltime 17417.948 |
Transformer | epoch 1 | step 34337 |avg loss 8.593 |avg tokens 4669.500 |tokens/s 9176.947 |walltime 17423.036 |
Transformer | epoch 1 | step 34347 |avg loss 8.633 |avg tokens 4559.700 |tokens/s 8925.983 |walltime 17428.145 |
Transformer | epoch 1 | step 34357 |avg loss 8.617 |avg tokens 4656.500 |tokens/s 9048.985 |walltime 17433.290 |
Transformer | epoch 1 | step 34367 |avg loss 8.296 |avg tokens 4644.800 |tokens/s 9042.930 |walltime 17438.427 |
Transformer | epoch 1 | step 34377 |avg loss 8.607 |avg tokens 4668.300 |tokens/s 9140.751 |walltime 17443.534 |
Transformer | epoch 1 | step 34387 |avg loss 8.490 |avg tokens 4604.600 |tokens/s 9219.042 |walltime 17448.529 |
Transformer | epoch 1 | step 34397 |avg loss 8.572 |avg tokens 4705.600 |tokens/s 9188.002 |walltime 17453.650 |
Transformer | epoch 1 | step 34407 |avg loss 8.439 |avg tokens 4841.400 |tokens/s 9121.255 |walltime 17458.958 |
Transformer | epoch 1 | step 34417 |avg loss 8.488 |avg tokens 4671.100 |tokens/s 9011.410 |walltime 17464.141 |
Transformer | epoch 1 | step 34427 |avg loss 8.687 |avg tokens 4071.100 |tokens/s 8651.812 |walltime 17468.847 |
Transformer | epoch 1 | step 34437 |avg loss 8.476 |avg tokens 4074.900 |tokens/s 8336.501 |walltime 17473.735 |
Transformer | epoch 1 | step 34447 |avg loss 8.601 |avg tokens 4509.700 |tokens/s 8793.908 |walltime 17478.863 |
Transformer | epoch 1 | step 34457 |avg loss 8.674 |avg tokens 4649.800 |tokens/s 9062.665 |walltime 17483.994 |
Transformer | epoch 1 | step 34467 |avg loss 8.472 |avg tokens 4533.400 |tokens/s 8978.277 |walltime 17489.043 |
Transformer | epoch 1 | step 34477 |avg loss 8.757 |avg tokens 4353.500 |tokens/s 8942.930 |walltime 17493.911 |
Transformer | epoch 1 | step 34487 |avg loss 8.462 |avg tokens 4496.600 |tokens/s 8674.990 |walltime 17499.095 |
Transformer | epoch 1 | step 34497 |avg loss 8.383 |avg tokens 4778.100 |tokens/s 9189.182 |walltime 17504.294 |
Transformer | epoch 1 | step 34507 |avg loss 8.578 |avg tokens 4362.800 |tokens/s 8913.989 |walltime 17509.189 |
Transformer | epoch 1 | step 34517 |avg loss 8.230 |avg tokens 4452.800 |tokens/s 8746.780 |walltime 17514.280 |
Transformer | epoch 1 | step 34527 |avg loss 8.480 |avg tokens 4726.900 |tokens/s 8968.281 |walltime 17519.550 |
Transformer | epoch 1 | step 34537 |avg loss 8.628 |avg tokens 4472.700 |tokens/s 9020.406 |walltime 17524.509 |
Transformer | epoch 1 | step 34547 |avg loss 8.495 |avg tokens 4858.400 |tokens/s 9235.197 |walltime 17529.769 |
Transformer | epoch 1 | step 34557 |avg loss 8.726 |avg tokens 4347.900 |tokens/s 8955.063 |walltime 17534.625 |
Transformer | epoch 1 | step 34567 |avg loss 8.660 |avg tokens 4526.600 |tokens/s 9125.127 |walltime 17539.585 |
Transformer | epoch 1 | step 34577 |avg loss 8.248 |avg tokens 4304.400 |tokens/s 8646.290 |walltime 17544.564 |
Transformer | epoch 1 | step 34587 |avg loss 8.288 |avg tokens 4728.900 |tokens/s 9030.195 |walltime 17549.800 |
Transformer | epoch 1 | step 34597 |avg loss 8.636 |avg tokens 4942.400 |tokens/s 9865.483 |walltime 17554.810 |
Transformer | epoch 1 | step 34607 |avg loss 8.476 |avg tokens 4342.000 |tokens/s 8603.751 |walltime 17559.857 |
Transformer | epoch 1 | step 34617 |avg loss 8.433 |avg tokens 4431.500 |tokens/s 8971.066 |walltime 17564.797 |
Transformer | epoch 1 | step 34627 |avg loss 8.476 |avg tokens 4111.000 |tokens/s 8319.498 |walltime 17569.738 |
Transformer | epoch 1 | step 34637 |avg loss 8.608 |avg tokens 3916.500 |tokens/s 8242.049 |walltime 17574.490 |
Transformer | epoch 1 | step 34647 |avg loss 8.702 |avg tokens 4368.100 |tokens/s 9012.578 |walltime 17579.336 |
Transformer | epoch 1 | step 34657 |avg loss 8.664 |avg tokens 4596.100 |tokens/s 9125.070 |walltime 17584.373 |
Transformer | epoch 1 | step 34667 |avg loss 8.835 |avg tokens 4633.100 |tokens/s 9534.960 |walltime 17589.232 |
Transformer | epoch 1 | step 34677 |avg loss 8.614 |avg tokens 4749.700 |tokens/s 9154.086 |walltime 17594.421 |
Transformer | epoch 1 | step 34687 |avg loss 8.649 |avg tokens 4639.500 |tokens/s 9343.424 |walltime 17599.386 |
Transformer | epoch 1 | step 34697 |avg loss 8.657 |avg tokens 4291.100 |tokens/s 8758.468 |walltime 17604.286 |
Transformer | epoch 1 | step 34707 |avg loss 8.566 |avg tokens 4734.700 |tokens/s 9039.113 |walltime 17609.524 |
Transformer | epoch 1 | step 34717 |avg loss 8.593 |avg tokens 4307.000 |tokens/s 8823.567 |walltime 17614.405 |
Transformer | epoch 1 | step 34727 |avg loss 8.359 |avg tokens 4402.700 |tokens/s 8617.181 |walltime 17619.514 |
Transformer | epoch 1 | step 34737 |avg loss 8.521 |avg tokens 4329.800 |tokens/s 8761.423 |walltime 17624.456 |
Transformer | epoch 1 | step 34747 |avg loss 8.650 |avg tokens 4161.800 |tokens/s 8898.926 |walltime 17629.133 |
Transformer | epoch 1 | step 34757 |avg loss 8.635 |avg tokens 4501.200 |tokens/s 9446.778 |walltime 17633.898 |
Transformer | epoch 1 | step 34767 |avg loss 8.305 |avg tokens 4596.000 |tokens/s 8981.026 |walltime 17639.015 |
Transformer | epoch 1 | step 34777 |avg loss 8.383 |avg tokens 4726.400 |tokens/s 9081.020 |walltime 17644.220 |
Transformer | epoch 1 | step 34787 |avg loss 8.553 |avg tokens 4451.800 |tokens/s 8910.091 |walltime 17649.216 |
Transformer | epoch 1 | step 34797 |avg loss 8.207 |avg tokens 4852.000 |tokens/s 9682.204 |walltime 17654.228 |
Transformer | epoch 1 | step 34807 |avg loss 8.616 |avg tokens 3887.800 |tokens/s 7882.708 |walltime 17659.160 |
Transformer | epoch 1 | step 34817 |avg loss 8.321 |avg tokens 4695.700 |tokens/s 9108.999 |walltime 17664.315 |
Transformer | epoch 1 | step 34827 |avg loss 8.755 |avg tokens 4637.800 |tokens/s 9330.253 |walltime 17669.285 |
Transformer | epoch 1 | step 34837 |avg loss 8.232 |avg tokens 4885.600 |tokens/s 9193.627 |walltime 17674.599 |
Transformer | epoch 1 | step 34847 |avg loss 8.771 |avg tokens 4689.300 |tokens/s 9379.627 |walltime 17679.599 |
Transformer | epoch 1 | step 34857 |avg loss 8.628 |avg tokens 3945.000 |tokens/s 8188.974 |walltime 17684.416 |
Transformer | epoch 1 | step 34867 |avg loss 8.343 |avg tokens 4757.900 |tokens/s 9140.661 |walltime 17689.622 |
Transformer | epoch 1 | step 34877 |avg loss 8.378 |avg tokens 4907.000 |tokens/s 9525.367 |walltime 17694.773 |
Transformer | epoch 1 | step 34887 |avg loss 8.642 |avg tokens 4516.400 |tokens/s 9073.204 |walltime 17699.751 |
Transformer | epoch 1 | step 34897 |avg loss 8.747 |avg tokens 4476.400 |tokens/s 9068.425 |walltime 17704.687 |
Transformer | epoch 1 | step 34907 |avg loss 8.444 |avg tokens 4875.200 |tokens/s 9090.710 |walltime 17710.050 |
Transformer | epoch 1 | step 34917 |avg loss 8.841 |avg tokens 3724.000 |tokens/s 8217.079 |walltime 17714.582 |
Transformer | epoch 1 | step 34927 |avg loss 8.511 |avg tokens 4864.700 |tokens/s 9470.728 |walltime 17719.719 |
Transformer | epoch 1 | step 34937 |avg loss 8.735 |avg tokens 4235.900 |tokens/s 9020.909 |walltime 17724.414 |
Transformer | epoch 1 | step 34947 |avg loss 8.741 |avg tokens 4372.900 |tokens/s 8887.845 |walltime 17729.334 |
Transformer | epoch 1 | step 34957 |avg loss 8.726 |avg tokens 4374.300 |tokens/s 8821.647 |walltime 17734.293 |
Transformer | epoch 1 | step 34967 |avg loss 8.482 |avg tokens 4514.500 |tokens/s 8746.526 |walltime 17739.454 |
Transformer | epoch 1 | step 34977 |avg loss 8.599 |avg tokens 4651.800 |tokens/s 9165.484 |walltime 17744.530 |
Transformer | epoch 1 | step 34987 |avg loss 8.512 |avg tokens 4338.900 |tokens/s 8598.466 |walltime 17749.576 |
Transformer | epoch 1 | step 34997 |avg loss 8.754 |avg tokens 4235.000 |tokens/s 8714.953 |walltime 17754.435 |
Transformer | epoch 1 | step 35007 |avg loss 8.481 |avg tokens 4408.300 |tokens/s 8978.377 |walltime 17759.345 |
Transformer | epoch 1 | step 35017 |avg loss 8.725 |avg tokens 4517.200 |tokens/s 8654.213 |walltime 17764.565 |
Transformer | epoch 1 | step 35027 |avg loss 8.520 |avg tokens 4566.900 |tokens/s 8907.394 |walltime 17769.692 |
Transformer | epoch 1 | step 35037 |avg loss 8.332 |avg tokens 4638.200 |tokens/s 9004.135 |walltime 17774.843 |
Transformer | epoch 1 | step 35047 |avg loss 8.345 |avg tokens 4574.600 |tokens/s 8847.981 |walltime 17780.013 |
Transformer | epoch 1 | step 35057 |avg loss 8.492 |avg tokens 4678.400 |tokens/s 9116.697 |walltime 17785.145 |
Transformer | epoch 1 | step 35067 |avg loss 8.688 |avg tokens 4003.100 |tokens/s 8187.523 |walltime 17790.034 |
Transformer | epoch 1 | step 35077 |avg loss 8.271 |avg tokens 4397.600 |tokens/s 8785.826 |walltime 17795.040 |
Transformer | epoch 1 | step 35087 |avg loss 8.775 |avg tokens 4499.200 |tokens/s 9348.813 |walltime 17799.852 |
Transformer | epoch 1 | step 35097 |avg loss 8.800 |avg tokens 4238.900 |tokens/s 9032.661 |walltime 17804.545 |
Transformer | epoch 1 | step 35107 |avg loss 8.406 |avg tokens 4323.300 |tokens/s 8879.494 |walltime 17809.414 |
Transformer | epoch 1 | step 35117 |avg loss 8.538 |avg tokens 4617.800 |tokens/s 9132.590 |walltime 17814.470 |
Transformer | epoch 1 | step 35127 |avg loss 8.451 |avg tokens 4672.500 |tokens/s 8923.887 |walltime 17819.706 |
Transformer | epoch 1 | step 35137 |avg loss 8.445 |avg tokens 4527.300 |tokens/s 8772.333 |walltime 17824.867 |
Transformer | epoch 1 | step 35147 |avg loss 8.404 |avg tokens 4632.000 |tokens/s 9108.203 |walltime 17829.953 |
Transformer | epoch 1 | step 35157 |avg loss 8.334 |avg tokens 4756.700 |tokens/s 9008.694 |walltime 17835.233 |
Transformer | epoch 1 | step 35167 |avg loss 8.408 |avg tokens 3935.000 |tokens/s 8267.193 |walltime 17839.993 |
Transformer | epoch 1 | step 35177 |avg loss 8.532 |avg tokens 4846.800 |tokens/s 9538.524 |walltime 17845.074 |
Transformer | epoch 1 | step 35187 |avg loss 8.480 |avg tokens 4594.100 |tokens/s 9037.477 |walltime 17850.157 |
Transformer | epoch 1 | step 35197 |avg loss 8.506 |avg tokens 4486.100 |tokens/s 8880.318 |walltime 17855.209 |
Transformer | epoch 1 | step 35207 |avg loss 8.420 |avg tokens 4813.400 |tokens/s 9265.246 |walltime 17860.404 |
Transformer | epoch 1 | step 35217 |avg loss 8.515 |avg tokens 4550.900 |tokens/s 8877.234 |walltime 17865.531 |
Transformer | epoch 1 | step 35227 |avg loss 8.272 |avg tokens 4965.700 |tokens/s 9368.947 |walltime 17870.831 |
Transformer | epoch 1 | step 35237 |avg loss 8.405 |avg tokens 4820.000 |tokens/s 9124.669 |walltime 17876.113 |
Transformer | epoch 1 | step 35247 |avg loss 8.578 |avg tokens 4241.800 |tokens/s 8288.182 |walltime 17881.231 |
Transformer | epoch 1 | step 35257 |avg loss 8.430 |avg tokens 4577.200 |tokens/s 9038.197 |walltime 17886.295 |
Transformer | epoch 1 | step 35267 |avg loss 8.599 |avg tokens 3952.100 |tokens/s 8265.011 |walltime 17891.077 |
Transformer | epoch 1 | step 35277 |avg loss 8.788 |avg tokens 4595.500 |tokens/s 9120.792 |walltime 17896.116 |
Transformer | epoch 1 | step 35287 |avg loss 8.401 |avg tokens 4298.000 |tokens/s 8520.374 |walltime 17901.160 |
Transformer | epoch 1 | step 35297 |avg loss 8.627 |avg tokens 4691.100 |tokens/s 9342.173 |walltime 17906.181 |
Transformer | epoch 1 | step 35307 |avg loss 8.546 |avg tokens 4577.400 |tokens/s 8732.566 |walltime 17911.423 |
Transformer | epoch 1 | step 35317 |avg loss 8.349 |avg tokens 4763.200 |tokens/s 9084.244 |walltime 17916.667 |
Transformer | epoch 1 | step 35327 |avg loss 8.354 |avg tokens 4785.800 |tokens/s 9096.684 |walltime 17921.928 |
Transformer | epoch 1 | step 35337 |avg loss 8.589 |avg tokens 4554.700 |tokens/s 9086.641 |walltime 17926.940 |
Transformer | epoch 1 | step 35347 |avg loss 8.527 |avg tokens 4450.000 |tokens/s 9077.352 |walltime 17931.842 |
Transformer | epoch 1 | step 35357 |avg loss 8.653 |avg tokens 4654.300 |tokens/s 9118.773 |walltime 17936.947 |
Transformer | epoch 1 | step 35367 |avg loss 8.487 |avg tokens 4918.800 |tokens/s 9350.160 |walltime 17942.207 |
Transformer | epoch 1 | step 35377 |avg loss 8.518 |avg tokens 4630.800 |tokens/s 8937.848 |walltime 17947.388 |
Transformer | epoch 1 | step 35387 |avg loss 8.623 |avg tokens 4825.900 |tokens/s 9446.082 |walltime 17952.497 |
Transformer | epoch 1 | step 35397 |avg loss 8.449 |avg tokens 4788.300 |tokens/s 9136.741 |walltime 17957.738 |
Transformer | epoch 1 | step 35407 |avg loss 8.564 |avg tokens 4207.300 |tokens/s 8474.024 |walltime 17962.703 |
Transformer | epoch 1 | step 35417 |avg loss 8.298 |avg tokens 4869.600 |tokens/s 9157.141 |walltime 17968.021 |
Transformer | epoch 1 | step 35427 |avg loss 8.339 |avg tokens 4814.400 |tokens/s 9328.224 |walltime 17973.182 |
Transformer | epoch 1 | step 35437 |avg loss 8.680 |avg tokens 4212.500 |tokens/s 8633.756 |walltime 17978.061 |
Transformer | epoch 1 | step 35447 |avg loss 8.673 |avg tokens 4496.500 |tokens/s 9159.867 |walltime 17982.970 |
Transformer | epoch 1 | step 35457 |avg loss 8.316 |avg tokens 4757.600 |tokens/s 9058.534 |walltime 17988.222 |
Transformer | epoch 1 | step 35467 |avg loss 8.249 |avg tokens 4565.200 |tokens/s 9189.190 |walltime 17993.190 |
Transformer | epoch 1 | step 35477 |avg loss 8.189 |avg tokens 4704.000 |tokens/s 9236.047 |walltime 17998.283 |
Transformer | epoch 1 | step 35487 |avg loss 8.542 |avg tokens 4688.500 |tokens/s 9107.403 |walltime 18003.431 |
Transformer | epoch 1 | step 35497 |avg loss 8.619 |avg tokens 4349.800 |tokens/s 8858.708 |walltime 18008.341 |
Transformer | epoch 1 | step 35507 |avg loss 8.668 |avg tokens 4788.100 |tokens/s 9819.495 |walltime 18013.217 |
Transformer | epoch 1 | step 35517 |avg loss 8.410 |avg tokens 4586.300 |tokens/s 9046.736 |walltime 18018.287 |
Transformer | epoch 1 | step 35527 |avg loss 8.461 |avg tokens 4543.000 |tokens/s 8826.725 |walltime 18023.434 |
Transformer | epoch 1 | step 35537 |avg loss 8.495 |avg tokens 4723.000 |tokens/s 9299.799 |walltime 18028.512 |
Transformer | epoch 1 | step 35547 |avg loss 8.445 |avg tokens 4276.200 |tokens/s 8674.890 |walltime 18033.442 |
Transformer | epoch 1 | step 35557 |avg loss 8.540 |avg tokens 4464.200 |tokens/s 9027.626 |walltime 18038.387 |
Transformer | epoch 1 | step 35567 |avg loss 8.546 |avg tokens 4574.200 |tokens/s 9010.343 |walltime 18043.463 |
Transformer | epoch 1 | step 35577 |avg loss 8.730 |avg tokens 4502.600 |tokens/s 9304.481 |walltime 18048.303 |
Transformer | epoch 1 | step 35587 |avg loss 8.551 |avg tokens 4779.000 |tokens/s 9183.039 |walltime 18053.507 |
Transformer | epoch 1 | step 35597 |avg loss 8.482 |avg tokens 4390.500 |tokens/s 8788.744 |walltime 18058.502 |
Transformer | epoch 1 | step 35607 |avg loss 8.632 |avg tokens 4471.100 |tokens/s 8826.273 |walltime 18063.568 |
Transformer | epoch 1 | step 35617 |avg loss 8.674 |avg tokens 4460.500 |tokens/s 9000.862 |walltime 18068.524 |
Transformer | epoch 1 | step 35627 |avg loss 8.608 |avg tokens 4419.600 |tokens/s 9066.738 |walltime 18073.398 |
Transformer | epoch 1 | step 35637 |avg loss 8.728 |avg tokens 4571.700 |tokens/s 9282.555 |walltime 18078.323 |
Transformer | epoch 1 | step 35647 |avg loss 8.399 |avg tokens 4832.600 |tokens/s 9064.911 |walltime 18083.654 |
Transformer | epoch 1 | step 35657 |avg loss 8.562 |avg tokens 4417.300 |tokens/s 8695.704 |walltime 18088.734 |
Transformer | epoch 1 | step 35667 |avg loss 8.302 |avg tokens 4776.600 |tokens/s 8949.147 |walltime 18094.072 |
Transformer | epoch 1 | step 35677 |avg loss 8.828 |avg tokens 4167.600 |tokens/s 8970.057 |walltime 18098.718 |
Transformer | epoch 1 | step 35687 |avg loss 8.681 |avg tokens 4444.400 |tokens/s 9185.714 |walltime 18103.556 |
Transformer | epoch 1 | step 35697 |avg loss 8.563 |avg tokens 4122.100 |tokens/s 8264.669 |walltime 18108.544 |
Transformer | epoch 1 | step 35707 |avg loss 8.563 |avg tokens 4601.300 |tokens/s 8859.324 |walltime 18113.738 |
Transformer | epoch 1 | step 35717 |avg loss 8.558 |avg tokens 4256.500 |tokens/s 8682.606 |walltime 18118.640 |
Transformer | epoch 1 | step 35727 |avg loss 8.767 |avg tokens 4083.700 |tokens/s 8540.945 |walltime 18123.421 |
Transformer | epoch 1 | step 35737 |avg loss 8.676 |avg tokens 4819.100 |tokens/s 9470.109 |walltime 18128.510 |
Transformer | epoch 1 | step 35747 |avg loss 8.541 |avg tokens 4344.300 |tokens/s 8312.305 |walltime 18133.736 |
Transformer | epoch 1 | step 35757 |avg loss 8.683 |avg tokens 4647.200 |tokens/s 9317.627 |walltime 18138.724 |
Transformer | epoch 1 | step 35767 |avg loss 8.499 |avg tokens 4794.100 |tokens/s 9169.780 |walltime 18143.952 |
Transformer | epoch 1 | step 35777 |avg loss 8.618 |avg tokens 4695.400 |tokens/s 9223.690 |walltime 18149.043 |
Transformer | epoch 1 | step 35787 |avg loss 8.726 |avg tokens 4894.400 |tokens/s 9627.644 |walltime 18154.126 |
Transformer | epoch 1 | step 35797 |avg loss 8.474 |avg tokens 4757.800 |tokens/s 9082.789 |walltime 18159.365 |
Transformer | epoch 1 | step 35807 |avg loss 8.419 |avg tokens 4493.100 |tokens/s 8718.615 |walltime 18164.518 |
Transformer | epoch 1 | step 35817 |avg loss 8.834 |avg tokens 4221.200 |tokens/s 8892.387 |walltime 18169.265 |
Transformer | epoch 1 | step 35827 |avg loss 8.469 |avg tokens 4215.200 |tokens/s 8616.505 |walltime 18174.157 |
Transformer | epoch 1 | step 35837 |avg loss 8.683 |avg tokens 4659.200 |tokens/s 9065.994 |walltime 18179.296 |
Transformer | epoch 1 | step 35847 |avg loss 8.825 |avg tokens 4358.200 |tokens/s 8970.769 |walltime 18184.154 |
Transformer | epoch 1 | step 35857 |avg loss 8.533 |avg tokens 4429.800 |tokens/s 9086.620 |walltime 18189.030 |
Transformer | epoch 1 | step 35867 |avg loss 8.721 |avg tokens 4741.000 |tokens/s 9175.168 |walltime 18194.197 |
Transformer | epoch 1 | step 35877 |avg loss 8.701 |avg tokens 4490.000 |tokens/s 9404.023 |walltime 18198.971 |
Transformer | epoch 1 | step 35887 |avg loss 8.597 |avg tokens 4329.300 |tokens/s 8526.524 |walltime 18204.049 |
Transformer | epoch 1 | step 35897 |avg loss 8.419 |avg tokens 4814.600 |tokens/s 8974.175 |walltime 18209.414 |
Transformer | epoch 1 | step 35907 |avg loss 8.600 |avg tokens 4354.400 |tokens/s 8633.709 |walltime 18214.457 |
Transformer | epoch 1 | step 35917 |avg loss 8.539 |avg tokens 4638.100 |tokens/s 9068.859 |walltime 18219.572 |
Transformer | epoch 1 | step 35927 |avg loss 8.778 |avg tokens 4646.000 |tokens/s 8919.054 |walltime 18224.781 |
Transformer | epoch 1 | step 35937 |avg loss 8.589 |avg tokens 4527.200 |tokens/s 9050.371 |walltime 18229.783 |
Transformer | epoch 1 | step 35947 |avg loss 8.773 |avg tokens 4569.700 |tokens/s 9247.426 |walltime 18234.724 |
Transformer | epoch 1 | step 35957 |avg loss 8.521 |avg tokens 4269.000 |tokens/s 8434.458 |walltime 18239.786 |
Transformer | epoch 1 | step 35967 |avg loss 8.664 |avg tokens 4791.600 |tokens/s 9211.123 |walltime 18244.988 |
Transformer | epoch 1 | step 35977 |avg loss 8.801 |avg tokens 4713.200 |tokens/s 9410.025 |walltime 18249.997 |
Transformer | epoch 1 | step 35987 |avg loss 8.582 |avg tokens 4713.300 |tokens/s 9182.961 |walltime 18255.129 |
Transformer | epoch 1 | step 35997 |avg loss 8.490 |avg tokens 4780.900 |tokens/s 9182.781 |walltime 18260.336 |
Transformer | epoch 1 | step 36007 |avg loss 8.474 |avg tokens 4751.200 |tokens/s 9046.375 |walltime 18265.588 |
Transformer | epoch 1 | step 36017 |avg loss 8.409 |avg tokens 4786.300 |tokens/s 9128.293 |walltime 18270.831 |
Transformer | epoch 1 | step 36027 |avg loss 8.540 |avg tokens 4534.000 |tokens/s 9146.696 |walltime 18275.788 |
Transformer | epoch 1 | step 36037 |avg loss 8.619 |avg tokens 4786.000 |tokens/s 9384.141 |walltime 18280.888 |
Transformer | epoch 1 | step 36047 |avg loss 8.472 |avg tokens 4552.800 |tokens/s 8826.318 |walltime 18286.046 |
Transformer | epoch 1 | step 36057 |avg loss 8.466 |avg tokens 4811.200 |tokens/s 9151.360 |walltime 18291.304 |
Transformer | epoch 1 | step 36067 |avg loss 8.347 |avg tokens 4701.600 |tokens/s 8844.042 |walltime 18296.620 |
Transformer | epoch 1 | step 36077 |avg loss 8.485 |avg tokens 4579.900 |tokens/s 9020.497 |walltime 18301.697 |
Transformer | epoch 1 | step 36087 |avg loss 8.254 |avg tokens 4863.200 |tokens/s 9134.266 |walltime 18307.021 |
Transformer | epoch 1 | step 36097 |avg loss 8.562 |avg tokens 4613.000 |tokens/s 8920.903 |walltime 18312.192 |
Transformer | epoch 1 | step 36107 |avg loss 8.755 |avg tokens 4473.100 |tokens/s 9387.612 |walltime 18316.957 |
Transformer | epoch 1 | step 36117 |avg loss 8.924 |avg tokens 4678.000 |tokens/s 9399.306 |walltime 18321.934 |
Transformer | epoch 1 | step 36127 |avg loss 8.510 |avg tokens 4632.200 |tokens/s 9044.555 |walltime 18327.056 |
Transformer | epoch 1 | step 36137 |avg loss 8.520 |avg tokens 4832.000 |tokens/s 9361.647 |walltime 18332.217 |
Transformer | epoch 1 | step 36147 |avg loss 8.572 |avg tokens 4669.600 |tokens/s 9147.008 |walltime 18337.322 |
Transformer | epoch 1 | step 36157 |avg loss 8.692 |avg tokens 4704.400 |tokens/s 9452.377 |walltime 18342.299 |
Transformer | epoch 1 | step 36167 |avg loss 8.590 |avg tokens 4588.700 |tokens/s 9350.905 |walltime 18347.206 |
Transformer | epoch 1 | step 36177 |avg loss 8.416 |avg tokens 4252.600 |tokens/s 8548.502 |walltime 18352.181 |
Transformer | epoch 1 | step 36187 |avg loss 8.603 |avg tokens 4850.700 |tokens/s 9578.603 |walltime 18357.245 |
Transformer | epoch 1 | step 36197 |avg loss 8.548 |avg tokens 4777.000 |tokens/s 9249.134 |walltime 18362.410 |
Transformer | epoch 1 | step 36207 |avg loss 8.577 |avg tokens 4473.300 |tokens/s 8822.385 |walltime 18367.480 |
Transformer | epoch 1 | step 36217 |avg loss 8.760 |avg tokens 4449.800 |tokens/s 8774.800 |walltime 18372.551 |
Transformer | epoch 1 | step 36227 |avg loss 8.644 |avg tokens 4294.900 |tokens/s 8538.942 |walltime 18377.581 |
Transformer | epoch 1 | step 36237 |avg loss 8.692 |avg tokens 4652.100 |tokens/s 9139.634 |walltime 18382.671 |
Transformer | epoch 1 | step 36247 |avg loss 8.710 |avg tokens 4618.000 |tokens/s 8923.496 |walltime 18387.846 |
Transformer | epoch 1 | step 36257 |avg loss 8.358 |avg tokens 4569.800 |tokens/s 9070.348 |walltime 18392.884 |
Transformer | epoch 1 | step 36267 |avg loss 8.549 |avg tokens 4452.600 |tokens/s 8969.035 |walltime 18397.849 |
Transformer | epoch 1 | step 36277 |avg loss 8.640 |avg tokens 4793.000 |tokens/s 9384.200 |walltime 18402.956 |
Transformer | epoch 1 | step 36287 |avg loss 8.739 |avg tokens 4379.500 |tokens/s 9129.063 |walltime 18407.754 |
Transformer | epoch 1 | step 36297 |avg loss 8.431 |avg tokens 4098.700 |tokens/s 8361.461 |walltime 18412.656 |
Transformer | epoch 1 | step 36307 |avg loss 8.755 |avg tokens 4795.600 |tokens/s 9329.845 |walltime 18417.796 |
Transformer | epoch 1 | step 36317 |avg loss 8.618 |avg tokens 4818.900 |tokens/s 9475.683 |walltime 18422.881 |
Transformer | epoch 1 | step 36327 |avg loss 8.412 |avg tokens 4575.600 |tokens/s 9024.540 |walltime 18427.951 |
Transformer | epoch 1 | step 36337 |avg loss 8.511 |avg tokens 4680.300 |tokens/s 9113.118 |walltime 18433.087 |
Transformer | epoch 1 | step 36347 |avg loss 8.263 |avg tokens 4639.900 |tokens/s 8765.690 |walltime 18438.380 |
Transformer | epoch 1 | step 36357 |avg loss 8.515 |avg tokens 4380.300 |tokens/s 8854.650 |walltime 18443.327 |
Transformer | epoch 1 | step 36367 |avg loss 8.755 |avg tokens 4495.800 |tokens/s 9452.450 |walltime 18448.084 |
Transformer | epoch 1 | step 36377 |avg loss 8.442 |avg tokens 4109.200 |tokens/s 8165.860 |walltime 18453.116 |
Transformer | epoch 1 | step 36387 |avg loss 8.541 |avg tokens 4215.700 |tokens/s 8761.697 |walltime 18457.927 |
Transformer | epoch 1 | step 36397 |avg loss 8.835 |avg tokens 4517.600 |tokens/s 9688.329 |walltime 18462.590 |
Transformer | epoch 1 | step 36407 |avg loss 8.647 |avg tokens 4624.700 |tokens/s 9181.130 |walltime 18467.627 |
Transformer | epoch 1 | step 36417 |avg loss 8.796 |avg tokens 4465.500 |tokens/s 8916.490 |walltime 18472.636 |
Transformer | epoch 1 | step 36427 |avg loss 8.741 |avg tokens 3950.500 |tokens/s 8485.368 |walltime 18477.291 |
Transformer | epoch 1 | step 36437 |avg loss 8.426 |avg tokens 4651.300 |tokens/s 8935.425 |walltime 18482.497 |
Transformer | epoch 1 | step 36447 |avg loss 8.337 |avg tokens 4756.000 |tokens/s 8963.744 |walltime 18487.802 |
Transformer | epoch 1 | step 36457 |avg loss 8.752 |avg tokens 4956.600 |tokens/s 9667.741 |walltime 18492.929 |
Transformer | epoch 1 | step 36467 |avg loss 8.667 |avg tokens 4127.900 |tokens/s 8472.814 |walltime 18497.801 |
Transformer | epoch 1 | step 36477 |avg loss 8.482 |avg tokens 4687.300 |tokens/s 9245.405 |walltime 18502.871 |
Transformer | epoch 1 | step 36487 |avg loss 8.430 |avg tokens 4881.600 |tokens/s 9195.670 |walltime 18508.180 |
Transformer | epoch 1 | step 36497 |avg loss 8.473 |avg tokens 4242.600 |tokens/s 8509.253 |walltime 18513.166 |
Transformer | epoch 1 | step 36507 |avg loss 8.579 |avg tokens 4491.700 |tokens/s 8865.724 |walltime 18518.232 |
Transformer | epoch 1 | step 36517 |avg loss 8.627 |avg tokens 4523.600 |tokens/s 9382.768 |walltime 18523.053 |
Transformer | epoch 1 | step 36527 |avg loss 8.322 |avg tokens 4713.400 |tokens/s 9019.466 |walltime 18528.279 |
Transformer | epoch 1 | step 36537 |avg loss 8.399 |avg tokens 4156.900 |tokens/s 8534.793 |walltime 18533.150 |
Transformer | epoch 1 | step 36547 |avg loss 8.583 |avg tokens 4390.700 |tokens/s 8521.293 |walltime 18538.302 |
Transformer | epoch 1 | step 36557 |avg loss 8.650 |avg tokens 4766.300 |tokens/s 9370.756 |walltime 18543.389 |
Transformer | epoch 1 | step 36567 |avg loss 8.400 |avg tokens 4902.400 |tokens/s 9248.594 |walltime 18548.689 |
Transformer | epoch 1 | step 36577 |avg loss 8.575 |avg tokens 4989.400 |tokens/s 9579.549 |walltime 18553.898 |
Transformer | epoch 1 | step 36587 |avg loss 8.552 |avg tokens 4797.900 |tokens/s 9347.038 |walltime 18559.031 |
Transformer | epoch 1 | step 36597 |avg loss 8.572 |avg tokens 4504.500 |tokens/s 9123.416 |walltime 18563.968 |
Transformer | epoch 1 | step 36607 |avg loss 8.576 |avg tokens 4638.000 |tokens/s 9419.445 |walltime 18568.892 |
Transformer | epoch 1 | step 36617 |avg loss 8.621 |avg tokens 4805.100 |tokens/s 9105.494 |walltime 18574.169 |
Transformer | epoch 1 | step 36627 |avg loss 8.455 |avg tokens 4646.700 |tokens/s 9019.875 |walltime 18579.321 |
Transformer | epoch 1 | step 36637 |avg loss 8.316 |avg tokens 4383.300 |tokens/s 8717.129 |walltime 18584.349 |
Transformer | epoch 1 | step 36647 |avg loss 8.600 |avg tokens 4249.900 |tokens/s 8594.560 |walltime 18589.294 |
Transformer | epoch 1 | step 36657 |avg loss 8.725 |avg tokens 4331.000 |tokens/s 8872.784 |walltime 18594.175 |
Transformer | epoch 1 | step 36667 |avg loss 8.379 |avg tokens 4812.500 |tokens/s 9238.580 |walltime 18599.384 |
Transformer | epoch 1 | step 36677 |avg loss 8.768 |avg tokens 4191.700 |tokens/s 8991.270 |walltime 18604.046 |
Transformer | epoch 1 | step 36687 |avg loss 8.768 |avg tokens 4412.900 |tokens/s 9078.249 |walltime 18608.907 |
Transformer | epoch 1 | step 36697 |avg loss 8.423 |avg tokens 4535.100 |tokens/s 9024.891 |walltime 18613.932 |
Transformer | epoch 1 | step 36707 |avg loss 8.477 |avg tokens 4782.600 |tokens/s 9260.048 |walltime 18619.097 |
Transformer | epoch 1 | step 36717 |avg loss 8.481 |avg tokens 4343.200 |tokens/s 8589.902 |walltime 18624.153 |
Transformer | epoch 1 | step 36727 |avg loss 8.465 |avg tokens 4861.800 |tokens/s 9362.477 |walltime 18629.346 |
Transformer | epoch 1 | step 36737 |avg loss 8.495 |avg tokens 4675.300 |tokens/s 9109.321 |walltime 18634.479 |
Transformer | epoch 1 | step 36747 |avg loss 8.550 |avg tokens 4755.400 |tokens/s 9221.169 |walltime 18639.636 |
Transformer | epoch 1 | step 36757 |avg loss 8.733 |avg tokens 4676.100 |tokens/s 9135.849 |walltime 18644.754 |
Transformer | epoch 1 | step 36767 |avg loss 8.703 |avg tokens 4184.200 |tokens/s 8435.875 |walltime 18649.714 |
Transformer | epoch 1 | step 36777 |avg loss 8.616 |avg tokens 4578.300 |tokens/s 9008.888 |walltime 18654.796 |
Transformer | epoch 1 | step 36787 |avg loss 8.608 |avg tokens 4419.000 |tokens/s 8843.866 |walltime 18659.793 |
Transformer | epoch 1 | step 36797 |avg loss 8.594 |avg tokens 4931.600 |tokens/s 9305.553 |walltime 18665.092 |
Transformer | epoch 1 | step 36807 |avg loss 8.451 |avg tokens 4574.600 |tokens/s 8996.385 |walltime 18670.177 |
Transformer | epoch 1 | step 36817 |avg loss 8.514 |avg tokens 4346.100 |tokens/s 8728.959 |walltime 18675.156 |
Transformer | epoch 1 | step 36827 |avg loss 8.607 |avg tokens 3928.500 |tokens/s 8394.193 |walltime 18679.836 |
Transformer | epoch 1 | step 36837 |avg loss 8.339 |avg tokens 4371.500 |tokens/s 8864.694 |walltime 18684.768 |
Transformer | epoch 1 | step 36847 |avg loss 8.712 |avg tokens 4564.200 |tokens/s 8920.560 |walltime 18689.884 |
Transformer | epoch 1 | step 36857 |avg loss 8.649 |avg tokens 3991.800 |tokens/s 8205.052 |walltime 18694.749 |
Transformer | epoch 1 | step 36867 |avg loss 8.469 |avg tokens 4843.900 |tokens/s 9374.684 |walltime 18699.916 |
Transformer | epoch 1 | step 36877 |avg loss 8.436 |avg tokens 4799.200 |tokens/s 9180.336 |walltime 18705.144 |
Transformer | epoch 1 | step 36887 |avg loss 8.596 |avg tokens 4762.200 |tokens/s 9248.187 |walltime 18710.293 |
Transformer | epoch 1 | step 36897 |avg loss 8.780 |avg tokens 4620.400 |tokens/s 9238.625 |walltime 18715.294 |
Transformer | epoch 1 | step 36907 |avg loss 8.686 |avg tokens 4762.200 |tokens/s 9360.910 |walltime 18720.382 |
Transformer | epoch 1 | step 36917 |avg loss 8.623 |avg tokens 4757.200 |tokens/s 9244.394 |walltime 18725.528 |
Transformer | epoch 1 | step 36927 |avg loss 8.639 |avg tokens 3810.100 |tokens/s 8332.362 |walltime 18730.100 |
Transformer | epoch 1 | step 36937 |avg loss 8.599 |avg tokens 4484.900 |tokens/s 8988.451 |walltime 18735.090 |
Transformer | epoch 1 | step 36947 |avg loss 8.458 |avg tokens 4213.400 |tokens/s 8430.978 |walltime 18740.088 |
Transformer | epoch 1 | step 36957 |avg loss 8.691 |avg tokens 4785.100 |tokens/s 9456.155 |walltime 18745.148 |
Transformer | epoch 1 | step 36967 |avg loss 8.632 |avg tokens 4828.900 |tokens/s 9331.610 |walltime 18750.323 |
Transformer | epoch 1 | step 36977 |avg loss 8.501 |avg tokens 3806.400 |tokens/s 8069.180 |walltime 18755.040 |
Transformer | epoch 1 | step 36987 |avg loss 8.683 |avg tokens 4636.400 |tokens/s 9012.589 |walltime 18760.184 |
Transformer | epoch 1 | step 36997 |avg loss 8.706 |avg tokens 4055.200 |tokens/s 8470.122 |walltime 18764.972 |
Transformer | epoch 1 | step 37007 |avg loss 8.614 |avg tokens 3782.100 |tokens/s 8186.714 |walltime 18769.592 |
Transformer | epoch 1 | step 37017 |avg loss 8.338 |avg tokens 4758.200 |tokens/s 8880.051 |walltime 18774.950 |
Transformer | epoch 1 | step 37027 |avg loss 8.472 |avg tokens 4234.200 |tokens/s 8542.000 |walltime 18779.907 |
Transformer | epoch 1 | step 37037 |avg loss 8.925 |avg tokens 4361.300 |tokens/s 9682.590 |walltime 18784.411 |
Transformer | epoch 1 | step 37047 |avg loss 8.669 |avg tokens 4471.800 |tokens/s 8845.363 |walltime 18789.467 |
Transformer | epoch 1 | step 37057 |avg loss 8.433 |avg tokens 4688.600 |tokens/s 8860.411 |walltime 18794.758 |
Transformer | epoch 1 | step 37067 |avg loss 8.495 |avg tokens 4730.900 |tokens/s 9287.420 |walltime 18799.852 |
Transformer | epoch 1 | step 37077 |avg loss 8.626 |avg tokens 4740.500 |tokens/s 9360.370 |walltime 18804.917 |
Transformer | epoch 1 | step 37087 |avg loss 8.620 |avg tokens 4543.000 |tokens/s 8866.910 |walltime 18810.040 |
Transformer | epoch 1 | step 37097 |avg loss 8.158 |avg tokens 4480.000 |tokens/s 8629.890 |walltime 18815.231 |
Transformer | epoch 1 | step 37107 |avg loss 8.215 |avg tokens 4680.500 |tokens/s 8885.823 |walltime 18820.499 |
Transformer | epoch 1 | step 37117 |avg loss 8.242 |avg tokens 4891.800 |tokens/s 9506.538 |walltime 18825.645 |
Transformer | epoch 1 | step 37127 |avg loss 8.472 |avg tokens 4466.900 |tokens/s 8912.222 |walltime 18830.657 |
Transformer | epoch 1 | step 37137 |avg loss 8.688 |avg tokens 3997.500 |tokens/s 8420.958 |walltime 18835.404 |
Transformer | epoch 1 | step 37147 |avg loss 8.663 |avg tokens 4694.700 |tokens/s 9275.999 |walltime 18840.465 |
Transformer | epoch 1 | step 37157 |avg loss 8.777 |avg tokens 4373.200 |tokens/s 8948.734 |walltime 18845.352 |
Transformer | epoch 1 | step 37167 |avg loss 8.626 |avg tokens 4544.300 |tokens/s 9088.633 |walltime 18850.352 |
Transformer | epoch 1 | step 37177 |avg loss 8.592 |avg tokens 4681.300 |tokens/s 9178.552 |walltime 18855.452 |
Transformer | epoch 1 | step 37187 |avg loss 8.616 |avg tokens 4497.500 |tokens/s 8874.510 |walltime 18860.520 |
Transformer | epoch 1 | step 37197 |avg loss 8.654 |avg tokens 4376.300 |tokens/s 8901.727 |walltime 18865.436 |
Transformer | epoch 1 | step 37207 |avg loss 8.425 |avg tokens 4873.600 |tokens/s 9028.307 |walltime 18870.834 |
Transformer | epoch 1 | step 37217 |avg loss 8.908 |avg tokens 3990.400 |tokens/s 8520.125 |walltime 18875.518 |
Transformer | epoch 1 | step 37227 |avg loss 8.742 |avg tokens 4413.600 |tokens/s 9030.567 |walltime 18880.405 |
Transformer | epoch 1 | step 37237 |avg loss 8.579 |avg tokens 4444.200 |tokens/s 8739.296 |walltime 18885.491 |
Transformer | epoch 1 | step 37247 |avg loss 8.557 |avg tokens 4452.600 |tokens/s 8709.362 |walltime 18890.603 |
Transformer | epoch 1 | step 37257 |avg loss 8.442 |avg tokens 3997.000 |tokens/s 8330.293 |walltime 18895.401 |
Transformer | epoch 1 | step 37267 |avg loss 8.489 |avg tokens 4681.200 |tokens/s 9182.667 |walltime 18900.499 |
Transformer | epoch 1 | step 37277 |avg loss 8.557 |avg tokens 4491.200 |tokens/s 8927.198 |walltime 18905.530 |
Transformer | epoch 1 | step 37287 |avg loss 8.702 |avg tokens 4299.300 |tokens/s 8757.629 |walltime 18910.439 |
Transformer | epoch 1 | step 37297 |avg loss 8.657 |avg tokens 4891.700 |tokens/s 9248.962 |walltime 18915.728 |
Transformer | epoch 1 | step 37307 |avg loss 8.695 |avg tokens 4704.500 |tokens/s 9327.101 |walltime 18920.772 |
Transformer | epoch 1 | step 37317 |avg loss 8.522 |avg tokens 4600.500 |tokens/s 8991.968 |walltime 18925.888 |
Transformer | epoch 1 | step 37327 |avg loss 8.459 |avg tokens 4378.300 |tokens/s 8595.887 |walltime 18930.982 |
Transformer | epoch 1 | step 37337 |avg loss 8.614 |avg tokens 4159.800 |tokens/s 8737.985 |walltime 18935.742 |
Transformer | epoch 1 | step 37347 |avg loss 8.727 |avg tokens 4719.200 |tokens/s 9477.504 |walltime 18940.722 |
Transformer | epoch 1 | step 37357 |avg loss 8.755 |avg tokens 3974.400 |tokens/s 8392.866 |walltime 18945.457 |
Transformer | epoch 1 | step 37367 |avg loss 8.604 |avg tokens 4562.600 |tokens/s 8864.689 |walltime 18950.604 |
Transformer | epoch 1 | step 37377 |avg loss 8.485 |avg tokens 4410.300 |tokens/s 8714.345 |walltime 18955.665 |
Transformer | epoch 1 | step 37387 |avg loss 8.470 |avg tokens 4140.500 |tokens/s 8479.013 |walltime 18960.548 |
Transformer | epoch 1 | step 37397 |avg loss 8.697 |avg tokens 4193.200 |tokens/s 8728.000 |walltime 18965.353 |
Transformer | epoch 1 | step 37407 |avg loss 8.507 |avg tokens 4862.400 |tokens/s 9082.897 |walltime 18970.706 |
Transformer | epoch 1 | step 37417 |avg loss 8.610 |avg tokens 4917.000 |tokens/s 9368.877 |walltime 18975.954 |
Transformer | epoch 1 | step 37427 |avg loss 8.393 |avg tokens 4790.400 |tokens/s 9097.717 |walltime 18981.220 |
Transformer | epoch 1 | step 37437 |avg loss 8.548 |avg tokens 4852.300 |tokens/s 9369.181 |walltime 18986.399 |
Transformer | epoch 1 | step 37447 |avg loss 8.449 |avg tokens 4564.500 |tokens/s 8914.211 |walltime 18991.519 |
Transformer | epoch 1 | step 37457 |avg loss 8.551 |avg tokens 4698.600 |tokens/s 9383.855 |walltime 18996.526 |
Transformer | epoch 1 | step 37467 |avg loss 8.568 |avg tokens 4539.300 |tokens/s 9086.540 |walltime 19001.522 |
Transformer | epoch 1 | step 37477 |avg loss 8.555 |avg tokens 4525.400 |tokens/s 9114.882 |walltime 19006.487 |
Transformer | epoch 1 | step 37487 |avg loss 8.636 |avg tokens 4550.200 |tokens/s 9206.595 |walltime 19011.429 |
Transformer | epoch 1 | step 37497 |avg loss 8.522 |avg tokens 4752.200 |tokens/s 9749.473 |walltime 19016.303 |
Transformer | epoch 1 | step 37507 |avg loss 8.470 |avg tokens 4664.900 |tokens/s 8964.986 |walltime 19021.507 |
Transformer | epoch 1 | step 37517 |avg loss 8.581 |avg tokens 4325.200 |tokens/s 8780.638 |walltime 19026.433 |
Transformer | epoch 1 | step 37527 |avg loss 8.436 |avg tokens 4287.000 |tokens/s 8542.364 |walltime 19031.451 |
Transformer | epoch 1 | step 37537 |avg loss 8.589 |avg tokens 4577.900 |tokens/s 9127.740 |walltime 19036.467 |
Transformer | epoch 1 | step 37547 |avg loss 8.623 |avg tokens 4608.400 |tokens/s 8999.881 |walltime 19041.587 |
Transformer | epoch 1 | step 37557 |avg loss 8.403 |avg tokens 4589.500 |tokens/s 8981.284 |walltime 19046.697 |
Transformer | epoch 1 | step 37567 |avg loss 8.345 |avg tokens 4827.200 |tokens/s 9222.781 |walltime 19051.931 |
Transformer | epoch 1 | step 37577 |avg loss 8.320 |avg tokens 4631.500 |tokens/s 8901.064 |walltime 19057.135 |
Transformer | epoch 1 | step 37587 |avg loss 8.437 |avg tokens 4834.400 |tokens/s 9049.897 |walltime 19062.476 |
Transformer | epoch 1 | step 37597 |avg loss 8.504 |avg tokens 4751.000 |tokens/s 9220.817 |walltime 19067.629 |
Transformer | epoch 1 | step 37607 |avg loss 8.626 |avg tokens 4349.000 |tokens/s 8530.767 |walltime 19072.727 |
Transformer | epoch 1 | step 37617 |avg loss 7.936 |avg tokens 4766.100 |tokens/s 9039.644 |walltime 19077.999 |
Transformer | epoch 1 | step 37627 |avg loss 8.603 |avg tokens 4307.900 |tokens/s 9069.491 |walltime 19082.749 |
Transformer | epoch 1 | step 37637 |avg loss 8.462 |avg tokens 4804.600 |tokens/s 9182.493 |walltime 19087.982 |
Transformer | epoch 1 | step 37647 |avg loss 8.494 |avg tokens 4820.800 |tokens/s 9231.663 |walltime 19093.204 |
Transformer | epoch 1 | step 37657 |avg loss 8.555 |avg tokens 4548.400 |tokens/s 9276.095 |walltime 19098.107 |
Transformer | epoch 1 | step 37667 |avg loss 8.704 |avg tokens 4394.100 |tokens/s 8975.198 |walltime 19103.003 |
Transformer | epoch 1 | step 37677 |avg loss 8.567 |avg tokens 4325.100 |tokens/s 8863.099 |walltime 19107.883 |
Transformer | epoch 1 | step 37687 |avg loss 8.388 |avg tokens 4687.500 |tokens/s 8876.987 |walltime 19113.163 |
Transformer | epoch 1 | step 37697 |avg loss 8.728 |avg tokens 4352.200 |tokens/s 9013.364 |walltime 19117.992 |
Transformer | epoch 1 | step 37707 |avg loss 8.449 |avg tokens 4685.100 |tokens/s 9324.512 |walltime 19123.016 |
Transformer | epoch 1 | step 37717 |avg loss 8.683 |avg tokens 4824.600 |tokens/s 9496.705 |walltime 19128.097 |
Transformer | epoch 1 | step 37727 |avg loss 8.457 |avg tokens 4656.900 |tokens/s 8895.863 |walltime 19133.332 |
Transformer | epoch 1 | step 37737 |avg loss 8.635 |avg tokens 3877.500 |tokens/s 8292.247 |walltime 19138.008 |
Transformer | epoch 1 | step 37747 |avg loss 8.676 |avg tokens 4616.500 |tokens/s 9577.319 |walltime 19142.828 |
Transformer | epoch 1 | step 37757 |avg loss 8.492 |avg tokens 4509.500 |tokens/s 8951.444 |walltime 19147.866 |
Transformer | epoch 1 | step 37767 |avg loss 8.691 |avg tokens 4523.400 |tokens/s 8986.720 |walltime 19152.899 |
Transformer | epoch 1 | step 37777 |avg loss 8.484 |avg tokens 4010.700 |tokens/s 8656.671 |walltime 19157.532 |
Transformer | epoch 1 | step 37787 |avg loss 8.461 |avg tokens 4305.600 |tokens/s 8486.228 |walltime 19162.606 |
Transformer | epoch 1 | step 37797 |avg loss 8.468 |avg tokens 4301.000 |tokens/s 8748.928 |walltime 19167.522 |
Transformer | epoch 1 | step 37807 |avg loss 8.634 |avg tokens 4062.500 |tokens/s 8282.443 |walltime 19172.427 |
Transformer | epoch 1 | step 37817 |avg loss 8.628 |avg tokens 4496.200 |tokens/s 8722.144 |walltime 19177.582 |
Transformer | epoch 1 | step 37827 |avg loss 8.351 |avg tokens 4843.200 |tokens/s 9242.280 |walltime 19182.822 |
Transformer | epoch 1 | step 37837 |avg loss 8.433 |avg tokens 4490.300 |tokens/s 8920.855 |walltime 19187.855 |
Transformer | epoch 1 | step 37847 |avg loss 8.708 |avg tokens 4730.300 |tokens/s 9148.292 |walltime 19193.026 |
Transformer | epoch 1 | step 37857 |avg loss 8.780 |avg tokens 4581.600 |tokens/s 9475.982 |walltime 19197.861 |
Transformer | epoch 1 | step 37867 |avg loss 8.657 |avg tokens 4387.300 |tokens/s 8942.679 |walltime 19202.767 |
Transformer | epoch 1 | step 37877 |avg loss 8.568 |avg tokens 4823.200 |tokens/s 9479.164 |walltime 19207.855 |
Transformer | epoch 1 | step 37887 |avg loss 8.919 |avg tokens 4391.500 |tokens/s 9557.496 |walltime 19212.450 |
Transformer | epoch 1 | step 37897 |avg loss 8.480 |avg tokens 4506.500 |tokens/s 8703.451 |walltime 19217.628 |
Transformer | epoch 1 | step 37907 |avg loss 8.741 |avg tokens 4169.800 |tokens/s 8997.803 |walltime 19222.262 |
Transformer | epoch 1 | step 37917 |avg loss 8.544 |avg tokens 4507.900 |tokens/s 9068.336 |walltime 19227.233 |
Transformer | epoch 1 | step 37927 |avg loss 8.886 |avg tokens 4090.800 |tokens/s 8621.973 |walltime 19231.978 |
Transformer | epoch 1 | step 37937 |avg loss 8.387 |avg tokens 4580.000 |tokens/s 8828.745 |walltime 19237.166 |
Transformer | epoch 1 | step 37947 |avg loss 8.592 |avg tokens 4801.600 |tokens/s 9576.910 |walltime 19242.179 |
Transformer | epoch 1 | step 37957 |avg loss 8.796 |avg tokens 4101.900 |tokens/s 8435.159 |walltime 19247.042 |
Transformer | epoch 1 | step 37967 |avg loss 8.508 |avg tokens 4757.100 |tokens/s 9315.901 |walltime 19252.149 |
Transformer | epoch 1 | step 37977 |avg loss 8.711 |avg tokens 4638.300 |tokens/s 8719.119 |walltime 19257.468 |
Transformer | epoch 1 | step 37987 |avg loss 8.395 |avg tokens 4582.400 |tokens/s 9018.375 |walltime 19262.549 |
Transformer | epoch 1 | step 37997 |avg loss 8.510 |avg tokens 4773.200 |tokens/s 9326.763 |walltime 19267.667 |
Transformer | epoch 1 | step 38007 |avg loss 8.572 |avg tokens 4000.400 |tokens/s 8345.261 |walltime 19272.461 |
Transformer | epoch 1 | step 38017 |avg loss 8.611 |avg tokens 4649.600 |tokens/s 9125.457 |walltime 19277.556 |
Transformer | epoch 1 | step 38027 |avg loss 8.420 |avg tokens 4525.000 |tokens/s 8737.040 |walltime 19282.735 |
Transformer | epoch 1 | step 38037 |avg loss 8.482 |avg tokens 4935.400 |tokens/s 9350.112 |walltime 19288.014 |
Transformer | epoch 1 | step 38047 |avg loss 8.619 |avg tokens 4331.400 |tokens/s 8915.200 |walltime 19292.872 |
Transformer | epoch 1 | step 38057 |avg loss 8.402 |avg tokens 4775.700 |tokens/s 9333.540 |walltime 19297.989 |
Transformer | epoch 1 | step 38067 |avg loss 8.627 |avg tokens 4698.700 |tokens/s 9560.463 |walltime 19302.903 |
Transformer | epoch 1 | step 38077 |avg loss 8.600 |avg tokens 4495.200 |tokens/s 8758.456 |walltime 19308.036 |
Transformer | epoch 1 | step 38087 |avg loss 8.604 |avg tokens 4698.000 |tokens/s 9178.540 |walltime 19313.154 |
Transformer | epoch 1 | step 38097 |avg loss 8.748 |avg tokens 4583.400 |tokens/s 9330.079 |walltime 19318.067 |
Transformer | epoch 1 | step 38107 |avg loss 8.536 |avg tokens 4826.400 |tokens/s 9213.958 |walltime 19323.305 |
Transformer | epoch 1 | step 38117 |avg loss 8.767 |avg tokens 3477.800 |tokens/s 7740.224 |walltime 19327.798 |
Transformer | epoch 1 | step 38127 |avg loss 8.487 |avg tokens 4727.300 |tokens/s 8998.429 |walltime 19333.052 |
Transformer | epoch 1 | step 38137 |avg loss 8.479 |avg tokens 4757.500 |tokens/s 9102.391 |walltime 19338.278 |
Transformer | epoch 1 | step 38147 |avg loss 8.649 |avg tokens 4773.800 |tokens/s 9374.400 |walltime 19343.371 |
Transformer | epoch 1 | step 38157 |avg loss 8.584 |avg tokens 4296.500 |tokens/s 8729.460 |walltime 19348.292 |
Transformer | epoch 1 | step 38167 |avg loss 8.670 |avg tokens 3946.800 |tokens/s 8037.073 |walltime 19353.203 |
Transformer | epoch 1 | step 38177 |avg loss 8.392 |avg tokens 4852.800 |tokens/s 9380.918 |walltime 19358.376 |
Transformer | epoch 1 | step 38187 |avg loss 8.555 |avg tokens 4452.500 |tokens/s 9034.861 |walltime 19363.304 |
Transformer | epoch 1 | step 38197 |avg loss 8.632 |avg tokens 4669.800 |tokens/s 9188.514 |walltime 19368.387 |
Transformer | epoch 1 | step 38207 |avg loss 8.817 |avg tokens 4422.000 |tokens/s 8856.076 |walltime 19373.380 |
Transformer | epoch 1 | step 38217 |avg loss 8.649 |avg tokens 4708.000 |tokens/s 9188.278 |walltime 19378.504 |
Transformer | epoch 1 | step 38227 |avg loss 8.537 |avg tokens 4551.900 |tokens/s 8962.158 |walltime 19383.583 |
Transformer | epoch 1 | step 38237 |avg loss 8.639 |avg tokens 4462.600 |tokens/s 8942.602 |walltime 19388.573 |
Transformer | epoch 1 | step 38247 |avg loss 8.460 |avg tokens 4506.800 |tokens/s 8905.310 |walltime 19393.634 |
Transformer | epoch 1 | step 38257 |avg loss 8.699 |avg tokens 3982.300 |tokens/s 8383.310 |walltime 19398.384 |
Transformer | epoch 1 | step 38267 |avg loss 8.575 |avg tokens 4611.700 |tokens/s 8930.610 |walltime 19403.548 |
Transformer | epoch 1 | step 38277 |avg loss 8.545 |avg tokens 4327.800 |tokens/s 8702.748 |walltime 19408.521 |
Transformer | epoch 1 | step 38287 |avg loss 8.667 |avg tokens 4790.500 |tokens/s 9418.800 |walltime 19413.607 |
Transformer | epoch 1 | step 38297 |avg loss 8.716 |avg tokens 4281.400 |tokens/s 8350.764 |walltime 19418.734 |
Transformer | epoch 1 | step 38307 |avg loss 8.733 |avg tokens 3814.000 |tokens/s 8529.918 |walltime 19423.205 |
Transformer | epoch 1 | step 38317 |avg loss 8.593 |avg tokens 4521.000 |tokens/s 8684.327 |walltime 19428.411 |
Transformer | epoch 1 | step 38327 |avg loss 8.584 |avg tokens 4325.500 |tokens/s 8754.106 |walltime 19433.352 |
Transformer | epoch 1 | step 38337 |avg loss 8.751 |avg tokens 4833.500 |tokens/s 9396.684 |walltime 19438.496 |
Transformer | epoch 1 | step 38347 |avg loss 8.370 |avg tokens 4582.500 |tokens/s 8991.259 |walltime 19443.593 |
Transformer | epoch 1 | step 38357 |avg loss 8.447 |avg tokens 4547.500 |tokens/s 8896.185 |walltime 19448.705 |
Transformer | epoch 1 | step 38367 |avg loss 8.749 |avg tokens 3618.300 |tokens/s 7813.939 |walltime 19453.335 |
Transformer | epoch 1 | step 38377 |avg loss 8.797 |avg tokens 4628.300 |tokens/s 9045.707 |walltime 19458.452 |
Transformer | epoch 1 | step 38387 |avg loss 8.710 |avg tokens 4186.000 |tokens/s 8540.577 |walltime 19463.353 |
Transformer | epoch 1 | step 38397 |avg loss 8.785 |avg tokens 4711.600 |tokens/s 9309.046 |walltime 19468.414 |
Transformer | epoch 1 | step 38407 |avg loss 8.834 |avg tokens 4538.900 |tokens/s 8831.854 |walltime 19473.554 |
Transformer | epoch 1 | step 38417 |avg loss 8.664 |avg tokens 4881.900 |tokens/s 9287.588 |walltime 19478.810 |
Transformer | epoch 1 | step 38427 |avg loss 8.672 |avg tokens 4584.300 |tokens/s 8777.904 |walltime 19484.033 |
Transformer | epoch 1 | step 38437 |avg loss 8.538 |avg tokens 4397.500 |tokens/s 8615.846 |walltime 19489.136 |
Transformer | epoch 1 | step 38447 |avg loss 8.804 |avg tokens 4351.600 |tokens/s 8797.684 |walltime 19494.083 |
Transformer | epoch 1 | step 38457 |avg loss 8.462 |avg tokens 4815.200 |tokens/s 9290.097 |walltime 19499.266 |
Transformer | epoch 1 | step 38467 |avg loss 8.909 |avg tokens 4418.500 |tokens/s 8972.152 |walltime 19504.191 |
Transformer | epoch 1 | step 38477 |avg loss 8.716 |avg tokens 4897.800 |tokens/s 9180.528 |walltime 19509.526 |
Transformer | epoch 1 | step 38487 |avg loss 8.946 |avg tokens 4166.600 |tokens/s 8791.702 |walltime 19514.265 |
Transformer | epoch 1 | step 38497 |avg loss 9.002 |avg tokens 4492.500 |tokens/s 8953.308 |walltime 19519.283 |
Transformer | epoch 1 | step 38507 |avg loss 8.705 |avg tokens 4435.500 |tokens/s 8578.836 |walltime 19524.453 |
Transformer | epoch 1 | step 38517 |avg loss 8.703 |avg tokens 4531.900 |tokens/s 8766.642 |walltime 19529.622 |
Transformer | epoch 1 | step 38527 |avg loss 8.619 |avg tokens 4812.700 |tokens/s 9222.253 |walltime 19534.841 |
Transformer | epoch 1 | step 38537 |avg loss 8.770 |avg tokens 4276.400 |tokens/s 8924.095 |walltime 19539.633 |
Transformer | epoch 1 | step 38547 |avg loss 8.647 |avg tokens 4538.300 |tokens/s 8981.280 |walltime 19544.686 |
Transformer | epoch 1 | step 38557 |avg loss 8.621 |avg tokens 4498.700 |tokens/s 9045.394 |walltime 19549.659 |
Transformer | epoch 1 | step 38567 |avg loss 8.543 |avg tokens 4434.400 |tokens/s 8749.855 |walltime 19554.727 |
Transformer | epoch 1 | step 38577 |avg loss 8.832 |avg tokens 4467.100 |tokens/s 9196.016 |walltime 19559.585 |
Transformer | epoch 1 | step 38587 |avg loss 8.700 |avg tokens 4222.100 |tokens/s 8608.495 |walltime 19564.490 |
Transformer | epoch 1 | step 38597 |avg loss 8.745 |avg tokens 4168.900 |tokens/s 8645.113 |walltime 19569.312 |
Transformer | epoch 1 | step 38607 |avg loss 8.465 |avg tokens 4795.900 |tokens/s 9072.966 |walltime 19574.598 |
Transformer | epoch 1 | step 38617 |avg loss 9.043 |avg tokens 4518.500 |tokens/s 9691.432 |walltime 19579.260 |
Transformer | epoch 1 | step 38627 |avg loss 8.840 |avg tokens 4137.000 |tokens/s 8495.052 |walltime 19584.130 |
Transformer | epoch 1 | step 38637 |avg loss 8.480 |avg tokens 4840.500 |tokens/s 9265.955 |walltime 19589.354 |
Transformer | epoch 1 | step 38647 |avg loss 8.783 |avg tokens 4299.300 |tokens/s 8740.101 |walltime 19594.273 |
Transformer | epoch 1 | step 38657 |avg loss 8.729 |avg tokens 4861.200 |tokens/s 9422.929 |walltime 19599.432 |
Transformer | epoch 1 | step 38667 |avg loss 8.531 |avg tokens 4634.200 |tokens/s 9046.457 |walltime 19604.555 |
Transformer | epoch 1 | step 38677 |avg loss 8.823 |avg tokens 3821.700 |tokens/s 8380.758 |walltime 19609.115 |
Transformer | epoch 1 | step 38687 |avg loss 8.860 |avg tokens 4086.800 |tokens/s 8639.603 |walltime 19613.845 |
Transformer | epoch 1 | step 38697 |avg loss 8.629 |avg tokens 4909.600 |tokens/s 9355.977 |walltime 19619.093 |
Transformer | epoch 1 | step 38707 |avg loss 8.733 |avg tokens 4109.400 |tokens/s 8443.638 |walltime 19623.960 |
Transformer | epoch 1 | step 38717 |avg loss 8.377 |avg tokens 4356.300 |tokens/s 8846.392 |walltime 19628.884 |
Transformer | epoch 1 | step 38727 |avg loss 8.587 |avg tokens 4265.700 |tokens/s 8724.697 |walltime 19633.773 |
Transformer | epoch 1 | step 38737 |avg loss 8.536 |avg tokens 4530.500 |tokens/s 8914.584 |walltime 19638.855 |
Transformer | epoch 1 | step 38747 |avg loss 8.330 |avg tokens 4786.300 |tokens/s 9093.563 |walltime 19644.119 |
Transformer | epoch 1 | step 38757 |avg loss 8.739 |avg tokens 4321.200 |tokens/s 8835.475 |walltime 19649.009 |
Transformer | epoch 1 | step 38767 |avg loss 8.629 |avg tokens 4858.800 |tokens/s 9212.370 |walltime 19654.284 |
Transformer | epoch 1 | step 38777 |avg loss 8.907 |avg tokens 4301.800 |tokens/s 8937.778 |walltime 19659.097 |
Transformer | epoch 1 | step 38787 |avg loss 8.890 |avg tokens 4136.700 |tokens/s 8812.955 |walltime 19663.791 |
Transformer | epoch 1 | step 38797 |avg loss 8.502 |avg tokens 4391.500 |tokens/s 8657.591 |walltime 19668.863 |
Transformer | epoch 1 | step 38807 |avg loss 8.566 |avg tokens 4781.100 |tokens/s 9178.184 |walltime 19674.072 |
Transformer | epoch 1 | step 38817 |avg loss 8.662 |avg tokens 3934.400 |tokens/s 8312.992 |walltime 19678.805 |
Transformer | epoch 1 | step 38827 |avg loss 8.433 |avg tokens 4452.200 |tokens/s 8730.551 |walltime 19683.905 |
Transformer | epoch 1 | step 38837 |avg loss 8.710 |avg tokens 4792.600 |tokens/s 9635.517 |walltime 19688.878 |
Transformer | epoch 1 | step 38847 |avg loss 8.519 |avg tokens 4703.000 |tokens/s 9157.740 |walltime 19694.014 |
Transformer | epoch 1 | step 38857 |avg loss 8.366 |avg tokens 4789.200 |tokens/s 9161.758 |walltime 19699.241 |
Transformer | epoch 1 | step 38867 |avg loss 8.630 |avg tokens 4534.300 |tokens/s 8803.891 |walltime 19704.392 |
Transformer | epoch 1 | step 38877 |avg loss 8.569 |avg tokens 4582.400 |tokens/s 9071.250 |walltime 19709.443 |
Transformer | epoch 1 | step 38887 |avg loss 8.634 |avg tokens 4815.200 |tokens/s 9593.314 |walltime 19714.463 |
Transformer | epoch 1 | step 38897 |avg loss 8.709 |avg tokens 4500.700 |tokens/s 8852.033 |walltime 19719.547 |
Transformer | epoch 1 | step 38907 |avg loss 8.369 |avg tokens 4605.600 |tokens/s 9072.995 |walltime 19724.623 |
Transformer | epoch 1 | step 38917 |avg loss 8.743 |avg tokens 4040.700 |tokens/s 8595.656 |walltime 19729.324 |
Transformer | epoch 1 | step 38927 |avg loss 8.543 |avg tokens 4505.600 |tokens/s 8920.585 |walltime 19734.375 |
Transformer | epoch 1 | step 38937 |avg loss 8.664 |avg tokens 4311.100 |tokens/s 8789.008 |walltime 19739.280 |
Transformer | epoch 1 | step 38947 |avg loss 8.698 |avg tokens 4614.000 |tokens/s 9407.936 |walltime 19744.184 |
Transformer | epoch 1 | step 38957 |avg loss 8.487 |avg tokens 3861.100 |tokens/s 8545.554 |walltime 19748.703 |
Transformer | epoch 1 | step 38967 |avg loss 8.451 |avg tokens 4708.800 |tokens/s 8963.213 |walltime 19753.956 |
Transformer | epoch 1 | step 38977 |avg loss 8.451 |avg tokens 4611.100 |tokens/s 8977.971 |walltime 19759.092 |
Transformer | epoch 1 | step 38987 |avg loss 8.524 |avg tokens 4614.200 |tokens/s 9051.951 |walltime 19764.190 |
Transformer | epoch 1 | step 38997 |avg loss 8.376 |avg tokens 4336.700 |tokens/s 8451.178 |walltime 19769.321 |
Transformer | epoch 1 | step 39007 |avg loss 8.368 |avg tokens 4556.800 |tokens/s 8814.483 |walltime 19774.491 |
Transformer | epoch 1 | step 39017 |avg loss 8.533 |avg tokens 4576.600 |tokens/s 9154.245 |walltime 19779.490 |
Transformer | epoch 1 | step 39027 |avg loss 8.830 |avg tokens 4097.600 |tokens/s 8762.788 |walltime 19784.166 |
Transformer | epoch 1 | step 39037 |avg loss 8.510 |avg tokens 4295.200 |tokens/s 8673.793 |walltime 19789.118 |
Transformer | epoch 1 | step 39047 |avg loss 8.363 |avg tokens 4461.200 |tokens/s 8635.821 |walltime 19794.284 |
Transformer | epoch 1 | step 39057 |avg loss 8.705 |avg tokens 4420.900 |tokens/s 8921.047 |walltime 19799.240 |
Transformer | epoch 1 | step 39067 |avg loss 8.628 |avg tokens 4455.000 |tokens/s 8947.374 |walltime 19804.219 |
Transformer | epoch 1 | step 39077 |avg loss 8.213 |avg tokens 4645.000 |tokens/s 8992.215 |walltime 19809.384 |
Transformer | epoch 1 | step 39087 |avg loss 8.603 |avg tokens 4606.300 |tokens/s 9017.202 |walltime 19814.493 |
Transformer | epoch 1 | step 39097 |avg loss 8.581 |avg tokens 4200.000 |tokens/s 8580.341 |walltime 19819.388 |
Transformer | epoch 1 | step 39107 |avg loss 8.757 |avg tokens 4807.600 |tokens/s 9526.357 |walltime 19824.434 |
Transformer | epoch 1 | step 39117 |avg loss 8.425 |avg tokens 4485.700 |tokens/s 8779.495 |walltime 19829.544 |
Transformer | epoch 1 | step 39127 |avg loss 8.503 |avg tokens 4388.700 |tokens/s 8701.707 |walltime 19834.587 |
Transformer | epoch 1 | step 39137 |avg loss 8.607 |avg tokens 4388.400 |tokens/s 9043.855 |walltime 19839.439 |
Transformer | epoch 1 | step 39147 |avg loss 8.514 |avg tokens 4291.300 |tokens/s 8831.947 |walltime 19844.298 |
Transformer | epoch 1 | step 39157 |avg loss 8.618 |avg tokens 4509.200 |tokens/s 9346.485 |walltime 19849.123 |
Transformer | epoch 1 | step 39167 |avg loss 8.495 |avg tokens 4685.500 |tokens/s 9095.738 |walltime 19854.274 |
Transformer | epoch 1 | step 39177 |avg loss 8.869 |avg tokens 3693.100 |tokens/s 8273.819 |walltime 19858.738 |
Transformer | epoch 1 | step 39187 |avg loss 8.654 |avg tokens 4546.700 |tokens/s 9127.899 |walltime 19863.719 |
Transformer | epoch 1 | step 39197 |avg loss 8.474 |avg tokens 4182.100 |tokens/s 8487.603 |walltime 19868.646 |
Transformer | epoch 1 | step 39207 |avg loss 8.469 |avg tokens 4688.200 |tokens/s 9224.573 |walltime 19873.728 |
Transformer | epoch 1 | step 39217 |avg loss 8.678 |avg tokens 3951.600 |tokens/s 8564.393 |walltime 19878.342 |
Transformer | epoch 1 | step 39227 |avg loss 8.620 |avg tokens 4792.300 |tokens/s 9165.606 |walltime 19883.571 |
Transformer | epoch 1 | step 39237 |avg loss 8.501 |avg tokens 4700.600 |tokens/s 9404.975 |walltime 19888.569 |
Transformer | epoch 1 | step 39247 |avg loss 8.212 |avg tokens 4769.700 |tokens/s 9117.512 |walltime 19893.800 |
Transformer | epoch 1 | step 39257 |avg loss 8.541 |avg tokens 4398.900 |tokens/s 8712.496 |walltime 19898.849 |
Transformer | epoch 1 | step 39267 |avg loss 8.531 |avg tokens 4410.300 |tokens/s 8360.357 |walltime 19904.125 |
Transformer | epoch 1 | step 39277 |avg loss 8.372 |avg tokens 4460.900 |tokens/s 8663.613 |walltime 19909.274 |
Transformer | epoch 1 | step 39287 |avg loss 8.262 |avg tokens 4929.000 |tokens/s 9648.043 |walltime 19914.382 |
Transformer | epoch 1 | step 39297 |avg loss 8.581 |avg tokens 3862.100 |tokens/s 8163.652 |walltime 19919.113 |
Transformer | epoch 1 | step 39307 |avg loss 8.369 |avg tokens 4232.500 |tokens/s 8534.197 |walltime 19924.073 |
Transformer | epoch 1 | step 39317 |avg loss 8.494 |avg tokens 4928.200 |tokens/s 9271.619 |walltime 19929.388 |
Transformer | epoch 1 | step 39327 |avg loss 8.434 |avg tokens 4654.900 |tokens/s 9190.558 |walltime 19934.453 |
Transformer | epoch 1 | step 39337 |avg loss 8.212 |avg tokens 4888.900 |tokens/s 9190.629 |walltime 19939.772 |
Transformer | epoch 1 | step 39347 |avg loss 8.415 |avg tokens 4911.400 |tokens/s 9501.341 |walltime 19944.942 |
Transformer | epoch 1 | step 39357 |avg loss 8.553 |avg tokens 4611.600 |tokens/s 9189.203 |walltime 19949.960 |
Transformer | epoch 1 | step 39367 |avg loss 8.546 |avg tokens 4427.800 |tokens/s 8637.656 |walltime 19955.086 |
Transformer | epoch 1 | step 39377 |avg loss 8.703 |avg tokens 4733.700 |tokens/s 9444.524 |walltime 19960.098 |
Transformer | epoch 1 | step 39387 |avg loss 8.658 |avg tokens 4328.600 |tokens/s 9340.198 |walltime 19964.733 |
Transformer | epoch 1 | step 39397 |avg loss 8.691 |avg tokens 4205.400 |tokens/s 8748.231 |walltime 19969.540 |
Transformer | epoch 1 | step 39407 |avg loss 8.525 |avg tokens 4426.100 |tokens/s 8718.926 |walltime 19974.616 |
Transformer | epoch 1 | step 39417 |avg loss 8.715 |avg tokens 4666.700 |tokens/s 9502.288 |walltime 19979.527 |
Transformer | epoch 1 | step 39427 |avg loss 8.552 |avg tokens 4759.700 |tokens/s 9725.754 |walltime 19984.421 |
Transformer | epoch 1 | step 39437 |avg loss 8.573 |avg tokens 4284.300 |tokens/s 8622.433 |walltime 19989.390 |
Transformer | epoch 1 | step 39447 |avg loss 8.531 |avg tokens 4871.200 |tokens/s 9273.653 |walltime 19994.643 |
Transformer | epoch 1 | step 39457 |avg loss 8.682 |avg tokens 4462.300 |tokens/s 8859.232 |walltime 19999.680 |
Transformer | epoch 1 | step 39467 |avg loss 8.615 |avg tokens 4576.600 |tokens/s 9208.962 |walltime 20004.649 |
Transformer | epoch 1 | step 39477 |avg loss 8.262 |avg tokens 4890.700 |tokens/s 9136.090 |walltime 20010.003 |
Transformer | epoch 1 | step 39487 |avg loss 8.601 |avg tokens 4534.300 |tokens/s 9324.178 |walltime 20014.866 |
Transformer | epoch 1 | step 39497 |avg loss 8.660 |avg tokens 4330.400 |tokens/s 8497.938 |walltime 20019.961 |
Transformer | epoch 1 | step 39507 |avg loss 8.766 |avg tokens 4579.600 |tokens/s 9292.977 |walltime 20024.889 |
Transformer | epoch 1 | step 39517 |avg loss 8.636 |avg tokens 4744.400 |tokens/s 9376.876 |walltime 20029.949 |
Transformer | epoch 1 | step 39527 |avg loss 8.550 |avg tokens 4387.700 |tokens/s 8774.099 |walltime 20034.950 |
Transformer | epoch 1 | step 39537 |avg loss 8.474 |avg tokens 4637.000 |tokens/s 9033.126 |walltime 20040.083 |
Transformer | epoch 1 | step 39547 |avg loss 8.589 |avg tokens 4706.100 |tokens/s 9119.490 |walltime 20045.244 |
Transformer | epoch 1 | step 39557 |avg loss 8.476 |avg tokens 4785.000 |tokens/s 9113.271 |walltime 20050.494 |
Transformer | epoch 1 | step 39567 |avg loss 8.516 |avg tokens 4380.500 |tokens/s 8772.381 |walltime 20055.488 |
Transformer | epoch 1 | step 39577 |avg loss 8.465 |avg tokens 4834.500 |tokens/s 9203.862 |walltime 20060.741 |
Transformer | epoch 1 | step 39587 |avg loss 8.312 |avg tokens 4753.000 |tokens/s 9125.640 |walltime 20065.949 |
Transformer | epoch 1 | step 39597 |avg loss 8.340 |avg tokens 4420.800 |tokens/s 8800.702 |walltime 20070.972 |
Transformer | epoch 1 | step 39607 |avg loss 8.672 |avg tokens 4611.200 |tokens/s 9298.885 |walltime 20075.931 |
Transformer | epoch 1 | step 39617 |avg loss 8.601 |avg tokens 4556.100 |tokens/s 8951.790 |walltime 20081.021 |
Transformer | epoch 1 | step 39627 |avg loss 8.551 |avg tokens 4537.900 |tokens/s 8885.854 |walltime 20086.128 |
Transformer | epoch 1 | step 39637 |avg loss 8.260 |avg tokens 4840.100 |tokens/s 9178.948 |walltime 20091.401 |
Transformer | epoch 1 | step 39647 |avg loss 8.649 |avg tokens 4703.000 |tokens/s 9457.010 |walltime 20096.374 |
Transformer | epoch 1 | step 39657 |avg loss 8.578 |avg tokens 4388.400 |tokens/s 8785.060 |walltime 20101.369 |
Transformer | epoch 1 | step 39667 |avg loss 8.504 |avg tokens 4700.500 |tokens/s 9292.946 |walltime 20106.427 |
Transformer | epoch 1 | step 39677 |avg loss 8.469 |avg tokens 4852.000 |tokens/s 9389.911 |walltime 20111.594 |
Transformer | epoch 1 | step 39687 |avg loss 8.534 |avg tokens 4597.100 |tokens/s 9018.680 |walltime 20116.692 |
Transformer | epoch 1 | step 39697 |avg loss 8.472 |avg tokens 4417.900 |tokens/s 9055.466 |walltime 20121.570 |
Transformer | epoch 1 | step 39707 |avg loss 8.619 |avg tokens 4518.300 |tokens/s 8836.142 |walltime 20126.684 |
Transformer | epoch 1 | step 39717 |avg loss 8.609 |avg tokens 4588.400 |tokens/s 9099.705 |walltime 20131.726 |
Transformer | epoch 1 | step 39727 |avg loss 8.544 |avg tokens 4148.700 |tokens/s 8566.125 |walltime 20136.569 |
Transformer | epoch 1 | step 39737 |avg loss 8.577 |avg tokens 4628.500 |tokens/s 8859.999 |walltime 20141.793 |
Transformer | epoch 1 | step 39747 |avg loss 8.507 |avg tokens 4655.800 |tokens/s 9121.540 |walltime 20146.897 |
Transformer | epoch 1 | step 39757 |avg loss 8.587 |avg tokens 4011.500 |tokens/s 8486.327 |walltime 20151.625 |
Transformer | epoch 1 | step 39767 |avg loss 8.790 |avg tokens 4580.300 |tokens/s 9661.355 |walltime 20156.365 |
Transformer | epoch 1 | step 39777 |avg loss 8.318 |avg tokens 4750.400 |tokens/s 9075.775 |walltime 20161.600 |
Transformer | epoch 1 | step 39787 |avg loss 8.444 |avg tokens 3676.100 |tokens/s 7921.192 |walltime 20166.240 |
Transformer | epoch 1 | step 39797 |avg loss 8.077 |avg tokens 4743.200 |tokens/s 9191.728 |walltime 20171.401 |
Transformer | epoch 1 | step 39807 |avg loss 8.612 |avg tokens 4807.800 |tokens/s 9567.204 |walltime 20176.426 |
Transformer | epoch 1 | step 39817 |avg loss 8.223 |avg tokens 4569.600 |tokens/s 8825.287 |walltime 20181.604 |
Transformer | epoch 1 | step 39827 |avg loss 8.459 |avg tokens 4637.000 |tokens/s 9189.096 |walltime 20186.650 |
Transformer | epoch 1 | step 39837 |avg loss 8.549 |avg tokens 4658.600 |tokens/s 9181.708 |walltime 20191.724 |
Transformer | epoch 1 | step 39847 |avg loss 8.044 |avg tokens 4699.100 |tokens/s 8950.794 |walltime 20196.974 |
Transformer | epoch 1 | step 39857 |avg loss 8.395 |avg tokens 4651.200 |tokens/s 8909.850 |walltime 20202.194 |
Transformer | epoch 1 | step 39867 |avg loss 8.476 |avg tokens 4552.200 |tokens/s 8907.258 |walltime 20207.305 |
Transformer | epoch 1 | step 39877 |avg loss 8.676 |avg tokens 4701.900 |tokens/s 9366.165 |walltime 20212.325 |
Transformer | epoch 1 | step 39887 |avg loss 8.545 |avg tokens 4519.400 |tokens/s 9020.115 |walltime 20217.335 |
Transformer | epoch 1 | step 39897 |avg loss 8.559 |avg tokens 4573.400 |tokens/s 9045.460 |walltime 20222.391 |
Transformer | epoch 1 | step 39907 |avg loss 8.425 |avg tokens 4351.200 |tokens/s 8816.010 |walltime 20227.327 |
Transformer | epoch 1 | step 39917 |avg loss 8.589 |avg tokens 4108.300 |tokens/s 8452.252 |walltime 20232.187 |
Transformer | epoch 1 | step 39927 |avg loss 8.473 |avg tokens 4513.300 |tokens/s 9130.896 |walltime 20237.130 |
Transformer | epoch 1 | step 39937 |avg loss 8.507 |avg tokens 4656.300 |tokens/s 9193.057 |walltime 20242.195 |
Transformer | epoch 1 | step 39947 |avg loss 8.397 |avg tokens 4028.500 |tokens/s 8285.778 |walltime 20247.057 |
Transformer | epoch 1 | step 39957 |avg loss 8.444 |avg tokens 4490.100 |tokens/s 8726.921 |walltime 20252.202 |
Transformer | epoch 1 | step 39967 |avg loss 8.447 |avg tokens 4611.100 |tokens/s 8878.102 |walltime 20257.396 |
Transformer | epoch 1 | step 39977 |avg loss 8.459 |avg tokens 4584.800 |tokens/s 8882.024 |walltime 20262.558 |
Transformer | epoch 1 | step 39987 |avg loss 8.573 |avg tokens 4325.400 |tokens/s 8524.861 |walltime 20267.632 |
Transformer | epoch 1 | step 39997 |avg loss 8.679 |avg tokens 4201.300 |tokens/s 8564.717 |walltime 20272.537 |
Transformer | epoch 1 | step 40007 |avg loss 8.361 |avg tokens 4652.700 |tokens/s 8828.307 |walltime 20277.807 |
Transformer | epoch 1 | step 40017 |avg loss 8.328 |avg tokens 4726.100 |tokens/s 9021.650 |walltime 20283.046 |
Transformer | epoch 1 | step 40027 |avg loss 8.460 |avg tokens 4605.800 |tokens/s 8750.234 |walltime 20288.310 |
Transformer | epoch 1 | step 40037 |avg loss 8.316 |avg tokens 4805.300 |tokens/s 9033.997 |walltime 20293.629 |
Transformer | epoch 1 | step 40047 |avg loss 8.433 |avg tokens 4871.700 |tokens/s 9214.255 |walltime 20298.916 |
Transformer | epoch 1 | step 40057 |avg loss 8.703 |avg tokens 4185.300 |tokens/s 8673.502 |walltime 20303.741 |
Transformer | epoch 1 | step 40067 |avg loss 8.842 |avg tokens 4535.500 |tokens/s 9634.215 |walltime 20308.449 |
Transformer | epoch 1 | step 40077 |avg loss 8.547 |avg tokens 4596.000 |tokens/s 8876.631 |walltime 20313.627 |
Transformer | epoch 1 | step 40087 |avg loss 8.217 |avg tokens 4457.300 |tokens/s 8653.021 |walltime 20318.778 |
Transformer | epoch 1 | step 40097 |avg loss 8.414 |avg tokens 4728.800 |tokens/s 9007.470 |walltime 20324.028 |
Transformer | epoch 1 | step 40107 |avg loss 8.578 |avg tokens 4421.700 |tokens/s 8967.526 |walltime 20328.959 |
Transformer | epoch 1 | step 40117 |avg loss 8.465 |avg tokens 4743.500 |tokens/s 9373.960 |walltime 20334.019 |
Transformer | epoch 1 | step 40127 |avg loss 8.288 |avg tokens 4741.200 |tokens/s 9017.647 |walltime 20339.276 |
Transformer | epoch 1 | step 40137 |avg loss 8.592 |avg tokens 4813.500 |tokens/s 9325.495 |walltime 20344.438 |
Transformer | epoch 1 | step 40147 |avg loss 8.340 |avg tokens 4825.000 |tokens/s 9014.271 |walltime 20349.791 |
Transformer | epoch 1 | step 40157 |avg loss 8.751 |avg tokens 4572.200 |tokens/s 9177.486 |walltime 20354.773 |
Transformer | epoch 1 | step 40167 |avg loss 8.504 |avg tokens 4540.500 |tokens/s 9265.472 |walltime 20359.673 |
Transformer | epoch 1 | step 40177 |avg loss 8.664 |avg tokens 4574.900 |tokens/s 9255.266 |walltime 20364.616 |
Transformer | epoch 1 | step 40187 |avg loss 8.425 |avg tokens 4431.000 |tokens/s 8575.981 |walltime 20369.783 |
Transformer | epoch 1 | step 40197 |avg loss 8.529 |avg tokens 4189.300 |tokens/s 8523.338 |walltime 20374.698 |
Transformer | epoch 1 | step 40207 |avg loss 8.326 |avg tokens 4857.000 |tokens/s 9023.525 |walltime 20380.081 |
Transformer | epoch 1 | step 40217 |avg loss 8.384 |avg tokens 4691.200 |tokens/s 9082.852 |walltime 20385.246 |
Transformer | epoch 1 | step 40227 |avg loss 8.616 |avg tokens 4418.800 |tokens/s 8859.905 |walltime 20390.233 |
Transformer | epoch 1 | step 40237 |avg loss 8.624 |avg tokens 4114.200 |tokens/s 8643.339 |walltime 20394.993 |
Transformer | epoch 1 | step 40247 |avg loss 8.495 |avg tokens 4564.300 |tokens/s 9005.648 |walltime 20400.061 |
Transformer | epoch 1 | step 40257 |avg loss 8.554 |avg tokens 4787.000 |tokens/s 9086.638 |walltime 20405.329 |
Transformer | epoch 1 | step 40267 |avg loss 8.573 |avg tokens 4410.600 |tokens/s 8599.573 |walltime 20410.458 |
Transformer | epoch 1 | step 40277 |avg loss 8.620 |avg tokens 4201.300 |tokens/s 8601.067 |walltime 20415.343 |
Transformer | epoch 1 | step 40287 |avg loss 8.724 |avg tokens 3920.000 |tokens/s 8530.734 |walltime 20419.938 |
Transformer | epoch 1 | step 40297 |avg loss 8.482 |avg tokens 4894.700 |tokens/s 9171.723 |walltime 20425.275 |
Transformer | epoch 1 | step 40307 |avg loss 8.590 |avg tokens 4508.000 |tokens/s 9060.991 |walltime 20430.250 |
Transformer | epoch 1 | step 40317 |avg loss 8.648 |avg tokens 4571.200 |tokens/s 9266.773 |walltime 20435.183 |
Transformer | epoch 1 | step 40327 |avg loss 8.421 |avg tokens 4493.300 |tokens/s 8592.923 |walltime 20440.412 |
Transformer | epoch 1 | step 40337 |avg loss 8.646 |avg tokens 4174.600 |tokens/s 8661.589 |walltime 20445.232 |
Transformer | epoch 1 | step 40347 |avg loss 8.579 |avg tokens 4848.900 |tokens/s 9427.587 |walltime 20450.375 |
Transformer | epoch 1 | step 40357 |avg loss 8.529 |avg tokens 4694.900 |tokens/s 9110.764 |walltime 20455.528 |
Transformer | epoch 1 | step 40367 |avg loss 8.791 |avg tokens 4715.000 |tokens/s 9259.678 |walltime 20460.620 |
Transformer | epoch 1 | step 40377 |avg loss 8.435 |avg tokens 4686.300 |tokens/s 9019.673 |walltime 20465.816 |
Transformer | epoch 1 | step 40387 |avg loss 8.482 |avg tokens 4700.100 |tokens/s 9105.291 |walltime 20470.978 |
Transformer | epoch 1 | step 40397 |avg loss 8.633 |avg tokens 4714.600 |tokens/s 9214.169 |walltime 20476.094 |
Transformer | epoch 1 | step 40407 |avg loss 8.585 |avg tokens 4320.100 |tokens/s 8509.546 |walltime 20481.171 |
Transformer | epoch 1 | step 40417 |avg loss 8.632 |avg tokens 4800.600 |tokens/s 9338.894 |walltime 20486.312 |
Transformer | epoch 1 | step 40427 |avg loss 8.622 |avg tokens 4511.000 |tokens/s 9296.014 |walltime 20491.164 |
Transformer | epoch 1 | step 40437 |avg loss 8.391 |avg tokens 4635.600 |tokens/s 9224.089 |walltime 20496.190 |
Transformer | epoch 1 | step 40447 |avg loss 8.560 |avg tokens 4668.100 |tokens/s 9367.660 |walltime 20501.173 |
Transformer | epoch 1 | step 40457 |avg loss 8.475 |avg tokens 4566.700 |tokens/s 8807.376 |walltime 20506.358 |
Transformer | epoch 1 | step 40467 |avg loss 8.704 |avg tokens 4698.900 |tokens/s 9283.354 |walltime 20511.420 |
Transformer | epoch 1 | step 40477 |avg loss 8.542 |avg tokens 4594.800 |tokens/s 9113.871 |walltime 20516.461 |
Transformer | epoch 1 | step 40487 |avg loss 8.425 |avg tokens 4948.100 |tokens/s 9140.021 |walltime 20521.875 |
Transformer | epoch 1 | step 40497 |avg loss 8.564 |avg tokens 4697.000 |tokens/s 9176.881 |walltime 20526.993 |
Transformer | epoch 1 | step 40507 |avg loss 8.484 |avg tokens 4626.300 |tokens/s 9067.628 |walltime 20532.095 |
Transformer | epoch 1 | step 40517 |avg loss 8.410 |avg tokens 4860.300 |tokens/s 9455.432 |walltime 20537.235 |
Transformer | epoch 1 | step 40527 |avg loss 8.666 |avg tokens 4048.100 |tokens/s 8534.768 |walltime 20541.978 |
Transformer | epoch 1 | step 40537 |avg loss 8.639 |avg tokens 4752.800 |tokens/s 9439.205 |walltime 20547.014 |
Transformer | epoch 1 | step 40547 |avg loss 8.567 |avg tokens 4732.000 |tokens/s 9011.118 |walltime 20552.265 |
Transformer | epoch 1 | step 40557 |avg loss 8.573 |avg tokens 4480.200 |tokens/s 8849.876 |walltime 20557.327 |
Transformer | epoch 1 | step 40567 |avg loss 8.316 |avg tokens 4655.000 |tokens/s 8953.414 |walltime 20562.527 |
Transformer | epoch 1 | step 40577 |avg loss 8.676 |avg tokens 4342.700 |tokens/s 9079.977 |walltime 20567.309 |
Transformer | epoch 1 | step 40587 |avg loss 8.558 |avg tokens 4600.400 |tokens/s 9162.729 |walltime 20572.330 |
Transformer | epoch 1 | step 40597 |avg loss 8.433 |avg tokens 4690.100 |tokens/s 8949.996 |walltime 20577.570 |
Transformer | epoch 1 | step 40607 |avg loss 8.445 |avg tokens 4879.700 |tokens/s 9431.466 |walltime 20582.744 |
Transformer | epoch 1 | step 40617 |avg loss 8.178 |avg tokens 4616.300 |tokens/s 8908.553 |walltime 20587.926 |
Transformer | epoch 1 | step 40627 |avg loss 8.489 |avg tokens 4755.500 |tokens/s 9159.258 |walltime 20593.118 |
Transformer | epoch 1 | step 40637 |avg loss 8.383 |avg tokens 4679.200 |tokens/s 8993.072 |walltime 20598.321 |
Transformer | epoch 1 | step 40647 |avg loss 8.546 |avg tokens 4286.200 |tokens/s 8486.663 |walltime 20603.372 |
Transformer | epoch 1 | step 40657 |avg loss 8.445 |avg tokens 4409.900 |tokens/s 8981.030 |walltime 20608.282 |
Transformer | epoch 1 | step 40667 |avg loss 8.469 |avg tokens 4619.200 |tokens/s 9169.015 |walltime 20613.320 |
Transformer | epoch 1 | step 40677 |avg loss 8.409 |avg tokens 4900.800 |tokens/s 9174.038 |walltime 20618.662 |
Transformer | epoch 1 | step 40687 |avg loss 8.558 |avg tokens 4501.100 |tokens/s 9072.258 |walltime 20623.623 |
Transformer | epoch 1 | step 40697 |avg loss 8.545 |avg tokens 4650.200 |tokens/s 8969.390 |walltime 20628.808 |
Transformer | epoch 1 | step 40707 |avg loss 8.487 |avg tokens 4549.800 |tokens/s 8862.849 |walltime 20633.941 |
Transformer | epoch 1 | step 40717 |avg loss 8.374 |avg tokens 4598.200 |tokens/s 8915.904 |walltime 20639.099 |
Transformer | epoch 1 | step 40727 |avg loss 8.562 |avg tokens 4735.300 |tokens/s 9570.235 |walltime 20644.047 |
Transformer | epoch 1 | step 40737 |avg loss 8.605 |avg tokens 4654.200 |tokens/s 9249.048 |walltime 20649.079 |
Transformer | epoch 1 | step 40747 |avg loss 8.449 |avg tokens 4942.400 |tokens/s 9502.786 |walltime 20654.280 |
Transformer | epoch 1 | step 40757 |avg loss 8.515 |avg tokens 4496.400 |tokens/s 8908.130 |walltime 20659.327 |
Transformer | epoch 1 | step 40767 |avg loss 8.585 |avg tokens 3966.400 |tokens/s 7972.131 |walltime 20664.303 |
Transformer | epoch 1 | step 40777 |avg loss 8.497 |avg tokens 4555.500 |tokens/s 9142.342 |walltime 20669.285 |
Transformer | epoch 1 | step 40787 |avg loss 8.524 |avg tokens 4579.400 |tokens/s 9102.961 |walltime 20674.316 |
Transformer | epoch 1 | step 40797 |avg loss 8.597 |avg tokens 4641.900 |tokens/s 9190.905 |walltime 20679.367 |
Transformer | epoch 1 | step 40807 |avg loss 8.356 |avg tokens 4505.000 |tokens/s 8877.307 |walltime 20684.441 |
Transformer | epoch 1 | step 40817 |avg loss 8.336 |avg tokens 4765.600 |tokens/s 9161.062 |walltime 20689.643 |
Transformer | epoch 1 | step 40827 |avg loss 8.413 |avg tokens 4133.500 |tokens/s 8340.529 |walltime 20694.599 |
Transformer | epoch 1 | step 40837 |avg loss 8.384 |avg tokens 4838.700 |tokens/s 9436.855 |walltime 20699.727 |
Transformer | epoch 1 | step 40847 |avg loss 8.505 |avg tokens 4206.000 |tokens/s 8578.590 |walltime 20704.630 |
Transformer | epoch 1 | step 40857 |avg loss 8.607 |avg tokens 4321.500 |tokens/s 8762.294 |walltime 20709.562 |
Transformer | epoch 1 | step 40867 |avg loss 8.528 |avg tokens 4467.300 |tokens/s 8847.561 |walltime 20714.611 |
Transformer | epoch 1 | step 40877 |avg loss 8.562 |avg tokens 4127.700 |tokens/s 8540.989 |walltime 20719.444 |
Transformer | epoch 1 | step 40887 |avg loss 8.309 |avg tokens 4635.300 |tokens/s 9098.224 |walltime 20724.538 |
Transformer | epoch 1 | step 40897 |avg loss 8.160 |avg tokens 4739.200 |tokens/s 8979.761 |walltime 20729.816 |
Transformer | epoch 1 | step 40907 |avg loss 8.351 |avg tokens 4766.400 |tokens/s 9063.022 |walltime 20735.075 |
Transformer | epoch 1 | step 40917 |avg loss 8.573 |avg tokens 4519.700 |tokens/s 9165.748 |walltime 20740.006 |
Transformer | epoch 1 | step 40927 |avg loss 8.476 |avg tokens 4535.500 |tokens/s 8889.240 |walltime 20745.108 |
Transformer | epoch 1 | step 40937 |avg loss 8.779 |avg tokens 4666.300 |tokens/s 9346.125 |walltime 20750.101 |
Transformer | epoch 1 | step 40947 |avg loss 8.532 |avg tokens 4455.900 |tokens/s 8679.166 |walltime 20755.235 |
Transformer | epoch 1 | step 40957 |avg loss 8.261 |avg tokens 4682.400 |tokens/s 8897.548 |walltime 20760.498 |
Transformer | epoch 1 | step 40967 |avg loss 8.414 |avg tokens 4704.000 |tokens/s 8985.959 |walltime 20765.733 |
Transformer | epoch 1 | step 40977 |avg loss 8.642 |avg tokens 4114.300 |tokens/s 8268.490 |walltime 20770.709 |
Transformer | epoch 1 | step 40987 |avg loss 8.578 |avg tokens 4402.600 |tokens/s 8908.092 |walltime 20775.651 |
Transformer | epoch 1 | step 40997 |avg loss 8.603 |avg tokens 4412.600 |tokens/s 8866.638 |walltime 20780.627 |
Transformer | epoch 1 | step 41007 |avg loss 8.362 |avg tokens 4695.400 |tokens/s 8916.478 |walltime 20785.893 |
Transformer | epoch 1 | step 41017 |avg loss 8.487 |avg tokens 4539.700 |tokens/s 8664.871 |walltime 20791.133 |
Transformer | epoch 1 | step 41027 |avg loss 8.508 |avg tokens 4898.500 |tokens/s 9227.079 |walltime 20796.441 |
Transformer | epoch 1 | step 41037 |avg loss 8.522 |avg tokens 4666.400 |tokens/s 9016.631 |walltime 20801.617 |
Transformer | epoch 1 | step 41047 |avg loss 8.388 |avg tokens 4547.700 |tokens/s 8884.134 |walltime 20806.736 |
Transformer | epoch 1 | step 41057 |avg loss 8.682 |avg tokens 4009.000 |tokens/s 8301.049 |walltime 20811.565 |
Transformer | epoch 1 | step 41067 |avg loss 8.524 |avg tokens 4569.400 |tokens/s 8957.424 |walltime 20816.666 |
Transformer | epoch 1 | step 41077 |avg loss 8.415 |avg tokens 3861.500 |tokens/s 8230.713 |walltime 20821.358 |
Transformer | epoch 1 | step 41087 |avg loss 8.365 |avg tokens 4974.000 |tokens/s 9242.294 |walltime 20826.740 |
Transformer | epoch 1 | step 41097 |avg loss 8.818 |avg tokens 4564.300 |tokens/s 9556.908 |walltime 20831.516 |
Transformer | epoch 1 | step 41107 |avg loss 8.396 |avg tokens 4602.700 |tokens/s 9161.106 |walltime 20836.540 |
Transformer | epoch 1 | step 41117 |avg loss 8.430 |avg tokens 4816.900 |tokens/s 9160.740 |walltime 20841.798 |
Transformer | epoch 1 | step 41127 |avg loss 8.655 |avg tokens 4651.300 |tokens/s 9297.019 |walltime 20846.801 |
Transformer | epoch 1 | step 41137 |avg loss 8.479 |avg tokens 4665.400 |tokens/s 9098.886 |walltime 20851.929 |
Transformer | epoch 1 | step 41147 |avg loss 8.395 |avg tokens 4739.100 |tokens/s 9321.093 |walltime 20857.013 |
Transformer | epoch 1 | step 41157 |avg loss 8.356 |avg tokens 4875.000 |tokens/s 9124.365 |walltime 20862.356 |
Transformer | epoch 1 | step 41167 |avg loss 8.210 |avg tokens 4747.200 |tokens/s 8986.145 |walltime 20867.638 |
Transformer | epoch 1 | step 41177 |avg loss 8.351 |avg tokens 4854.400 |tokens/s 9288.828 |walltime 20872.865 |
Transformer | epoch 1 | step 41187 |avg loss 8.198 |avg tokens 4739.300 |tokens/s 9027.767 |walltime 20878.114 |
Transformer | epoch 1 | step 41197 |avg loss 8.697 |avg tokens 4223.200 |tokens/s 8439.849 |walltime 20883.118 |
Transformer | epoch 1 | step 41207 |avg loss 8.301 |avg tokens 5000.000 |tokens/s 9424.543 |walltime 20888.423 |
Transformer | epoch 1 | step 41217 |avg loss 8.458 |avg tokens 4524.500 |tokens/s 8907.132 |walltime 20893.503 |
Transformer | epoch 1 | step 41227 |avg loss 8.142 |avg tokens 4725.800 |tokens/s 9135.684 |walltime 20898.676 |
Transformer | epoch 1 | step 41237 |avg loss 8.304 |avg tokens 4742.600 |tokens/s 8929.677 |walltime 20903.987 |
Transformer | epoch 1 | step 41247 |avg loss 8.526 |avg tokens 4899.100 |tokens/s 9466.527 |walltime 20909.162 |
Transformer | epoch 1 | step 41257 |avg loss 8.386 |avg tokens 4084.800 |tokens/s 8312.495 |walltime 20914.076 |
Transformer | epoch 1 | step 41267 |avg loss 8.557 |avg tokens 4282.200 |tokens/s 8701.992 |walltime 20918.997 |
Transformer | epoch 1 | step 41277 |avg loss 8.566 |avg tokens 4266.800 |tokens/s 8914.194 |walltime 20923.784 |
Transformer | epoch 1 | step 41287 |avg loss 8.378 |avg tokens 4396.600 |tokens/s 8614.090 |walltime 20928.888 |
Transformer | epoch 1 | step 41297 |avg loss 8.454 |avg tokens 4369.700 |tokens/s 8831.954 |walltime 20933.835 |
Transformer | epoch 1 | step 41307 |avg loss 8.627 |avg tokens 4022.800 |tokens/s 8494.269 |walltime 20938.571 |
Transformer | epoch 1 | step 41317 |avg loss 8.370 |avg tokens 4519.400 |tokens/s 8732.745 |walltime 20943.746 |
Transformer | epoch 1 | step 41327 |avg loss 8.907 |avg tokens 4222.000 |tokens/s 8753.298 |walltime 20948.570 |
Transformer | epoch 1 | step 41337 |avg loss 8.478 |avg tokens 4667.600 |tokens/s 9327.797 |walltime 20953.574 |
Transformer | epoch 1 | step 41347 |avg loss 8.506 |avg tokens 4465.800 |tokens/s 9013.678 |walltime 20958.528 |
Transformer | epoch 1 | step 41357 |avg loss 8.021 |avg tokens 4717.900 |tokens/s 9345.879 |walltime 20963.576 |
Transformer | epoch 1 | step 41367 |avg loss 8.457 |avg tokens 4135.200 |tokens/s 8446.928 |walltime 20968.472 |
Transformer | epoch 1 | step 41377 |avg loss 8.442 |avg tokens 4563.900 |tokens/s 8889.215 |walltime 20973.606 |
Transformer | epoch 1 | step 41387 |avg loss 8.325 |avg tokens 4908.000 |tokens/s 9279.666 |walltime 20978.895 |
Transformer | epoch 1 | step 41397 |avg loss 8.353 |avg tokens 4582.000 |tokens/s 9028.171 |walltime 20983.970 |
Transformer | epoch 1 | step 41407 |avg loss 8.389 |avg tokens 4477.500 |tokens/s 8797.149 |walltime 20989.060 |
Transformer | epoch 1 | step 41417 |avg loss 8.346 |avg tokens 4522.800 |tokens/s 9011.919 |walltime 20994.079 |
Transformer | epoch 1 | step 41427 |avg loss 8.528 |avg tokens 4392.900 |tokens/s 9112.420 |walltime 20998.899 |
Transformer | epoch 1 | step 41437 |avg loss 8.437 |avg tokens 4783.800 |tokens/s 9339.745 |walltime 21004.021 |
Transformer | epoch 1 | step 41447 |avg loss 8.593 |avg tokens 4012.500 |tokens/s 8056.640 |walltime 21009.002 |
Transformer | epoch 1 | step 41457 |avg loss 8.669 |avg tokens 4089.200 |tokens/s 8817.445 |walltime 21013.639 |
Transformer | epoch 1 | step 41467 |avg loss 8.557 |avg tokens 4744.400 |tokens/s 9263.518 |walltime 21018.761 |
Transformer | epoch 1 | step 41477 |avg loss 8.584 |avg tokens 4551.700 |tokens/s 8957.075 |walltime 21023.843 |
Transformer | epoch 1 | step 41487 |avg loss 8.421 |avg tokens 4654.400 |tokens/s 9218.729 |walltime 21028.892 |
Transformer | epoch 1 | step 41497 |avg loss 8.780 |avg tokens 4335.800 |tokens/s 9003.924 |walltime 21033.707 |
Transformer | epoch 1 | step 41507 |avg loss 8.774 |avg tokens 4497.200 |tokens/s 9351.821 |walltime 21038.516 |
Transformer | epoch 1 | step 41517 |avg loss 8.506 |avg tokens 4617.100 |tokens/s 9052.271 |walltime 21043.616 |
Transformer | epoch 1 | step 41527 |avg loss 8.460 |avg tokens 4698.300 |tokens/s 9216.033 |walltime 21048.714 |
Transformer | epoch 1 | step 41537 |avg loss 8.325 |avg tokens 4562.000 |tokens/s 8758.630 |walltime 21053.923 |
Transformer | epoch 1 | step 41547 |avg loss 8.469 |avg tokens 4947.000 |tokens/s 9388.251 |walltime 21059.192 |
Transformer | epoch 1 | step 41557 |avg loss 8.428 |avg tokens 4039.600 |tokens/s 8467.266 |walltime 21063.963 |
Transformer | epoch 1 | step 41567 |avg loss 8.688 |avg tokens 4467.000 |tokens/s 8917.082 |walltime 21068.973 |
Transformer | epoch 1 | step 41577 |avg loss 8.419 |avg tokens 4818.100 |tokens/s 9169.105 |walltime 21074.227 |
Transformer | epoch 1 | step 41587 |avg loss 8.751 |avg tokens 4141.000 |tokens/s 8482.059 |walltime 21079.109 |
Transformer | epoch 1 | step 41597 |avg loss 8.453 |avg tokens 4577.100 |tokens/s 9253.624 |walltime 21084.056 |
Transformer | epoch 1 | step 41607 |avg loss 8.238 |avg tokens 4928.800 |tokens/s 9248.189 |walltime 21089.385 |
Transformer | epoch 1 | step 41617 |avg loss 8.528 |avg tokens 4587.900 |tokens/s 9025.893 |walltime 21094.468 |
Transformer | epoch 1 | step 41627 |avg loss 8.380 |avg tokens 4848.400 |tokens/s 9228.267 |walltime 21099.722 |
Transformer | epoch 1 | step 41637 |avg loss 8.470 |avg tokens 4820.100 |tokens/s 9145.734 |walltime 21104.992 |
Transformer | epoch 1 | step 41647 |avg loss 8.461 |avg tokens 4644.000 |tokens/s 9225.281 |walltime 21110.026 |
Transformer | epoch 1 | step 41657 |avg loss 8.638 |avg tokens 4599.400 |tokens/s 9254.252 |walltime 21114.997 |
Transformer | epoch 1 | step 41667 |avg loss 8.434 |avg tokens 4071.800 |tokens/s 8354.459 |walltime 21119.870 |
Transformer | epoch 1 | step 41677 |avg loss 8.055 |avg tokens 4465.400 |tokens/s 8819.150 |walltime 21124.934 |
Transformer | epoch 1 | step 41687 |avg loss 8.691 |avg tokens 4274.100 |tokens/s 8689.466 |walltime 21129.852 |
Transformer | epoch 1 | step 41697 |avg loss 8.449 |avg tokens 4460.400 |tokens/s 8764.298 |walltime 21134.942 |
Transformer | epoch 1 | step 41707 |avg loss 8.356 |avg tokens 4610.200 |tokens/s 8935.092 |walltime 21140.101 |
Transformer | epoch 1 | step 41717 |avg loss 8.583 |avg tokens 4624.500 |tokens/s 9479.975 |walltime 21144.979 |
Transformer | epoch 1 | step 41727 |avg loss 8.362 |avg tokens 4540.100 |tokens/s 8833.259 |walltime 21150.119 |
Transformer | epoch 1 | step 41737 |avg loss 8.432 |avg tokens 4613.000 |tokens/s 9031.533 |walltime 21155.227 |
Transformer | epoch 1 | step 41747 |avg loss 8.353 |avg tokens 4714.600 |tokens/s 9381.912 |walltime 21160.252 |
Transformer | epoch 1 | step 41757 |avg loss 8.569 |avg tokens 4557.400 |tokens/s 9077.528 |walltime 21165.273 |
Transformer | epoch 1 | step 41767 |avg loss 8.455 |avg tokens 4764.100 |tokens/s 9358.366 |walltime 21170.363 |
Transformer | epoch 1 | step 41777 |avg loss 8.295 |avg tokens 4691.900 |tokens/s 8921.599 |walltime 21175.622 |
Transformer | epoch 1 | step 41787 |avg loss 8.491 |avg tokens 4677.200 |tokens/s 9518.932 |walltime 21180.536 |
Transformer | epoch 1 | step 41797 |avg loss 8.656 |avg tokens 4838.200 |tokens/s 9697.344 |walltime 21185.525 |
Transformer | epoch 1 | step 41807 |avg loss 8.556 |avg tokens 4593.700 |tokens/s 9096.596 |walltime 21190.575 |
Transformer | epoch 1 | step 41817 |avg loss 8.549 |avg tokens 4419.000 |tokens/s 9006.124 |walltime 21195.482 |
Transformer | epoch 1 | step 41827 |avg loss 8.689 |avg tokens 4526.200 |tokens/s 9223.579 |walltime 21200.389 |
Transformer | epoch 1 | step 41837 |avg loss 8.589 |avg tokens 4079.800 |tokens/s 8380.751 |walltime 21205.257 |
Transformer | epoch 1 | step 41847 |avg loss 8.516 |avg tokens 4526.600 |tokens/s 8754.239 |walltime 21210.428 |
Transformer | epoch 1 | step 41857 |avg loss 8.527 |avg tokens 4746.300 |tokens/s 9195.894 |walltime 21215.589 |
Transformer | epoch 1 | step 41867 |avg loss 8.678 |avg tokens 4254.400 |tokens/s 8790.338 |walltime 21220.429 |
Transformer | epoch 1 | step 41877 |avg loss 8.532 |avg tokens 4530.000 |tokens/s 9063.559 |walltime 21225.427 |
Transformer | epoch 1 | step 41887 |avg loss 8.358 |avg tokens 4531.800 |tokens/s 8751.797 |walltime 21230.605 |
Transformer | epoch 1 | step 41897 |avg loss 8.402 |avg tokens 4326.900 |tokens/s 8861.845 |walltime 21235.488 |
Transformer | epoch 1 | step 41907 |avg loss 8.588 |avg tokens 4517.400 |tokens/s 9012.033 |walltime 21240.500 |
Transformer | epoch 1 | step 41917 |avg loss 8.719 |avg tokens 4373.500 |tokens/s 8881.261 |walltime 21245.425 |
Transformer | epoch 1 | step 41927 |avg loss 8.489 |avg tokens 4329.400 |tokens/s 8603.965 |walltime 21250.457 |
Transformer | epoch 1 | step 41937 |avg loss 8.576 |avg tokens 4483.300 |tokens/s 9235.732 |walltime 21255.311 |
Transformer | epoch 1 | step 41947 |avg loss 8.239 |avg tokens 4763.200 |tokens/s 8975.096 |walltime 21260.618 |
Transformer | epoch 1 | step 41957 |avg loss 8.459 |avg tokens 4123.400 |tokens/s 8370.637 |walltime 21265.544 |
Transformer | epoch 1 | step 41967 |avg loss 8.632 |avg tokens 4176.600 |tokens/s 8416.040 |walltime 21270.507 |
Transformer | epoch 1 | step 41977 |avg loss 8.488 |avg tokens 4470.900 |tokens/s 8861.873 |walltime 21275.552 |
Transformer | epoch 1 | step 41987 |avg loss 8.441 |avg tokens 4325.200 |tokens/s 8550.042 |walltime 21280.611 |
Transformer | epoch 1 | step 41997 |avg loss 8.516 |avg tokens 4698.300 |tokens/s 9376.300 |walltime 21285.621 |
Transformer | epoch 1 | step 42007 |avg loss 8.409 |avg tokens 4507.200 |tokens/s 8941.114 |walltime 21290.662 |
Transformer | epoch 1 | step 42017 |avg loss 8.398 |avg tokens 4564.500 |tokens/s 8873.310 |walltime 21295.807 |
Transformer | epoch 1 | step 42027 |avg loss 8.564 |avg tokens 4405.900 |tokens/s 8719.391 |walltime 21300.860 |
Transformer | epoch 1 | step 42037 |avg loss 8.555 |avg tokens 4284.400 |tokens/s 8905.905 |walltime 21305.670 |
Transformer | epoch 1 | step 42047 |avg loss 8.599 |avg tokens 4640.600 |tokens/s 9208.862 |walltime 21310.710 |
Transformer | epoch 1 | step 42057 |avg loss 8.324 |avg tokens 4154.700 |tokens/s 8553.176 |walltime 21315.567 |
Transformer | epoch 1 | step 42067 |avg loss 8.309 |avg tokens 4599.200 |tokens/s 8840.863 |walltime 21320.769 |
Transformer | epoch 1 | step 42077 |avg loss 8.523 |avg tokens 4042.800 |tokens/s 8289.003 |walltime 21325.647 |
Transformer | epoch 1 | step 42087 |avg loss 8.631 |avg tokens 4307.800 |tokens/s 8591.620 |walltime 21330.661 |
Transformer | epoch 1 | step 42097 |avg loss 8.561 |avg tokens 4915.400 |tokens/s 9774.063 |walltime 21335.690 |
Transformer | epoch 1 | step 42107 |avg loss 8.510 |avg tokens 4561.300 |tokens/s 9132.677 |walltime 21340.684 |
Transformer | epoch 1 | step 42117 |avg loss 8.571 |avg tokens 4419.900 |tokens/s 8963.496 |walltime 21345.615 |
Transformer | epoch 1 | step 42127 |avg loss 8.656 |avg tokens 3761.300 |tokens/s 8367.976 |walltime 21350.110 |
Transformer | epoch 1 | step 42137 |avg loss 8.608 |avg tokens 4624.200 |tokens/s 9601.912 |walltime 21354.926 |
Transformer | epoch 1 | step 42147 |avg loss 8.608 |avg tokens 4947.100 |tokens/s 9701.068 |walltime 21360.025 |
Transformer | epoch 1 | step 42157 |avg loss 8.300 |avg tokens 4856.800 |tokens/s 9060.359 |walltime 21365.386 |
Transformer | epoch 1 | step 42167 |avg loss 8.433 |avg tokens 3973.200 |tokens/s 8729.047 |walltime 21369.938 |
Transformer | epoch 1 | step 42177 |avg loss 8.423 |avg tokens 4501.800 |tokens/s 8982.269 |walltime 21374.949 |
Transformer | epoch 1 | step 42187 |avg loss 8.673 |avg tokens 4235.500 |tokens/s 9025.778 |walltime 21379.642 |
Transformer | epoch 1 | step 42197 |avg loss 8.338 |avg tokens 4622.000 |tokens/s 8985.371 |walltime 21384.786 |
Transformer | epoch 1 | step 42207 |avg loss 8.302 |avg tokens 4688.800 |tokens/s 9066.173 |walltime 21389.958 |
Transformer | epoch 1 | step 42217 |avg loss 8.348 |avg tokens 4509.800 |tokens/s 9317.596 |walltime 21394.798 |
Transformer | epoch 1 | step 42227 |avg loss 8.338 |avg tokens 4580.800 |tokens/s 8988.683 |walltime 21399.894 |
Transformer | epoch 1 | step 42237 |avg loss 8.280 |avg tokens 4645.800 |tokens/s 9199.369 |walltime 21404.944 |
Transformer | epoch 1 | step 42247 |avg loss 8.615 |avg tokens 4709.800 |tokens/s 8999.376 |walltime 21410.178 |
Transformer | epoch 1 | step 42257 |avg loss 8.363 |avg tokens 4006.900 |tokens/s 8287.822 |walltime 21415.012 |
Transformer | epoch 1 | step 42267 |avg loss 8.489 |avg tokens 4583.900 |tokens/s 8972.376 |walltime 21420.121 |
Transformer | epoch 1 | step 42277 |avg loss 8.379 |avg tokens 4698.000 |tokens/s 8886.890 |walltime 21425.408 |
Transformer | epoch 1 | step 42287 |avg loss 8.307 |avg tokens 4670.500 |tokens/s 9086.011 |walltime 21430.548 |
Transformer | epoch 1 | step 42297 |avg loss 8.658 |avg tokens 3991.100 |tokens/s 8802.210 |walltime 21435.082 |
Transformer | epoch 1 | step 42307 |avg loss 8.264 |avg tokens 4911.000 |tokens/s 9435.409 |walltime 21440.287 |
Transformer | epoch 1 | step 42317 |avg loss 8.505 |avg tokens 4246.000 |tokens/s 8487.436 |walltime 21445.290 |
Transformer | epoch 1 | step 42327 |avg loss 8.499 |avg tokens 4653.200 |tokens/s 9152.997 |walltime 21450.374 |
Transformer | epoch 1 | step 42337 |avg loss 8.353 |avg tokens 4932.300 |tokens/s 9559.929 |walltime 21455.533 |
Transformer | epoch 1 | step 42347 |avg loss 8.651 |avg tokens 4615.900 |tokens/s 8945.322 |walltime 21460.693 |
Transformer | epoch 1 | step 42357 |avg loss 8.442 |avg tokens 4250.500 |tokens/s 8585.299 |walltime 21465.644 |
Transformer | epoch 1 | step 42367 |avg loss 8.256 |avg tokens 4677.300 |tokens/s 9054.556 |walltime 21470.810 |
Transformer | epoch 1 | step 42377 |avg loss 8.295 |avg tokens 4934.400 |tokens/s 9225.408 |walltime 21476.158 |
Transformer | epoch 1 | step 42387 |avg loss 8.283 |avg tokens 4740.500 |tokens/s 9306.756 |walltime 21481.252 |
Transformer | epoch 1 | step 42397 |avg loss 8.399 |avg tokens 4662.400 |tokens/s 8903.978 |walltime 21486.488 |
Transformer | epoch 1 | step 42407 |avg loss 8.508 |avg tokens 4334.700 |tokens/s 8741.621 |walltime 21491.447 |
Transformer | epoch 1 | step 42417 |avg loss 8.067 |avg tokens 4610.100 |tokens/s 8934.182 |walltime 21496.607 |
Transformer | epoch 1 | step 42427 |avg loss 8.518 |avg tokens 4385.700 |tokens/s 8890.984 |walltime 21501.540 |
Transformer | epoch 1 | step 42437 |avg loss 8.587 |avg tokens 4409.400 |tokens/s 8715.525 |walltime 21506.599 |
Transformer | epoch 1 | step 42447 |avg loss 8.490 |avg tokens 4715.600 |tokens/s 9284.229 |walltime 21511.678 |
Transformer | epoch 1 | step 42457 |avg loss 8.439 |avg tokens 4450.500 |tokens/s 8888.615 |walltime 21516.685 |
Transformer | epoch 1 | step 42467 |avg loss 8.089 |avg tokens 4346.000 |tokens/s 8502.685 |walltime 21521.797 |
Transformer | epoch 1 | step 42477 |avg loss 8.660 |avg tokens 3909.200 |tokens/s 8219.929 |walltime 21526.552 |
Transformer | epoch 1 | step 42487 |avg loss 8.703 |avg tokens 4351.400 |tokens/s 8816.539 |walltime 21531.488 |
Transformer | epoch 1 | step 42497 |avg loss 8.400 |avg tokens 4710.500 |tokens/s 9128.558 |walltime 21536.648 |
Transformer | epoch 1 | step 42507 |avg loss 8.742 |avg tokens 3999.700 |tokens/s 8298.629 |walltime 21541.468 |
Transformer | epoch 1 | step 42517 |avg loss 8.330 |avg tokens 4705.600 |tokens/s 8932.637 |walltime 21546.736 |
Transformer | epoch 1 | step 42527 |avg loss 8.515 |avg tokens 4630.100 |tokens/s 9152.429 |walltime 21551.795 |
Transformer | epoch 1 | step 42537 |avg loss 8.439 |avg tokens 4478.200 |tokens/s 9034.836 |walltime 21556.751 |
Transformer | epoch 1 | step 42547 |avg loss 8.434 |avg tokens 4397.000 |tokens/s 8998.352 |walltime 21561.638 |
Transformer | epoch 1 | step 42557 |avg loss 8.351 |avg tokens 4731.900 |tokens/s 8970.266 |walltime 21566.913 |
Transformer | epoch 1 | step 42567 |avg loss 8.520 |avg tokens 4240.100 |tokens/s 8661.832 |walltime 21571.808 |
Transformer | epoch 1 | step 42577 |avg loss 8.694 |avg tokens 4315.500 |tokens/s 8403.332 |walltime 21576.943 |
Transformer | epoch 1 | step 42587 |avg loss 8.655 |avg tokens 4637.700 |tokens/s 9336.896 |walltime 21581.910 |
Transformer | epoch 1 | step 42597 |avg loss 8.459 |avg tokens 4577.300 |tokens/s 8884.605 |walltime 21587.062 |
Transformer | epoch 1 | step 42607 |avg loss 8.377 |avg tokens 4620.700 |tokens/s 9028.948 |walltime 21592.180 |
Transformer | epoch 1 | step 42617 |avg loss 8.463 |avg tokens 4255.100 |tokens/s 8467.374 |walltime 21597.205 |
Transformer | epoch 1 | step 42627 |avg loss 8.554 |avg tokens 4612.200 |tokens/s 9102.737 |walltime 21602.272 |
Transformer | epoch 1 | step 42637 |avg loss 8.512 |avg tokens 4740.000 |tokens/s 9259.195 |walltime 21607.391 |
Transformer | epoch 1 | step 42647 |avg loss 8.350 |avg tokens 4885.100 |tokens/s 9018.354 |walltime 21612.808 |
Transformer | epoch 1 | step 42657 |avg loss 8.317 |avg tokens 4454.000 |tokens/s 8829.149 |walltime 21617.853 |
Transformer | epoch 1 | step 42667 |avg loss 8.344 |avg tokens 4761.200 |tokens/s 9242.722 |walltime 21623.004 |
Transformer | epoch 1 | step 42677 |avg loss 8.195 |avg tokens 4375.900 |tokens/s 8667.223 |walltime 21628.053 |
Transformer | epoch 1 | step 42687 |avg loss 8.398 |avg tokens 4428.400 |tokens/s 8742.494 |walltime 21633.118 |
Transformer | epoch 1 | step 42697 |avg loss 8.238 |avg tokens 4799.400 |tokens/s 9075.239 |walltime 21638.407 |
Transformer | epoch 1 | step 42707 |avg loss 8.393 |avg tokens 4579.000 |tokens/s 8979.096 |walltime 21643.506 |
Transformer | epoch 1 | step 42717 |avg loss 8.186 |avg tokens 4489.600 |tokens/s 8720.521 |walltime 21648.655 |
Transformer | epoch 1 | step 42727 |avg loss 8.417 |avg tokens 4869.300 |tokens/s 9510.722 |walltime 21653.774 |
Transformer | epoch 1 | step 42737 |avg loss 8.459 |avg tokens 4702.400 |tokens/s 9087.734 |walltime 21658.949 |
Transformer | epoch 1 | step 42747 |avg loss 8.298 |avg tokens 4812.700 |tokens/s 9013.278 |walltime 21664.289 |
Transformer | epoch 1 | step 42757 |avg loss 8.747 |avg tokens 4266.700 |tokens/s 9063.595 |walltime 21668.996 |
Transformer | epoch 1 | step 42767 |avg loss 8.330 |avg tokens 4778.200 |tokens/s 9195.500 |walltime 21674.192 |
Transformer | epoch 1 | step 42777 |avg loss 8.363 |avg tokens 4732.400 |tokens/s 9159.509 |walltime 21679.359 |
Transformer | epoch 1 | step 42787 |avg loss 8.522 |avg tokens 4553.300 |tokens/s 9251.292 |walltime 21684.281 |
Transformer | epoch 1 | step 42797 |avg loss 8.521 |avg tokens 4409.500 |tokens/s 8664.284 |walltime 21689.370 |
Transformer | epoch 1 | step 42807 |avg loss 8.288 |avg tokens 4753.900 |tokens/s 9080.236 |walltime 21694.605 |
Transformer | epoch 1 | step 42817 |avg loss 8.574 |avg tokens 4799.300 |tokens/s 9632.350 |walltime 21699.588 |
Transformer | epoch 1 | step 42827 |avg loss 8.423 |avg tokens 4356.800 |tokens/s 8796.683 |walltime 21704.541 |
Transformer | epoch 1 | step 42837 |avg loss 8.567 |avg tokens 4751.000 |tokens/s 9152.160 |walltime 21709.732 |
Transformer | epoch 1 | step 42847 |avg loss 8.828 |avg tokens 4407.800 |tokens/s 8960.151 |walltime 21714.651 |
Transformer | epoch 1 | step 42857 |avg loss 8.544 |avg tokens 4442.000 |tokens/s 8856.139 |walltime 21719.667 |
Transformer | epoch 1 | step 42867 |avg loss 8.451 |avg tokens 4511.600 |tokens/s 8822.092 |walltime 21724.781 |
Transformer | epoch 1 | step 42877 |avg loss 8.491 |avg tokens 4603.200 |tokens/s 8998.838 |walltime 21729.896 |
Transformer | epoch 1 | step 42887 |avg loss 8.443 |avg tokens 4352.200 |tokens/s 8575.980 |walltime 21734.971 |
Transformer | epoch 1 | step 42897 |avg loss 8.377 |avg tokens 4712.800 |tokens/s 9141.831 |walltime 21740.126 |
Transformer | epoch 1 | step 42907 |avg loss 8.304 |avg tokens 4320.700 |tokens/s 8769.370 |walltime 21745.053 |
Transformer | epoch 1 | step 42917 |avg loss 8.489 |avg tokens 4095.900 |tokens/s 8603.768 |walltime 21749.814 |
Transformer | epoch 1 | step 42927 |avg loss 8.580 |avg tokens 4238.900 |tokens/s 9003.163 |walltime 21754.522 |
Transformer | epoch 1 | step 42937 |avg loss 8.250 |avg tokens 4579.500 |tokens/s 9058.466 |walltime 21759.578 |
Transformer | epoch 1 | step 42947 |avg loss 8.381 |avg tokens 4447.500 |tokens/s 8692.908 |walltime 21764.694 |
Transformer | epoch 1 | step 42957 |avg loss 8.605 |avg tokens 3962.700 |tokens/s 8523.449 |walltime 21769.343 |
Transformer | epoch 1 | step 42967 |avg loss 8.504 |avg tokens 4354.500 |tokens/s 8537.286 |walltime 21774.444 |
Transformer | epoch 1 | step 42977 |avg loss 8.706 |avg tokens 4276.200 |tokens/s 8771.757 |walltime 21779.319 |
Transformer | epoch 1 | step 42987 |avg loss 8.456 |avg tokens 4496.600 |tokens/s 8900.745 |walltime 21784.371 |
Transformer | epoch 1 | step 42997 |avg loss 8.425 |avg tokens 4239.000 |tokens/s 8599.713 |walltime 21789.300 |
Transformer | epoch 1 | step 43007 |avg loss 8.328 |avg tokens 4921.800 |tokens/s 9547.833 |walltime 21794.455 |
Transformer | epoch 1 | step 43017 |avg loss 8.425 |avg tokens 4466.600 |tokens/s 8865.812 |walltime 21799.493 |
Transformer | epoch 1 | step 43027 |avg loss 8.339 |avg tokens 4882.800 |tokens/s 9305.769 |walltime 21804.740 |
Transformer | epoch 1 | step 43037 |avg loss 8.507 |avg tokens 4279.800 |tokens/s 8831.156 |walltime 21809.586 |
Transformer | epoch 1 | step 43047 |avg loss 8.272 |avg tokens 4761.200 |tokens/s 9082.077 |walltime 21814.828 |
Transformer | epoch 1 | step 43057 |avg loss 8.431 |avg tokens 4025.100 |tokens/s 8249.396 |walltime 21819.708 |
Transformer | epoch 1 | step 43067 |avg loss 8.409 |avg tokens 4815.800 |tokens/s 9350.566 |walltime 21824.858 |
Transformer | epoch 1 | step 43077 |avg loss 8.483 |avg tokens 4729.500 |tokens/s 9387.241 |walltime 21829.896 |
Transformer | epoch 1 | step 43087 |avg loss 8.090 |avg tokens 4426.200 |tokens/s 8584.803 |walltime 21835.052 |
Transformer | epoch 1 | step 43097 |avg loss 8.644 |avg tokens 4131.500 |tokens/s 8368.413 |walltime 21839.989 |
Transformer | epoch 1 | step 43107 |avg loss 8.429 |avg tokens 4880.000 |tokens/s 9285.557 |walltime 21845.245 |
Transformer | epoch 1 | step 43117 |avg loss 8.466 |avg tokens 4063.300 |tokens/s 8416.578 |walltime 21850.072 |
Transformer | epoch 1 | step 43127 |avg loss 8.190 |avg tokens 4874.100 |tokens/s 9282.262 |walltime 21855.323 |
Transformer | epoch 1 | step 43137 |avg loss 8.627 |avg tokens 4838.100 |tokens/s 9762.668 |walltime 21860.279 |
Transformer | epoch 1 | step 43147 |avg loss 8.372 |avg tokens 4219.600 |tokens/s 8806.180 |walltime 21865.071 |
Transformer | epoch 1 | step 43157 |avg loss 8.416 |avg tokens 4360.400 |tokens/s 8709.702 |walltime 21870.077 |
Transformer | epoch 1 | step 43167 |avg loss 8.303 |avg tokens 4488.600 |tokens/s 8687.437 |walltime 21875.244 |
Transformer | epoch 1 | step 43177 |avg loss 8.690 |avg tokens 3971.900 |tokens/s 8407.128 |walltime 21879.968 |
Transformer | epoch 1 | step 43187 |avg loss 8.333 |avg tokens 4635.700 |tokens/s 8952.520 |walltime 21885.146 |
Transformer | epoch 1 | step 43197 |avg loss 8.182 |avg tokens 4751.100 |tokens/s 9308.449 |walltime 21890.250 |
Transformer | epoch 1 | step 43207 |avg loss 8.388 |avg tokens 4506.600 |tokens/s 9007.803 |walltime 21895.253 |
Transformer | epoch 1 | step 43217 |avg loss 8.459 |avg tokens 4455.100 |tokens/s 9125.433 |walltime 21900.135 |
Transformer | epoch 1 | step 43227 |avg loss 8.636 |avg tokens 4102.600 |tokens/s 8475.274 |walltime 21904.976 |
Transformer | epoch 1 | step 43237 |avg loss 8.515 |avg tokens 4060.900 |tokens/s 8371.722 |walltime 21909.827 |
Transformer | epoch 1 | step 43247 |avg loss 8.502 |avg tokens 4461.600 |tokens/s 8894.099 |walltime 21914.843 |
Transformer | epoch 1 | step 43257 |avg loss 8.435 |avg tokens 4932.400 |tokens/s 9472.443 |walltime 21920.050 |
Transformer | epoch 1 | step 43267 |avg loss 8.378 |avg tokens 4708.000 |tokens/s 8964.898 |walltime 21925.302 |
Transformer | epoch 1 | step 43277 |avg loss 8.430 |avg tokens 4570.300 |tokens/s 9027.985 |walltime 21930.364 |
Transformer | epoch 1 | step 43287 |avg loss 8.347 |avg tokens 4646.500 |tokens/s 8980.152 |walltime 21935.539 |
Transformer | epoch 1 | step 43297 |avg loss 8.566 |avg tokens 4060.200 |tokens/s 8644.804 |walltime 21940.235 |
Transformer | epoch 1 | step 43307 |avg loss 8.585 |avg tokens 4843.000 |tokens/s 9389.359 |walltime 21945.393 |
Transformer | epoch 1 | step 43317 |avg loss 8.606 |avg tokens 4604.700 |tokens/s 9424.317 |walltime 21950.279 |
Transformer | epoch 1 | step 43327 |avg loss 8.376 |avg tokens 4532.700 |tokens/s 9128.713 |walltime 21955.245 |
Transformer | epoch 1 | step 43337 |avg loss 8.617 |avg tokens 4198.800 |tokens/s 8404.785 |walltime 21960.240 |
Transformer | epoch 1 | step 43347 |avg loss 8.603 |avg tokens 4424.700 |tokens/s 9220.576 |walltime 21965.039 |
Transformer | epoch 1 | step 43357 |avg loss 8.355 |avg tokens 4759.600 |tokens/s 9083.207 |walltime 21970.279 |
Transformer | epoch 1 | step 43367 |avg loss 8.343 |avg tokens 4754.400 |tokens/s 9117.946 |walltime 21975.493 |
Transformer | epoch 1 | step 43377 |avg loss 8.396 |avg tokens 4361.700 |tokens/s 8631.172 |walltime 21980.547 |
Transformer | epoch 1 | step 43387 |avg loss 8.404 |avg tokens 4680.500 |tokens/s 9047.186 |walltime 21985.720 |
Transformer | epoch 1 | step 43397 |avg loss 8.316 |avg tokens 4823.000 |tokens/s 9307.256 |walltime 21990.902 |
Transformer | epoch 1 | step 43407 |avg loss 8.340 |avg tokens 4504.600 |tokens/s 8786.637 |walltime 21996.029 |
Transformer | epoch 1 | step 43417 |avg loss 8.490 |avg tokens 4468.900 |tokens/s 9014.471 |walltime 22000.986 |
Transformer | epoch 1 | step 43427 |avg loss 8.193 |avg tokens 4221.200 |tokens/s 9299.399 |walltime 22005.525 |
Transformer | epoch 1 | step 43437 |avg loss 8.224 |avg tokens 4781.600 |tokens/s 9166.622 |walltime 22010.742 |
Transformer | epoch 1 | step 43447 |avg loss 8.672 |avg tokens 4381.700 |tokens/s 8363.798 |walltime 22015.981 |
Transformer | epoch 1 | step 43457 |avg loss 8.578 |avg tokens 3866.600 |tokens/s 8208.044 |walltime 22020.691 |
Transformer | epoch 1 | step 43467 |avg loss 8.385 |avg tokens 4895.300 |tokens/s 9239.998 |walltime 22025.989 |
Transformer | epoch 1 | step 43477 |avg loss 8.558 |avg tokens 4337.400 |tokens/s 8909.088 |walltime 22030.858 |
Transformer | epoch 1 | step 43487 |avg loss 8.520 |avg tokens 4451.600 |tokens/s 8920.219 |walltime 22035.848 |
Transformer | epoch 1 | step 43497 |avg loss 8.645 |avg tokens 4802.800 |tokens/s 9529.452 |walltime 22040.888 |
Transformer | epoch 1 | step 43507 |avg loss 8.396 |avg tokens 4784.000 |tokens/s 9274.100 |walltime 22046.047 |
Transformer | epoch 1 | step 43517 |avg loss 8.444 |avg tokens 4813.600 |tokens/s 9109.026 |walltime 22051.331 |
Transformer | epoch 1 | step 43527 |avg loss 8.442 |avg tokens 4847.900 |tokens/s 9226.412 |walltime 22056.586 |
Transformer | epoch 1 | step 43537 |avg loss 8.590 |avg tokens 4574.500 |tokens/s 9241.126 |walltime 22061.536 |
Transformer | epoch 1 | step 43547 |avg loss 8.553 |avg tokens 4630.500 |tokens/s 9218.510 |walltime 22066.559 |
Transformer | epoch 1 | step 43557 |avg loss 8.592 |avg tokens 4778.700 |tokens/s 9353.629 |walltime 22071.668 |
Transformer | epoch 1 | step 43567 |avg loss 8.437 |avg tokens 4329.000 |tokens/s 8705.410 |walltime 22076.641 |
Transformer | epoch 1 | step 43577 |avg loss 8.279 |avg tokens 4869.100 |tokens/s 9269.357 |walltime 22081.893 |
Transformer | epoch 1 | step 43587 |avg loss 8.410 |avg tokens 4941.200 |tokens/s 9613.747 |walltime 22087.033 |
Transformer | epoch 1 | step 43597 |avg loss 8.370 |avg tokens 4453.300 |tokens/s 8723.166 |walltime 22092.138 |
Transformer | epoch 1 | step 43607 |avg loss 8.525 |avg tokens 4222.800 |tokens/s 8483.153 |walltime 22097.116 |
Transformer | epoch 1 | step 43617 |avg loss 8.336 |avg tokens 4597.400 |tokens/s 8971.485 |walltime 22102.241 |
Transformer | epoch 1 | step 43627 |avg loss 8.382 |avg tokens 4522.300 |tokens/s 8833.654 |walltime 22107.360 |
Transformer | epoch 1 | step 43637 |avg loss 8.556 |avg tokens 4361.600 |tokens/s 8577.690 |walltime 22112.445 |
Transformer | epoch 1 | step 43647 |avg loss 8.221 |avg tokens 4408.400 |tokens/s 8458.786 |walltime 22117.656 |
Transformer | epoch 1 | step 43657 |avg loss 8.387 |avg tokens 4625.600 |tokens/s 8884.865 |walltime 22122.863 |
Transformer | epoch 1 | step 43667 |avg loss 8.404 |avg tokens 4432.900 |tokens/s 8655.443 |walltime 22127.984 |
Transformer | epoch 1 | step 43677 |avg loss 8.583 |avg tokens 4589.000 |tokens/s 9061.222 |walltime 22133.049 |
Transformer | epoch 1 | step 43687 |avg loss 8.072 |avg tokens 4263.300 |tokens/s 8470.574 |walltime 22138.082 |
Transformer | epoch 1 | step 43697 |avg loss 8.495 |avg tokens 4404.500 |tokens/s 8651.102 |walltime 22143.173 |
Transformer | epoch 1 | step 43707 |avg loss 8.309 |avg tokens 4713.000 |tokens/s 9070.179 |walltime 22148.369 |
