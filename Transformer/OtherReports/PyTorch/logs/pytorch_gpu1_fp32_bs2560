| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 8421, WORLD_SIZE: 1, RANK: 0
| distributed init done!
| initialized host 5b14333bb733 as rank 0 and device id 0
Namespace(adam_betas=[0.9, 0.997], adam_eps=1e-09, amp=False, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data/', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=1, do_sanity_check=False, dropout=0.1, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, file=None, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=100, lr=[0.000846], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=1, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=2560, max_update=0, min_len=1, min_lr=0.0, momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=True, optimizer='adam', pad_sequence=1, path=None, prefix_size=0, print_alignment=False, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe='@@ ', replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='/workspace/checkpoint', save_interval=1, save_predictions=False, seed=1, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
filename test.en-de.de.bin
args.data /data/
args.source_lang en
args.target_lang de
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| /data/ train 4575637 examples
| Sentences are being padded to multiples of: 1
| /data/ valid 3000 examples
| Sentences are being padded to multiples of: 1
| /data/ test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
| NOTICE: your device may support faster training with --amp
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 1 GPUs
| max tokens per GPU = 2560 and max sentences per GPU = None
Transformer | epoch 0 | step 100 |avg loss 13.930 |avg tokens 2149.640 |tokens/s 8654.577 |walltime 33.756 |
Transformer | epoch 0 | step 200 |avg loss 12.098 |avg tokens 2207.940 |tokens/s 8724.068 |walltime 59.064 |
Transformer | epoch 0 | step 300 |avg loss 11.487 |avg tokens 2221.730 |tokens/s 8731.999 |walltime 84.508 |
Transformer | epoch 0 | step 400 |avg loss 11.347 |avg tokens 2194.980 |tokens/s 8638.238 |walltime 109.918 |
Transformer | epoch 0 | step 500 |avg loss 11.158 |avg tokens 2204.800 |tokens/s 8759.885 |walltime 135.087 |
Transformer | epoch 0 | step 600 |avg loss 10.892 |avg tokens 2224.040 |tokens/s 8785.015 |walltime 160.403 |
Transformer | epoch 0 | step 700 |avg loss 10.669 |avg tokens 2169.530 |tokens/s 8515.263 |walltime 185.882 |
Transformer | epoch 0 | step 800 |avg loss 10.365 |avg tokens 2250.270 |tokens/s 8715.250 |walltime 211.702 |
Transformer | epoch 0 | step 900 |avg loss 10.224 |avg tokens 2216.380 |tokens/s 8691.915 |walltime 237.201 |
Transformer | epoch 0 | step 1000 |avg loss 10.185 |avg tokens 2150.180 |tokens/s 8633.960 |walltime 262.105 |
Transformer | epoch 0 | step 1100 |avg loss 9.986 |avg tokens 2248.210 |tokens/s 8842.264 |walltime 287.530 |
Transformer | epoch 0 | step 1200 |avg loss 9.847 |avg tokens 2222.450 |tokens/s 8768.571 |walltime 312.876 |
Transformer | epoch 0 | step 1300 |avg loss 9.809 |avg tokens 2187.360 |tokens/s 8737.198 |walltime 337.911 |
Transformer | epoch 0 | step 1400 |avg loss 9.659 |avg tokens 2180.180 |tokens/s 8692.560 |walltime 362.992 |
Transformer | epoch 0 | step 1500 |avg loss 9.644 |avg tokens 2196.610 |tokens/s 8816.437 |walltime 387.907 |
Transformer | epoch 0 | step 1600 |avg loss 9.423 |avg tokens 2194.020 |tokens/s 8682.016 |walltime 413.178 |
Transformer | epoch 0 | step 1700 |avg loss 9.415 |avg tokens 2065.360 |tokens/s 8358.777 |walltime 437.887 |
Transformer | epoch 0 | step 1800 |avg loss 9.196 |avg tokens 2223.660 |tokens/s 8703.598 |walltime 463.435 |
Transformer | epoch 0 | step 1900 |avg loss 9.270 |avg tokens 2145.980 |tokens/s 8591.963 |walltime 488.412 |
Transformer | epoch 0 | step 2000 |avg loss 9.113 |avg tokens 2226.900 |tokens/s 8866.699 |walltime 513.527 |
Transformer | epoch 0 | step 2100 |avg loss 9.056 |avg tokens 2196.570 |tokens/s 8782.744 |walltime 538.537 |
Transformer | epoch 0 | step 2200 |avg loss 8.915 |avg tokens 2208.970 |tokens/s 8772.402 |walltime 563.718 |
Transformer | epoch 0 | step 2300 |avg loss 8.948 |avg tokens 2188.380 |tokens/s 8718.349 |walltime 588.819 |
Transformer | epoch 0 | step 2400 |avg loss 8.760 |avg tokens 2231.030 |tokens/s 8780.724 |walltime 614.227 |
Transformer | epoch 0 | step 2500 |avg loss 8.912 |avg tokens 2102.530 |tokens/s 8532.392 |walltime 638.869 |
Transformer | epoch 0 | step 2600 |avg loss 8.669 |avg tokens 2186.910 |tokens/s 8700.072 |walltime 664.006 |
Transformer | epoch 0 | step 2700 |avg loss 8.715 |avg tokens 2187.950 |tokens/s 8725.826 |walltime 689.080 |
Transformer | epoch 0 | step 2800 |avg loss 8.589 |avg tokens 2231.350 |tokens/s 8755.229 |walltime 714.566 |
Transformer | epoch 0 | step 2900 |avg loss 8.610 |avg tokens 2229.630 |tokens/s 8881.914 |walltime 739.669 |
Transformer | epoch 0 | step 3000 |avg loss 8.621 |avg tokens 2185.560 |tokens/s 8739.709 |walltime 764.677 |
Transformer | epoch 0 | step 3100 |avg loss 8.583 |avg tokens 2180.980 |tokens/s 8685.684 |walltime 789.787 |
Transformer | epoch 0 | step 3200 |avg loss 8.538 |avg tokens 2193.300 |tokens/s 8656.765 |walltime 815.123 |
Transformer | epoch 0 | step 3300 |avg loss 8.534 |avg tokens 2214.170 |tokens/s 8826.510 |walltime 840.208 |
Transformer | epoch 0 | step 3400 |avg loss 8.488 |avg tokens 2156.600 |tokens/s 8701.929 |walltime 864.991 |
Transformer | epoch 0 | step 3500 |avg loss 8.427 |avg tokens 2234.590 |tokens/s 8777.016 |walltime 890.451 |
Transformer | epoch 0 | step 3600 |avg loss 8.432 |avg tokens 2186.950 |tokens/s 8696.232 |walltime 915.599 |
Transformer | epoch 0 | step 3700 |avg loss 8.529 |avg tokens 2191.450 |tokens/s 8706.530 |walltime 940.769 |
Transformer | epoch 0 | step 3800 |avg loss 8.499 |avg tokens 2208.900 |tokens/s 8863.196 |walltime 965.691 |
Transformer | epoch 0 | step 3900 |avg loss 8.548 |avg tokens 2203.190 |tokens/s 8762.773 |walltime 990.834 |
Transformer | epoch 0 | step 4000 |avg loss 8.635 |avg tokens 2241.820 |tokens/s 8862.668 |walltime 1016.129 |
Transformer | epoch 0 | step 4100 |avg loss 8.611 |avg tokens 2103.120 |tokens/s 8509.508 |walltime 1040.844 |
Transformer | epoch 0 | step 4200 |avg loss 8.742 |avg tokens 2166.960 |tokens/s 8758.204 |walltime 1065.586 |
Transformer | epoch 0 | step 4300 |avg loss 8.872 |avg tokens 2181.110 |tokens/s 8809.876 |walltime 1090.344 |
Transformer | epoch 0 | step 4400 |avg loss 8.627 |avg tokens 2248.420 |tokens/s 8832.393 |walltime 1115.800 |
Transformer | epoch 0 | step 4500 |avg loss 8.763 |avg tokens 2212.430 |tokens/s 8751.610 |walltime 1141.081 |
Transformer | epoch 0 | step 4600 |avg loss 8.717 |avg tokens 2205.340 |tokens/s 8715.364 |walltime 1166.385 |
Transformer | epoch 0 | step 4700 |avg loss 8.746 |avg tokens 2121.490 |tokens/s 8611.457 |walltime 1191.020 |
Transformer | epoch 0 | step 4800 |avg loss 8.807 |avg tokens 2221.290 |tokens/s 8802.909 |walltime 1216.254 |
Transformer | epoch 0 | step 4900 |avg loss 8.753 |avg tokens 2237.430 |tokens/s 8819.044 |walltime 1241.624 |
Transformer | epoch 0 | step 5000 |avg loss 8.826 |avg tokens 2174.480 |tokens/s 8750.635 |walltime 1266.474 |
Transformer | epoch 0 | step 5100 |avg loss 8.819 |avg tokens 2027.940 |tokens/s 8300.790 |walltime 1290.904 |
Transformer | epoch 0 | step 5200 |avg loss 8.859 |avg tokens 2192.210 |tokens/s 8767.825 |walltime 1315.907 |
Transformer | epoch 0 | step 5300 |avg loss 8.872 |avg tokens 2117.070 |tokens/s 8606.461 |walltime 1340.506 |
Transformer | epoch 0 | step 5400 |avg loss 8.845 |avg tokens 2200.090 |tokens/s 8784.088 |walltime 1365.552 |
Transformer | epoch 0 | step 5500 |avg loss 8.916 |avg tokens 2216.780 |tokens/s 8807.766 |walltime 1390.721 |
Transformer | epoch 0 | step 5600 |avg loss 8.896 |avg tokens 2176.970 |tokens/s 8641.795 |walltime 1415.912 |
Transformer | epoch 0 | step 5700 |avg loss 8.948 |avg tokens 2184.930 |tokens/s 8737.061 |walltime 1440.919 |
Transformer | epoch 0 | step 5800 |avg loss 8.966 |avg tokens 2245.100 |tokens/s 8810.747 |walltime 1466.401 |
Transformer | epoch 0 | step 5900 |avg loss 9.086 |avg tokens 2151.580 |tokens/s 8610.383 |walltime 1491.389 |
Transformer | epoch 0 | step 6000 |avg loss 9.042 |avg tokens 2163.700 |tokens/s 8785.249 |walltime 1516.018 |
Transformer | epoch 0 | step 6100 |avg loss 9.080 |avg tokens 2172.990 |tokens/s 8616.180 |walltime 1541.238 |
Transformer | epoch 0 | step 6200 |avg loss 8.983 |avg tokens 2182.110 |tokens/s 8700.273 |walltime 1566.319 |
Transformer | epoch 0 | step 6300 |avg loss 9.064 |avg tokens 2167.290 |tokens/s 8645.627 |walltime 1591.387 |
Transformer | epoch 0 | step 6400 |avg loss 9.106 |avg tokens 2198.350 |tokens/s 8784.193 |walltime 1616.413 |
Transformer | epoch 0 | step 6500 |avg loss 9.185 |avg tokens 2223.480 |tokens/s 8863.570 |walltime 1641.499 |
Transformer | epoch 0 | step 6600 |avg loss 9.121 |avg tokens 2160.730 |tokens/s 8653.099 |walltime 1666.469 |
Transformer | epoch 0 | step 6700 |avg loss 9.249 |avg tokens 2152.360 |tokens/s 8589.738 |walltime 1691.526 |
Transformer | epoch 0 | step 6800 |avg loss 9.304 |avg tokens 2197.270 |tokens/s 8805.946 |walltime 1716.479 |
Transformer | epoch 0 | step 6900 |avg loss 9.299 |avg tokens 2138.840 |tokens/s 8639.649 |walltime 1741.235 |
Transformer | epoch 0 | step 7000 |avg loss 9.223 |avg tokens 2196.280 |tokens/s 8728.627 |walltime 1766.397 |
Transformer | epoch 0 | step 7100 |avg loss 9.226 |avg tokens 2204.400 |tokens/s 8794.906 |walltime 1791.461 |
Transformer | epoch 0 | step 7200 |avg loss 9.285 |avg tokens 2231.970 |tokens/s 8820.474 |walltime 1816.765 |
Transformer | epoch 0 | step 7300 |avg loss 9.303 |avg tokens 2158.220 |tokens/s 8629.396 |walltime 1841.776 |
Transformer | epoch 0 | step 7400 |avg loss 9.287 |avg tokens 2168.250 |tokens/s 8689.419 |walltime 1866.728 |
Transformer | epoch 0 | step 7500 |avg loss 9.273 |avg tokens 2239.220 |tokens/s 8905.282 |walltime 1891.873 |
Transformer | epoch 0 | step 7600 |avg loss 9.372 |avg tokens 2253.900 |tokens/s 8888.969 |walltime 1917.229 |
Transformer | epoch 0 | step 7700 |avg loss 9.379 |avg tokens 2122.790 |tokens/s 8751.267 |walltime 1941.486 |
Transformer | epoch 0 | step 7800 |avg loss 9.282 |avg tokens 2126.340 |tokens/s 8616.150 |walltime 1966.165 |
Transformer | epoch 0 | step 7900 |avg loss 9.312 |avg tokens 2196.950 |tokens/s 8786.323 |walltime 1991.169 |
Transformer | epoch 0 | step 8000 |avg loss 9.211 |avg tokens 2196.920 |tokens/s 8689.673 |walltime 2016.451 |
Transformer | epoch 0 | step 8100 |avg loss 9.305 |avg tokens 2170.710 |tokens/s 8744.457 |walltime 2041.275 |
Transformer | epoch 0 | step 8200 |avg loss 9.328 |avg tokens 2156.100 |tokens/s 8695.037 |walltime 2066.072 |
Transformer | epoch 0 | step 8300 |avg loss 9.335 |avg tokens 2217.710 |tokens/s 8823.375 |walltime 2091.206 |
Transformer | epoch 0 | step 8400 |avg loss 9.318 |avg tokens 2255.320 |tokens/s 8866.340 |walltime 2116.643 |
Transformer | epoch 0 | step 8500 |avg loss 9.257 |avg tokens 2206.840 |tokens/s 8765.217 |walltime 2141.820 |
Transformer | epoch 0 | step 8600 |avg loss 9.214 |avg tokens 2143.540 |tokens/s 8516.171 |walltime 2166.991 |
Transformer | epoch 0 | step 8700 |avg loss 9.179 |avg tokens 2246.230 |tokens/s 8816.332 |walltime 2192.469 |
Transformer | epoch 0 | step 8800 |avg loss 9.177 |avg tokens 2176.050 |tokens/s 8654.230 |walltime 2217.613 |
Transformer | epoch 0 | step 8900 |avg loss 9.297 |avg tokens 2093.890 |tokens/s 8406.774 |walltime 2242.520 |
Transformer | epoch 0 | step 9000 |avg loss 9.282 |avg tokens 2270.920 |tokens/s 8897.945 |walltime 2268.042 |
Transformer | epoch 0 | step 9100 |avg loss 9.338 |avg tokens 2202.860 |tokens/s 8802.344 |walltime 2293.068 |
Transformer | epoch 0 | step 9200 |avg loss 9.299 |avg tokens 2157.670 |tokens/s 8658.309 |walltime 2317.988 |
Transformer | epoch 0 | step 9300 |avg loss 9.279 |avg tokens 2219.060 |tokens/s 8784.000 |walltime 2343.251 |
Transformer | epoch 0 | step 9400 |avg loss 9.228 |avg tokens 2230.870 |tokens/s 8763.030 |walltime 2368.708 |
Transformer | epoch 0 | step 9500 |avg loss 9.280 |avg tokens 2160.280 |tokens/s 8627.924 |walltime 2393.747 |
Transformer | epoch 0 | step 9600 |avg loss 9.390 |avg tokens 2217.880 |tokens/s 8813.229 |walltime 2418.912 |
Transformer | epoch 0 | step 9700 |avg loss 9.316 |avg tokens 2181.680 |tokens/s 8718.480 |walltime 2443.936 |
Transformer | epoch 0 | step 9800 |avg loss 9.397 |avg tokens 2173.040 |tokens/s 8678.377 |walltime 2468.975 |
Transformer | epoch 0 | step 9900 |avg loss 9.365 |avg tokens 2228.770 |tokens/s 8758.711 |walltime 2494.422 |
Transformer | epoch 0 | step 10000 |avg loss 9.297 |avg tokens 2159.530 |tokens/s 8599.131 |walltime 2519.535 |
Transformer | epoch 0 | step 10100 |avg loss 9.335 |avg tokens 2113.810 |tokens/s 8478.372 |walltime 2544.467 |
Transformer | epoch 0 | step 10200 |avg loss 9.260 |avg tokens 2184.380 |tokens/s 8647.947 |walltime 2569.726 |
Transformer | epoch 0 | step 10300 |avg loss 9.352 |avg tokens 2168.490 |tokens/s 8679.349 |walltime 2594.710 |
Transformer | epoch 0 | step 10400 |avg loss 9.370 |avg tokens 2145.430 |tokens/s 8647.966 |walltime 2619.519 |
Transformer | epoch 0 | step 10500 |avg loss 9.306 |avg tokens 2133.890 |tokens/s 8542.240 |walltime 2644.499 |
Transformer | epoch 0 | step 10600 |avg loss 9.399 |avg tokens 2139.740 |tokens/s 8657.013 |walltime 2669.216 |
Transformer | epoch 0 | step 10700 |avg loss 9.366 |avg tokens 2127.560 |tokens/s 8601.099 |walltime 2693.952 |
Transformer | epoch 0 | step 10800 |avg loss 9.435 |avg tokens 2131.830 |tokens/s 8647.472 |walltime 2718.605 |
Transformer | epoch 0 | step 10900 |avg loss 9.251 |avg tokens 2230.620 |tokens/s 8830.025 |walltime 2743.866 |
Transformer | epoch 0 | step 11000 |avg loss 9.388 |avg tokens 2219.270 |tokens/s 8793.021 |walltime 2769.105 |
Transformer | epoch 0 | step 11100 |avg loss 9.390 |avg tokens 2126.270 |tokens/s 8624.297 |walltime 2793.760 |
Transformer | epoch 0 | step 11200 |avg loss 9.380 |avg tokens 2186.220 |tokens/s 8670.361 |walltime 2818.975 |
Transformer | epoch 0 | step 11300 |avg loss 9.462 |avg tokens 2156.170 |tokens/s 8663.755 |walltime 2843.862 |
Transformer | epoch 0 | step 11400 |avg loss 9.471 |avg tokens 2153.370 |tokens/s 8710.030 |walltime 2868.585 |
Transformer | epoch 0 | step 11500 |avg loss 9.367 |avg tokens 2223.210 |tokens/s 8739.355 |walltime 2894.024 |
Transformer | epoch 0 | step 11600 |avg loss 9.379 |avg tokens 2228.420 |tokens/s 8858.852 |walltime 2919.179 |
Transformer | epoch 0 | step 11700 |avg loss 9.424 |avg tokens 2222.210 |tokens/s 8879.442 |walltime 2944.205 |
Transformer | epoch 0 | step 11800 |avg loss 9.499 |avg tokens 2094.800 |tokens/s 8444.859 |walltime 2969.011 |
Transformer | epoch 0 | step 11900 |avg loss 9.498 |avg tokens 2163.800 |tokens/s 8700.999 |walltime 2993.879 |
Transformer | epoch 0 | step 12000 |avg loss 9.418 |avg tokens 2275.360 |tokens/s 9006.858 |walltime 3019.142 |
Transformer | epoch 0 | step 12100 |avg loss 9.445 |avg tokens 2156.870 |tokens/s 8682.551 |walltime 3043.983 |
Transformer | epoch 0 | step 12200 |avg loss 9.387 |avg tokens 2236.070 |tokens/s 8859.233 |walltime 3069.223 |
Transformer | epoch 0 | step 12300 |avg loss 9.287 |avg tokens 2174.410 |tokens/s 8540.080 |walltime 3094.684 |
Transformer | epoch 0 | step 12400 |avg loss 9.328 |avg tokens 2175.870 |tokens/s 8599.056 |walltime 3119.988 |
Transformer | epoch 0 | step 12500 |avg loss 9.296 |avg tokens 2170.900 |tokens/s 8637.433 |walltime 3145.121 |
Transformer | epoch 0 | step 12600 |avg loss 9.281 |avg tokens 2213.930 |tokens/s 8784.562 |walltime 3170.324 |
Transformer | epoch 0 | step 12700 |avg loss 9.285 |avg tokens 2227.200 |tokens/s 8758.175 |walltime 3195.754 |
Transformer | epoch 0 | step 12800 |avg loss 9.306 |avg tokens 2132.240 |tokens/s 8627.396 |walltime 3220.469 |
Transformer | epoch 0 | step 12900 |avg loss 9.294 |avg tokens 2208.340 |tokens/s 8737.648 |walltime 3245.743 |
Transformer | epoch 0 | step 13000 |avg loss 9.310 |avg tokens 2235.070 |tokens/s 8874.408 |walltime 3270.928 |
Transformer | epoch 0 | step 13100 |avg loss 9.334 |avg tokens 2208.270 |tokens/s 8802.021 |walltime 3296.016 |
Transformer | epoch 0 | step 13200 |avg loss 9.408 |avg tokens 2233.310 |tokens/s 8782.829 |walltime 3321.444 |
Transformer | epoch 0 | step 13300 |avg loss 9.321 |avg tokens 2160.150 |tokens/s 8693.907 |walltime 3346.291 |
Transformer | epoch 0 | step 13400 |avg loss 9.361 |avg tokens 2233.030 |tokens/s 8933.975 |walltime 3371.286 |
Transformer | epoch 0 | step 13500 |avg loss 9.316 |avg tokens 2250.690 |tokens/s 8984.274 |walltime 3396.337 |
Transformer | epoch 0 | step 13600 |avg loss 9.352 |avg tokens 2137.590 |tokens/s 8630.823 |walltime 3421.104 |
Transformer | epoch 0 | step 13700 |avg loss 9.292 |avg tokens 2181.480 |tokens/s 8631.952 |walltime 3446.377 |
Transformer | epoch 0 | step 13800 |avg loss 9.322 |avg tokens 2153.290 |tokens/s 8621.740 |walltime 3471.352 |
Transformer | epoch 0 | step 13900 |avg loss 9.326 |avg tokens 2175.760 |tokens/s 8584.986 |walltime 3496.695 |
Transformer | epoch 0 | step 14000 |avg loss 9.323 |avg tokens 2185.060 |tokens/s 8654.014 |walltime 3521.945 |
Transformer | epoch 0 | step 14100 |avg loss 9.400 |avg tokens 2218.150 |tokens/s 8816.641 |walltime 3547.103 |
Transformer | epoch 0 | step 14200 |avg loss 9.401 |avg tokens 2114.820 |tokens/s 8548.964 |walltime 3571.841 |
Transformer | epoch 0 | step 14300 |avg loss 9.422 |avg tokens 2169.070 |tokens/s 8698.888 |walltime 3596.776 |
Transformer | epoch 0 | step 14400 |avg loss 9.456 |avg tokens 2176.780 |tokens/s 8656.132 |walltime 3621.923 |
Transformer | epoch 0 | step 14500 |avg loss 9.408 |avg tokens 2173.420 |tokens/s 8642.192 |walltime 3647.072 |
Transformer | epoch 0 | step 14600 |avg loss 9.355 |avg tokens 2215.860 |tokens/s 8782.867 |walltime 3672.302 |
Transformer | epoch 0 | step 14700 |avg loss 9.376 |avg tokens 2239.600 |tokens/s 8947.116 |walltime 3697.333 |
Transformer | epoch 0 | step 14800 |avg loss 9.352 |avg tokens 2244.050 |tokens/s 8853.308 |walltime 3722.680 |
Transformer | epoch 0 | step 14900 |avg loss 9.418 |avg tokens 2172.220 |tokens/s 8749.971 |walltime 3747.506 |
Transformer | epoch 0 | step 15000 |avg loss 9.365 |avg tokens 2135.760 |tokens/s 8576.814 |walltime 3772.407 |
Transformer | epoch 0 | step 15100 |avg loss 9.353 |avg tokens 2144.790 |tokens/s 8525.521 |walltime 3797.564 |
Transformer | epoch 0 | step 15200 |avg loss 9.360 |avg tokens 2189.160 |tokens/s 8712.691 |walltime 3822.691 |
Transformer | epoch 0 | step 15300 |avg loss 9.329 |avg tokens 2234.400 |tokens/s 8786.226 |walltime 3848.121 |
Transformer | epoch 0 | step 15400 |avg loss 9.361 |avg tokens 2192.800 |tokens/s 8769.421 |walltime 3873.126 |
Transformer | epoch 0 | step 15500 |avg loss 9.308 |avg tokens 2201.700 |tokens/s 8756.195 |walltime 3898.271 |
Transformer | epoch 0 | step 15600 |avg loss 9.195 |avg tokens 2245.880 |tokens/s 8850.169 |walltime 3923.647 |
Transformer | epoch 0 | step 15700 |avg loss 9.307 |avg tokens 2158.540 |tokens/s 8768.374 |walltime 3948.265 |
Transformer | epoch 0 | step 15800 |avg loss 9.275 |avg tokens 2261.110 |tokens/s 8881.669 |walltime 3973.723 |
Transformer | epoch 0 | step 15900 |avg loss 9.333 |avg tokens 2180.900 |tokens/s 8681.145 |walltime 3998.845 |
Transformer | epoch 0 | step 16000 |avg loss 9.335 |avg tokens 2189.200 |tokens/s 8734.131 |walltime 4023.910 |
Transformer | epoch 0 | step 16100 |avg loss 9.376 |avg tokens 2199.240 |tokens/s 8898.694 |walltime 4048.624 |
Transformer | epoch 0 | step 16200 |avg loss 9.289 |avg tokens 2190.130 |tokens/s 8684.571 |walltime 4073.843 |
Transformer | epoch 0 | step 16300 |avg loss 9.376 |avg tokens 2179.780 |tokens/s 8736.801 |walltime 4098.792 |
Transformer | epoch 0 | step 16400 |avg loss 9.345 |avg tokens 2165.420 |tokens/s 8689.489 |walltime 4123.712 |
Transformer | epoch 0 | step 16500 |avg loss 9.429 |avg tokens 2196.440 |tokens/s 8882.252 |walltime 4148.441 |
Transformer | epoch 0 | step 16600 |avg loss 9.387 |avg tokens 2169.320 |tokens/s 8810.443 |walltime 4173.063 |
Transformer | epoch 0 | step 16700 |avg loss 9.313 |avg tokens 2230.180 |tokens/s 8835.468 |walltime 4198.304 |
Transformer | epoch 0 | step 16800 |avg loss 9.422 |avg tokens 2173.230 |tokens/s 8702.597 |walltime 4223.276 |
Transformer | epoch 0 | step 16900 |avg loss 9.269 |avg tokens 2240.320 |tokens/s 8785.247 |walltime 4248.777 |
Transformer | epoch 0 | step 17000 |avg loss 9.295 |avg tokens 2135.670 |tokens/s 8608.727 |walltime 4273.586 |
Transformer | epoch 0 | step 17100 |avg loss 9.337 |avg tokens 2174.330 |tokens/s 8708.868 |walltime 4298.552 |
Transformer | epoch 0 | step 17200 |avg loss 9.352 |avg tokens 2119.200 |tokens/s 8525.564 |walltime 4323.409 |
Transformer | epoch 0 | step 17300 |avg loss 9.272 |avg tokens 2199.870 |tokens/s 8809.204 |walltime 4348.382 |
Transformer | epoch 0 | step 17400 |avg loss 9.306 |avg tokens 2199.670 |tokens/s 8743.721 |walltime 4373.539 |
Transformer | epoch 0 | step 17500 |avg loss 9.398 |avg tokens 2152.390 |tokens/s 8670.664 |walltime 4398.363 |
Transformer | epoch 0 | step 17600 |avg loss 9.323 |avg tokens 2160.700 |tokens/s 8771.120 |walltime 4422.997 |
Transformer | epoch 0 | step 17700 |avg loss 9.310 |avg tokens 2252.970 |tokens/s 8901.506 |walltime 4448.307 |
Transformer | epoch 0 | step 17800 |avg loss 9.416 |avg tokens 2103.130 |tokens/s 8645.969 |walltime 4472.632 |
Transformer | epoch 0 | step 17900 |avg loss 9.354 |avg tokens 2144.930 |tokens/s 8693.443 |walltime 4497.305 |
Transformer | epoch 0 | step 18000 |avg loss 9.249 |avg tokens 2165.100 |tokens/s 8693.161 |walltime 4522.211 |
Transformer | epoch 0 | step 18100 |avg loss 9.282 |avg tokens 2193.330 |tokens/s 8848.442 |walltime 4546.998 |
Transformer | epoch 0 | step 18200 |avg loss 9.243 |avg tokens 2234.280 |tokens/s 8821.938 |walltime 4572.325 |
Transformer | epoch 0 | step 18300 |avg loss 9.186 |avg tokens 2155.750 |tokens/s 8639.521 |walltime 4597.277 |
Transformer | epoch 0 | step 18400 |avg loss 9.235 |avg tokens 2224.200 |tokens/s 8800.382 |walltime 4622.551 |
Transformer | epoch 0 | step 18500 |avg loss 9.264 |avg tokens 2176.330 |tokens/s 8807.174 |walltime 4647.262 |
Transformer | epoch 0 | step 18600 |avg loss 9.269 |avg tokens 2202.050 |tokens/s 8781.488 |walltime 4672.338 |
Transformer | epoch 0 | step 18700 |avg loss 9.269 |avg tokens 2144.450 |tokens/s 8718.446 |walltime 4696.935 |
Transformer | epoch 0 | step 18800 |avg loss 9.312 |avg tokens 2138.000 |tokens/s 8652.732 |walltime 4721.644 |
Transformer | epoch 0 | step 18900 |avg loss 9.198 |avg tokens 2220.220 |tokens/s 8736.472 |walltime 4747.057 |
Transformer | epoch 0 | step 19000 |avg loss 9.293 |avg tokens 2218.970 |tokens/s 8916.090 |walltime 4771.944 |
Transformer | epoch 0 | step 19100 |avg loss 9.210 |avg tokens 2126.600 |tokens/s 8546.320 |walltime 4796.827 |
Transformer | epoch 0 | step 19200 |avg loss 9.194 |avg tokens 2170.200 |tokens/s 8700.313 |walltime 4821.771 |
Transformer | epoch 0 | step 19300 |avg loss 9.199 |avg tokens 2159.590 |tokens/s 8577.517 |walltime 4846.949 |
Transformer | epoch 0 | step 19400 |avg loss 9.260 |avg tokens 2185.230 |tokens/s 8761.975 |walltime 4871.888 |
Transformer | epoch 0 | step 19500 |avg loss 9.247 |avg tokens 2190.090 |tokens/s 8857.976 |walltime 4896.613 |
Transformer | epoch 0 | step 19600 |avg loss 9.248 |avg tokens 2227.930 |tokens/s 8763.328 |walltime 4922.036 |
Transformer | epoch 0 | step 19700 |avg loss 9.240 |avg tokens 2125.040 |tokens/s 8543.841 |walltime 4946.909 |
Transformer | epoch 0 | step 19800 |avg loss 9.281 |avg tokens 2155.800 |tokens/s 8690.411 |walltime 4971.715 |
Transformer | epoch 0 | step 19900 |avg loss 9.394 |avg tokens 2122.810 |tokens/s 8780.666 |walltime 4995.891 |
Transformer | epoch 0 | step 20000 |avg loss 9.287 |avg tokens 2177.990 |tokens/s 8676.500 |walltime 5020.993 |
Transformer | epoch 0 | step 20100 |avg loss 9.249 |avg tokens 2149.360 |tokens/s 8675.225 |walltime 5045.769 |
Transformer | epoch 0 | step 20200 |avg loss 9.256 |avg tokens 2194.600 |tokens/s 8791.330 |walltime 5070.732 |
Transformer | epoch 0 | step 20300 |avg loss 9.301 |avg tokens 2170.610 |tokens/s 8746.411 |walltime 5095.550 |
Transformer | epoch 0 | step 20400 |avg loss 9.187 |avg tokens 2135.290 |tokens/s 8563.698 |walltime 5120.484 |
Transformer | epoch 0 | step 20500 |avg loss 9.248 |avg tokens 2180.520 |tokens/s 8693.028 |walltime 5145.567 |
Transformer | epoch 0 | step 20600 |avg loss 9.322 |avg tokens 2234.280 |tokens/s 8926.886 |walltime 5170.596 |
Transformer | epoch 0 | step 20700 |avg loss 9.316 |avg tokens 2135.640 |tokens/s 8756.941 |walltime 5194.984 |
Transformer | epoch 0 | step 20800 |avg loss 9.299 |avg tokens 2235.160 |tokens/s 8908.267 |walltime 5220.075 |
Transformer | epoch 0 | step 20900 |avg loss 9.241 |avg tokens 2173.070 |tokens/s 8730.160 |walltime 5244.966 |
Transformer | epoch 0 | step 21000 |avg loss 9.192 |avg tokens 2180.250 |tokens/s 8799.117 |walltime 5269.744 |
Transformer | epoch 0 | step 21100 |avg loss 9.238 |avg tokens 2200.330 |tokens/s 8804.683 |walltime 5294.735 |
Transformer | epoch 0 | step 21200 |avg loss 9.205 |avg tokens 2182.290 |tokens/s 8769.092 |walltime 5319.621 |
Transformer | epoch 0 | step 21300 |avg loss 9.141 |avg tokens 2245.240 |tokens/s 8817.164 |walltime 5345.085 |
Transformer | epoch 0 | step 21400 |avg loss 9.258 |avg tokens 2154.600 |tokens/s 8725.211 |walltime 5369.779 |
Transformer | epoch 0 | step 21500 |avg loss 9.296 |avg tokens 2189.750 |tokens/s 8830.604 |walltime 5394.577 |
Transformer | epoch 0 | step 21600 |avg loss 9.201 |avg tokens 2196.380 |tokens/s 8798.538 |walltime 5419.540 |
Transformer | epoch 0 | step 21700 |avg loss 9.174 |avg tokens 2191.180 |tokens/s 8711.310 |walltime 5444.693 |
Transformer | epoch 0 | step 21800 |avg loss 9.217 |avg tokens 2221.740 |tokens/s 8849.100 |walltime 5469.800 |
Transformer | epoch 0 | step 21900 |avg loss 9.259 |avg tokens 2160.400 |tokens/s 8730.380 |walltime 5494.546 |
Transformer | epoch 0 | step 22000 |avg loss 9.191 |avg tokens 2251.470 |tokens/s 8944.973 |walltime 5519.716 |
Transformer | epoch 0 | step 22100 |avg loss 9.271 |avg tokens 2137.420 |tokens/s 8733.212 |walltime 5544.191 |
Transformer | epoch 0 | step 22200 |avg loss 9.195 |avg tokens 2137.930 |tokens/s 8674.031 |walltime 5568.838 |
Transformer | epoch 0 | step 22300 |avg loss 9.217 |avg tokens 2219.570 |tokens/s 8896.331 |walltime 5593.787 |
Transformer | epoch 0 | step 22400 |avg loss 9.199 |avg tokens 2216.520 |tokens/s 8780.271 |walltime 5619.032 |
Transformer | epoch 0 | step 22500 |avg loss 9.267 |avg tokens 2149.580 |tokens/s 8705.142 |walltime 5643.725 |
Transformer | epoch 0 | step 22600 |avg loss 9.160 |avg tokens 2132.800 |tokens/s 8662.939 |walltime 5668.345 |
Transformer | epoch 0 | step 22700 |avg loss 9.151 |avg tokens 2229.480 |tokens/s 8889.910 |walltime 5693.423 |
Transformer | epoch 0 | step 22800 |avg loss 9.207 |avg tokens 2117.290 |tokens/s 8537.502 |walltime 5718.223 |
Transformer | epoch 0 | step 22900 |avg loss 9.195 |avg tokens 2183.060 |tokens/s 8691.827 |walltime 5743.340 |
Transformer | epoch 0 | step 23000 |avg loss 9.179 |avg tokens 2198.640 |tokens/s 8710.611 |walltime 5768.581 |
Transformer | epoch 0 | step 23100 |avg loss 9.171 |avg tokens 2210.530 |tokens/s 8878.884 |walltime 5793.477 |
Transformer | epoch 0 | step 23200 |avg loss 9.284 |avg tokens 2139.470 |tokens/s 8803.928 |walltime 5817.778 |
Transformer | epoch 0 | step 23300 |avg loss 9.198 |avg tokens 2208.740 |tokens/s 8799.935 |walltime 5842.878 |
Transformer | epoch 0 | step 23400 |avg loss 9.273 |avg tokens 2176.570 |tokens/s 8797.953 |walltime 5867.617 |
Transformer | epoch 0 | step 23500 |avg loss 9.160 |avg tokens 2155.540 |tokens/s 8493.517 |walltime 5892.996 |
Transformer | epoch 0 | step 23600 |avg loss 9.213 |avg tokens 2206.770 |tokens/s 8722.735 |walltime 5918.295 |
Transformer | epoch 0 | step 23700 |avg loss 9.233 |avg tokens 2111.030 |tokens/s 8643.110 |walltime 5942.719 |
Transformer | epoch 0 | step 23800 |avg loss 9.286 |avg tokens 2223.370 |tokens/s 8853.708 |walltime 5967.832 |
Transformer | epoch 0 | step 23900 |avg loss 9.262 |avg tokens 2181.660 |tokens/s 8721.672 |walltime 5992.846 |
Transformer | epoch 0 | step 24000 |avg loss 9.245 |avg tokens 2095.420 |tokens/s 8572.098 |walltime 6017.291 |
Transformer | epoch 0 | step 24100 |avg loss 9.154 |avg tokens 2163.640 |tokens/s 8622.008 |walltime 6042.385 |
Transformer | epoch 0 | step 24200 |avg loss 9.238 |avg tokens 2185.830 |tokens/s 8676.633 |walltime 6067.577 |
Transformer | epoch 0 | step 24300 |avg loss 9.168 |avg tokens 2175.590 |tokens/s 8633.856 |walltime 6092.776 |
Transformer | epoch 0 | step 24400 |avg loss 9.218 |avg tokens 2151.270 |tokens/s 8791.469 |walltime 6117.246 |
Transformer | epoch 0 | step 24500 |avg loss 9.178 |avg tokens 2171.640 |tokens/s 8753.538 |walltime 6142.054 |
Transformer | epoch 0 | step 24600 |avg loss 9.161 |avg tokens 2267.650 |tokens/s 8979.447 |walltime 6167.308 |
Transformer | epoch 0 | step 24700 |avg loss 9.171 |avg tokens 2140.130 |tokens/s 8577.881 |walltime 6192.257 |
Transformer | epoch 0 | step 24800 |avg loss 9.100 |avg tokens 2220.850 |tokens/s 8787.519 |walltime 6217.530 |
Transformer | epoch 0 | step 24900 |avg loss 9.214 |avg tokens 2188.810 |tokens/s 8713.538 |walltime 6242.650 |
Transformer | epoch 0 | step 25000 |avg loss 9.187 |avg tokens 2169.540 |tokens/s 8765.884 |walltime 6267.400 |
Transformer | epoch 0 | step 25100 |avg loss 9.116 |avg tokens 2212.920 |tokens/s 8585.363 |walltime 6293.175 |
Transformer | epoch 0 | step 25200 |avg loss 9.190 |avg tokens 2235.870 |tokens/s 8872.352 |walltime 6318.376 |
Transformer | epoch 0 | step 25300 |avg loss 9.156 |avg tokens 2207.470 |tokens/s 8767.669 |walltime 6343.553 |
Transformer | epoch 0 | step 25400 |avg loss 9.222 |avg tokens 2198.990 |tokens/s 8737.259 |walltime 6368.721 |
Transformer | epoch 0 | step 25500 |avg loss 9.197 |avg tokens 2170.130 |tokens/s 8700.696 |walltime 6393.663 |
Transformer | epoch 0 | step 25600 |avg loss 9.152 |avg tokens 2204.180 |tokens/s 8743.781 |walltime 6418.872 |
Transformer | epoch 0 | step 25700 |avg loss 9.165 |avg tokens 2170.440 |tokens/s 8695.924 |walltime 6443.831 |
Transformer | epoch 0 | step 25800 |avg loss 9.162 |avg tokens 2263.660 |tokens/s 9008.895 |walltime 6468.958 |
Transformer | epoch 0 | step 25900 |avg loss 9.082 |avg tokens 2284.550 |tokens/s 8828.759 |walltime 6494.834 |
Transformer | epoch 0 | step 26000 |avg loss 9.276 |avg tokens 2155.980 |tokens/s 8742.638 |walltime 6519.495 |
Transformer | epoch 0 | step 26100 |avg loss 9.252 |avg tokens 2245.680 |tokens/s 8343.774 |walltime 6546.409 |
Transformer | epoch 0 | step 26200 |avg loss 9.118 |avg tokens 2182.760 |tokens/s 8566.994 |walltime 6571.888 |
Transformer | epoch 0 | step 26300 |avg loss 9.232 |avg tokens 2181.140 |tokens/s 8774.903 |walltime 6596.744 |
Transformer | epoch 0 | step 26400 |avg loss 9.174 |avg tokens 2226.380 |tokens/s 8880.427 |walltime 6621.815 |
Transformer | epoch 0 | step 26500 |avg loss 9.205 |avg tokens 2181.460 |tokens/s 8683.898 |walltime 6646.936 |
Transformer | epoch 0 | step 26600 |avg loss 9.162 |avg tokens 2216.310 |tokens/s 8877.307 |walltime 6671.902 |
Transformer | epoch 0 | step 26700 |avg loss 9.173 |avg tokens 2157.230 |tokens/s 8645.472 |walltime 6696.854 |
Transformer | epoch 0 | step 26800 |avg loss 9.202 |avg tokens 2157.530 |tokens/s 8691.098 |walltime 6721.678 |
Transformer | epoch 0 | step 26900 |avg loss 9.161 |avg tokens 2198.620 |tokens/s 8767.571 |walltime 6746.755 |
Transformer | epoch 0 | step 27000 |avg loss 9.157 |avg tokens 2155.750 |tokens/s 8581.609 |walltime 6771.876 |
Transformer | epoch 0 | step 27100 |avg loss 9.139 |avg tokens 2231.810 |tokens/s 8784.661 |walltime 6797.282 |
Transformer | epoch 0 | step 27200 |avg loss 9.175 |avg tokens 2173.870 |tokens/s 8751.264 |walltime 6822.122 |
Transformer | epoch 0 | step 27300 |avg loss 9.181 |avg tokens 2212.170 |tokens/s 8797.862 |walltime 6847.267 |
Transformer | epoch 0 | step 27400 |avg loss 9.136 |avg tokens 2167.670 |tokens/s 8657.701 |walltime 6872.304 |
Transformer | epoch 0 | step 27500 |avg loss 9.164 |avg tokens 2146.650 |tokens/s 8663.645 |walltime 6897.082 |
Transformer | epoch 0 | step 27600 |avg loss 9.136 |avg tokens 2231.530 |tokens/s 8792.960 |walltime 6922.460 |
Transformer | epoch 0 | step 27700 |avg loss 9.219 |avg tokens 2162.180 |tokens/s 8708.277 |walltime 6947.289 |
Transformer | epoch 0 | step 27800 |avg loss 9.226 |avg tokens 2164.750 |tokens/s 8672.500 |walltime 6972.250 |
Transformer | epoch 0 | step 27900 |avg loss 9.169 |avg tokens 2174.600 |tokens/s 8692.186 |walltime 6997.268 |
Transformer | epoch 0 | step 28000 |avg loss 9.216 |avg tokens 2241.370 |tokens/s 8877.340 |walltime 7022.517 |
Transformer | epoch 0 | step 28100 |avg loss 9.144 |avg tokens 2160.230 |tokens/s 8739.394 |walltime 7047.235 |
Transformer | epoch 0 | step 28200 |avg loss 9.144 |avg tokens 2256.680 |tokens/s 8994.859 |walltime 7072.323 |
Transformer | epoch 0 | step 28300 |avg loss 9.200 |avg tokens 2201.060 |tokens/s 7426.657 |walltime 7101.961 |
Transformer | epoch 0 | step 28400 |avg loss 9.116 |avg tokens 2207.380 |tokens/s 7347.086 |walltime 7132.005 |
Transformer | epoch 0 | step 28500 |avg loss 9.144 |avg tokens 2148.680 |tokens/s 8701.583 |walltime 7156.698 |
Transformer | epoch 0 | step 28600 |avg loss 9.149 |avg tokens 2121.740 |tokens/s 7659.539 |walltime 7184.399 |
Transformer | epoch 0 | step 28700 |avg loss 9.188 |avg tokens 2198.280 |tokens/s 8174.611 |walltime 7211.290 |
Transformer | epoch 0 | step 28800 |avg loss 9.167 |avg tokens 2221.340 |tokens/s 8850.237 |walltime 7236.389 |
Transformer | epoch 0 | step 28900 |avg loss 9.203 |avg tokens 2081.060 |tokens/s 8529.194 |walltime 7260.789 |
Transformer | epoch 0 | step 29000 |avg loss 9.139 |avg tokens 2198.780 |tokens/s 8762.613 |walltime 7285.881 |
Transformer | epoch 0 | step 29100 |avg loss 9.186 |avg tokens 2214.620 |tokens/s 8903.293 |walltime 7310.756 |
Transformer | epoch 0 | step 29200 |avg loss 9.238 |avg tokens 2164.870 |tokens/s 8713.245 |walltime 7335.601 |
Transformer | epoch 0 | step 29300 |avg loss 9.135 |avg tokens 2240.520 |tokens/s 8802.572 |walltime 7361.054 |
Transformer | epoch 0 | step 29400 |avg loss 9.105 |avg tokens 2227.260 |tokens/s 8764.536 |walltime 7386.467 |
Transformer | epoch 0 | step 29500 |avg loss 9.195 |avg tokens 2150.440 |tokens/s 8730.353 |walltime 7411.098 |
Transformer | epoch 0 | step 29600 |avg loss 9.136 |avg tokens 2168.860 |tokens/s 8661.505 |walltime 7436.139 |
Transformer | epoch 0 | step 29700 |avg loss 9.184 |avg tokens 2155.290 |tokens/s 8765.724 |walltime 7460.726 |
Transformer | epoch 0 | step 29800 |avg loss 9.170 |avg tokens 2166.030 |tokens/s 8648.378 |walltime 7485.772 |
Transformer | epoch 0 | step 29900 |avg loss 9.194 |avg tokens 2147.140 |tokens/s 8721.349 |walltime 7510.391 |
Transformer | epoch 0 | step 30000 |avg loss 9.124 |avg tokens 2202.310 |tokens/s 8736.150 |walltime 7535.600 |
Transformer | epoch 0 | step 30100 |avg loss 9.220 |avg tokens 2158.440 |tokens/s 8704.529 |walltime 7560.397 |
Transformer | epoch 0 | step 30200 |avg loss 9.051 |avg tokens 2252.100 |tokens/s 8809.361 |walltime 7585.962 |
Transformer | epoch 0 | step 30300 |avg loss 9.267 |avg tokens 2179.460 |tokens/s 8779.623 |walltime 7610.786 |
Transformer | epoch 0 | step 30400 |avg loss 9.251 |avg tokens 2180.070 |tokens/s 8780.699 |walltime 7635.614 |
Transformer | epoch 0 | step 30500 |avg loss 9.215 |avg tokens 2206.060 |tokens/s 8823.907 |walltime 7660.615 |
Transformer | epoch 0 | step 30600 |avg loss 9.148 |avg tokens 2213.330 |tokens/s 8804.526 |walltime 7685.753 |
Transformer | epoch 0 | step 30700 |avg loss 9.204 |avg tokens 2204.000 |tokens/s 8881.025 |walltime 7710.570 |
Transformer | epoch 0 | step 30800 |avg loss 9.194 |avg tokens 2183.680 |tokens/s 8839.253 |walltime 7735.275 |
Transformer | epoch 0 | step 30900 |avg loss 9.168 |avg tokens 2142.380 |tokens/s 8682.988 |walltime 7759.948 |
Transformer | epoch 0 | step 31000 |avg loss 9.214 |avg tokens 2161.510 |tokens/s 8682.615 |walltime 7784.843 |
Transformer | epoch 0 | step 31100 |avg loss 9.150 |avg tokens 2113.410 |tokens/s 8482.704 |walltime 7809.757 |
Transformer | epoch 0 | step 31200 |avg loss 9.226 |avg tokens 2251.750 |tokens/s 8931.264 |walltime 7834.969 |
Transformer | epoch 0 | step 31300 |avg loss 9.145 |avg tokens 2156.130 |tokens/s 8592.941 |walltime 7860.061 |
Transformer | epoch 0 | step 31400 |avg loss 9.169 |avg tokens 2205.830 |tokens/s 8855.293 |walltime 7884.971 |
Transformer | epoch 0 | step 31500 |avg loss 9.161 |avg tokens 2176.690 |tokens/s 8743.060 |walltime 7909.867 |
Transformer | epoch 0 | step 31600 |avg loss 9.142 |avg tokens 2187.740 |tokens/s 8798.543 |walltime 7934.732 |
Transformer | epoch 0 | step 31700 |avg loss 9.119 |avg tokens 2239.080 |tokens/s 8930.458 |walltime 7959.804 |
Transformer | epoch 0 | step 31800 |avg loss 9.125 |avg tokens 2217.910 |tokens/s 8886.681 |walltime 7984.762 |
Transformer | epoch 0 | step 31900 |avg loss 9.159 |avg tokens 2167.010 |tokens/s 8782.949 |walltime 8009.435 |
Transformer | epoch 0 | step 32000 |avg loss 9.147 |avg tokens 2156.590 |tokens/s 8593.017 |walltime 8034.532 |
Transformer | epoch 0 | step 32100 |avg loss 9.221 |avg tokens 2183.450 |tokens/s 8699.369 |walltime 8059.631 |
Transformer | epoch 0 | step 32200 |avg loss 9.054 |avg tokens 2196.480 |tokens/s 8767.217 |walltime 8084.684 |
Transformer | epoch 0 | step 32300 |avg loss 9.142 |avg tokens 2142.560 |tokens/s 8703.832 |walltime 8109.300 |
Transformer | epoch 0 | step 32400 |avg loss 9.202 |avg tokens 2102.950 |tokens/s 8639.408 |walltime 8133.642 |
Transformer | epoch 0 | step 32500 |avg loss 9.122 |avg tokens 2203.540 |tokens/s 8812.225 |walltime 8158.647 |
Transformer | epoch 0 | step 32600 |avg loss 9.243 |avg tokens 2110.850 |tokens/s 8600.165 |walltime 8183.191 |
Transformer | epoch 0 | step 32700 |avg loss 9.122 |avg tokens 2232.670 |tokens/s 8752.105 |walltime 8208.702 |
Transformer | epoch 0 | step 32800 |avg loss 9.110 |avg tokens 2199.080 |tokens/s 8826.482 |walltime 8233.616 |
Transformer | epoch 0 | step 32900 |avg loss 9.090 |avg tokens 2170.500 |tokens/s 7684.651 |walltime 8261.861 |
Transformer | epoch 0 | step 33000 |avg loss 9.120 |avg tokens 2199.210 |tokens/s 8823.554 |walltime 8286.785 |
Transformer | epoch 0 | step 33100 |avg loss 9.116 |avg tokens 2191.750 |tokens/s 8690.760 |walltime 8312.004 |
Transformer | epoch 0 | step 33200 |avg loss 9.163 |avg tokens 2211.430 |tokens/s 8911.129 |walltime 8336.821 |
Transformer | epoch 0 | step 33300 |avg loss 9.222 |avg tokens 2202.950 |tokens/s 8881.507 |walltime 8361.625 |
Transformer | epoch 0 | step 33400 |avg loss 9.095 |avg tokens 2206.690 |tokens/s 8720.782 |walltime 8386.928 |
Transformer | epoch 0 | step 33500 |avg loss 9.083 |avg tokens 2187.550 |tokens/s 8704.945 |walltime 8412.058 |
Transformer | epoch 0 | step 33600 |avg loss 9.094 |avg tokens 2225.310 |tokens/s 8814.920 |walltime 8437.303 |
Transformer | epoch 0 | step 33700 |avg loss 9.155 |avg tokens 2196.780 |tokens/s 8771.325 |walltime 8462.348 |
Transformer | epoch 0 | step 33800 |avg loss 9.157 |avg tokens 2135.270 |tokens/s 8610.286 |walltime 8487.147 |
Transformer | epoch 0 | step 33900 |avg loss 9.061 |avg tokens 2211.210 |tokens/s 8758.641 |walltime 8512.393 |
Transformer | epoch 0 | step 34000 |avg loss 9.046 |avg tokens 2288.490 |tokens/s 9014.934 |walltime 8537.779 |
Transformer | epoch 0 | step 34100 |avg loss 9.052 |avg tokens 2159.780 |tokens/s 8607.914 |walltime 8562.870 |
Transformer | epoch 0 | step 34200 |avg loss 9.166 |avg tokens 2243.000 |tokens/s 8970.024 |walltime 8587.875 |
Transformer | epoch 0 | step 34300 |avg loss 9.115 |avg tokens 2114.250 |tokens/s 8556.915 |walltime 8612.583 |
Transformer | epoch 0 | step 34400 |avg loss 9.190 |avg tokens 2184.300 |tokens/s 8719.592 |walltime 8637.634 |
Transformer | epoch 0 | step 34500 |avg loss 9.116 |avg tokens 2172.790 |tokens/s 8697.323 |walltime 8662.616 |
Transformer | epoch 0 | step 34600 |avg loss 9.127 |avg tokens 2072.270 |tokens/s 8419.922 |walltime 8687.227 |
Transformer | epoch 0 | step 34700 |avg loss 9.114 |avg tokens 2223.300 |tokens/s 8843.011 |walltime 8712.369 |
Transformer | epoch 0 | step 34800 |avg loss 9.024 |avg tokens 2213.610 |tokens/s 8858.817 |walltime 8737.357 |
Transformer | epoch 0 | step 34900 |avg loss 9.082 |avg tokens 2188.850 |tokens/s 8481.510 |walltime 8763.164 |
Transformer | epoch 0 | step 35000 |avg loss 9.166 |avg tokens 2174.790 |tokens/s 5754.149 |walltime 8800.959 |
Transformer | epoch 0 | step 35100 |avg loss 9.179 |avg tokens 2174.230 |tokens/s 5273.832 |walltime 8842.186 |
Transformer | epoch 0 | step 35200 |avg loss 9.107 |avg tokens 2129.570 |tokens/s 3474.614 |walltime 8903.476 |
Transformer | epoch 0 | step 35300 |avg loss 9.157 |avg tokens 2192.040 |tokens/s 3526.720 |walltime 8965.631 |
Transformer | epoch 0 | step 35400 |avg loss 9.129 |avg tokens 2180.660 |tokens/s 3501.769 |walltime 9027.904 |
Transformer | epoch 0 | step 35500 |avg loss 9.126 |avg tokens 2134.240 |tokens/s 3432.451 |walltime 9090.082 |
Transformer | epoch 0 | step 35600 |avg loss 9.050 |avg tokens 2127.520 |tokens/s 3395.842 |walltime 9152.733 |
Transformer | epoch 0 | step 35700 |avg loss 9.160 |avg tokens 2189.090 |tokens/s 3500.155 |walltime 9215.276 |
Transformer | epoch 0 | step 35800 |avg loss 9.068 |avg tokens 2182.660 |tokens/s 3454.026 |walltime 9278.467 |
Transformer | epoch 0 | step 35900 |avg loss 9.122 |avg tokens 2165.600 |tokens/s 3489.096 |walltime 9340.535 |
Transformer | epoch 0 | step 36000 |avg loss 9.123 |avg tokens 2165.030 |tokens/s 3436.951 |walltime 9403.528 |
Transformer | epoch 0 | step 36100 |avg loss 9.129 |avg tokens 2247.150 |tokens/s 3547.949 |walltime 9466.864 |
Transformer | epoch 0 | step 36200 |avg loss 9.156 |avg tokens 2187.040 |tokens/s 3491.026 |walltime 9529.512 |
Transformer | epoch 0 | step 36300 |avg loss 9.134 |avg tokens 2212.310 |tokens/s 3515.331 |walltime 9592.445 |
Transformer | epoch 0 | step 36400 |avg loss 9.222 |avg tokens 2161.000 |tokens/s 6816.196 |walltime 9624.149 |
Transformer | epoch 0 | step 36500 |avg loss 9.163 |avg tokens 2210.620 |tokens/s 8714.971 |walltime 9649.515 |
Transformer | epoch 0 | step 36600 |avg loss 9.170 |avg tokens 2152.750 |tokens/s 8502.842 |walltime 9674.833 |
Transformer | epoch 0 | step 36700 |avg loss 9.218 |avg tokens 2128.710 |tokens/s 8588.524 |walltime 9699.618 |
Transformer | epoch 0 | step 36800 |avg loss 9.165 |avg tokens 2189.620 |tokens/s 8716.077 |walltime 9724.740 |
Transformer | epoch 0 | step 36900 |avg loss 9.202 |avg tokens 2194.190 |tokens/s 8795.639 |walltime 9749.686 |
Transformer | epoch 0 | step 37000 |avg loss 9.184 |avg tokens 2152.210 |tokens/s 8568.860 |walltime 9774.803 |
Transformer | epoch 0 | step 37100 |avg loss 9.078 |avg tokens 2214.450 |tokens/s 8674.956 |walltime 9800.330 |
Transformer | epoch 0 | step 37200 |avg loss 9.287 |avg tokens 2082.050 |tokens/s 8548.109 |walltime 9824.687 |
Transformer | epoch 0 | step 37300 |avg loss 9.162 |avg tokens 2247.580 |tokens/s 8954.047 |walltime 9849.788 |
Transformer | epoch 0 | step 37400 |avg loss 9.244 |avg tokens 2178.180 |tokens/s 8691.837 |walltime 9874.848 |
Transformer | epoch 0 | step 37500 |avg loss 9.158 |avg tokens 2141.280 |tokens/s 8599.468 |walltime 9899.748 |
Transformer | epoch 0 | step 37600 |avg loss 9.285 |avg tokens 2136.610 |tokens/s 8695.386 |walltime 9924.320 |
Transformer | epoch 0 | step 37700 |avg loss 9.131 |avg tokens 2246.130 |tokens/s 8839.814 |walltime 9949.729 |
Transformer | epoch 0 | step 37800 |avg loss 9.230 |avg tokens 2184.980 |tokens/s 8698.175 |walltime 9974.849 |
Transformer | epoch 0 | step 37900 |avg loss 9.166 |avg tokens 2203.170 |tokens/s 8705.709 |walltime 10000.156 |
Transformer | epoch 0 | step 38000 |avg loss 9.181 |avg tokens 2229.520 |tokens/s 8819.036 |walltime 10025.437 |
Transformer | epoch 0 | step 38100 |avg loss 9.165 |avg tokens 2207.890 |tokens/s 8690.648 |walltime 10050.843 |
Transformer | epoch 0 | step 38200 |avg loss 9.255 |avg tokens 2165.720 |tokens/s 8786.300 |walltime 10075.491 |
Transformer | epoch 0 | step 38300 |avg loss 9.186 |avg tokens 2229.170 |tokens/s 8823.419 |walltime 10100.756 |
Transformer | epoch 0 | step 38400 |avg loss 9.167 |avg tokens 2141.520 |tokens/s 8556.148 |walltime 10125.785 |
Transformer | epoch 0 | step 38500 |avg loss 9.155 |avg tokens 2231.340 |tokens/s 8835.217 |walltime 10151.040 |
Transformer | epoch 0 | step 38600 |avg loss 9.168 |avg tokens 2168.010 |tokens/s 8641.653 |walltime 10176.128 |
Transformer | epoch 0 | step 38700 |avg loss 9.163 |avg tokens 2199.470 |tokens/s 8815.460 |walltime 10201.078 |
Transformer | epoch 0 | step 38800 |avg loss 9.069 |avg tokens 2281.530 |tokens/s 8960.203 |walltime 10226.541 |
Transformer | epoch 0 | step 38900 |avg loss 9.125 |avg tokens 2211.910 |tokens/s 8833.697 |walltime 10251.580 |
Transformer | epoch 0 | step 39000 |avg loss 9.082 |avg tokens 2251.470 |tokens/s 8838.365 |walltime 10277.054 |
Transformer | epoch 0 | step 39100 |avg loss 9.057 |avg tokens 2198.380 |tokens/s 8744.930 |walltime 10302.193 |
Transformer | epoch 0 | step 39200 |avg loss 9.173 |avg tokens 2137.790 |tokens/s 8742.107 |walltime 10326.647 |
Transformer | epoch 0 | step 39300 |avg loss 9.160 |avg tokens 2221.030 |tokens/s 8861.719 |walltime 10351.710 |
Transformer | epoch 0 | step 39400 |avg loss 9.169 |avg tokens 2219.470 |tokens/s 8913.230 |walltime 10376.611 |
Transformer | epoch 0 | step 39500 |avg loss 9.183 |avg tokens 2199.030 |tokens/s 8803.850 |walltime 10401.589 |
Transformer | epoch 0 | step 39600 |avg loss 9.139 |avg tokens 2131.450 |tokens/s 8634.837 |walltime 10426.273 |
Transformer | epoch 0 | step 39700 |avg loss 9.093 |avg tokens 2202.600 |tokens/s 8718.953 |walltime 10451.535 |
Transformer | epoch 0 | step 39800 |avg loss 9.094 |avg tokens 2234.710 |tokens/s 8870.650 |walltime 10476.728 |
Transformer | epoch 0 | step 39900 |avg loss 9.124 |avg tokens 2152.380 |tokens/s 8561.989 |walltime 10501.866 |
Transformer | epoch 0 | step 40000 |avg loss 9.163 |avg tokens 2207.310 |tokens/s 8783.542 |walltime 10526.997 |
Transformer | epoch 0 | step 40100 |avg loss 9.115 |avg tokens 2233.270 |tokens/s 8866.832 |walltime 10552.183 |
Transformer | epoch 0 | step 40200 |avg loss 9.085 |avg tokens 2183.380 |tokens/s 8721.818 |walltime 10577.217 |
Transformer | epoch 0 | step 40300 |avg loss 9.133 |avg tokens 2200.450 |tokens/s 8805.300 |walltime 10602.207 |
Transformer | epoch 0 | step 40400 |avg loss 9.113 |avg tokens 2234.360 |tokens/s 8941.413 |walltime 10627.196 |
Transformer | epoch 0 | step 40500 |avg loss 9.015 |avg tokens 2184.640 |tokens/s 8684.848 |walltime 10652.350 |
Transformer | epoch 0 | step 40600 |avg loss 9.057 |avg tokens 2214.840 |tokens/s 8750.756 |walltime 10677.661 |
Transformer | epoch 0 | step 40700 |avg loss 9.054 |avg tokens 2115.330 |tokens/s 8580.834 |walltime 10702.313 |
Transformer | epoch 0 | step 40800 |avg loss 9.047 |avg tokens 2202.130 |tokens/s 8729.979 |walltime 10727.537 |
Transformer | epoch 0 | step 40900 |avg loss 9.141 |avg tokens 2205.360 |tokens/s 8783.319 |walltime 10752.646 |
Transformer | epoch 0 | step 41000 |avg loss 9.100 |avg tokens 2197.690 |tokens/s 8747.977 |walltime 10777.768 |
Transformer | epoch 0 | step 41100 |avg loss 9.075 |avg tokens 2194.250 |tokens/s 8730.492 |walltime 10802.901 |
Transformer | epoch 0 | step 41200 |avg loss 9.060 |avg tokens 2121.510 |tokens/s 8541.075 |walltime 10827.740 |
Transformer | epoch 0 | step 41300 |avg loss 9.140 |avg tokens 2182.210 |tokens/s 8819.081 |walltime 10852.484 |
Transformer | epoch 0 | step 41400 |avg loss 9.142 |avg tokens 2120.470 |tokens/s 8635.927 |walltime 10877.039 |
Transformer | epoch 0 | step 41500 |avg loss 9.145 |avg tokens 2196.710 |tokens/s 8866.797 |walltime 10901.813 |
Transformer | epoch 0 | step 41600 |avg loss 9.051 |avg tokens 2162.280 |tokens/s 8751.064 |walltime 10926.522 |
Transformer | epoch 0 | step 41700 |avg loss 9.037 |avg tokens 2119.910 |tokens/s 8559.685 |walltime 10951.288 |
Transformer | epoch 0 | step 41800 |avg loss 9.116 |avg tokens 2149.080 |tokens/s 8696.624 |walltime 10976.000 |
Transformer | epoch 0 | step 41900 |avg loss 9.096 |avg tokens 2196.660 |tokens/s 8795.088 |walltime 11000.976 |
Transformer | epoch 0 | step 42000 |avg loss 9.130 |avg tokens 2136.480 |tokens/s 8656.893 |walltime 11025.655 |
Transformer | epoch 0 | step 42100 |avg loss 9.085 |avg tokens 2185.600 |tokens/s 8719.182 |walltime 11050.722 |
Transformer | epoch 0 | step 42200 |avg loss 9.072 |avg tokens 2184.950 |tokens/s 8777.138 |walltime 11075.616 |
Transformer | epoch 0 | step 42300 |avg loss 9.084 |avg tokens 2256.730 |tokens/s 8951.976 |walltime 11100.825 |
Transformer | epoch 0 | step 42400 |avg loss 9.081 |avg tokens 2199.640 |tokens/s 8680.328 |walltime 11126.165 |
Transformer | epoch 0 | step 42500 |avg loss 9.010 |avg tokens 2257.330 |tokens/s 8899.609 |walltime 11151.530 |
Transformer | epoch 0 | step 42600 |avg loss 9.072 |avg tokens 2140.480 |tokens/s 8557.872 |walltime 11176.542 |
Transformer | epoch 0 | step 42700 |avg loss 9.059 |avg tokens 2230.700 |tokens/s 8768.696 |walltime 11201.981 |
Transformer | epoch 0 | step 42800 |avg loss 9.165 |avg tokens 2111.100 |tokens/s 8676.644 |walltime 11226.312 |
Transformer | epoch 0 | step 42900 |avg loss 9.171 |avg tokens 2227.580 |tokens/s 8824.001 |walltime 11251.556 |
Transformer | epoch 0 | step 43000 |avg loss 9.060 |avg tokens 2166.130 |tokens/s 8667.087 |walltime 11276.549 |
Transformer | epoch 0 | step 43100 |avg loss 9.044 |avg tokens 2230.950 |tokens/s 8788.225 |walltime 11301.935 |
Transformer | epoch 0 | step 43200 |avg loss 9.129 |avg tokens 2190.120 |tokens/s 8709.003 |walltime 11327.082 |
Transformer | epoch 0 | step 43300 |avg loss 9.080 |avg tokens 2088.120 |tokens/s 8407.470 |walltime 11351.919 |
Transformer | epoch 0 | step 43400 |avg loss 9.140 |avg tokens 2231.360 |tokens/s 8910.422 |walltime 11376.961 |
Transformer | epoch 0 | step 43500 |avg loss 9.119 |avg tokens 2196.680 |tokens/s 8794.235 |walltime 11401.940 |
Transformer | epoch 0 | step 43600 |avg loss 9.085 |avg tokens 2248.540 |tokens/s 8990.346 |walltime 11426.950 |
Transformer | epoch 0 | step 43700 |avg loss 9.016 |avg tokens 2200.860 |tokens/s 8803.699 |walltime 11451.949 |
Transformer | epoch 0 | step 43800 |avg loss 9.053 |avg tokens 2222.330 |tokens/s 8714.549 |walltime 11477.451 |
Transformer | epoch 0 | step 43900 |avg loss 9.108 |avg tokens 2173.190 |tokens/s 8727.815 |walltime 11502.350 |
Transformer | epoch 0 | step 44000 |avg loss 9.121 |avg tokens 2123.560 |tokens/s 8596.054 |walltime 11527.054 |
Transformer | epoch 0 | step 44100 |avg loss 9.123 |avg tokens 2192.020 |tokens/s 8826.728 |walltime 11551.888 |
Transformer | epoch 0 | step 44200 |avg loss 9.044 |avg tokens 2157.170 |tokens/s 8569.017 |walltime 11577.062 |
Transformer | epoch 0 | step 44300 |avg loss 9.146 |avg tokens 2076.230 |tokens/s 8422.058 |walltime 11601.715 |
Transformer | epoch 0 | step 44400 |avg loss 9.032 |avg tokens 2193.290 |tokens/s 8727.390 |walltime 11626.846 |
Transformer | epoch 0 | step 44500 |avg loss 9.110 |avg tokens 2217.440 |tokens/s 8971.246 |walltime 11651.563 |
Transformer | epoch 0 | step 44600 |avg loss 9.087 |avg tokens 2181.150 |tokens/s 8795.966 |walltime 11676.360 |
Transformer | epoch 0 | step 44700 |avg loss 9.045 |avg tokens 2090.340 |tokens/s 8508.003 |walltime 11700.929 |
Transformer | epoch 0 | step 44800 |avg loss 9.060 |avg tokens 2214.670 |tokens/s 8771.635 |walltime 11726.177 |
Transformer | epoch 0 | step 44900 |avg loss 9.053 |avg tokens 2142.970 |tokens/s 8622.818 |walltime 11751.030 |
Transformer | epoch 0 | step 45000 |avg loss 9.094 |avg tokens 2183.610 |tokens/s 8756.204 |walltime 11775.967 |
Transformer | epoch 0 | step 45100 |avg loss 9.080 |avg tokens 2202.880 |tokens/s 8788.909 |walltime 11801.032 |
Transformer | epoch 0 | step 45200 |avg loss 9.009 |avg tokens 2159.780 |tokens/s 8615.696 |walltime 11826.100 |
Transformer | epoch 0 | step 45300 |avg loss 9.059 |avg tokens 2108.480 |tokens/s 8621.182 |walltime 11850.557 |
Transformer | epoch 0 | step 45400 |avg loss 9.072 |avg tokens 2183.940 |tokens/s 8667.402 |walltime 11875.754 |
Transformer | epoch 0 | step 45500 |avg loss 9.006 |avg tokens 2238.080 |tokens/s 8843.943 |walltime 11901.060 |
Transformer | epoch 0 | step 45600 |avg loss 9.125 |avg tokens 2162.080 |tokens/s 8772.520 |walltime 11925.706 |
Transformer | epoch 0 | step 45700 |avg loss 9.019 |avg tokens 2215.720 |tokens/s 8746.868 |walltime 11951.038 |
Transformer | epoch 0 | step 45800 |avg loss 8.989 |avg tokens 2242.880 |tokens/s 8834.437 |walltime 11976.426 |
Transformer | epoch 0 | step 45900 |avg loss 9.070 |avg tokens 2158.240 |tokens/s 8644.707 |walltime 12001.392 |
Transformer | epoch 0 | step 46000 |avg loss 9.094 |avg tokens 2053.660 |tokens/s 8480.783 |walltime 12025.607 |
Transformer | epoch 0 | step 46100 |avg loss 9.039 |avg tokens 2225.720 |tokens/s 8822.357 |walltime 12050.835 |
Transformer | epoch 0 | step 46200 |avg loss 9.131 |avg tokens 2192.580 |tokens/s 8850.315 |walltime 12075.610 |
Transformer | epoch 0 | step 46300 |avg loss 9.103 |avg tokens 2167.080 |tokens/s 8570.064 |walltime 12100.896 |
Transformer | epoch 0 | step 46400 |avg loss 9.109 |avg tokens 2142.670 |tokens/s 8706.398 |walltime 12125.506 |
Transformer | epoch 0 | step 46500 |avg loss 9.156 |avg tokens 2158.000 |tokens/s 8684.733 |walltime 12150.355 |
Transformer | epoch 0 | step 46600 |avg loss 9.049 |avg tokens 2189.810 |tokens/s 8721.827 |walltime 12175.462 |
Transformer | epoch 0 | step 46700 |avg loss 9.020 |avg tokens 2144.490 |tokens/s 8568.281 |walltime 12200.490 |
Transformer | epoch 0 | step 46800 |avg loss 9.062 |avg tokens 2093.850 |tokens/s 8547.000 |walltime 12224.988 |
Transformer | epoch 0 | step 46900 |avg loss 9.118 |avg tokens 2168.970 |tokens/s 8699.009 |walltime 12249.922 |
Transformer | epoch 0 | step 47000 |avg loss 9.107 |avg tokens 2202.710 |tokens/s 8791.241 |walltime 12274.978 |
Transformer | epoch 0 | step 47100 |avg loss 8.919 |avg tokens 2170.870 |tokens/s 8680.484 |walltime 12299.986 |
Transformer | epoch 0 | step 47200 |avg loss 9.042 |avg tokens 2234.810 |tokens/s 8847.033 |walltime 12325.247 |
Transformer | epoch 0 | step 47300 |avg loss 9.089 |avg tokens 2147.300 |tokens/s 8672.478 |walltime 12350.007 |
Transformer | epoch 0 | step 47400 |avg loss 9.058 |avg tokens 2170.460 |tokens/s 8659.288 |walltime 12375.072 |
Transformer | epoch 0 | step 47500 |avg loss 9.008 |avg tokens 2193.110 |tokens/s 8722.359 |walltime 12400.215 |
Transformer | epoch 0 | step 47600 |avg loss 9.095 |avg tokens 2229.470 |tokens/s 8893.975 |walltime 12425.282 |
Transformer | epoch 0 | step 47700 |avg loss 9.007 |avg tokens 2285.100 |tokens/s 8905.699 |walltime 12450.941 |
Transformer | epoch 0 | step 47800 |avg loss 9.046 |avg tokens 2176.770 |tokens/s 8592.915 |walltime 12476.273 |
Transformer | epoch 0 | step 47900 |avg loss 9.128 |avg tokens 2151.510 |tokens/s 8744.878 |walltime 12500.877 |
Transformer | epoch 0 | step 48000 |avg loss 9.088 |avg tokens 2195.140 |tokens/s 8825.152 |walltime 12525.750 |
Transformer | epoch 0 | step 48100 |avg loss 9.132 |avg tokens 2236.200 |tokens/s 8905.087 |walltime 12550.862 |
Transformer | epoch 0 | step 48200 |avg loss 9.027 |avg tokens 2148.430 |tokens/s 8588.543 |walltime 12575.877 |
Transformer | epoch 0 | step 48300 |avg loss 9.120 |avg tokens 2099.990 |tokens/s 8681.530 |walltime 12600.066 |
Transformer | epoch 0 | step 48400 |avg loss 9.097 |avg tokens 2243.190 |tokens/s 8888.067 |walltime 12625.304 |
Transformer | epoch 0 | step 48500 |avg loss 9.015 |avg tokens 2143.460 |tokens/s 8646.547 |walltime 12650.094 |
Transformer | epoch 0 | step 48600 |avg loss 9.059 |avg tokens 2149.920 |tokens/s 8606.388 |walltime 12675.075 |
Transformer | epoch 0 | step 48700 |avg loss 9.058 |avg tokens 2121.720 |tokens/s 8715.977 |walltime 12699.417 |
Transformer | epoch 0 | step 48800 |avg loss 9.003 |avg tokens 2263.620 |tokens/s 8860.874 |walltime 12724.964 |
Transformer | epoch 0 | step 48900 |avg loss 9.060 |avg tokens 2149.610 |tokens/s 8654.170 |walltime 12749.803 |
Transformer | epoch 0 | step 49000 |avg loss 9.053 |avg tokens 2218.250 |tokens/s 8893.470 |walltime 12774.745 |
Transformer | epoch 0 | step 49100 |avg loss 9.028 |avg tokens 2149.730 |tokens/s 8655.367 |walltime 12799.582 |
Transformer | epoch 0 | step 49200 |avg loss 9.096 |avg tokens 2142.600 |tokens/s 8770.195 |walltime 12824.013 |
Transformer | epoch 0 | step 49300 |avg loss 9.111 |avg tokens 2119.730 |tokens/s 8603.207 |walltime 12848.651 |
Transformer | epoch 0 | step 49400 |avg loss 9.041 |avg tokens 2201.070 |tokens/s 8789.937 |walltime 12873.692 |
Transformer | epoch 0 | step 49500 |avg loss 8.990 |avg tokens 2168.580 |tokens/s 8583.145 |walltime 12898.958 |
Transformer | epoch 0 | step 49600 |avg loss 9.006 |avg tokens 2188.110 |tokens/s 8686.986 |walltime 12924.146 |
Transformer | epoch 0 | step 49700 |avg loss 9.019 |avg tokens 2225.290 |tokens/s 8739.448 |walltime 12949.609 |
Transformer | epoch 0 | step 49800 |avg loss 9.054 |avg tokens 2188.540 |tokens/s 8710.503 |walltime 12974.734 |
Transformer | epoch 0 | step 49900 |avg loss 9.024 |avg tokens 2183.370 |tokens/s 8738.057 |walltime 12999.721 |
Transformer | epoch 0 | step 50000 |avg loss 8.992 |avg tokens 2211.900 |tokens/s 8849.934 |walltime 13024.714 |
Transformer | epoch 0 | step 50100 |avg loss 9.097 |avg tokens 2132.540 |tokens/s 8716.661 |walltime 13049.179 |
Transformer | epoch 0 | step 50200 |avg loss 9.010 |avg tokens 2174.400 |tokens/s 8764.532 |walltime 13073.989 |
Transformer | epoch 0 | step 50300 |avg loss 9.014 |avg tokens 2174.690 |tokens/s 8791.801 |walltime 13098.724 |
Transformer | epoch 0 | step 50400 |avg loss 9.044 |avg tokens 2147.530 |tokens/s 8602.980 |walltime 13123.687 |
Transformer | epoch 0 | step 50500 |avg loss 9.051 |avg tokens 2191.820 |tokens/s 8818.490 |walltime 13148.541 |
Transformer | epoch 0 | step 50600 |avg loss 9.066 |avg tokens 2193.300 |tokens/s 8762.872 |walltime 13173.571 |
Transformer | epoch 0 | step 50700 |avg loss 9.057 |avg tokens 2261.140 |tokens/s 8988.812 |walltime 13198.726 |
Transformer | epoch 0 | step 50800 |avg loss 9.036 |avg tokens 2215.040 |tokens/s 8859.424 |walltime 13223.728 |
Transformer | epoch 0 | step 50900 |avg loss 9.063 |avg tokens 2192.930 |tokens/s 8754.313 |walltime 13248.778 |
Transformer | epoch 0 | step 51000 |avg loss 9.011 |avg tokens 2191.750 |tokens/s 8699.089 |walltime 13273.973 |
Transformer | epoch 0 | step 51100 |avg loss 9.044 |avg tokens 2150.000 |tokens/s 8651.270 |walltime 13298.825 |
Transformer | epoch 0 | step 51200 |avg loss 9.047 |avg tokens 2133.410 |tokens/s 8602.678 |walltime 13323.624 |
Transformer | epoch 0 | step 51300 |avg loss 9.049 |avg tokens 2195.380 |tokens/s 8761.588 |walltime 13348.681 |
Transformer | epoch 0 | step 51400 |avg loss 8.947 |avg tokens 2241.250 |tokens/s 8795.332 |walltime 13374.163 |
Transformer | epoch 0 | step 51500 |avg loss 9.052 |avg tokens 2131.320 |tokens/s 8560.906 |walltime 13399.059 |
Transformer | epoch 0 | step 51600 |avg loss 9.004 |avg tokens 2192.540 |tokens/s 8722.344 |walltime 13424.196 |
Transformer | epoch 0 | step 51700 |avg loss 9.069 |avg tokens 2175.540 |tokens/s 8691.255 |walltime 13449.228 |
Transformer | epoch 0 | step 51800 |avg loss 9.026 |avg tokens 2136.590 |tokens/s 8659.566 |walltime 13473.901 |
Transformer | epoch 0 | step 51900 |avg loss 9.004 |avg tokens 2127.030 |tokens/s 8643.556 |walltime 13498.509 |
Transformer | epoch 0 | step 52000 |avg loss 9.125 |avg tokens 2090.900 |tokens/s 8597.284 |walltime 13522.830 |
Transformer | epoch 0 | step 52100 |avg loss 8.921 |avg tokens 2240.640 |tokens/s 8774.332 |walltime 13548.366 |
Transformer | epoch 0 | step 52200 |avg loss 8.969 |avg tokens 2224.190 |tokens/s 8725.336 |walltime 13573.857 |
Transformer | epoch 0 | step 52300 |avg loss 8.986 |avg tokens 2084.660 |tokens/s 8477.323 |walltime 13598.448 |
Transformer | epoch 0 | step 52400 |avg loss 9.060 |avg tokens 2140.230 |tokens/s 8616.891 |walltime 13623.286 |
Transformer | epoch 0 | step 52500 |avg loss 9.027 |avg tokens 2179.470 |tokens/s 8762.029 |walltime 13648.160 |
Transformer | epoch 0 | step 52600 |avg loss 9.010 |avg tokens 2179.000 |tokens/s 8626.760 |walltime 13673.418 |
Transformer | epoch 0 | step 52700 |avg loss 8.986 |avg tokens 2215.570 |tokens/s 8746.114 |walltime 13698.750 |
Transformer | epoch 0 | step 52800 |avg loss 8.987 |avg tokens 2215.700 |tokens/s 8760.957 |walltime 13724.041 |
Transformer | epoch 0 | step 52900 |avg loss 9.064 |avg tokens 2155.770 |tokens/s 8711.532 |walltime 13748.787 |
Transformer | epoch 0 | step 53000 |avg loss 9.018 |avg tokens 2166.890 |tokens/s 8678.525 |walltime 13773.756 |
Transformer | epoch 0 | step 53100 |avg loss 9.054 |avg tokens 2158.470 |tokens/s 8628.883 |walltime 13798.770 |
Transformer | epoch 0 | step 53200 |avg loss 9.007 |avg tokens 2102.890 |tokens/s 8524.085 |walltime 13823.440 |
Transformer | epoch 0 | step 53300 |avg loss 9.077 |avg tokens 2155.000 |tokens/s 8740.118 |walltime 13848.097 |
Transformer | epoch 0 | step 53400 |avg loss 9.021 |avg tokens 2244.070 |tokens/s 8819.930 |walltime 13873.540 |
Transformer | epoch 0 | step 53500 |avg loss 9.035 |avg tokens 2151.920 |tokens/s 8672.617 |walltime 13898.353 |
Transformer | epoch 0 | step 53600 |avg loss 9.126 |avg tokens 2158.340 |tokens/s 8636.642 |walltime 13923.343 |
Transformer | epoch 0 | step 53700 |avg loss 9.011 |avg tokens 2168.970 |tokens/s 8686.076 |walltime 13948.314 |
Transformer | epoch 0 | step 53800 |avg loss 9.051 |avg tokens 2251.200 |tokens/s 8889.519 |walltime 13973.638 |
Transformer | epoch 0 | step 53900 |avg loss 9.036 |avg tokens 2194.960 |tokens/s 8790.138 |walltime 13998.609 |
Transformer | epoch 0 | step 54000 |avg loss 9.039 |avg tokens 2152.250 |tokens/s 8602.525 |walltime 14023.627 |
Transformer | epoch 0 | step 54100 |avg loss 9.066 |avg tokens 2224.080 |tokens/s 8849.900 |walltime 14048.759 |
Transformer | epoch 0 | step 54200 |avg loss 9.063 |avg tokens 2130.310 |tokens/s 8655.091 |walltime 14073.372 |
Transformer | epoch 0 | step 54300 |avg loss 9.166 |avg tokens 2112.150 |tokens/s 8618.267 |walltime 14097.880 |
Transformer | epoch 0 | step 54400 |avg loss 8.908 |avg tokens 2241.710 |tokens/s 8774.597 |walltime 14123.427 |
Transformer | epoch 0 | step 54500 |avg loss 9.057 |avg tokens 2196.800 |tokens/s 8754.307 |walltime 14148.521 |
Transformer | epoch 0 | step 54600 |avg loss 9.074 |avg tokens 2181.090 |tokens/s 8784.455 |walltime 14173.350 |
Transformer | epoch 0 | step 54700 |avg loss 9.006 |avg tokens 2168.680 |tokens/s 8686.526 |walltime 14198.316 |
Transformer | epoch 0 | step 54800 |avg loss 9.049 |avg tokens 2045.180 |tokens/s 8316.459 |walltime 14222.908 |
Transformer | epoch 0 | step 54900 |avg loss 9.015 |avg tokens 2219.980 |tokens/s 8799.422 |walltime 14248.137 |
Transformer | epoch 0 | step 55000 |avg loss 8.937 |avg tokens 2208.740 |tokens/s 8700.494 |walltime 14273.523 |
Transformer | epoch 0 | step 55100 |avg loss 9.063 |avg tokens 2161.400 |tokens/s 8642.559 |walltime 14298.532 |
Transformer | epoch 0 | step 55200 |avg loss 9.033 |avg tokens 2151.890 |tokens/s 8687.791 |walltime 14323.301 |
Transformer | epoch 0 | step 55300 |avg loss 9.024 |avg tokens 2194.900 |tokens/s 8757.392 |walltime 14348.365 |
Transformer | epoch 0 | step 55400 |avg loss 8.983 |avg tokens 2198.220 |tokens/s 8765.881 |walltime 14373.442 |
Transformer | epoch 0 | step 55500 |avg loss 9.003 |avg tokens 2168.980 |tokens/s 8616.113 |walltime 14398.615 |
Transformer | epoch 0 | step 55600 |avg loss 9.040 |avg tokens 2124.990 |tokens/s 8681.043 |walltime 14423.094 |
Transformer | epoch 0 | step 55700 |avg loss 8.930 |avg tokens 2201.300 |tokens/s 8685.716 |walltime 14448.438 |
Transformer | epoch 0 | step 55800 |avg loss 9.066 |avg tokens 2081.650 |tokens/s 8478.654 |walltime 14472.989 |
Transformer | epoch 0 | step 55900 |avg loss 9.069 |avg tokens 2198.320 |tokens/s 8810.399 |walltime 14497.941 |
Transformer | epoch 0 | step 56000 |avg loss 9.011 |avg tokens 2223.660 |tokens/s 8828.963 |walltime 14523.127 |
Transformer | epoch 0 | step 56100 |avg loss 9.005 |avg tokens 2247.850 |tokens/s 8822.737 |walltime 14548.605 |
Transformer | epoch 0 | step 56200 |avg loss 9.013 |avg tokens 2210.970 |tokens/s 8727.154 |walltime 14573.939 |
Transformer | epoch 0 | step 56300 |avg loss 9.083 |avg tokens 2125.350 |tokens/s 8700.725 |walltime 14598.366 |
Transformer | epoch 0 | step 56400 |avg loss 9.042 |avg tokens 2217.860 |tokens/s 8921.211 |walltime 14623.227 |
Transformer | epoch 0 | step 56500 |avg loss 8.978 |avg tokens 2199.550 |tokens/s 8751.998 |walltime 14648.359 |
Transformer | epoch 0 | step 56600 |avg loss 9.092 |avg tokens 2166.910 |tokens/s 8601.394 |walltime 14673.551 |
Transformer | epoch 0 | step 56700 |avg loss 9.019 |avg tokens 2113.500 |tokens/s 8528.026 |walltime 14698.334 |
Transformer | epoch 0 | step 56800 |avg loss 9.124 |avg tokens 2143.810 |tokens/s 8627.607 |walltime 14723.183 |
Transformer | epoch 0 | step 56900 |avg loss 9.065 |avg tokens 2133.160 |tokens/s 8569.232 |walltime 14748.076 |
Transformer | epoch 0 | step 57000 |avg loss 9.127 |avg tokens 2236.860 |tokens/s 8866.036 |walltime 14773.306 |
Transformer | epoch 0 | step 57100 |avg loss 9.088 |avg tokens 2177.120 |tokens/s 8657.046 |walltime 14798.454 |
Transformer | epoch 0 | step 57200 |avg loss 9.080 |avg tokens 2212.800 |tokens/s 8784.063 |walltime 14823.645 |
Transformer | epoch 0 | step 57300 |avg loss 9.065 |avg tokens 2141.500 |tokens/s 8549.559 |walltime 14848.693 |
Transformer | epoch 0 | step 57400 |avg loss 9.060 |avg tokens 2162.100 |tokens/s 8675.027 |walltime 14873.616 |
Transformer | epoch 0 | step 57500 |avg loss 8.979 |avg tokens 2146.700 |tokens/s 8663.519 |walltime 14898.395 |
Transformer | epoch 0 | step 57600 |avg loss 9.050 |avg tokens 2158.190 |tokens/s 8631.208 |walltime 14923.400 |
Transformer | epoch 0 | step 57700 |avg loss 8.972 |avg tokens 2199.830 |tokens/s 8679.181 |walltime 14948.746 |
Transformer | epoch 0 | step 57800 |avg loss 9.118 |avg tokens 2138.140 |tokens/s 8564.277 |walltime 14973.711 |
Transformer | epoch 0 | step 57900 |avg loss 8.989 |avg tokens 2197.050 |tokens/s 8648.648 |walltime 14999.115 |
Transformer | epoch 0 | step 58000 |avg loss 9.179 |avg tokens 2099.890 |tokens/s 8554.864 |walltime 15023.661 |
Transformer | epoch 0 | step 58100 |avg loss 9.132 |avg tokens 2182.710 |tokens/s 8717.670 |walltime 15048.699 |
Transformer | epoch 0 | step 58200 |avg loss 9.057 |avg tokens 2184.130 |tokens/s 8729.849 |walltime 15073.718 |
Transformer | epoch 0 | step 58300 |avg loss 9.022 |avg tokens 2211.670 |tokens/s 8834.716 |walltime 15098.752 |
Transformer | epoch 0 | step 58400 |avg loss 9.110 |avg tokens 2209.560 |tokens/s 8852.240 |walltime 15123.712 |
Transformer | epoch 0 | step 58500 |avg loss 9.095 |avg tokens 2156.220 |tokens/s 8681.344 |walltime 15148.550 |
Transformer | epoch 0 | step 58600 |avg loss 8.972 |avg tokens 2218.940 |tokens/s 8723.665 |walltime 15173.985 |
Transformer | epoch 0 | step 58700 |avg loss 9.125 |avg tokens 2116.560 |tokens/s 8642.041 |walltime 15198.477 |
Transformer | epoch 0 | step 58800 |avg loss 9.080 |avg tokens 2194.820 |tokens/s 8822.879 |walltime 15223.353 |
Transformer | epoch 0 | step 58900 |avg loss 9.073 |avg tokens 2180.550 |tokens/s 8752.618 |walltime 15248.266 |
Transformer | epoch 0 | step 59000 |avg loss 9.059 |avg tokens 2131.580 |tokens/s 8554.524 |walltime 15273.184 |
Transformer | epoch 0 | step 59100 |avg loss 9.035 |avg tokens 2188.370 |tokens/s 8769.881 |walltime 15298.137 |
Transformer | epoch 0 | step 59200 |avg loss 9.062 |avg tokens 2150.550 |tokens/s 8630.100 |walltime 15323.057 |
Transformer | epoch 0 | step 59300 |avg loss 9.081 |avg tokens 2115.060 |tokens/s 8526.960 |walltime 15347.861 |
Transformer | epoch 0 | step 59400 |avg loss 9.087 |avg tokens 2172.090 |tokens/s 8699.426 |walltime 15372.829 |
Transformer | epoch 0 | step 59500 |avg loss 9.066 |avg tokens 2149.530 |tokens/s 8562.452 |walltime 15397.933 |
Transformer | epoch 0 | step 59600 |avg loss 9.004 |avg tokens 2210.470 |tokens/s 8749.869 |walltime 15423.196 |
Transformer | epoch 0 | step 59700 |avg loss 9.044 |avg tokens 2192.410 |tokens/s 8751.233 |walltime 15448.249 |
Transformer | epoch 0 | step 59800 |avg loss 8.995 |avg tokens 2186.540 |tokens/s 8666.932 |walltime 15473.477 |
Transformer | epoch 0 | step 59900 |avg loss 8.992 |avg tokens 2188.540 |tokens/s 8718.231 |walltime 15498.580 |
Transformer | epoch 0 | step 60000 |avg loss 9.049 |avg tokens 2153.120 |tokens/s 8550.090 |walltime 15523.763 |
Transformer | epoch 0 | step 60100 |avg loss 8.971 |avg tokens 2199.860 |tokens/s 8839.589 |walltime 15548.649 |
Transformer | epoch 0 | step 60200 |avg loss 9.039 |avg tokens 2200.250 |tokens/s 8793.187 |walltime 15573.671 |
Transformer | epoch 0 | step 60300 |avg loss 9.006 |avg tokens 2179.010 |tokens/s 8623.919 |walltime 15598.938 |
Transformer | epoch 0 | step 60400 |avg loss 9.052 |avg tokens 2135.720 |tokens/s 8532.083 |walltime 15623.970 |
Transformer | epoch 0 | step 60500 |avg loss 9.120 |avg tokens 2176.150 |tokens/s 8777.614 |walltime 15648.762 |
Transformer | epoch 0 | step 60600 |avg loss 9.085 |avg tokens 2123.090 |tokens/s 8525.955 |walltime 15673.664 |
Transformer | epoch 0 | step 60700 |avg loss 9.060 |avg tokens 2181.570 |tokens/s 8745.575 |walltime 15698.608 |
Transformer | epoch 0 | step 60800 |avg loss 9.103 |avg tokens 2174.300 |tokens/s 8760.484 |walltime 15723.428 |
Transformer | epoch 0 | step 60900 |avg loss 9.014 |avg tokens 2183.920 |tokens/s 8710.956 |walltime 15748.499 |
Transformer | epoch 0 | step 61000 |avg loss 8.958 |avg tokens 2110.200 |tokens/s 8472.684 |walltime 15773.405 |
Transformer | epoch 0 | step 61100 |avg loss 9.018 |avg tokens 2094.530 |tokens/s 8535.199 |walltime 15797.945 |
Transformer | epoch 0 | step 61200 |avg loss 9.102 |avg tokens 2186.070 |tokens/s 8845.104 |walltime 15822.660 |
Transformer | epoch 0 | step 61300 |avg loss 9.062 |avg tokens 2186.430 |tokens/s 8753.861 |walltime 15847.636 |
Transformer | epoch 0 | step 61400 |avg loss 8.990 |avg tokens 2228.040 |tokens/s 8891.764 |walltime 15872.694 |
Transformer | epoch 0 | step 61500 |avg loss 9.090 |avg tokens 2179.220 |tokens/s 8654.707 |walltime 15897.873 |
Transformer | epoch 0 | step 61600 |avg loss 9.092 |avg tokens 2155.910 |tokens/s 8730.648 |walltime 15922.567 |
Transformer | epoch 0 | step 61700 |avg loss 9.009 |avg tokens 2201.940 |tokens/s 8724.198 |walltime 15947.806 |
Transformer | epoch 0 | step 61800 |avg loss 9.113 |avg tokens 2114.660 |tokens/s 8567.391 |walltime 15972.489 |
Transformer | epoch 0 | step 61900 |avg loss 9.056 |avg tokens 2181.020 |tokens/s 8635.962 |walltime 15997.744 |
Transformer | epoch 0 | step 62000 |avg loss 9.108 |avg tokens 2158.870 |tokens/s 8596.824 |walltime 16022.857 |
Transformer | epoch 0 | step 62100 |avg loss 9.109 |avg tokens 2163.680 |tokens/s 8706.131 |walltime 16047.709 |
Transformer | epoch 0 | step 62200 |avg loss 9.061 |avg tokens 2110.530 |tokens/s 8532.706 |walltime 16072.444 |
Transformer | epoch 0 | step 62300 |avg loss 9.015 |avg tokens 2168.070 |tokens/s 8636.718 |walltime 16097.547 |
Transformer | epoch 0 | step 62400 |avg loss 9.138 |avg tokens 2152.500 |tokens/s 8676.065 |walltime 16122.356 |
Transformer | epoch 0 | step 62500 |avg loss 9.040 |avg tokens 2226.570 |tokens/s 8715.826 |walltime 16147.902 |
Transformer | epoch 0 | step 62600 |avg loss 9.079 |avg tokens 2192.480 |tokens/s 8810.902 |walltime 16172.786 |
Transformer | epoch 0 | step 62700 |avg loss 9.054 |avg tokens 2220.910 |tokens/s 8884.036 |walltime 16197.785 |
Transformer | epoch 0 | step 62800 |avg loss 9.045 |avg tokens 2184.300 |tokens/s 8715.921 |walltime 16222.846 |
Transformer | epoch 0 | step 62900 |avg loss 9.105 |avg tokens 2208.410 |tokens/s 8795.293 |walltime 16247.955 |
Transformer | epoch 0 | step 63000 |avg loss 9.105 |avg tokens 2078.690 |tokens/s 8491.824 |walltime 16272.434 |
Transformer | epoch 0 | step 63100 |avg loss 9.075 |avg tokens 2201.240 |tokens/s 8821.796 |walltime 16297.386 |
Transformer | epoch 0 | step 63200 |avg loss 9.127 |avg tokens 2224.350 |tokens/s 8927.018 |walltime 16322.303 |
Transformer | epoch 0 | step 63300 |avg loss 9.088 |avg tokens 2115.710 |tokens/s 8552.301 |walltime 16347.042 |
Transformer | epoch 0 | step 63400 |avg loss 9.097 |avg tokens 2158.950 |tokens/s 8667.397 |walltime 16371.951 |
Transformer | epoch 0 | step 63500 |avg loss 9.039 |avg tokens 2174.170 |tokens/s 8711.704 |walltime 16396.907 |
Transformer | epoch 0 | step 63600 |avg loss 9.048 |avg tokens 2213.310 |tokens/s 8760.460 |walltime 16422.172 |
Transformer | epoch 0 | step 63700 |avg loss 9.058 |avg tokens 2217.350 |tokens/s 8778.010 |walltime 16447.432 |
Transformer | epoch 0 | step 63800 |avg loss 9.057 |avg tokens 2194.130 |tokens/s 8860.215 |walltime 16472.196 |
Transformer | epoch 0 | step 63900 |avg loss 9.081 |avg tokens 2161.640 |tokens/s 8690.448 |walltime 16497.070 |
Transformer | epoch 0 | step 64000 |avg loss 8.991 |avg tokens 2254.540 |tokens/s 8851.920 |walltime 16522.540 |
Transformer | epoch 0 | step 64100 |avg loss 9.108 |avg tokens 2159.930 |tokens/s 8717.847 |walltime 16547.316 |
Transformer | epoch 0 | step 64200 |avg loss 9.111 |avg tokens 2182.420 |tokens/s 8699.876 |walltime 16572.401 |
Transformer | epoch 0 | step 64300 |avg loss 9.131 |avg tokens 2157.230 |tokens/s 8714.542 |walltime 16597.156 |
Transformer | epoch 0 | step 64400 |avg loss 9.047 |avg tokens 2182.630 |tokens/s 8655.537 |walltime 16622.372 |
Transformer | epoch 0 | step 64500 |avg loss 9.010 |avg tokens 2241.900 |tokens/s 8791.526 |walltime 16647.873 |
Transformer | epoch 0 | step 64600 |avg loss 9.066 |avg tokens 2128.360 |tokens/s 8517.670 |walltime 16672.860 |
Transformer | epoch 0 | step 64700 |avg loss 9.026 |avg tokens 2215.560 |tokens/s 8792.625 |walltime 16698.058 |
Transformer | epoch 0 | step 64800 |avg loss 9.033 |avg tokens 2220.790 |tokens/s 8791.430 |walltime 16723.319 |
Transformer | epoch 0 | step 64900 |avg loss 8.987 |avg tokens 2241.270 |tokens/s 8863.660 |walltime 16748.605 |
Transformer | epoch 0 | step 65000 |avg loss 9.082 |avg tokens 2216.580 |tokens/s 8858.883 |walltime 16773.626 |
Transformer | epoch 0 | step 65100 |avg loss 9.075 |avg tokens 2150.230 |tokens/s 8636.724 |walltime 16798.523 |
Epoch time: 16814.343109369278
Transformer | epoch 0 | step 65198 |avg loss 9.189 |avg tokens 2138.286 |tokens/s 8486.920 |walltime 16823.214 |
Validation loss on subset valid: 9.09947784580923
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [98].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [98].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [86].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [98], which does not match the required output shape [86].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [120].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [86], which does not match the required output shape [120].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [120], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [5, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [4, 8], which does not match the required output shape [3, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [3, 8], which does not match the required output shape [2, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [71].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [71].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [101].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [101].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [101], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [11, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [2, 8], which does not match the required output shape [1, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [64].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [64], which does not match the required output shape [114].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [114], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [18, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [16, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [13, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [10, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [7, 8], which does not match the required output shape [5, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [56].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [90].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [56], which does not match the required output shape [90].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [90], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [47].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [47], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [59].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [59], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [12, 8], which does not match the required output shape [11, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [10, 8], which does not match the required output shape [9, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [7, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [88].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [88].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [88], which does not match the required output shape [49].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [49], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [125, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [66].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [66].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [66], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [21, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [9, 8], which does not match the required output shape [8, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [8, 8], which does not match the required output shape [6, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [6, 8], which does not match the required output shape [4, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [26, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [26, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [24, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [24, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [22, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [17, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [17, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [87].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [87], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [50].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [50], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [38, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [36, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [22, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [19, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [15, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [12, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [43].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [43], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [44].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [44], which does not match the required output shape [53].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [53], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [35, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [32, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [27, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [23, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [71].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [71].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [71], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [39, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [37, 8], which does not match the required output shape [34, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [31, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [27, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [20, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [20, 8], which does not match the required output shape [18, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [16, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [14, 8], which does not match the required output shape [13, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [57].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [57], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [55].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [120, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [55], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [28, 8], which does not match the required output shape [25, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [21, 8], which does not match the required output shape [19, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [15, 8], which does not match the required output shape [14, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [42].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [42], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [45], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [49, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [34, 8], which does not match the required output shape [30, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [30, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [29, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [54].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [54], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [122, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [58, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [41, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [41, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [37, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [35, 8], which does not match the required output shape [33, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [33, 8], which does not match the required output shape [32, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [36].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [36], which does not match the required output shape [30].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [30], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [108, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [31].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [31], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [50, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [47, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [47, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [28, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [28].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [28], which does not match the required output shape [39].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [39], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [118, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [118, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [104, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [71, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [57, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [54, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [54, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [48, 8], which does not match the required output shape [46, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [46, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [35].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [40].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [40], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [111, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [45].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [68, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [68, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [63, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [63, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [45, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [45, 8], which does not match the required output shape [44, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [44, 8], which does not match the required output shape [43, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [43, 8], which does not match the required output shape [42, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [42, 8], which does not match the required output shape [40, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [40, 8], which does not match the required output shape [36, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [31, 8], which does not match the required output shape [29, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [25, 8], which does not match the required output shape [23, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [34].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [34], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [124, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [121, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [116, 8], which does not match the required output shape [110, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [101, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [101, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [32], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [78, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [74, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [69, 8], which does not match the required output shape [67, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [51, 8], which does not match the required output shape [50, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [39, 8], which does not match the required output shape [38, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [27].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [27], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [125, 8], which does not match the required output shape [121, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [70, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [66, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [65, 8], which does not match the required output shape [64, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [64, 8], which does not match the required output shape [62, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [62, 8], which does not match the required output shape [61, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [60, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [59, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [56, 8], which does not match the required output shape [55, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [53, 8], which does not match the required output shape [52, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [51, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [49, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [37].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [37], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [23].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [23], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [113, 8], which does not match the required output shape [112, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [112, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [29].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [29], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [91, 8], which does not match the required output shape [90, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [89, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [82, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [12].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [72, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [61, 8], which does not match the required output shape [59, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [38].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [38], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [21].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [21], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [119, 8], which does not match the required output shape [117, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [104, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [92, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [86, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [77, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [76, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [75, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [73, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [67, 8], which does not match the required output shape [65, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [60, 8], which does not match the required output shape [58, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [57, 8], which does not match the required output shape [56, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [55, 8], which does not match the required output shape [53, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [9].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [41].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [41], which does not match the required output shape [2].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [25].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [25], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [126, 8], which does not match the required output shape [123, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [120, 8], which does not match the required output shape [116, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [22].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [22], which does not match the required output shape [32].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [111, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [100, 8], which does not match the required output shape [99, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [97, 8], which does not match the required output shape [95, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [93, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [93, 8], which does not match the required output shape [88, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [18].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [77, 8], which does not match the required output shape [75, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [74, 8], which does not match the required output shape [73, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [70, 8], which does not match the required output shape [66, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [16].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [14].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [14], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [124, 8], which does not match the required output shape [122, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [17], which does not match the required output shape [15].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [20], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [102, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [96, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [96, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [89, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [88, 8], which does not match the required output shape [86, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [85, 8], which does not match the required output shape [84, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [84, 8], which does not match the required output shape [83, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [83, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [76, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [1].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [69, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [18], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [33].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [33], which does not match the required output shape [4].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [7], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [123, 8], which does not match the required output shape [119, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [17].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [115, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [16], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [115, 8], which does not match the required output shape [113, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [110, 8], which does not match the required output shape [107, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [107, 8], which does not match the required output shape [106, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [106, 8], which does not match the required output shape [105, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [105, 8], which does not match the required output shape [103, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [100, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [99, 8], which does not match the required output shape [98, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [98, 8], which does not match the required output shape [97, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [3], which does not match the required output shape [5].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [95, 8], which does not match the required output shape [94, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [92, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [82, 8], which does not match the required output shape [81, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [81, 8], which does not match the required output shape [80, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [80, 8], which does not match the required output shape [79, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [79, 8], which does not match the required output shape [78, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [72, 8], which does not match the required output shape [71, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [2], which does not match the required output shape [26].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [26], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [9], which does not match the required output shape [8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [6].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [7].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [126, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [12], which does not match the required output shape [13].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [13], which does not match the required output shape [24].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [24], which does not match the required output shape [11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [117, 8], which does not match the required output shape [114, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [114, 8], which does not match the required output shape [109, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [109, 8], which does not match the required output shape [108, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [4], which does not match the required output shape [19].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [19], which does not match the required output shape [3].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [103, 8], which does not match the required output shape [102, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [6], which does not match the required output shape [20].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [94, 8], which does not match the required output shape [91, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [90, 8], which does not match the required output shape [87, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [87, 8], which does not match the required output shape [85, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [8], which does not match the required output shape [0].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:405: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:411: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [10].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.masked_select(
/workspace/translation/fairseq/sequence_generator.py:458: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.add(
/workspace/translation/fairseq/sequence_generator.py:379: UserWarning: An output with one or more elements was resized since it had shape [52, 8], which does not match the required output shape [48, 8].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:16.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams, rounding_mode='trunc')
Traceback (most recent call last):
  File "train.py", line 430, in <module>
    main(ARGS)
  File "train.py", line 128, in main
    valid_bleu = score(args, trainer, datasets[valid_subsets[0]], src_dict, tgt_dict, 'valid.raw.de')
  File "train.py", line 305, in score
    sacrebleu_score = sacrebleu.corpus_bleu(predictions, refs, lowercase=not args.test_cased_bleu).score
  File "/opt/conda/lib/python3.8/site-packages/sacrebleu.py", line 1031, in corpus_bleu
    raise EOFError("Source and reference streams have different lengths!")
EOFError: Source and reference streams have different lengths!
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'train.py', '--local_rank=0', '/data/', '--arch', 'transformer_wmt_en_de_big_t2t', '--share-all-embeddings', '--optimizer', 'adam', '--adam-betas', '0.9', '0.997', '--adam-eps', '1e-9', '--clip-norm', '0.0', '--lr-scheduler', 'inverse_sqrt', '--warmup-init-lr', '0.0', '--warmup-updates', '4000', '--lr', '0.000846', '--min-lr', '0.0', '--dropout', '0.1', '--weight-decay', '0.0', '--criterion', 'label_smoothed_cross_entropy', '--label-smoothing', '0.1', '--max-tokens', '2560', '--seed', '1', '--max-epoch', '1', '--no-epoch-checkpoints', '--fuse-layer-norm', '--online-eval', '--log-interval', '100', '--distributed-init-method', 'env://', '--save-dir', '/workspace/checkpoint']' returned non-zero exit status 1.
Killing subprocess 4253
