Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, amp=True, amp_level='O2', arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='./data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=200, lr=[0.0006], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='./checkpoints/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=1, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| ./data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 1 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Transformer | epoch 0 | step 200 |avg loss 13.166 |avg tokens 4554.600 |tokens/s 31105.843 |walltime 38.742 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Transformer | epoch 0 | step 400 |avg loss 11.458 |avg tokens 4559.575 |tokens/s 31251.729 |walltime 67.922 |
Transformer | epoch 0 | step 600 |avg loss 11.052 |avg tokens 4528.205 |tokens/s 31150.485 |walltime 96.995 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Transformer | epoch 0 | step 800 |avg loss 10.513 |avg tokens 4534.425 |tokens/s 31137.225 |walltime 126.120 |
Transformer | epoch 0 | step 1000 |avg loss 10.096 |avg tokens 4565.310 |tokens/s 32023.186 |walltime 154.633 |
Transformer | epoch 0 | step 1200 |avg loss 9.686 |avg tokens 4350.955 |tokens/s 30366.533 |walltime 183.289 |
Transformer | epoch 0 | step 1400 |avg loss 9.423 |avg tokens 4481.065 |tokens/s 31053.022 |walltime 212.150 |
Transformer | epoch 0 | step 1600 |avg loss 9.183 |avg tokens 4495.965 |tokens/s 31322.397 |walltime 240.857 |
Transformer | epoch 0 | step 1800 |avg loss 8.948 |avg tokens 4454.465 |tokens/s 31016.585 |walltime 269.580 |
Transformer | epoch 0 | step 2000 |avg loss 8.620 |avg tokens 4582.655 |tokens/s 31478.629 |walltime 298.696 |
Transformer | epoch 0 | step 2200 |avg loss 8.340 |avg tokens 4579.990 |tokens/s 31350.646 |walltime 327.914 |
Transformer | epoch 0 | step 2400 |avg loss 8.043 |avg tokens 4513.945 |tokens/s 30907.890 |walltime 357.123 |
Transformer | epoch 0 | step 2600 |avg loss 7.927 |avg tokens 4546.485 |tokens/s 31199.765 |walltime 386.268 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Transformer | epoch 0 | step 2800 |avg loss 7.677 |avg tokens 4616.820 |tokens/s 31550.577 |walltime 415.534 |
Transformer | epoch 0 | step 3000 |avg loss 7.548 |avg tokens 4597.040 |tokens/s 31643.680 |walltime 444.589 |
Transformer | epoch 0 | step 3200 |avg loss 7.425 |avg tokens 4445.885 |tokens/s 30620.657 |walltime 473.627 |
Transformer | epoch 0 | step 3400 |avg loss 7.304 |avg tokens 4546.840 |tokens/s 31643.036 |walltime 502.366 |
Transformer | epoch 0 | step 3600 |avg loss 7.138 |avg tokens 4524.515 |tokens/s 31114.990 |walltime 531.448 |
Transformer | epoch 0 | step 3800 |avg loss 7.107 |avg tokens 4426.670 |tokens/s 30711.416 |walltime 560.276 |
Transformer | epoch 0 | step 4000 |avg loss 7.031 |avg tokens 4484.730 |tokens/s 31350.660 |walltime 588.886 |
Transformer | epoch 0 | step 4200 |avg loss 6.878 |avg tokens 4530.920 |tokens/s 31266.574 |walltime 617.868 |
Transformer | epoch 0 | step 4400 |avg loss 6.595 |avg tokens 4688.405 |tokens/s 32087.471 |walltime 647.091 |
Transformer | epoch 0 | step 4600 |avg loss 6.584 |avg tokens 4593.495 |tokens/s 31709.720 |walltime 676.063 |
Transformer | epoch 0 | step 4800 |avg loss 6.686 |avg tokens 4435.075 |tokens/s 31104.549 |walltime 704.580 |
Transformer | epoch 0 | step 5000 |avg loss 6.706 |avg tokens 4495.740 |tokens/s 31760.602 |walltime 732.891 |
Transformer | epoch 0 | step 5200 |avg loss 6.460 |avg tokens 4540.670 |tokens/s 31224.117 |walltime 761.975 |
Transformer | epoch 0 | step 5400 |avg loss 6.495 |avg tokens 4558.505 |tokens/s 31839.388 |walltime 790.609 |
Transformer | epoch 0 | step 5600 |avg loss 6.274 |avg tokens 4538.520 |tokens/s 31531.606 |walltime 819.396 |
Transformer | epoch 0 | step 5800 |avg loss 6.234 |avg tokens 4633.095 |tokens/s 32124.914 |walltime 848.241 |
Transformer | epoch 0 | step 6000 |avg loss 6.263 |avg tokens 4425.685 |tokens/s 30856.957 |walltime 876.926 |
Transformer | epoch 0 | step 6200 |avg loss 6.215 |avg tokens 4378.890 |tokens/s 30145.983 |walltime 905.977 |
Transformer | epoch 0 | step 6400 |avg loss 6.112 |avg tokens 4644.230 |tokens/s 32263.512 |walltime 934.766 |
Transformer | epoch 0 | step 6600 |avg loss 6.189 |avg tokens 4504.825 |tokens/s 31253.459 |walltime 963.594 |
Transformer | epoch 0 | step 6800 |avg loss 6.057 |avg tokens 4519.605 |tokens/s 31547.263 |walltime 992.247 |
Transformer | epoch 0 | step 7000 |avg loss 6.046 |avg tokens 4549.040 |tokens/s 31705.130 |walltime 1020.943 |
Transformer | epoch 0 | step 7200 |avg loss 6.154 |avg tokens 4475.315 |tokens/s 31173.253 |walltime 1049.656 |
Transformer | epoch 0 | step 7400 |avg loss 5.899 |avg tokens 4561.840 |tokens/s 31708.341 |walltime 1078.429 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Transformer | epoch 0 | step 7600 |avg loss 5.799 |avg tokens 4593.650 |tokens/s 31442.632 |walltime 1107.649 |
Transformer | epoch 0 | step 7800 |avg loss 5.959 |avg tokens 4536.890 |tokens/s 31227.911 |walltime 1136.705 |
Transformer | epoch 0 | step 8000 |avg loss 5.879 |avg tokens 4519.105 |tokens/s 31339.597 |walltime 1165.545 |
Transformer | epoch 0 | step 8200 |avg loss 5.746 |avg tokens 4631.810 |tokens/s 31677.579 |walltime 1194.788 |
