| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 6
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 5
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 7
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 2
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 3
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 1
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| initialized host df6665c7a5b6 as rank 0 and device id 0
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, amp=True, amp_level='O2', arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, bpe_codes=None, buffer_size=64, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='./data/wmt14_en_de_joined_dict', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=8, do_sanity_check=False, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=False, fuse_dropout_add=False, fuse_layer_norm=True, fuse_relu_dropout=False, gen_subset='test', keep_interval_updates=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, lenpen=1, local_rank=0, log_interval=100, lr=[0.0006], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=0, max_len_a=0, max_len_b=200, max_positions=(1024, 1024), max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_save=False, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=False, profiler_file=None, profiler_steps=100, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='./checkpoints/', save_interval=1, save_interval_updates=0, save_predictions=False, score_reference=False, seed=1, sentence_avg=False, sentencepiece=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, stat_file='run_log.json', target_bleu=0.0, target_lang=None, test_cased_bleu=False, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=4000, weight_decay=0.0)
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| ./data/wmt14_en_de_joined_dict train 4575637 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict valid 3000 examples
| Sentences are being padded to multiples of: 1
| ./data/wmt14_en_de_joined_dict test 3003 examples
| Sentences are being padded to multiples of: 1
| num. model params: 210808832
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| training on 8 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Transformer | epoch 0 | step 100 |avg loss 14.053 |avg tokens 36353.610 |tokens/s 205231.454 |walltime 30.317 |
Transformer | epoch 0 | step 200 |avg loss 11.945 |avg tokens 35786.590 |tokens/s 212486.107 |walltime 47.159 |
Transformer | epoch 0 | step 300 |avg loss 11.270 |avg tokens 36262.110 |tokens/s 213323.873 |walltime 64.157 |
Transformer | epoch 0 | step 400 |avg loss 10.895 |avg tokens 36412.460 |tokens/s 213894.451 |walltime 81.181 |
Transformer | epoch 0 | step 500 |avg loss 10.443 |avg tokens 35965.510 |tokens/s 212926.686 |walltime 98.072 |
Transformer | epoch 0 | step 600 |avg loss 9.912 |avg tokens 36495.790 |tokens/s 216544.407 |walltime 114.926 |
Transformer | epoch 0 | step 700 |avg loss 9.545 |avg tokens 36266.870 |tokens/s 214373.277 |walltime 131.843 |
Transformer | epoch 0 | step 800 |avg loss 9.141 |avg tokens 36163.800 |tokens/s 211395.545 |walltime 148.950 |
Transformer | epoch 0 | step 900 |avg loss 8.860 |avg tokens 36097.570 |tokens/s 211433.769 |walltime 166.023 |
Transformer | epoch 0 | step 1000 |avg loss 8.505 |avg tokens 36422.970 |tokens/s 212811.498 |walltime 183.138 |
Transformer | epoch 0 | step 1100 |avg loss 8.289 |avg tokens 36349.340 |tokens/s 214436.387 |walltime 200.089 |
Transformer | epoch 0 | step 1200 |avg loss 7.951 |avg tokens 35992.260 |tokens/s 210443.242 |walltime 217.193 |
Transformer | epoch 0 | step 1300 |avg loss 7.698 |avg tokens 36245.200 |tokens/s 213405.430 |walltime 234.177 |
Transformer | epoch 0 | step 1400 |avg loss 7.398 |avg tokens 36101.440 |tokens/s 211832.161 |walltime 251.219 |
Transformer | epoch 0 | step 1500 |avg loss 7.160 |avg tokens 36097.960 |tokens/s 214450.317 |walltime 268.052 |
Transformer | epoch 0 | step 1600 |avg loss 6.902 |avg tokens 35857.810 |tokens/s 212041.244 |walltime 284.963 |
Transformer | epoch 0 | step 1700 |avg loss 6.659 |avg tokens 35886.470 |tokens/s 210698.895 |walltime 301.995 |
Transformer | epoch 0 | step 1800 |avg loss 6.417 |avg tokens 36472.800 |tokens/s 214383.992 |walltime 319.008 |
Transformer | epoch 0 | step 1900 |avg loss 6.384 |avg tokens 36059.150 |tokens/s 214279.858 |walltime 335.836 |
Transformer | epoch 0 | step 2000 |avg loss 6.250 |avg tokens 36203.190 |tokens/s 213968.269 |walltime 352.756 |
Transformer | epoch 0 | step 2100 |avg loss 6.073 |avg tokens 36194.740 |tokens/s 210843.186 |walltime 369.922 |
Transformer | epoch 0 | step 2200 |avg loss 5.981 |avg tokens 36001.400 |tokens/s 213891.180 |walltime 386.754 |
Transformer | epoch 0 | step 2300 |avg loss 5.933 |avg tokens 36061.920 |tokens/s 213902.767 |walltime 403.613 |
Transformer | epoch 0 | step 2400 |avg loss 5.830 |avg tokens 36140.300 |tokens/s 213210.049 |walltime 420.564 |
Transformer | epoch 0 | step 2500 |avg loss 5.790 |avg tokens 36324.380 |tokens/s 214961.328 |walltime 437.462 |
Transformer | epoch 0 | step 2600 |avg loss 5.755 |avg tokens 36176.370 |tokens/s 213182.249 |walltime 454.431 |
Transformer | epoch 0 | step 2700 |avg loss 5.674 |avg tokens 35999.920 |tokens/s 213191.563 |walltime 471.318 |
Transformer | epoch 0 | step 2800 |avg loss 5.697 |avg tokens 36263.430 |tokens/s 213071.359 |walltime 488.337 |
Transformer | epoch 0 | step 2900 |avg loss 5.565 |avg tokens 35973.760 |tokens/s 212599.259 |walltime 505.258 |
Transformer | epoch 0 | step 3000 |avg loss 5.537 |avg tokens 35927.070 |tokens/s 211500.167 |walltime 522.245 |
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Transformer | epoch 0 | step 3100 |avg loss 5.467 |avg tokens 36114.570 |tokens/s 213802.641 |walltime 539.136 |
Transformer | epoch 0 | step 3200 |avg loss 5.491 |avg tokens 35998.340 |tokens/s 212159.469 |walltime 556.104 |
Transformer | epoch 0 | step 3300 |avg loss 5.385 |avg tokens 36113.130 |tokens/s 213760.547 |walltime 572.998 |
Transformer | epoch 0 | step 3400 |avg loss 5.369 |avg tokens 35947.060 |tokens/s 212207.541 |walltime 589.938 |
Transformer | epoch 0 | step 3500 |avg loss 5.371 |avg tokens 36689.520 |tokens/s 217070.385 |walltime 606.840 |
Transformer | epoch 0 | step 3600 |avg loss 5.435 |avg tokens 35624.780 |tokens/s 212244.458 |walltime 623.625 |
Transformer | epoch 0 | step 3700 |avg loss 5.338 |avg tokens 36514.170 |tokens/s 216774.451 |walltime 640.469 |
Transformer | epoch 0 | step 3800 |avg loss 5.349 |avg tokens 36249.430 |tokens/s 215124.397 |walltime 657.319 |
Transformer | epoch 0 | step 3900 |avg loss 5.275 |avg tokens 36099.800 |tokens/s 213585.642 |walltime 674.221 |
Epoch time: 667.8423280715942
Transformer | epoch 0 | step 3935 |avg loss 5.150 |avg tokens 36188.571 |tokens/s 206730.816 |walltime 680.348 |
Validation loss on subset valid: 4.82952582425895
