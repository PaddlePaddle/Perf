grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
WARNING: Logging before InitGoogleLogging() is written to STDERR
W1118 12:14:20.656213 29809 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.
The TensorRT that Paddle depends on is not configured correctly.
  Suggestions:
  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;%PATH%`
  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`
{'batch_size': 2560,
 'beam_size': 5,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'device': 'gpu',
 'dropout': 0.1,
 'eos_idx': 1,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'use_amp': False,
 'use_pure_fp16': False,
 'warmup_steps': 4000,
 'weight_sharing': True}
2021-11-18 12:14:22,286-INFO: unique_endpoints {'127.0.0.1:36733'}
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:822: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  elif dtype == np.bool:
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:2334: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  "used at the same time." % type)
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:672
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:675
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:679
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:686
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:411
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:587
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:595
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:914
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:926
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:934
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:273
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/distributed/fleet/base/fleet_base.py:931: UserWarning: It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
  "It is recommended to use DistributedStrategy "
W1118 12:17:24.149637 29809 device_context.cc:451] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1118 12:17:24.154779 29809 device_context.cc:469] device: 5, cuDNN Version: 8.1.
Traceback (most recent call last):
  File "train.py", line 282, in <module>
    do_train(args)
  File "train.py", line 161, in do_train
    exe.run(startup_program)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1270, in run
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python3.7/dist-packages/six.py", line 703, in reraise
    raise value
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1268, in run
    return_merged=return_merged)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1445, in _run_impl
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1535, in _run_program
    [fetch_var_name])
OSError: In user code:

    File "train.py", line 282, in <module>
      do_train(args)
    File "train.py", line 98, in do_train
      eos_id=args.eos_idx)
    File "/usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py", line 579, in __init__
      vocab_size=src_vocab_size, emb_dim=d_model, bos_id=self.bos_id)
    File "/usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py", line 95, in __init__
      initializer=nn.initializer.Normal(0., emb_dim**-0.5)))
    File "/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/common.py", line 1390, in __init__
      is_bias=False)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py", line 421, in create_parameter
      default_initializer)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/layer_helper_base.py", line 384, in create_parameter
      **attr._to_kwargs(with_initializer=True))
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 3218, in create_parameter
      initializer(param, self)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/initializer.py", line 366, in __call__
      stop_gradient=True)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 3265, in append_op
      attrs=kwargs.get("attrs", None))
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 2305, in __init__
      for frame in traceback.extract_stack():

    ExternalError: CU error(1). 
      [Hint: Please search for the error code(1) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CU Error.] (at /paddle/paddle/fluid/memory/allocation/cuda_virtual_mem_allocator.cc:209)
      [operator < gaussian_random > error]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1637237870 (unix time) try "date -d @1637237870" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x740a) received by PID 29809 (TID 0x7f29c3b02740) from PID 29706 ***]

WARNING: Logging before InitGoogleLogging() is written to STDERR
W1206 03:40:24.810285 81443 tensorrt.cc:56] You are using Paddle compiled with TensorRT, but TensorRT dynamic library is not found. Ignore this if TensorRT is not needed.
The TensorRT that Paddle depends on is not configured correctly.
  Suggestions:
  1. Check if the TensorRT is installed correctly and its version is matched with paddlepaddle you installed.
  2. Configure environment variables as follows:
  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`
  - Windows: set PATH by `set PATH=XXX;%PATH%`
  - Mac: set  DYLD_LIBRARY_PATH by `export DYLD_LIBRARY_PATH=...`
{'batch_size': 2560,
 'beam_size': 5,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'device': 'gpu',
 'dropout': 0.1,
 'eos_idx': 1,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'use_amp': False,
 'use_pure_fp16': False,
 'warmup_steps': 4000,
 'weight_sharing': True}
2021-12-06 03:40:26,280-INFO: unique_endpoints {'127.0.0.1:33035'}
/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py:2334: UserWarning: The Attr(force_cpu) of Op(fill_constant) will be deprecated in the future, please use 'device_guard' instead. 'device_guard' has higher priority when they are used at the same time.
  "used at the same time." % type)
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:672
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:675
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:679
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:686
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:411
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:587
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:595
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:914
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:926
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddle/nn/layer/transformer.py:934
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/fluid/layers/math_op_patch.py:341: UserWarning: /usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py:273
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/usr/local/lib/python3.7/dist-packages/paddle/distributed/fleet/base/fleet_base.py:931: UserWarning: It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
  "It is recommended to use DistributedStrategy "
W1206 03:43:23.004690 81443 device_context.cc:451] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1206 03:43:23.008988 81443 device_context.cc:469] device: 5, cuDNN Version: 8.1.
Traceback (most recent call last):
  File "train.py", line 282, in <module>
    do_train(args)
  File "train.py", line 161, in do_train
    exe.run(startup_program)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1270, in run
    six.reraise(*sys.exc_info())
  File "/usr/local/lib/python3.7/dist-packages/six.py", line 703, in reraise
    raise value
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1268, in run
    return_merged=return_merged)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1445, in _run_impl
    use_program_cache=use_program_cache)
  File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/executor.py", line 1535, in _run_program
    [fetch_var_name])
OSError: 

  Compile Traceback (most recent call last):
    File "train.py", line 282, in <module>
      do_train(args)
    File "train.py", line 98, in do_train
      eos_id=args.eos_idx)
    File "/usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py", line 579, in __init__
      vocab_size=src_vocab_size, emb_dim=d_model, bos_id=self.bos_id)
    File "/usr/local/lib/python3.7/dist-packages/paddlenlp/transformers/transformer/modeling.py", line 95, in __init__
      initializer=nn.initializer.Normal(0., emb_dim**-0.5)))
    File "/usr/local/lib/python3.7/dist-packages/paddle/nn/layer/common.py", line 1390, in __init__
      is_bias=False)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/layers.py", line 421, in create_parameter
      default_initializer)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/layer_helper_base.py", line 384, in create_parameter
      **attr._to_kwargs(with_initializer=True))
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 3218, in create_parameter
      initializer(param, self)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/initializer.py", line 366, in __call__
      stop_gradient=True)
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 3265, in append_op
      attrs=kwargs.get("attrs", None))
    File "/usr/local/lib/python3.7/dist-packages/paddle/fluid/framework.py", line 2305, in __init__
      for frame in traceback.extract_stack():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::Executor::Run(paddle::framework::ProgramDesc const&, paddle::framework::Scope*, int, bool, bool, std::vector<std::string, std::allocator<std::string > > const&, bool, bool)
1   paddle::framework::Executor::RunPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, bool, bool, bool)
2   paddle::framework::Executor::RunPartialPreparedContext(paddle::framework::ExecutorPrepareContext*, paddle::framework::Scope*, long, long, bool, bool, bool)
3   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
4   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
5   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
6   std::_Function_handler<void (paddle::framework::ExecutionContext const&), paddle::framework::OpKernelRegistrarFunctor<paddle::platform::CUDAPlace, false, 0ul, paddle::operators::GPUGaussianRandomKernel<float>, paddle::operators::GPUGaussianRandomKernel<double> >::operator()(char const*, char const*, int) const::{lambda(paddle::framework::ExecutionContext const&)#1}>::_M_invoke(std::_Any_data const&, paddle::framework::ExecutionContext const&)
7   paddle::framework::Tensor::mutable_data(paddle::platform::Place const&, paddle::framework::proto::VarType_Type, unsigned long)
8   paddle::memory::AllocShared(paddle::platform::Place const&, unsigned long)
9   paddle::memory::allocation::AllocatorFacade::AllocShared(paddle::platform::Place const&, unsigned long)
10  paddle::memory::allocation::AllocatorFacade::Alloc(paddle::platform::Place const&, unsigned long)
11  paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
12  paddle::memory::allocation::VirtualMemoryAutoGrowthBestFitAllocator::AllocateImpl(unsigned long)
13  paddle::memory::allocation::VirtualMemoryAutoGrowthBestFitAllocator::ExtendAndMerge(unsigned long)
14  paddle::memory::allocation::AlignedAllocator::AllocateImpl(unsigned long)
15  paddle::memory::allocation::CUDAVirtualMemAllocator::AllocateImpl(unsigned long)
16  paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
17  paddle::platform::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ExternalError: CU error(1). 
  [Hint: Please search for the error code(1) on website (https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) to get Nvidia's official solution and advice about CU Error.] (at /paddle/paddle/fluid/memory/allocation/cuda_virtual_mem_allocator.cc:209)
  [operator < gaussian_random > error]


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ScopePool::Clear()
1   paddle::framework::Scope::~Scope()
2   paddle::framework::Scope::~Scope()

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1638762231 (unix time) try "date -d @1638762231" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x13dba) received by PID 81443 (TID 0x7fa510b8f740) from PID 81338 ***]

{'alpha': 0.6,
 'batch_size': 2560,
 'beam_search_version': 'v1',
 'beam_size': 4,
 'benchmark': True,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bos_token': None,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'dev_file': None,
 'device': 'gpu',
 'diversity_rate': 0.0,
 'dropout': 0.1,
 'eos_idx': 1,
 'eos_token': None,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'input_dtype': 'int64',
 'is_distributed': True,
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'train_file': None,
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'unk_token': None,
 'use_amp': False,
 'use_pure_fp16': False,
 'use_rel_len': False,
 'vocab_file': None,
 'warmup_steps': 4000,
 'weight_sharing': True}
INFO:paddle.utils.download:unique_endpoints {'127.0.0.1:33207'}
Traceback (most recent call last):
  File "train.py", line 330, in <module>
    do_train(args)
  File "train.py", line 136, in do_train
    eos_id=args.eos_idx)
TypeError: __init__() got an unexpected keyword argument 'num_encoder_layers'


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639126708 (unix time) try "date -d @1639126708" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0xec43) received by PID 60592 (TID 0x7f1b33241740) from PID 60483 ***]

{'alpha': 0.6,
 'batch_size': 2560,
 'beam_search_version': 'v1',
 'beam_size': 4,
 'benchmark': True,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bos_token': None,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'dev_file': None,
 'device': 'gpu',
 'diversity_rate': 0.0,
 'dropout': 0.1,
 'eos_idx': 1,
 'eos_token': None,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'input_dtype': 'int64',
 'is_distributed': True,
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'train_file': None,
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'unk_token': None,
 'use_amp': False,
 'use_pure_fp16': False,
 'use_rel_len': False,
 'vocab_file': None,
 'warmup_steps': 4000,
 'weight_sharing': True}
INFO:paddle.utils.download:unique_endpoints {'127.0.0.1:36385'}
Traceback (most recent call last):
  File "train.py", line 330, in <module>
    do_train(args)
  File "train.py", line 136, in do_train
    eos_id=args.eos_idx)
TypeError: __init__() got an unexpected keyword argument 'num_encoder_layers'


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639129845 (unix time) try "date -d @1639129845" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x1023d) received by PID 66214 (TID 0x7f674b92a740) from PID 66109 ***]

{'alpha': 0.6,
 'batch_size': 2560,
 'beam_search_version': 'v1',
 'beam_size': 4,
 'benchmark': True,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bos_token': None,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'dev_file': None,
 'device': 'gpu',
 'diversity_rate': 0.0,
 'dropout': 0.1,
 'eos_idx': 1,
 'eos_token': None,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'input_dtype': 'int64',
 'is_distributed': True,
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'train_file': None,
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'unk_token': None,
 'use_amp': False,
 'use_pure_fp16': False,
 'use_rel_len': False,
 'vocab_file': None,
 'warmup_steps': 4000,
 'weight_sharing': True}
INFO:paddle.utils.download:unique_endpoints {'127.0.0.1:36244'}
Traceback (most recent call last):
  File "train.py", line 330, in <module>
    do_train(args)
  File "train.py", line 136, in do_train
    eos_id=args.eos_idx)
TypeError: __init__() got an unexpected keyword argument 'num_encoder_layers'
{'alpha': 0.6,
 'batch_size': 2560,
 'beam_search_version': 'v1',
 'beam_size': 4,
 'benchmark': True,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bos_token': None,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'dev_file': None,
 'device': 'gpu',
 'diversity_rate': 0.0,
 'dropout': 0.1,
 'eos_idx': 1,
 'eos_token': None,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'input_dtype': 'int64',
 'is_distributed': True,
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'train_file': None,
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'unk_token': None,
 'use_amp': False,
 'use_pure_fp16': False,
 'use_rel_len': False,
 'vocab_file': None,
 'warmup_steps': 4000,
 'weight_sharing': True}
INFO:paddle.utils.download:unique_endpoints {'127.0.0.1:34066'}
W1213 02:34:38.692816 75272 device_context.cc:447] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1213 02:34:38.697999 75272 device_context.cc:465] device: 5, cuDNN Version: 8.1.
I1213 02:34:43.450909 75272 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:52331 successful.
W1213 02:35:08.710889 75272 build_strategy.cc:110] Currently, fuse_broadcast_ops only works under Reduce mode.
W1213 02:35:08.848450 75272 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 257. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 43.
INFO:root:step_idx: 0, epoch: 0, batch: 0, avg loss: 11.194307, normalized loss: 9.826666, ppl: 72715.320312
INFO:root:step_idx: 100, epoch: 0, batch: 100, avg loss: 8.685120, normalized loss: 7.317479, ppl: 5914.248047, avg_speed: 3.44 step/s, batch_cost: 0.29109 sec, reader_cost: 0.00011 sec, tokens: 221480, ips: 7608.69068 words/sec
INFO:root:step_idx: 200, epoch: 0, batch: 200, avg loss: 7.599742, normalized loss: 6.232101, ppl: 1997.681274, avg_speed: 3.42 step/s, batch_cost: 0.29266 sec, reader_cost: 0.00009 sec, tokens: 223290, ips: 7629.68924 words/sec
INFO:root:step_idx: 300, epoch: 0, batch: 300, avg loss: 8.339890, normalized loss: 6.972249, ppl: 4187.626953, avg_speed: 3.42 step/s, batch_cost: 0.29245 sec, reader_cost: 0.00009 sec, tokens: 219866, ips: 7518.18090 words/sec
INFO:root:step_idx: 400, epoch: 0, batch: 400, avg loss: 7.664037, normalized loss: 6.296396, ppl: 2130.339844, avg_speed: 3.41 step/s, batch_cost: 0.29359 sec, reader_cost: 0.00010 sec, tokens: 217004, ips: 7391.30020 words/sec
{'alpha': 0.6,
 'batch_size': 2560,
 'beam_search_version': 'v1',
 'beam_size': 4,
 'benchmark': True,
 'beta1': 0.9,
 'beta2': 0.997,
 'bos_idx': 0,
 'bos_token': None,
 'bsz_multi': 8,
 'd_inner_hid': 4096,
 'd_model': 1024,
 'dev_file': None,
 'device': 'gpu',
 'diversity_rate': 0.0,
 'dropout': 0.1,
 'eos_idx': 1,
 'eos_token': None,
 'epoch': 30,
 'eps': '1e-9',
 'infer_batch_size': 8,
 'inference_model_dir': 'infer_model',
 'init_from_checkpoint': '',
 'init_from_params': './trained_models/step_final/',
 'init_from_pretrain_model': '',
 'input_dtype': 'int64',
 'is_distributed': True,
 'label_smooth_eps': 0.1,
 'learning_rate': 2.0,
 'max_iter': 1500,
 'max_length': 1024,
 'max_out_len': 1024,
 'n_best': 1,
 'n_head': 16,
 'n_layer': 6,
 'output_file': 'predict.txt',
 'pad_factor': 8,
 'pad_seq': 1,
 'pool_size': 200000,
 'print_step': 100,
 'random_seed': 'None',
 'save_model': 'trained_models',
 'save_step': 10000,
 'scale_loss': 128.0,
 'shuffle': True,
 'shuffle_batch': True,
 'shuffle_seed': 128,
 'sort_type': 'global',
 'special_token': ['<s>', '<e>', '<unk>'],
 'src_lang': 'en',
 'src_vocab_size': 10000,
 'task_name': 'de-en',
 'train_file': None,
 'trg_lang': 'de',
 'trg_vocab_size': 10000,
 'unk_idx': 2,
 'unk_token': None,
 'use_amp': False,
 'use_pure_fp16': False,
 'use_rel_len': False,
 'vocab_file': None,
 'warmup_steps': 4000,
 'weight_sharing': True}
INFO:paddle.utils.download:unique_endpoints {'127.0.0.1:33111'}
W1213 08:20:02.448436 78502 device_context.cc:447] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1213 08:20:02.452773 78502 device_context.cc:465] device: 5, cuDNN Version: 8.1.
I1213 08:20:11.205103 78502 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:40284 successful.
W1213 08:20:31.180584 78502 build_strategy.cc:110] Currently, fuse_broadcast_ops only works under Reduce mode.
W1213 08:20:31.326997 78502 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 257. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 43.
INFO:root:step_idx: 0, epoch: 0, batch: 0, avg loss: 11.330845, normalized loss: 9.963204, ppl: 83353.414062
INFO:root:step_idx: 100, epoch: 0, batch: 100, avg loss: 8.674680, normalized loss: 7.307039, ppl: 5852.825195, avg_speed: 3.44 step/s, batch_cost: 0.29109 sec, reader_cost: 0.00009 sec, tokens: 221480, ips: 7608.52087 words/sec
INFO:root:step_idx: 200, epoch: 0, batch: 200, avg loss: 7.600883, normalized loss: 6.233242, ppl: 1999.960205, avg_speed: 3.42 step/s, batch_cost: 0.29210 sec, reader_cost: 0.00009 sec, tokens: 223290, ips: 7644.33196 words/sec
INFO:root:step_idx: 300, epoch: 0, batch: 300, avg loss: 8.363994, normalized loss: 6.996353, ppl: 4289.792969, avg_speed: 3.42 step/s, batch_cost: 0.29238 sec, reader_cost: 0.00008 sec, tokens: 219866, ips: 7519.78269 words/sec
INFO:root:step_idx: 400, epoch: 0, batch: 400, avg loss: 7.687802, normalized loss: 6.320161, ppl: 2181.573975, avg_speed: 3.41 step/s, batch_cost: 0.29344 sec, reader_cost: 0.00009 sec, tokens: 217004, ips: 7395.17325 words/sec
INFO:root:step_idx: 500, epoch: 0, batch: 500, avg loss: 7.337993, normalized loss: 5.970352, ppl: 1537.623291, avg_speed: 3.40 step/s, batch_cost: 0.29449 sec, reader_cost: 0.00008 sec, tokens: 224219, ips: 7613.88912 words/sec
INFO:root:step_idx: 600, epoch: 0, batch: 600, avg loss: 7.113766, normalized loss: 5.746125, ppl: 1228.765991, avg_speed: 3.39 step/s, batch_cost: 0.29539 sec, reader_cost: 0.00008 sec, tokens: 215391, ips: 7291.85523 words/sec
INFO:root:step_idx: 700, epoch: 0, batch: 700, avg loss: 7.675186, normalized loss: 6.307545, ppl: 2154.224609, avg_speed: 3.38 step/s, batch_cost: 0.29555 sec, reader_cost: 0.00008 sec, tokens: 223170, ips: 7551.06256 words/sec
INFO:root:step_idx: 800, epoch: 0, batch: 800, avg loss: 5.935853, normalized loss: 4.568213, ppl: 378.362823, avg_speed: 3.40 step/s, batch_cost: 0.29446 sec, reader_cost: 0.00009 sec, tokens: 215871, ips: 7331.13562 words/sec
INFO:root:step_idx: 900, epoch: 0, batch: 900, avg loss: 5.476254, normalized loss: 4.108614, ppl: 238.950012, avg_speed: 3.39 step/s, batch_cost: 0.29532 sec, reader_cost: 0.00012 sec, tokens: 217305, ips: 7358.21599 words/sec
INFO:root:step_idx: 1000, epoch: 0, batch: 1000, avg loss: 4.869919, normalized loss: 3.502278, ppl: 130.310394, avg_speed: 3.39 step/s, batch_cost: 0.29507 sec, reader_cost: 0.00008 sec, tokens: 222471, ips: 7539.50709 words/sec
INFO:root:step_idx: 1100, epoch: 0, batch: 1100, avg loss: 6.115318, normalized loss: 4.747677, ppl: 452.739929, avg_speed: 3.38 step/s, batch_cost: 0.29609 sec, reader_cost: 0.00013 sec, tokens: 214428, ips: 7242.01162 words/sec
INFO:root:step_idx: 1200, epoch: 0, batch: 1200, avg loss: 6.117687, normalized loss: 4.750046, ppl: 453.813904, avg_speed: 3.38 step/s, batch_cost: 0.29574 sec, reader_cost: 0.00008 sec, tokens: 216086, ips: 7306.50903 words/sec
INFO:root:step_idx: 1300, epoch: 0, batch: 1300, avg loss: 5.206422, normalized loss: 3.838781, ppl: 182.440094, avg_speed: 3.41 step/s, batch_cost: 0.29337 sec, reader_cost: 0.00008 sec, tokens: 212105, ips: 7229.93774 words/sec
INFO:root:step_idx: 1400, epoch: 0, batch: 1400, avg loss: 4.988533, normalized loss: 3.620893, ppl: 146.721085, avg_speed: 3.38 step/s, batch_cost: 0.29596 sec, reader_cost: 0.00009 sec, tokens: 222992, ips: 7534.59896 words/sec
