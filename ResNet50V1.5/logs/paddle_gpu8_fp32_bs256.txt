LAUNCH INFO 2022-12-02 18:10:12,795 -----------  Configuration  ----------------------
LAUNCH INFO 2022-12-02 18:10:12,795 devices: 0,1,2,3,4,5,6,7
LAUNCH INFO 2022-12-02 18:10:12,795 elastic_level: -1
LAUNCH INFO 2022-12-02 18:10:12,795 elastic_timeout: 30
LAUNCH INFO 2022-12-02 18:10:12,795 gloo_port: 6767
LAUNCH INFO 2022-12-02 18:10:12,795 host: None
LAUNCH INFO 2022-12-02 18:10:12,795 ips: None
LAUNCH INFO 2022-12-02 18:10:12,795 job_id: default
LAUNCH INFO 2022-12-02 18:10:12,795 legacy: False
LAUNCH INFO 2022-12-02 18:10:12,795 log_dir: log
LAUNCH INFO 2022-12-02 18:10:12,795 log_level: INFO
LAUNCH INFO 2022-12-02 18:10:12,795 master: None
LAUNCH INFO 2022-12-02 18:10:12,795 max_restart: 3
LAUNCH INFO 2022-12-02 18:10:12,795 nnodes: 1
LAUNCH INFO 2022-12-02 18:10:12,795 nproc_per_node: None
LAUNCH INFO 2022-12-02 18:10:12,795 rank: -1
LAUNCH INFO 2022-12-02 18:10:12,795 run_mode: collective
LAUNCH INFO 2022-12-02 18:10:12,795 server_num: None
LAUNCH INFO 2022-12-02 18:10:12,795 servers: 
LAUNCH INFO 2022-12-02 18:10:12,795 start_port: 6070
LAUNCH INFO 2022-12-02 18:10:12,796 trainer_num: None
LAUNCH INFO 2022-12-02 18:10:12,796 trainers: 
LAUNCH INFO 2022-12-02 18:10:12,796 training_script: ppcls/static/train.py
LAUNCH INFO 2022-12-02 18:10:12,796 training_script_args: ['-c', 'ppcls/configs/ImageNet/ResNet/ResNet50.yaml', '-o', 'DataLoader.Train.sampler.batch_size=256', '-o', 'Global.epochs=8', '-o', 'DataLoader.Train.loader.num_workers=8', '-o', 'Global.eval_during_train=False', '-o', 'fuse_elewise_add_act_ops=True', '-o', 'enable_addto=True']
LAUNCH INFO 2022-12-02 18:10:12,796 with_gloo: 1
LAUNCH INFO 2022-12-02 18:10:12,796 --------------------------------------------------
LAUNCH INFO 2022-12-02 18:10:12,796 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2022-12-02 18:10:12,804 Run Pod: qlcunm, replicas 8, status ready
LAUNCH INFO 2022-12-02 18:10:12,892 Watching Pod: qlcunm, replicas 8, status running
/paddle/perf/pa240_perf/PaddleClas/ppcls/data/preprocess/ops/timm_autoaugment.py:39: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)
/paddle/perf/pa240_perf/PaddleClas/ppcls/data/preprocess/ops/timm_autoaugment.py:39: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)
A new field (fuse_elewise_add_act_ops) detected!
A new field (enable_addto) detected!
[2022/12/02 18:10:15] ppcls INFO: 
===========================================================
==        PaddleClas is powered by PaddlePaddle !        ==
===========================================================
==                                                       ==
==   For more info please go to the following website.   ==
==                                                       ==
==       https://github.com/PaddlePaddle/PaddleClas      ==
===========================================================

[2022/12/02 18:10:15] ppcls INFO: Global : 
[2022/12/02 18:10:15] ppcls INFO:     checkpoints : None
[2022/12/02 18:10:15] ppcls INFO:     pretrained_model : None
[2022/12/02 18:10:15] ppcls INFO:     output_dir : ./output/
[2022/12/02 18:10:15] ppcls INFO:     device : gpu
[2022/12/02 18:10:15] ppcls INFO:     save_interval : 1
[2022/12/02 18:10:15] ppcls INFO:     eval_during_train : False
[2022/12/02 18:10:15] ppcls INFO:     eval_interval : 1
[2022/12/02 18:10:15] ppcls INFO:     epochs : 8
[2022/12/02 18:10:15] ppcls INFO:     print_batch_step : 10
[2022/12/02 18:10:15] ppcls INFO:     use_visualdl : False
[2022/12/02 18:10:15] ppcls INFO:     image_shape : [3, 224, 224]
[2022/12/02 18:10:15] ppcls INFO:     save_inference_dir : ./inference
[2022/12/02 18:10:15] ppcls INFO:     to_static : False
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: Arch : 
[2022/12/02 18:10:15] ppcls INFO:     name : ResNet50
[2022/12/02 18:10:15] ppcls INFO:     class_num : 1000
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: Loss : 
[2022/12/02 18:10:15] ppcls INFO:     Train : 
[2022/12/02 18:10:15] ppcls INFO:         CELoss : 
[2022/12/02 18:10:15] ppcls INFO:             weight : 1.0
[2022/12/02 18:10:15] ppcls INFO:     Eval : 
[2022/12/02 18:10:15] ppcls INFO:         CELoss : 
[2022/12/02 18:10:15] ppcls INFO:             weight : 1.0
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: Optimizer : 
[2022/12/02 18:10:15] ppcls INFO:     name : Momentum
[2022/12/02 18:10:15] ppcls INFO:     momentum : 0.9
[2022/12/02 18:10:15] ppcls INFO:     lr : 
[2022/12/02 18:10:15] ppcls INFO:         name : Piecewise
[2022/12/02 18:10:15] ppcls INFO:         learning_rate : 0.1
[2022/12/02 18:10:15] ppcls INFO:         decay_epochs : [30, 60, 90]
[2022/12/02 18:10:15] ppcls INFO:         values : [0.1, 0.01, 0.001, 0.0001]
[2022/12/02 18:10:15] ppcls INFO:     regularizer : 
[2022/12/02 18:10:15] ppcls INFO:         name : L2
[2022/12/02 18:10:15] ppcls INFO:         coeff : 0.0001
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: DataLoader : 
[2022/12/02 18:10:15] ppcls INFO:     Train : 
[2022/12/02 18:10:15] ppcls INFO:         dataset : 
[2022/12/02 18:10:15] ppcls INFO:             name : ImageNetDataset
[2022/12/02 18:10:15] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2022/12/02 18:10:15] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/train_list.txt
[2022/12/02 18:10:15] ppcls INFO:             transform_ops : 
[2022/12/02 18:10:15] ppcls INFO:                 DecodeImage : 
[2022/12/02 18:10:15] ppcls INFO:                     to_rgb : True
[2022/12/02 18:10:15] ppcls INFO:                     channel_first : False
[2022/12/02 18:10:15] ppcls INFO:                 RandCropImage : 
[2022/12/02 18:10:15] ppcls INFO:                     size : 224
[2022/12/02 18:10:15] ppcls INFO:                 RandFlipImage : 
[2022/12/02 18:10:15] ppcls INFO:                     flip_code : 1
[2022/12/02 18:10:15] ppcls INFO:                 NormalizeImage : 
[2022/12/02 18:10:15] ppcls INFO:                     scale : 1.0/255.0
[2022/12/02 18:10:15] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2022/12/02 18:10:15] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2022/12/02 18:10:15] ppcls INFO:                     order : 
[2022/12/02 18:10:15] ppcls INFO:         sampler : 
[2022/12/02 18:10:15] ppcls INFO:             name : DistributedBatchSampler
[2022/12/02 18:10:15] ppcls INFO:             batch_size : 256
[2022/12/02 18:10:15] ppcls INFO:             drop_last : False
[2022/12/02 18:10:15] ppcls INFO:             shuffle : True
[2022/12/02 18:10:15] ppcls INFO:         loader : 
[2022/12/02 18:10:15] ppcls INFO:             num_workers : 8
[2022/12/02 18:10:15] ppcls INFO:             use_shared_memory : True
[2022/12/02 18:10:15] ppcls INFO:     Eval : 
[2022/12/02 18:10:15] ppcls INFO:         dataset : 
[2022/12/02 18:10:15] ppcls INFO:             name : ImageNetDataset
[2022/12/02 18:10:15] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2022/12/02 18:10:15] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/val_list.txt
[2022/12/02 18:10:15] ppcls INFO:             transform_ops : 
[2022/12/02 18:10:15] ppcls INFO:                 DecodeImage : 
[2022/12/02 18:10:15] ppcls INFO:                     to_rgb : True
[2022/12/02 18:10:15] ppcls INFO:                     channel_first : False
[2022/12/02 18:10:15] ppcls INFO:                 ResizeImage : 
[2022/12/02 18:10:15] ppcls INFO:                     resize_short : 256
[2022/12/02 18:10:15] ppcls INFO:                 CropImage : 
[2022/12/02 18:10:15] ppcls INFO:                     size : 224
[2022/12/02 18:10:15] ppcls INFO:                 NormalizeImage : 
[2022/12/02 18:10:15] ppcls INFO:                     scale : 1.0/255.0
[2022/12/02 18:10:15] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2022/12/02 18:10:15] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2022/12/02 18:10:15] ppcls INFO:                     order : 
[2022/12/02 18:10:15] ppcls INFO:         sampler : 
[2022/12/02 18:10:15] ppcls INFO:             name : DistributedBatchSampler
[2022/12/02 18:10:15] ppcls INFO:             batch_size : 64
[2022/12/02 18:10:15] ppcls INFO:             drop_last : False
[2022/12/02 18:10:15] ppcls INFO:             shuffle : False
[2022/12/02 18:10:15] ppcls INFO:         loader : 
[2022/12/02 18:10:15] ppcls INFO:             num_workers : 4
[2022/12/02 18:10:15] ppcls INFO:             use_shared_memory : True
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: Infer : 
[2022/12/02 18:10:15] ppcls INFO:     infer_imgs : docs/images/inference_deployment/whl_demo.jpg
[2022/12/02 18:10:15] ppcls INFO:     batch_size : 10
[2022/12/02 18:10:15] ppcls INFO:     transforms : 
[2022/12/02 18:10:15] ppcls INFO:         DecodeImage : 
[2022/12/02 18:10:15] ppcls INFO:             to_rgb : True
[2022/12/02 18:10:15] ppcls INFO:             channel_first : False
[2022/12/02 18:10:15] ppcls INFO:         ResizeImage : 
[2022/12/02 18:10:15] ppcls INFO:             resize_short : 256
[2022/12/02 18:10:15] ppcls INFO:         CropImage : 
[2022/12/02 18:10:15] ppcls INFO:             size : 224
[2022/12/02 18:10:15] ppcls INFO:         NormalizeImage : 
[2022/12/02 18:10:15] ppcls INFO:             scale : 1.0/255.0
[2022/12/02 18:10:15] ppcls INFO:             mean : [0.485, 0.456, 0.406]
[2022/12/02 18:10:15] ppcls INFO:             std : [0.229, 0.224, 0.225]
[2022/12/02 18:10:15] ppcls INFO:             order : 
[2022/12/02 18:10:15] ppcls INFO:         ToCHWImage : None
[2022/12/02 18:10:15] ppcls INFO:     PostProcess : 
[2022/12/02 18:10:15] ppcls INFO:         name : Topk
[2022/12/02 18:10:15] ppcls INFO:         topk : 5
[2022/12/02 18:10:15] ppcls INFO:         class_id_map_file : ppcls/utils/imagenet1k_label_list.txt
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: Metric : 
[2022/12/02 18:10:15] ppcls INFO:     Train : 
[2022/12/02 18:10:15] ppcls INFO:         TopkAcc : 
[2022/12/02 18:10:15] ppcls INFO:             topk : [1, 5]
[2022/12/02 18:10:15] ppcls INFO:     Eval : 
[2022/12/02 18:10:15] ppcls INFO:         TopkAcc : 
[2022/12/02 18:10:15] ppcls INFO:             topk : [1, 5]
[2022/12/02 18:10:15] ppcls INFO: ------------------------------------------------------------
[2022/12/02 18:10:15] ppcls INFO: fuse_elewise_add_act_ops : True
[2022/12/02 18:10:15] ppcls INFO: enable_addto : True
[2022/12/02 18:10:22] ppcls WARNING: "init_res" will be deprecated, please use "init_net" instead.
[2022-12-02 18:10:23,042] [ WARNING] fleet.py:1073 - It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
server not ready, wait 3 sec to retry...
not ready endpoints:['10.21.226.170:57455', '10.21.226.170:57456', '10.21.226.170:57457', '10.21.226.170:57458', '10.21.226.170:57459', '10.21.226.170:57460', '10.21.226.170:57461']
W1202 18:10:26.428673 76923 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1202 18:10:26.428718 76923 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
W1202 18:10:37.377180 76923 build_strategy.cc:124] Currently, fuse_broadcast_ops only works under Reduce mode.
I1202 18:10:37.394516 76923 fuse_pass_base.cc:59] ---  detected 16 subgraphs
I1202 18:10:37.410384 76923 fuse_pass_base.cc:59] ---  detected 16 subgraphs
W1202 18:10:37.445449 76923 fuse_all_reduce_op_pass.cc:79] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 7.
W1202 18:10:38.126017 77401 gpu_resources.cc:217] WARNING: device:  . The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[2022/12/02 18:10:52] ppcls INFO: epoch:0   train step:10   lr: 0.100000, loss:  7.1632 top1:  0.0000 top5:  0.0000 batch_cost: 0.67284 s, reader_cost: 0.00063 s, ips: 380.47789 samples/sec.
[2022/12/02 18:10:59] ppcls INFO: epoch:0   train step:20   lr: 0.100000, loss:  7.0892 top1:  0.0039 top5:  0.0117 batch_cost: 0.67544 s, reader_cost: 0.00070 s, ips: 379.01447 samples/sec.
[2022/12/02 18:11:06] ppcls INFO: epoch:0   train step:30   lr: 0.100000, loss:  7.0782 top1:  0.0039 top5:  0.0039 batch_cost: 0.68038 s, reader_cost: 0.00106 s, ips: 376.26089 samples/sec.
[2022/12/02 18:11:12] ppcls INFO: epoch:0   train step:40   lr: 0.100000, loss:  7.0407 top1:  0.0000 top5:  0.0039 batch_cost: 0.67945 s, reader_cost: 0.00096 s, ips: 376.77433 samples/sec.
[2022/12/02 18:11:19] ppcls INFO: epoch:0   train step:50   lr: 0.100000, loss:  7.2854 top1:  0.0117 top5:  0.0234 batch_cost: 0.67904 s, reader_cost: 0.00099 s, ips: 377.00450 samples/sec.
[2022/12/02 18:11:26] ppcls INFO: epoch:0   train step:60   lr: 0.100000, loss:  6.9052 top1:  0.0039 top5:  0.0078 batch_cost: 0.67851 s, reader_cost: 0.00096 s, ips: 377.29640 samples/sec.
[2022/12/02 18:11:33] ppcls INFO: epoch:0   train step:70   lr: 0.100000, loss:  6.8831 top1:  0.0000 top5:  0.0117 batch_cost: 0.67812 s, reader_cost: 0.00091 s, ips: 377.51701 samples/sec.
[2022/12/02 18:11:39] ppcls INFO: epoch:0   train step:80   lr: 0.100000, loss:  6.9036 top1:  0.0000 top5:  0.0078 batch_cost: 0.67794 s, reader_cost: 0.00086 s, ips: 377.61712 samples/sec.
[2022/12/02 18:11:46] ppcls INFO: epoch:0   train step:90   lr: 0.100000, loss:  6.9150 top1:  0.0039 top5:  0.0078 batch_cost: 0.67779 s, reader_cost: 0.00084 s, ips: 377.69902 samples/sec.
[2022/12/02 18:11:53] ppcls INFO: epoch:0   train step:100  lr: 0.100000, loss:  6.9054 top1:  0.0000 top5:  0.0234 batch_cost: 0.67788 s, reader_cost: 0.00081 s, ips: 377.64903 samples/sec.
[2022/12/02 18:12:00] ppcls INFO: epoch:0   train step:110  lr: 0.100000, loss:  6.9492 top1:  0.0039 top5:  0.0117 batch_cost: 0.67805 s, reader_cost: 0.00077 s, ips: 377.55200 samples/sec.
[2022/12/02 18:12:06] ppcls INFO: epoch:0   train step:120  lr: 0.100000, loss:  6.8734 top1:  0.0039 top5:  0.0078 batch_cost: 0.67805 s, reader_cost: 0.00077 s, ips: 377.55536 samples/sec.
[2022/12/02 18:12:13] ppcls INFO: epoch:0   train step:130  lr: 0.100000, loss:  6.8518 top1:  0.0078 top5:  0.0312 batch_cost: 0.67813 s, reader_cost: 0.00075 s, ips: 377.50846 samples/sec.
[2022/12/02 18:12:20] ppcls INFO: epoch:0   train step:140  lr: 0.100000, loss:  6.8339 top1:  0.0039 top5:  0.0156 batch_cost: 0.67817 s, reader_cost: 0.00073 s, ips: 377.48481 samples/sec.
[2022/12/02 18:12:27] ppcls INFO: epoch:0   train step:150  lr: 0.100000, loss:  6.8218 top1:  0.0039 top5:  0.0195 batch_cost: 0.67830 s, reader_cost: 0.00071 s, ips: 377.41520 samples/sec.
[2022/12/02 18:12:34] ppcls INFO: epoch:0   train step:160  lr: 0.100000, loss:  6.7131 top1:  0.0039 top5:  0.0234 batch_cost: 0.67842 s, reader_cost: 0.00070 s, ips: 377.34825 samples/sec.
[2022/12/02 18:12:40] ppcls INFO: epoch:0   train step:170  lr: 0.100000, loss:  6.6824 top1:  0.0039 top5:  0.0312 batch_cost: 0.67847 s, reader_cost: 0.00069 s, ips: 377.31994 samples/sec.
[2022/12/02 18:12:47] ppcls INFO: epoch:0   train step:180  lr: 0.100000, loss:  6.5460 top1:  0.0078 top5:  0.0391 batch_cost: 0.67853 s, reader_cost: 0.00068 s, ips: 377.28606 samples/sec.
[2022/12/02 18:12:54] ppcls INFO: epoch:0   train step:190  lr: 0.100000, loss:  6.5451 top1:  0.0156 top5:  0.0273 batch_cost: 0.67865 s, reader_cost: 0.00067 s, ips: 377.22068 samples/sec.
[2022/12/02 18:13:01] ppcls INFO: epoch:0   train step:200  lr: 0.100000, loss:  6.5107 top1:  0.0117 top5:  0.0352 batch_cost: 0.67872 s, reader_cost: 0.00066 s, ips: 377.18192 samples/sec.
[2022/12/02 18:13:08] ppcls INFO: epoch:0   train step:210  lr: 0.100000, loss:  6.3870 top1:  0.0078 top5:  0.0430 batch_cost: 0.67878 s, reader_cost: 0.00066 s, ips: 377.14737 samples/sec.
[2022/12/02 18:13:14] ppcls INFO: epoch:0   train step:220  lr: 0.100000, loss:  6.3949 top1:  0.0039 top5:  0.0625 batch_cost: 0.67888 s, reader_cost: 0.00065 s, ips: 377.09168 samples/sec.
[2022/12/02 18:13:21] ppcls INFO: epoch:0   train step:230  lr: 0.100000, loss:  6.4059 top1:  0.0078 top5:  0.0352 batch_cost: 0.67886 s, reader_cost: 0.00065 s, ips: 377.10375 samples/sec.
[2022/12/02 18:13:28] ppcls INFO: epoch:0   train step:240  lr: 0.100000, loss:  6.3456 top1:  0.0117 top5:  0.0312 batch_cost: 0.67893 s, reader_cost: 0.00064 s, ips: 377.06525 samples/sec.
[2022/12/02 18:13:35] ppcls INFO: epoch:0   train step:250  lr: 0.100000, loss:  6.2118 top1:  0.0117 top5:  0.0547 batch_cost: 0.67898 s, reader_cost: 0.00064 s, ips: 377.03399 samples/sec.
[2022/12/02 18:13:42] ppcls INFO: epoch:0   train step:260  lr: 0.100000, loss:  6.2446 top1:  0.0117 top5:  0.0742 batch_cost: 0.67902 s, reader_cost: 0.00064 s, ips: 377.01125 samples/sec.
[2022/12/02 18:13:48] ppcls INFO: epoch:0   train step:270  lr: 0.100000, loss:  6.1998 top1:  0.0156 top5:  0.0703 batch_cost: 0.67910 s, reader_cost: 0.00063 s, ips: 376.96802 samples/sec.
[2022/12/02 18:13:55] ppcls INFO: epoch:0   train step:280  lr: 0.100000, loss:  6.1361 top1:  0.0234 top5:  0.0547 batch_cost: 0.67918 s, reader_cost: 0.00063 s, ips: 376.92731 samples/sec.
[2022/12/02 18:14:02] ppcls INFO: epoch:0   train step:290  lr: 0.100000, loss:  6.1337 top1:  0.0195 top5:  0.0781 batch_cost: 0.67922 s, reader_cost: 0.00063 s, ips: 376.90272 samples/sec.
[2022/12/02 18:14:09] ppcls INFO: epoch:0   train step:300  lr: 0.100000, loss:  5.9668 top1:  0.0312 top5:  0.0898 batch_cost: 0.67928 s, reader_cost: 0.00063 s, ips: 376.87066 samples/sec.
[2022/12/02 18:14:16] ppcls INFO: epoch:0   train step:310  lr: 0.100000, loss:  5.9797 top1:  0.0352 top5:  0.1055 batch_cost: 0.67929 s, reader_cost: 0.00063 s, ips: 376.86390 samples/sec.
[2022/12/02 18:14:23] ppcls INFO: epoch:0   train step:320  lr: 0.100000, loss:  5.9454 top1:  0.0352 top5:  0.1133 batch_cost: 0.67934 s, reader_cost: 0.00063 s, ips: 376.83451 samples/sec.
[2022/12/02 18:14:29] ppcls INFO: epoch:0   train step:330  lr: 0.100000, loss:  5.8276 top1:  0.0156 top5:  0.1055 batch_cost: 0.67936 s, reader_cost: 0.00063 s, ips: 376.82362 samples/sec.
[2022/12/02 18:14:36] ppcls INFO: epoch:0   train step:340  lr: 0.100000, loss:  5.7139 top1:  0.0312 top5:  0.1094 batch_cost: 0.67941 s, reader_cost: 0.00062 s, ips: 376.79491 samples/sec.
[2022/12/02 18:14:43] ppcls INFO: epoch:0   train step:350  lr: 0.100000, loss:  5.8304 top1:  0.0430 top5:  0.0859 batch_cost: 0.67947 s, reader_cost: 0.00062 s, ips: 376.76576 samples/sec.
[2022/12/02 18:14:50] ppcls INFO: epoch:0   train step:360  lr: 0.100000, loss:  5.6736 top1:  0.0391 top5:  0.1719 batch_cost: 0.67948 s, reader_cost: 0.00062 s, ips: 376.75790 samples/sec.
[2022/12/02 18:14:57] ppcls INFO: epoch:0   train step:370  lr: 0.100000, loss:  5.7069 top1:  0.0430 top5:  0.1406 batch_cost: 0.67954 s, reader_cost: 0.00062 s, ips: 376.72275 samples/sec.
[2022/12/02 18:15:03] ppcls INFO: epoch:0   train step:380  lr: 0.100000, loss:  5.6924 top1:  0.0430 top5:  0.0977 batch_cost: 0.67970 s, reader_cost: 0.00065 s, ips: 376.63629 samples/sec.
[2022/12/02 18:15:10] ppcls INFO: epoch:0   train step:390  lr: 0.100000, loss:  5.7564 top1:  0.0508 top5:  0.1094 batch_cost: 0.67972 s, reader_cost: 0.00063 s, ips: 376.62566 samples/sec.
[2022/12/02 18:15:13] ppcls INFO: END epoch:0   train  loss:  6.4939 top1:  0.0141 top5:  0.0483 batch_cost: 0.68320 s, reader_cost: 0.00063 s, batch_cost_sum: 264.39889 s,
[2022/12/02 18:15:14] ppcls INFO: Already save model in ./output/ResNet50/0
[2022/12/02 18:15:26] ppcls INFO: epoch:1   train step:10   lr: 0.100000, loss:  5.5424 top1:  0.0312 top5:  0.1133 batch_cost: 0.70390 s, reader_cost: 0.00883 s, ips: 363.68871 samples/sec.
[2022/12/02 18:15:33] ppcls INFO: epoch:1   train step:20   lr: 0.100000, loss:  5.6224 top1:  0.0508 top5:  0.1289 batch_cost: 0.70730 s, reader_cost: 0.00850 s, ips: 361.93977 samples/sec.
[2022/12/02 18:15:40] ppcls INFO: epoch:1   train step:30   lr: 0.100000, loss:  5.5120 top1:  0.0547 top5:  0.1406 batch_cost: 0.69745 s, reader_cost: 0.00552 s, ips: 367.05129 samples/sec.
[2022/12/02 18:15:46] ppcls INFO: epoch:1   train step:40   lr: 0.100000, loss:  5.4382 top1:  0.0664 top5:  0.1680 batch_cost: 0.69284 s, reader_cost: 0.00426 s, ips: 369.49239 samples/sec.
[2022/12/02 18:15:53] ppcls INFO: epoch:1   train step:50   lr: 0.100000, loss:  5.3947 top1:  0.0352 top5:  0.1602 batch_cost: 0.69057 s, reader_cost: 0.00351 s, ips: 370.70834 samples/sec.
[2022/12/02 18:16:00] ppcls INFO: epoch:1   train step:60   lr: 0.100000, loss:  5.4128 top1:  0.0547 top5:  0.1680 batch_cost: 0.68895 s, reader_cost: 0.00300 s, ips: 371.58129 samples/sec.
[2022/12/02 18:16:07] ppcls INFO: epoch:1   train step:70   lr: 0.100000, loss:  5.4614 top1:  0.0469 top5:  0.1680 batch_cost: 0.68778 s, reader_cost: 0.00266 s, ips: 372.21409 samples/sec.
[2022/12/02 18:16:14] ppcls INFO: epoch:1   train step:80   lr: 0.100000, loss:  5.3261 top1:  0.0625 top5:  0.1992 batch_cost: 0.68698 s, reader_cost: 0.00240 s, ips: 372.64706 samples/sec.
[2022/12/02 18:16:20] ppcls INFO: epoch:1   train step:90   lr: 0.100000, loss:  5.4854 top1:  0.0430 top5:  0.1602 batch_cost: 0.68617 s, reader_cost: 0.00220 s, ips: 373.08752 samples/sec.
[2022/12/02 18:16:27] ppcls INFO: epoch:1   train step:100  lr: 0.100000, loss:  5.4142 top1:  0.0859 top5:  0.1875 batch_cost: 0.68604 s, reader_cost: 0.00205 s, ips: 373.15817 samples/sec.
[2022/12/02 18:16:34] ppcls INFO: epoch:1   train step:110  lr: 0.100000, loss:  5.0349 top1:  0.0859 top5:  0.2305 batch_cost: 0.68567 s, reader_cost: 0.00193 s, ips: 373.35899 samples/sec.
[2022/12/02 18:16:41] ppcls INFO: epoch:1   train step:120  lr: 0.100000, loss:  5.2533 top1:  0.0781 top5:  0.2070 batch_cost: 0.68530 s, reader_cost: 0.00186 s, ips: 373.55901 samples/sec.
[2022/12/02 18:16:48] ppcls INFO: epoch:1   train step:130  lr: 0.100000, loss:  5.0735 top1:  0.0625 top5:  0.1641 batch_cost: 0.68502 s, reader_cost: 0.00176 s, ips: 373.71234 samples/sec.
[2022/12/02 18:16:55] ppcls INFO: epoch:1   train step:140  lr: 0.100000, loss:  5.0839 top1:  0.0977 top5:  0.2148 batch_cost: 0.68474 s, reader_cost: 0.00168 s, ips: 373.86322 samples/sec.
[2022/12/02 18:17:01] ppcls INFO: epoch:1   train step:150  lr: 0.100000, loss:  4.8861 top1:  0.0938 top5:  0.2266 batch_cost: 0.68448 s, reader_cost: 0.00160 s, ips: 374.00390 samples/sec.
[2022/12/02 18:17:08] ppcls INFO: epoch:1   train step:160  lr: 0.100000, loss:  5.0696 top1:  0.0742 top5:  0.2344 batch_cost: 0.68427 s, reader_cost: 0.00154 s, ips: 374.12075 samples/sec.
[2022/12/02 18:17:15] ppcls INFO: epoch:1   train step:170  lr: 0.100000, loss:  5.1574 top1:  0.0586 top5:  0.1719 batch_cost: 0.68404 s, reader_cost: 0.00148 s, ips: 374.24760 samples/sec.
[2022/12/02 18:17:22] ppcls INFO: epoch:1   train step:180  lr: 0.100000, loss:  4.9909 top1:  0.1094 top5:  0.2383 batch_cost: 0.68390 s, reader_cost: 0.00144 s, ips: 374.32355 samples/sec.
[2022/12/02 18:17:29] ppcls INFO: epoch:1   train step:190  lr: 0.100000, loss:  5.0798 top1:  0.0586 top5:  0.2227 batch_cost: 0.68372 s, reader_cost: 0.00140 s, ips: 374.42465 samples/sec.
[2022/12/02 18:17:35] ppcls INFO: epoch:1   train step:200  lr: 0.100000, loss:  5.0049 top1:  0.1133 top5:  0.2188 batch_cost: 0.68357 s, reader_cost: 0.00137 s, ips: 374.50569 samples/sec.
[2022/12/02 18:17:42] ppcls INFO: epoch:1   train step:210  lr: 0.100000, loss:  4.8267 top1:  0.0977 top5:  0.2852 batch_cost: 0.68346 s, reader_cost: 0.00134 s, ips: 374.56619 samples/sec.
[2022/12/02 18:17:49] ppcls INFO: epoch:1   train step:220  lr: 0.100000, loss:  5.1076 top1:  0.0820 top5:  0.2227 batch_cost: 0.68337 s, reader_cost: 0.00130 s, ips: 374.61168 samples/sec.
[2022/12/02 18:17:56] ppcls INFO: epoch:1   train step:230  lr: 0.100000, loss:  4.9834 top1:  0.0742 top5:  0.2344 batch_cost: 0.68328 s, reader_cost: 0.00128 s, ips: 374.66369 samples/sec.
[2022/12/02 18:18:03] ppcls INFO: epoch:1   train step:240  lr: 0.100000, loss:  4.9258 top1:  0.0938 top5:  0.2305 batch_cost: 0.68318 s, reader_cost: 0.00126 s, ips: 374.71907 samples/sec.
[2022/12/02 18:18:09] ppcls INFO: epoch:1   train step:250  lr: 0.100000, loss:  4.6936 top1:  0.1250 top5:  0.2852 batch_cost: 0.68312 s, reader_cost: 0.00127 s, ips: 374.74945 samples/sec.
[2022/12/02 18:18:16] ppcls INFO: epoch:1   train step:260  lr: 0.100000, loss:  4.8311 top1:  0.1133 top5:  0.2656 batch_cost: 0.68301 s, reader_cost: 0.00124 s, ips: 374.81317 samples/sec.
[2022/12/02 18:18:23] ppcls INFO: epoch:1   train step:270  lr: 0.100000, loss:  4.6601 top1:  0.1367 top5:  0.2891 batch_cost: 0.68295 s, reader_cost: 0.00122 s, ips: 374.84631 samples/sec.
[2022/12/02 18:18:30] ppcls INFO: epoch:1   train step:280  lr: 0.100000, loss:  4.8704 top1:  0.0977 top5:  0.2773 batch_cost: 0.68288 s, reader_cost: 0.00121 s, ips: 374.88063 samples/sec.
[2022/12/02 18:18:37] ppcls INFO: epoch:1   train step:290  lr: 0.100000, loss:  4.7200 top1:  0.1523 top5:  0.3086 batch_cost: 0.68283 s, reader_cost: 0.00118 s, ips: 374.90931 samples/sec.
[2022/12/02 18:18:44] ppcls INFO: epoch:1   train step:300  lr: 0.100000, loss:  4.7899 top1:  0.1016 top5:  0.3125 batch_cost: 0.68275 s, reader_cost: 0.00116 s, ips: 374.95331 samples/sec.
[2022/12/02 18:18:50] ppcls INFO: epoch:1   train step:310  lr: 0.100000, loss:  4.5822 top1:  0.1211 top5:  0.2852 batch_cost: 0.68270 s, reader_cost: 0.00114 s, ips: 374.98291 samples/sec.
[2022/12/02 18:18:57] ppcls INFO: epoch:1   train step:320  lr: 0.100000, loss:  4.7736 top1:  0.1250 top5:  0.2891 batch_cost: 0.68268 s, reader_cost: 0.00112 s, ips: 374.99287 samples/sec.
[2022/12/02 18:19:04] ppcls INFO: epoch:1   train step:330  lr: 0.100000, loss:  4.5890 top1:  0.1289 top5:  0.2969 batch_cost: 0.68262 s, reader_cost: 0.00111 s, ips: 375.02783 samples/sec.
LAUNCH INFO 2022-12-02 18:20:10,933 Terminating with signal 15
LAUNCH INFO 2022-12-02 18:20:49,947 Exit with signal 15
[2022/12/02 18:19:11] ppcls INFO: epoch:1   train step:340  lr: 0.100000, loss:  4.5626 top1:  0.1328 top5:  0.2930 batch_cost: 0.68259 s, reader_cost: 0.00109 s, ips: 375.04435 samples/sec.
[2022/12/02 18:19:18] ppcls INFO: epoch:1   train step:350  lr: 0.100000, loss:  4.4336 top1:  0.1367 top5:  0.2891 batch_cost: 0.68248 s, reader_cost: 0.00108 s, ips: 375.10140 samples/sec.
[2022/12/02 18:19:24] ppcls INFO: epoch:1   train step:360  lr: 0.100000, loss:  4.4671 top1:  0.1094 top5:  0.3320 batch_cost: 0.68245 s, reader_cost: 0.00106 s, ips: 375.12176 samples/sec.
[2022/12/02 18:19:31] ppcls INFO: epoch:1   train step:370  lr: 0.100000, loss:  4.1926 top1:  0.1875 top5:  0.3516 batch_cost: 0.68239 s, reader_cost: 0.00105 s, ips: 375.15227 samples/sec.
[2022/12/02 18:19:38] ppcls INFO: epoch:1   train step:380  lr: 0.100000, loss:  4.3325 top1:  0.1836 top5:  0.3555 batch_cost: 0.68247 s, reader_cost: 0.00117 s, ips: 375.10670 samples/sec.
[2022/12/02 18:19:45] ppcls INFO: epoch:1   train step:390  lr: 0.100000, loss:  4.2702 top1:  0.1680 top5:  0.3750 batch_cost: 0.68240 s, reader_cost: 0.00114 s, ips: 375.14494 samples/sec.
[2022/12/02 18:19:47] ppcls INFO: END epoch:1   train  loss:  4.9917 top1:  0.0964 top5:  0.2410 batch_cost: 0.68176 s, reader_cost: 0.00114 s, batch_cost_sum: 263.84099 s,
[2022/12/02 18:19:47] ppcls INFO: Already save model in ./output/ResNet50/1
[2022/12/02 18:19:59] ppcls INFO: epoch:2   train step:10   lr: 0.100000, loss:  4.5084 top1:  0.1016 top5:  0.2773 batch_cost: 0.70440 s, reader_cost: 0.00732 s, ips: 363.43000 samples/sec.
[2022/12/02 18:20:06] ppcls INFO: epoch:2   train step:20   lr: 0.100000, loss:  4.2478 top1:  0.1797 top5:  0.3750 batch_cost: 0.69673 s, reader_cost: 0.00588 s, ips: 367.43255 samples/sec.
