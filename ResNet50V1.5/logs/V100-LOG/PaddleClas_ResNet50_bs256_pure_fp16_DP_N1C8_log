LAUNCH INFO 2022-11-30 20:24:58,694 -----------  Configuration  ----------------------
LAUNCH INFO 2022-11-30 20:24:58,694 devices: 0,1,2,3,4,5,6,7
LAUNCH INFO 2022-11-30 20:24:58,694 elastic_level: -1
LAUNCH INFO 2022-11-30 20:24:58,694 elastic_timeout: 30
LAUNCH INFO 2022-11-30 20:24:58,695 gloo_port: 6767
LAUNCH INFO 2022-11-30 20:24:58,695 host: None
LAUNCH INFO 2022-11-30 20:24:58,695 ips: None
LAUNCH INFO 2022-11-30 20:24:58,695 job_id: default
LAUNCH INFO 2022-11-30 20:24:58,695 legacy: False
LAUNCH INFO 2022-11-30 20:24:58,695 log_dir: log
LAUNCH INFO 2022-11-30 20:24:58,695 log_level: INFO
LAUNCH INFO 2022-11-30 20:24:58,695 master: None
LAUNCH INFO 2022-11-30 20:24:58,695 max_restart: 3
LAUNCH INFO 2022-11-30 20:24:58,695 nnodes: 1
LAUNCH INFO 2022-11-30 20:24:58,695 nproc_per_node: None
LAUNCH INFO 2022-11-30 20:24:58,695 rank: -1
LAUNCH INFO 2022-11-30 20:24:58,695 run_mode: collective
LAUNCH INFO 2022-11-30 20:24:58,695 server_num: None
LAUNCH INFO 2022-11-30 20:24:58,695 servers: 
LAUNCH INFO 2022-11-30 20:24:58,695 start_port: 6070
LAUNCH INFO 2022-11-30 20:24:58,695 trainer_num: None
LAUNCH INFO 2022-11-30 20:24:58,695 trainers: 
LAUNCH INFO 2022-11-30 20:24:58,695 training_script: ppcls/static/train.py
LAUNCH INFO 2022-11-30 20:24:58,695 training_script_args: ['-c', 'ppcls/configs/ImageNet/ResNet/ResNet50_amp_O2_ultra.yaml', '-o', 'DataLoader.Train.sampler.batch_size=256', '-o', 'Global.epochs=8', '-o', 'DataLoader.Train.loader.num_workers=4', '-o', 'Global.eval_during_train=False', '-o', 'fuse_elewise_add_act_ops=True', '-o', 'enable_addto=True']
LAUNCH INFO 2022-11-30 20:24:58,695 with_gloo: 1
LAUNCH INFO 2022-11-30 20:24:58,695 --------------------------------------------------
LAUNCH INFO 2022-11-30 20:24:58,696 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2022-11-30 20:24:58,703 Run Pod: pqtmkr, replicas 8, status ready
LAUNCH INFO 2022-11-30 20:24:58,801 Watching Pod: pqtmkr, replicas 8, status running
/paddle/perf/pa240_perf/PaddleClas/ppcls/data/preprocess/ops/timm_autoaugment.py:39: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)
/paddle/perf/pa240_perf/PaddleClas/ppcls/data/preprocess/ops/timm_autoaugment.py:39: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)
A new field (fuse_elewise_add_act_ops) detected!
A new field (enable_addto) detected!
[2022/11/30 20:25:01] ppcls INFO: 
===========================================================
==        PaddleClas is powered by PaddlePaddle !        ==
===========================================================
==                                                       ==
==   For more info please go to the following website.   ==
==                                                       ==
==       https://github.com/PaddlePaddle/PaddleClas      ==
===========================================================

[2022/11/30 20:25:01] ppcls INFO: Global : 
[2022/11/30 20:25:01] ppcls INFO:     checkpoints : None
[2022/11/30 20:25:01] ppcls INFO:     pretrained_model : None
[2022/11/30 20:25:01] ppcls INFO:     output_dir : ./output/
[2022/11/30 20:25:01] ppcls INFO:     device : gpu
[2022/11/30 20:25:01] ppcls INFO:     save_interval : 1
[2022/11/30 20:25:01] ppcls INFO:     eval_during_train : False
[2022/11/30 20:25:01] ppcls INFO:     eval_interval : 1
[2022/11/30 20:25:01] ppcls INFO:     epochs : 8
[2022/11/30 20:25:01] ppcls INFO:     print_batch_step : 10
[2022/11/30 20:25:01] ppcls INFO:     use_visualdl : False
[2022/11/30 20:25:01] ppcls INFO:     image_channel : 4
[2022/11/30 20:25:01] ppcls INFO:     image_shape : [4, 224, 224]
[2022/11/30 20:25:01] ppcls INFO:     save_inference_dir : ./inference
[2022/11/30 20:25:01] ppcls INFO:     to_static : False
[2022/11/30 20:25:01] ppcls INFO:     use_dali : True
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: AMP : 
[2022/11/30 20:25:01] ppcls INFO:     scale_loss : 128.0
[2022/11/30 20:25:01] ppcls INFO:     use_dynamic_loss_scaling : True
[2022/11/30 20:25:01] ppcls INFO:     level : O2
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: Arch : 
[2022/11/30 20:25:01] ppcls INFO:     name : ResNet50
[2022/11/30 20:25:01] ppcls INFO:     class_num : 1000
[2022/11/30 20:25:01] ppcls INFO:     input_image_channel : 4
[2022/11/30 20:25:01] ppcls INFO:     data_format : NHWC
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: Loss : 
[2022/11/30 20:25:01] ppcls INFO:     Train : 
[2022/11/30 20:25:01] ppcls INFO:         CELoss : 
[2022/11/30 20:25:01] ppcls INFO:             weight : 1.0
[2022/11/30 20:25:01] ppcls INFO:     Eval : 
[2022/11/30 20:25:01] ppcls INFO:         CELoss : 
[2022/11/30 20:25:01] ppcls INFO:             weight : 1.0
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: Optimizer : 
[2022/11/30 20:25:01] ppcls INFO:     name : Momentum
[2022/11/30 20:25:01] ppcls INFO:     momentum : 0.9
[2022/11/30 20:25:01] ppcls INFO:     multi_precision : True
[2022/11/30 20:25:01] ppcls INFO:     lr : 
[2022/11/30 20:25:01] ppcls INFO:         name : Piecewise
[2022/11/30 20:25:01] ppcls INFO:         learning_rate : 0.1
[2022/11/30 20:25:01] ppcls INFO:         decay_epochs : [30, 60, 90]
[2022/11/30 20:25:01] ppcls INFO:         values : [0.1, 0.01, 0.001, 0.0001]
[2022/11/30 20:25:01] ppcls INFO:     regularizer : 
[2022/11/30 20:25:01] ppcls INFO:         name : L2
[2022/11/30 20:25:01] ppcls INFO:         coeff : 0.0001
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: DataLoader : 
[2022/11/30 20:25:01] ppcls INFO:     Train : 
[2022/11/30 20:25:01] ppcls INFO:         dataset : 
[2022/11/30 20:25:01] ppcls INFO:             name : ImageNetDataset
[2022/11/30 20:25:01] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2022/11/30 20:25:01] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/train_list.txt
[2022/11/30 20:25:01] ppcls INFO:             transform_ops : 
[2022/11/30 20:25:01] ppcls INFO:                 DecodeImage : 
[2022/11/30 20:25:01] ppcls INFO:                     to_rgb : True
[2022/11/30 20:25:01] ppcls INFO:                     channel_first : False
[2022/11/30 20:25:01] ppcls INFO:                 RandCropImage : 
[2022/11/30 20:25:01] ppcls INFO:                     size : 224
[2022/11/30 20:25:01] ppcls INFO:                 RandFlipImage : 
[2022/11/30 20:25:01] ppcls INFO:                     flip_code : 1
[2022/11/30 20:25:01] ppcls INFO:                 NormalizeImage : 
[2022/11/30 20:25:01] ppcls INFO:                     scale : 1.0/255.0
[2022/11/30 20:25:01] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2022/11/30 20:25:01] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2022/11/30 20:25:01] ppcls INFO:                     order : 
[2022/11/30 20:25:01] ppcls INFO:                     output_fp16 : True
[2022/11/30 20:25:01] ppcls INFO:                     channel_num : 4
[2022/11/30 20:25:01] ppcls INFO:         sampler : 
[2022/11/30 20:25:01] ppcls INFO:             name : DistributedBatchSampler
[2022/11/30 20:25:01] ppcls INFO:             batch_size : 256
[2022/11/30 20:25:01] ppcls INFO:             drop_last : False
[2022/11/30 20:25:01] ppcls INFO:             shuffle : True
[2022/11/30 20:25:01] ppcls INFO:         loader : 
[2022/11/30 20:25:01] ppcls INFO:             num_workers : 4
[2022/11/30 20:25:01] ppcls INFO:             use_shared_memory : True
[2022/11/30 20:25:01] ppcls INFO:     Eval : 
[2022/11/30 20:25:01] ppcls INFO:         dataset : 
[2022/11/30 20:25:01] ppcls INFO:             name : ImageNetDataset
[2022/11/30 20:25:01] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2022/11/30 20:25:01] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/val_list.txt
[2022/11/30 20:25:01] ppcls INFO:             transform_ops : 
[2022/11/30 20:25:01] ppcls INFO:                 DecodeImage : 
[2022/11/30 20:25:01] ppcls INFO:                     to_rgb : True
[2022/11/30 20:25:01] ppcls INFO:                     channel_first : False
[2022/11/30 20:25:01] ppcls INFO:                 ResizeImage : 
[2022/11/30 20:25:01] ppcls INFO:                     resize_short : 256
[2022/11/30 20:25:01] ppcls INFO:                 CropImage : 
[2022/11/30 20:25:01] ppcls INFO:                     size : 224
[2022/11/30 20:25:01] ppcls INFO:                 NormalizeImage : 
[2022/11/30 20:25:01] ppcls INFO:                     scale : 1.0/255.0
[2022/11/30 20:25:01] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2022/11/30 20:25:01] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2022/11/30 20:25:01] ppcls INFO:                     order : 
[2022/11/30 20:25:01] ppcls INFO:                     channel_num : 4
[2022/11/30 20:25:01] ppcls INFO:         sampler : 
[2022/11/30 20:25:01] ppcls INFO:             name : DistributedBatchSampler
[2022/11/30 20:25:01] ppcls INFO:             batch_size : 64
[2022/11/30 20:25:01] ppcls INFO:             drop_last : False
[2022/11/30 20:25:01] ppcls INFO:             shuffle : False
[2022/11/30 20:25:01] ppcls INFO:         loader : 
[2022/11/30 20:25:01] ppcls INFO:             num_workers : 4
[2022/11/30 20:25:01] ppcls INFO:             use_shared_memory : True
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: Infer : 
[2022/11/30 20:25:01] ppcls INFO:     infer_imgs : docs/images/inference_deployment/whl_demo.jpg
[2022/11/30 20:25:01] ppcls INFO:     batch_size : 10
[2022/11/30 20:25:01] ppcls INFO:     transforms : 
[2022/11/30 20:25:01] ppcls INFO:         DecodeImage : 
[2022/11/30 20:25:01] ppcls INFO:             to_rgb : True
[2022/11/30 20:25:01] ppcls INFO:             channel_first : False
[2022/11/30 20:25:01] ppcls INFO:         ResizeImage : 
[2022/11/30 20:25:01] ppcls INFO:             resize_short : 256
[2022/11/30 20:25:01] ppcls INFO:         CropImage : 
[2022/11/30 20:25:01] ppcls INFO:             size : 224
[2022/11/30 20:25:01] ppcls INFO:         NormalizeImage : 
[2022/11/30 20:25:01] ppcls INFO:             scale : 1.0/255.0
[2022/11/30 20:25:01] ppcls INFO:             mean : [0.485, 0.456, 0.406]
[2022/11/30 20:25:01] ppcls INFO:             std : [0.229, 0.224, 0.225]
[2022/11/30 20:25:01] ppcls INFO:             order : 
[2022/11/30 20:25:01] ppcls INFO:             channel_num : 4
[2022/11/30 20:25:01] ppcls INFO:         ToCHWImage : None
[2022/11/30 20:25:01] ppcls INFO:     PostProcess : 
[2022/11/30 20:25:01] ppcls INFO:         name : Topk
[2022/11/30 20:25:01] ppcls INFO:         topk : 5
[2022/11/30 20:25:01] ppcls INFO:         class_id_map_file : ppcls/utils/imagenet1k_label_list.txt
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: Metric : 
[2022/11/30 20:25:01] ppcls INFO:     Train : 
[2022/11/30 20:25:01] ppcls INFO:         TopkAcc : 
[2022/11/30 20:25:01] ppcls INFO:             topk : [1, 5]
[2022/11/30 20:25:01] ppcls INFO:     Eval : 
[2022/11/30 20:25:01] ppcls INFO:         TopkAcc : 
[2022/11/30 20:25:01] ppcls INFO:             topk : [1, 5]
[2022/11/30 20:25:01] ppcls INFO: ------------------------------------------------------------
[2022/11/30 20:25:01] ppcls INFO: fuse_elewise_add_act_ops : True
[2022/11/30 20:25:01] ppcls INFO: enable_addto : True
[/opt/dali/dali/operators/image/resize/resampling_attr.cc:103] The default behavior for LINEAR interpolation type has been changed to apply an antialiasing filter. If you didn't mean to apply an antialiasing filter, please use `antialias=False`
W1130 20:25:07.435849 17987 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1130 20:25:07.439637 17987 gpu_resources.cc:91] device: 0, cuDNN Version: 8.1.
[2022/11/30 20:25:07] ppcls WARNING: "init_res" will be deprecated, please use "init_net" instead.
[2022-11-30 20:25:07,886] [ WARNING] fleet.py:1073 - It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
server not ready, wait 3 sec to retry...
[2022/11/30 20:25:11] ppcls WARNING: Only support FP16 evaluation when AMP O2 is enabled.
W1130 20:25:13.150528 17987 build_strategy.cc:124] Currently, fuse_broadcast_ops only works under Reduce mode.
I1130 20:25:13.219179 17987 fuse_pass_base.cc:59] ---  detected 33 subgraphs
I1130 20:25:13.357825 17987 fuse_pass_base.cc:59] ---  detected 33 subgraphs
I1130 20:25:13.406193 17987 fuse_pass_base.cc:59] ---  detected 16 subgraphs
I1130 20:25:13.447719 17987 fuse_pass_base.cc:59] ---  detected 16 subgraphs
W1130 20:25:13.488313 17987 fuse_all_reduce_op_pass.cc:79] Find all_reduce operators: 161. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 10.
W1130 20:25:14.333513 18295 gpu_resources.cc:217] WARNING: device:  . The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
[2022/11/30 20:25:30] ppcls INFO: epoch:0   train step:10   lr: 0.100000, loss:  7.1879 top1:  0.0000 top5:  0.0039 batch_cost: 0.18833 s, reader_cost: 0.00079 s, ips: 1359.29190 samples/sec.
[2022/11/30 20:25:32] ppcls INFO: epoch:0   train step:20   lr: 0.100000, loss:  7.2344 top1:  0.0039 top5:  0.0078 batch_cost: 0.18845 s, reader_cost: 0.00079 s, ips: 1358.47430 samples/sec.
[2022/11/30 20:25:34] ppcls INFO: epoch:0   train step:30   lr: 0.100000, loss:  7.0560 top1:  0.0000 top5:  0.0039 batch_cost: 0.18843 s, reader_cost: 0.00076 s, ips: 1358.58699 samples/sec.
[2022/11/30 20:25:35] ppcls INFO: epoch:0   train step:40   lr: 0.100000, loss:  7.3534 top1:  0.0000 top5:  0.0000 batch_cost: 0.18879 s, reader_cost: 0.00083 s, ips: 1356.01294 samples/sec.
[2022/11/30 20:25:37] ppcls INFO: epoch:0   train step:50   lr: 0.100000, loss:  6.9446 top1:  0.0000 top5:  0.0039 batch_cost: 0.18877 s, reader_cost: 0.00082 s, ips: 1356.14361 samples/sec.
[2022/11/30 20:25:39] ppcls INFO: epoch:0   train step:60   lr: 0.100000, loss:  6.9075 top1:  0.0039 top5:  0.0078 batch_cost: 0.18892 s, reader_cost: 0.00083 s, ips: 1355.10434 samples/sec.
[2022/11/30 20:25:41] ppcls INFO: epoch:0   train step:70   lr: 0.100000, loss:  6.8597 top1:  0.0039 top5:  0.0117 batch_cost: 0.18909 s, reader_cost: 0.00096 s, ips: 1353.88347 samples/sec.
[2022/11/30 20:25:43] ppcls INFO: epoch:0   train step:80   lr: 0.100000, loss:  6.7253 top1:  0.0078 top5:  0.0156 batch_cost: 0.18913 s, reader_cost: 0.00094 s, ips: 1353.57826 samples/sec.
[2022/11/30 20:25:45] ppcls INFO: epoch:0   train step:90   lr: 0.100000, loss:  6.7009 top1:  0.0000 top5:  0.0273 batch_cost: 0.18907 s, reader_cost: 0.00092 s, ips: 1353.98211 samples/sec.
[2022/11/30 20:25:47] ppcls INFO: epoch:0   train step:100  lr: 0.100000, loss:  6.6754 top1:  0.0078 top5:  0.0352 batch_cost: 0.18911 s, reader_cost: 0.00092 s, ips: 1353.71195 samples/sec.
[2022/11/30 20:25:49] ppcls INFO: epoch:0   train step:110  lr: 0.100000, loss:  6.4690 top1:  0.0078 top5:  0.0234 batch_cost: 0.18909 s, reader_cost: 0.00092 s, ips: 1353.85577 samples/sec.
[2022/11/30 20:25:51] ppcls INFO: epoch:0   train step:120  lr: 0.100000, loss:  6.4427 top1:  0.0078 top5:  0.0352 batch_cost: 0.18916 s, reader_cost: 0.00091 s, ips: 1353.34968 samples/sec.
[2022/11/30 20:25:52] ppcls INFO: epoch:0   train step:130  lr: 0.100000, loss:  6.4415 top1:  0.0078 top5:  0.0352 batch_cost: 0.18919 s, reader_cost: 0.00091 s, ips: 1353.13541 samples/sec.
[2022/11/30 20:25:54] ppcls INFO: epoch:0   train step:140  lr: 0.100000, loss:  6.3631 top1:  0.0078 top5:  0.0391 batch_cost: 0.18923 s, reader_cost: 0.00090 s, ips: 1352.82315 samples/sec.
[2022/11/30 20:25:56] ppcls INFO: epoch:0   train step:150  lr: 0.100000, loss:  6.2561 top1:  0.0117 top5:  0.0586 batch_cost: 0.18922 s, reader_cost: 0.00089 s, ips: 1352.94842 samples/sec.
[2022/11/30 20:25:58] ppcls INFO: epoch:0   train step:160  lr: 0.100000, loss:  6.2514 top1:  0.0234 top5:  0.0547 batch_cost: 0.18926 s, reader_cost: 0.00089 s, ips: 1352.65574 samples/sec.
[2022/11/30 20:26:00] ppcls INFO: epoch:0   train step:170  lr: 0.100000, loss:  6.2649 top1:  0.0078 top5:  0.0508 batch_cost: 0.18927 s, reader_cost: 0.00089 s, ips: 1352.56911 samples/sec.
[2022/11/30 20:26:02] ppcls INFO: epoch:0   train step:180  lr: 0.100000, loss:  6.1681 top1:  0.0078 top5:  0.0625 batch_cost: 0.18927 s, reader_cost: 0.00088 s, ips: 1352.58243 samples/sec.
[2022/11/30 20:26:04] ppcls INFO: epoch:0   train step:190  lr: 0.100000, loss:  6.0279 top1:  0.0312 top5:  0.0742 batch_cost: 0.18933 s, reader_cost: 0.00088 s, ips: 1352.16898 samples/sec.
[2022/11/30 20:26:06] ppcls INFO: epoch:0   train step:200  lr: 0.100000, loss:  6.0824 top1:  0.0195 top5:  0.0898 batch_cost: 0.18932 s, reader_cost: 0.00088 s, ips: 1352.20313 samples/sec.
[2022/11/30 20:26:08] ppcls INFO: epoch:0   train step:210  lr: 0.100000, loss:  5.8622 top1:  0.0234 top5:  0.1211 batch_cost: 0.18931 s, reader_cost: 0.00087 s, ips: 1352.28200 samples/sec.
[2022/11/30 20:26:10] ppcls INFO: epoch:0   train step:220  lr: 0.100000, loss:  5.9701 top1:  0.0234 top5:  0.0977 batch_cost: 0.18932 s, reader_cost: 0.00087 s, ips: 1352.20514 samples/sec.
[2022/11/30 20:26:11] ppcls INFO: epoch:0   train step:230  lr: 0.100000, loss:  6.0554 top1:  0.0273 top5:  0.0703 batch_cost: 0.18930 s, reader_cost: 0.00086 s, ips: 1352.35563 samples/sec.
[2022/11/30 20:26:13] ppcls INFO: epoch:0   train step:240  lr: 0.100000, loss:  5.7303 top1:  0.0547 top5:  0.1211 batch_cost: 0.18929 s, reader_cost: 0.00086 s, ips: 1352.39551 samples/sec.
[2022/11/30 20:26:15] ppcls INFO: epoch:0   train step:250  lr: 0.100000, loss:  5.8237 top1:  0.0430 top5:  0.1250 batch_cost: 0.18931 s, reader_cost: 0.00085 s, ips: 1352.25677 samples/sec.
[2022/11/30 20:26:17] ppcls INFO: epoch:0   train step:260  lr: 0.100000, loss:  5.7975 top1:  0.0508 top5:  0.1016 batch_cost: 0.18932 s, reader_cost: 0.00085 s, ips: 1352.20681 samples/sec.
[2022/11/30 20:26:19] ppcls INFO: epoch:0   train step:270  lr: 0.100000, loss:  5.6438 top1:  0.0469 top5:  0.1289 batch_cost: 0.18934 s, reader_cost: 0.00085 s, ips: 1352.04751 samples/sec.
[2022/11/30 20:26:21] ppcls INFO: epoch:0   train step:280  lr: 0.100000, loss:  5.6623 top1:  0.0312 top5:  0.1016 batch_cost: 0.18936 s, reader_cost: 0.00085 s, ips: 1351.93613 samples/sec.
[2022/11/30 20:26:23] ppcls INFO: epoch:0   train step:290  lr: 0.100000, loss:  5.6890 top1:  0.0312 top5:  0.1250 batch_cost: 0.18932 s, reader_cost: 0.00084 s, ips: 1352.17370 samples/sec.
[2022/11/30 20:26:25] ppcls INFO: epoch:0   train step:300  lr: 0.100000, loss:  5.6233 top1:  0.0586 top5:  0.1523 batch_cost: 0.18930 s, reader_cost: 0.00084 s, ips: 1352.38263 samples/sec.
[2022/11/30 20:26:27] ppcls INFO: epoch:0   train step:310  lr: 0.100000, loss:  5.5147 top1:  0.0234 top5:  0.1445 batch_cost: 0.18930 s, reader_cost: 0.00084 s, ips: 1352.36545 samples/sec.
[2022/11/30 20:26:28] ppcls INFO: epoch:0   train step:320  lr: 0.100000, loss:  5.5554 top1:  0.0469 top5:  0.1562 batch_cost: 0.18926 s, reader_cost: 0.00084 s, ips: 1352.61077 samples/sec.
[2022/11/30 20:26:30] ppcls INFO: epoch:0   train step:330  lr: 0.100000, loss:  5.3843 top1:  0.0508 top5:  0.1758 batch_cost: 0.18932 s, reader_cost: 0.00083 s, ips: 1352.19744 samples/sec.
[2022/11/30 20:26:32] ppcls INFO: epoch:0   train step:340  lr: 0.100000, loss:  5.5854 top1:  0.0625 top5:  0.1250 batch_cost: 0.18934 s, reader_cost: 0.00085 s, ips: 1352.03866 samples/sec.
[2022/11/30 20:26:34] ppcls INFO: epoch:0   train step:350  lr: 0.100000, loss:  5.4074 top1:  0.0469 top5:  0.1758 batch_cost: 0.18937 s, reader_cost: 0.00084 s, ips: 1351.84314 samples/sec.
[2022/11/30 20:26:36] ppcls INFO: epoch:0   train step:360  lr: 0.100000, loss:  5.3722 top1:  0.0664 top5:  0.1875 batch_cost: 0.18935 s, reader_cost: 0.00084 s, ips: 1352.02851 samples/sec.
[2022/11/30 20:26:38] ppcls INFO: epoch:0   train step:370  lr: 0.100000, loss:  5.3756 top1:  0.0586 top5:  0.1602 batch_cost: 0.18936 s, reader_cost: 0.00084 s, ips: 1351.88762 samples/sec.
[2022/11/30 20:26:40] ppcls INFO: epoch:0   train step:380  lr: 0.100000, loss:  5.3872 top1:  0.0820 top5:  0.1992 batch_cost: 0.18935 s, reader_cost: 0.00083 s, ips: 1351.97586 samples/sec.
[2022/11/30 20:26:42] ppcls INFO: epoch:0   train step:390  lr: 0.100000, loss:  5.6255 top1:  0.0352 top5:  0.1133 batch_cost: 0.18934 s, reader_cost: 0.00083 s, ips: 1352.03920 samples/sec.
[2022/11/30 20:26:42] ppcls INFO: END epoch:0   train  loss:  6.1406 top1:  0.0272 top5:  0.0833 batch_cost: 0.18934 s, reader_cost: 0.00083 s, batch_cost_sum: 73.27409 s,
[2022/11/30 20:26:43] ppcls INFO: Already save model in ./output/ResNet50/0
[2022/11/30 20:26:45] ppcls INFO: epoch:1   train step:10   lr: 0.100000, loss:  5.2105 top1:  0.0742 top5:  0.1836 batch_cost: 0.18861 s, reader_cost: 0.00083 s, ips: 1357.26612 samples/sec.
[2022/11/30 20:26:47] ppcls INFO: epoch:1   train step:20   lr: 0.100000, loss:  5.3236 top1:  0.0625 top5:  0.1719 batch_cost: 0.18921 s, reader_cost: 0.00087 s, ips: 1352.96528 samples/sec.
[2022/11/30 20:26:49] ppcls INFO: epoch:1   train step:30   lr: 0.100000, loss:  5.2983 top1:  0.0547 top5:  0.1562 batch_cost: 0.18906 s, reader_cost: 0.00081 s, ips: 1354.09251 samples/sec.
[2022/11/30 20:26:51] ppcls INFO: epoch:1   train step:40   lr: 0.100000, loss:  5.1897 top1:  0.0742 top5:  0.1953 batch_cost: 0.18907 s, reader_cost: 0.00079 s, ips: 1353.96073 samples/sec.
[2022/11/30 20:26:53] ppcls INFO: epoch:1   train step:50   lr: 0.100000, loss:  5.0217 top1:  0.0820 top5:  0.2617 batch_cost: 0.18909 s, reader_cost: 0.00089 s, ips: 1353.82592 samples/sec.
[2022/11/30 20:26:55] ppcls INFO: epoch:1   train step:60   lr: 0.100000, loss:  5.1580 top1:  0.0977 top5:  0.1914 batch_cost: 0.18909 s, reader_cost: 0.00088 s, ips: 1353.88460 samples/sec.
[2022/11/30 20:26:56] ppcls INFO: epoch:1   train step:70   lr: 0.100000, loss:  5.0470 top1:  0.0664 top5:  0.2148 batch_cost: 0.18897 s, reader_cost: 0.00087 s, ips: 1354.68448 samples/sec.
[2022/11/30 20:26:58] ppcls INFO: epoch:1   train step:80   lr: 0.100000, loss:  5.3579 top1:  0.0664 top5:  0.1992 batch_cost: 0.18899 s, reader_cost: 0.00086 s, ips: 1354.59493 samples/sec.
[2022/11/30 20:27:00] ppcls INFO: epoch:1   train step:90   lr: 0.100000, loss:  5.1136 top1:  0.0938 top5:  0.2109 batch_cost: 0.18907 s, reader_cost: 0.00086 s, ips: 1354.00314 samples/sec.
[2022/11/30 20:27:02] ppcls INFO: epoch:1   train step:100  lr: 0.100000, loss:  4.6543 top1:  0.1211 top5:  0.3008 batch_cost: 0.18904 s, reader_cost: 0.00084 s, ips: 1354.23854 samples/sec.
[2022/11/30 20:27:04] ppcls INFO: epoch:1   train step:110  lr: 0.100000, loss:  5.0243 top1:  0.0820 top5:  0.2188 batch_cost: 0.18904 s, reader_cost: 0.00085 s, ips: 1354.20556 samples/sec.
[2022/11/30 20:27:06] ppcls INFO: epoch:1   train step:120  lr: 0.100000, loss:  4.7667 top1:  0.1172 top5:  0.3008 batch_cost: 0.18909 s, reader_cost: 0.00084 s, ips: 1353.83035 samples/sec.
[2022/11/30 20:27:08] ppcls INFO: epoch:1   train step:130  lr: 0.100000, loss:  4.8733 top1:  0.0898 top5:  0.2539 batch_cost: 0.18911 s, reader_cost: 0.00084 s, ips: 1353.67785 samples/sec.
[2022/11/30 20:27:10] ppcls INFO: epoch:1   train step:140  lr: 0.100000, loss:  4.9048 top1:  0.1055 top5:  0.2656 batch_cost: 0.18911 s, reader_cost: 0.00084 s, ips: 1353.73175 samples/sec.
[2022/11/30 20:27:12] ppcls INFO: epoch:1   train step:150  lr: 0.100000, loss:  4.8729 top1:  0.1133 top5:  0.2617 batch_cost: 0.18910 s, reader_cost: 0.00083 s, ips: 1353.75223 samples/sec.
[2022/11/30 20:27:14] ppcls INFO: epoch:1   train step:160  lr: 0.100000, loss:  4.7865 top1:  0.0820 top5:  0.2266 batch_cost: 0.18911 s, reader_cost: 0.00084 s, ips: 1353.68618 samples/sec.
[2022/11/30 20:27:15] ppcls INFO: epoch:1   train step:170  lr: 0.100000, loss:  4.8112 top1:  0.0977 top5:  0.2500 batch_cost: 0.18917 s, reader_cost: 0.00083 s, ips: 1353.29933 samples/sec.
[2022/11/30 20:27:17] ppcls INFO: epoch:1   train step:180  lr: 0.100000, loss:  4.6943 top1:  0.1328 top5:  0.2539 batch_cost: 0.18914 s, reader_cost: 0.00083 s, ips: 1353.52883 samples/sec.
[2022/11/30 20:27:19] ppcls INFO: epoch:1   train step:190  lr: 0.100000, loss:  4.7431 top1:  0.1172 top5:  0.2969 batch_cost: 0.18911 s, reader_cost: 0.00082 s, ips: 1353.73027 samples/sec.
[2022/11/30 20:27:21] ppcls INFO: epoch:1   train step:200  lr: 0.100000, loss:  4.7658 top1:  0.1133 top5:  0.2773 batch_cost: 0.18911 s, reader_cost: 0.00082 s, ips: 1353.70290 samples/sec.
[2022/11/30 20:27:23] ppcls INFO: epoch:1   train step:210  lr: 0.100000, loss:  4.6675 top1:  0.1367 top5:  0.3008 batch_cost: 0.18907 s, reader_cost: 0.00083 s, ips: 1353.99499 samples/sec.
[2022/11/30 20:27:25] ppcls INFO: epoch:1   train step:220  lr: 0.100000, loss:  4.5830 top1:  0.1406 top5:  0.3008 batch_cost: 0.18919 s, reader_cost: 0.00083 s, ips: 1353.15858 samples/sec.
[2022/11/30 20:27:27] ppcls INFO: epoch:1   train step:230  lr: 0.100000, loss:  4.6624 top1:  0.1445 top5:  0.2852 batch_cost: 0.18917 s, reader_cost: 0.00083 s, ips: 1353.31110 samples/sec.
[2022/11/30 20:27:29] ppcls INFO: epoch:1   train step:240  lr: 0.100000, loss:  4.6525 top1:  0.1562 top5:  0.2773 batch_cost: 0.18915 s, reader_cost: 0.00083 s, ips: 1353.38811 samples/sec.
[2022/11/30 20:27:31] ppcls INFO: epoch:1   train step:250  lr: 0.100000, loss:  4.7051 top1:  0.1289 top5:  0.3164 batch_cost: 0.18917 s, reader_cost: 0.00082 s, ips: 1353.28850 samples/sec.
[2022/11/30 20:27:32] ppcls INFO: epoch:1   train step:260  lr: 0.100000, loss:  4.3469 top1:  0.1992 top5:  0.3398 batch_cost: 0.18914 s, reader_cost: 0.00082 s, ips: 1353.51392 samples/sec.
[2022/11/30 20:27:34] ppcls INFO: epoch:1   train step:270  lr: 0.100000, loss:  4.2002 top1:  0.1719 top5:  0.3750 batch_cost: 0.18913 s, reader_cost: 0.00082 s, ips: 1353.59877 samples/sec.
[2022/11/30 20:27:36] ppcls INFO: epoch:1   train step:280  lr: 0.100000, loss:  4.5308 top1:  0.1328 top5:  0.3086 batch_cost: 0.18911 s, reader_cost: 0.00081 s, ips: 1353.74383 samples/sec.
[2022/11/30 20:27:38] ppcls INFO: epoch:1   train step:290  lr: 0.100000, loss:  4.3940 top1:  0.1523 top5:  0.3438 batch_cost: 0.18912 s, reader_cost: 0.00081 s, ips: 1353.61301 samples/sec.
[2022/11/30 20:27:40] ppcls INFO: epoch:1   train step:300  lr: 0.100000, loss:  4.3686 top1:  0.1367 top5:  0.3242 batch_cost: 0.18910 s, reader_cost: 0.00081 s, ips: 1353.76373 samples/sec.
[2022/11/30 20:27:42] ppcls INFO: epoch:1   train step:310  lr: 0.100000, loss:  4.1027 top1:  0.1680 top5:  0.4062 batch_cost: 0.18911 s, reader_cost: 0.00080 s, ips: 1353.74128 samples/sec.
[2022/11/30 20:27:44] ppcls INFO: epoch:1   train step:320  lr: 0.100000, loss:  4.1230 top1:  0.1406 top5:  0.3828 batch_cost: 0.18908 s, reader_cost: 0.00080 s, ips: 1353.90925 samples/sec.
[2022/11/30 20:27:46] ppcls INFO: epoch:1   train step:330  lr: 0.100000, loss:  4.3460 top1:  0.1562 top5:  0.3711 batch_cost: 0.18909 s, reader_cost: 0.00080 s, ips: 1353.86310 samples/sec.
[2022/11/30 20:27:48] ppcls INFO: epoch:1   train step:340  lr: 0.100000, loss:  4.2123 top1:  0.1602 top5:  0.3438 batch_cost: 0.18911 s, reader_cost: 0.00080 s, ips: 1353.67667 samples/sec.
[2022/11/30 20:27:49] ppcls INFO: epoch:1   train step:350  lr: 0.100000, loss:  4.2599 top1:  0.1250 top5:  0.3672 batch_cost: 0.18911 s, reader_cost: 0.00080 s, ips: 1353.71385 samples/sec.
[2022/11/30 20:27:51] ppcls INFO: epoch:1   train step:360  lr: 0.100000, loss:  4.2178 top1:  0.1641 top5:  0.3477 batch_cost: 0.18912 s, reader_cost: 0.00080 s, ips: 1353.65550 samples/sec.
[2022/11/30 20:27:53] ppcls INFO: epoch:1   train step:370  lr: 0.100000, loss:  4.1326 top1:  0.1797 top5:  0.4297 batch_cost: 0.18910 s, reader_cost: 0.00079 s, ips: 1353.74611 samples/sec.
[2022/11/30 20:27:55] ppcls INFO: epoch:1   train step:380  lr: 0.100000, loss:  4.0775 top1:  0.1641 top5:  0.3945 batch_cost: 0.18918 s, reader_cost: 0.00079 s, ips: 1353.23047 samples/sec.
[2022/11/30 20:27:57] ppcls INFO: epoch:1   train step:390  lr: 0.100000, loss:  4.2241 top1:  0.1641 top5:  0.3750 batch_cost: 0.18915 s, reader_cost: 0.00079 s, ips: 1353.42990 samples/sec.
[2022/11/30 20:27:57] ppcls INFO: END epoch:1   train  loss:  4.6978 top1:  0.1217 top5:  0.2885 batch_cost: 0.18914 s, reader_cost: 0.00079 s, batch_cost_sum: 73.19875 s,
[2022/11/30 20:27:58] ppcls INFO: Already save model in ./output/ResNet50/1
[2022/11/30 20:28:00] ppcls INFO: epoch:2   train step:10   lr: 0.100000, loss:  4.1842 top1:  0.1602 top5:  0.3555 batch_cost: 0.18990 s, reader_cost: 0.00072 s, ips: 1348.05311 samples/sec.
[2022/11/30 20:28:02] ppcls INFO: epoch:2   train step:20   lr: 0.100000, loss:  3.9264 top1:  0.2344 top5:  0.4570 batch_cost: 0.18948 s, reader_cost: 0.00076 s, ips: 1351.03182 samples/sec.
[2022/11/30 20:28:04] ppcls INFO: epoch:2   train step:30   lr: 0.100000, loss:  4.2082 top1:  0.1719 top5:  0.3672 batch_cost: 0.18930 s, reader_cost: 0.00079 s, ips: 1352.38198 samples/sec.
[2022/11/30 20:28:06] ppcls INFO: epoch:2   train step:40   lr: 0.100000, loss:  3.8950 top1:  0.1953 top5:  0.4375 batch_cost: 0.18923 s, reader_cost: 0.00078 s, ips: 1352.85487 samples/sec.
[2022/11/30 20:28:08] ppcls INFO: epoch:2   train step:50   lr: 0.100000, loss:  3.9976 top1:  0.1875 top5:  0.3945 batch_cost: 0.18919 s, reader_cost: 0.00077 s, ips: 1353.15705 samples/sec.
[2022/11/30 20:28:09] ppcls INFO: epoch:2   train step:60   lr: 0.100000, loss:  3.9958 top1:  0.1797 top5:  0.4102 batch_cost: 0.18904 s, reader_cost: 0.00078 s, ips: 1354.17924 samples/sec.
[2022/11/30 20:28:11] ppcls INFO: epoch:2   train step:70   lr: 0.100000, loss:  4.0117 top1:  0.2148 top5:  0.4023 batch_cost: 0.18924 s, reader_cost: 0.00079 s, ips: 1352.77174 samples/sec.
[2022/11/30 20:28:13] ppcls INFO: epoch:2   train step:80   lr: 0.100000, loss:  3.8842 top1:  0.1680 top5:  0.4102 batch_cost: 0.18918 s, reader_cost: 0.00079 s, ips: 1353.17992 samples/sec.
[2022/11/30 20:28:15] ppcls INFO: epoch:2   train step:90   lr: 0.100000, loss:  4.0219 top1:  0.1680 top5:  0.4141 batch_cost: 0.18905 s, reader_cost: 0.00078 s, ips: 1354.10752 samples/sec.
[2022/11/30 20:28:17] ppcls INFO: epoch:2   train step:100  lr: 0.100000, loss:  3.8279 top1:  0.2344 top5:  0.4570 batch_cost: 0.18907 s, reader_cost: 0.00077 s, ips: 1353.98173 samples/sec.
[2022/11/30 20:28:19] ppcls INFO: epoch:2   train step:110  lr: 0.100000, loss:  4.1572 top1:  0.2031 top5:  0.3945 batch_cost: 0.18914 s, reader_cost: 0.00078 s, ips: 1353.50947 samples/sec.
[2022/11/30 20:28:21] ppcls INFO: epoch:2   train step:120  lr: 0.100000, loss:  3.8824 top1:  0.2383 top5:  0.4492 batch_cost: 0.18907 s, reader_cost: 0.00078 s, ips: 1354.02457 samples/sec.
[2022/11/30 20:28:23] ppcls INFO: epoch:2   train step:130  lr: 0.100000, loss:  4.0414 top1:  0.1992 top5:  0.4219 batch_cost: 0.18908 s, reader_cost: 0.00078 s, ips: 1353.93699 samples/sec.
[2022/11/30 20:28:25] ppcls INFO: epoch:2   train step:140  lr: 0.100000, loss:  3.5264 top1:  0.2852 top5:  0.5156 batch_cost: 0.18912 s, reader_cost: 0.00078 s, ips: 1353.63386 samples/sec.
[2022/11/30 20:28:26] ppcls INFO: epoch:2   train step:150  lr: 0.100000, loss:  3.8318 top1:  0.2031 top5:  0.4102 batch_cost: 0.18910 s, reader_cost: 0.00078 s, ips: 1353.76161 samples/sec.
[2022/11/30 20:28:28] ppcls INFO: epoch:2   train step:160  lr: 0.100000, loss:  3.6929 top1:  0.2656 top5:  0.4922 batch_cost: 0.18916 s, reader_cost: 0.00078 s, ips: 1353.37930 samples/sec.
[2022/11/30 20:28:30] ppcls INFO: epoch:2   train step:170  lr: 0.100000, loss:  3.7636 top1:  0.2344 top5:  0.4492 batch_cost: 0.18920 s, reader_cost: 0.00077 s, ips: 1353.03265 samples/sec.
[2022/11/30 20:28:32] ppcls INFO: epoch:2   train step:180  lr: 0.100000, loss:  3.8782 top1:  0.2344 top5:  0.4453 batch_cost: 0.18915 s, reader_cost: 0.00077 s, ips: 1353.45574 samples/sec.
[2022/11/30 20:28:34] ppcls INFO: epoch:2   train step:190  lr: 0.100000, loss:  4.0167 top1:  0.2188 top5:  0.4062 batch_cost: 0.18915 s, reader_cost: 0.00077 s, ips: 1353.44238 samples/sec.
[2022/11/30 20:28:36] ppcls INFO: epoch:2   train step:200  lr: 0.100000, loss:  3.5976 top1:  0.2461 top5:  0.4844 batch_cost: 0.18911 s, reader_cost: 0.00077 s, ips: 1353.68823 samples/sec.
[2022/11/30 20:28:38] ppcls INFO: epoch:2   train step:210  lr: 0.100000, loss:  3.6340 top1:  0.2617 top5:  0.4531 batch_cost: 0.18911 s, reader_cost: 0.00077 s, ips: 1353.68391 samples/sec.
[2022/11/30 20:28:40] ppcls INFO: epoch:2   train step:220  lr: 0.100000, loss:  3.9329 top1:  0.1992 top5:  0.3945 batch_cost: 0.18909 s, reader_cost: 0.00077 s, ips: 1353.83471 samples/sec.
[2022/11/30 20:28:42] ppcls INFO: epoch:2   train step:230  lr: 0.100000, loss:  3.4645 top1:  0.2852 top5:  0.5078 batch_cost: 0.18909 s, reader_cost: 0.00078 s, ips: 1353.83089 samples/sec.
[2022/11/30 20:28:43] ppcls INFO: epoch:2   train step:240  lr: 0.100000, loss:  3.5897 top1:  0.2227 top5:  0.4648 batch_cost: 0.18912 s, reader_cost: 0.00077 s, ips: 1353.62619 samples/sec.
[2022/11/30 20:28:45] ppcls INFO: epoch:2   train step:250  lr: 0.100000, loss:  3.4550 top1:  0.2891 top5:  0.5117 batch_cost: 0.18910 s, reader_cost: 0.00077 s, ips: 1353.81588 samples/sec.
[2022/11/30 20:28:47] ppcls INFO: epoch:2   train step:260  lr: 0.100000, loss:  3.7327 top1:  0.2227 top5:  0.4453 batch_cost: 0.18918 s, reader_cost: 0.00077 s, ips: 1353.22001 samples/sec.
[2022/11/30 20:28:49] ppcls INFO: epoch:2   train step:270  lr: 0.100000, loss:  3.3366 top1:  0.3008 top5:  0.5430 batch_cost: 0.18923 s, reader_cost: 0.00077 s, ips: 1352.85896 samples/sec.
[2022/11/30 20:28:51] ppcls INFO: epoch:2   train step:280  lr: 0.100000, loss:  3.4777 top1:  0.3008 top5:  0.5117 batch_cost: 0.18927 s, reader_cost: 0.00077 s, ips: 1352.58800 samples/sec.
[2022/11/30 20:28:53] ppcls INFO: epoch:2   train step:290  lr: 0.100000, loss:  3.4219 top1:  0.2852 top5:  0.5469 batch_cost: 0.18927 s, reader_cost: 0.00076 s, ips: 1352.58733 samples/sec.
[2022/11/30 20:28:55] ppcls INFO: epoch:2   train step:300  lr: 0.100000, loss:  3.5273 top1:  0.2539 top5:  0.5547 batch_cost: 0.18928 s, reader_cost: 0.00077 s, ips: 1352.51039 samples/sec.
[2022/11/30 20:28:57] ppcls INFO: epoch:2   train step:310  lr: 0.100000, loss:  3.3569 top1:  0.2891 top5:  0.5703 batch_cost: 0.18927 s, reader_cost: 0.00076 s, ips: 1352.53644 samples/sec.
[2022/11/30 20:28:59] ppcls INFO: epoch:2   train step:320  lr: 0.100000, loss:  3.6546 top1:  0.2227 top5:  0.4883 batch_cost: 0.18925 s, reader_cost: 0.00076 s, ips: 1352.72795 samples/sec.
[2022/11/30 20:29:01] ppcls INFO: epoch:2   train step:330  lr: 0.100000, loss:  3.3808 top1:  0.2930 top5:  0.5195 batch_cost: 0.18923 s, reader_cost: 0.00076 s, ips: 1352.87333 samples/sec.
[2022/11/30 20:29:02] ppcls INFO: epoch:2   train step:340  lr: 0.100000, loss:  3.5090 top1:  0.3047 top5:  0.4922 batch_cost: 0.18922 s, reader_cost: 0.00076 s, ips: 1352.91330 samples/sec.
[2022/11/30 20:29:04] ppcls INFO: epoch:2   train step:350  lr: 0.100000, loss:  3.2107 top1:  0.3398 top5:  0.5781 batch_cost: 0.18922 s, reader_cost: 0.00076 s, ips: 1352.94076 samples/sec.
[2022/11/30 20:29:06] ppcls INFO: epoch:2   train step:360  lr: 0.100000, loss:  3.2306 top1:  0.3203 top5:  0.5625 batch_cost: 0.18924 s, reader_cost: 0.00075 s, ips: 1352.81179 samples/sec.
[2022/11/30 20:29:08] ppcls INFO: epoch:2   train step:370  lr: 0.100000, loss:  3.3557 top1:  0.2891 top5:  0.5508 batch_cost: 0.18923 s, reader_cost: 0.00075 s, ips: 1352.87222 samples/sec.
[2022/11/30 20:29:10] ppcls INFO: epoch:2   train step:380  lr: 0.100000, loss:  3.1994 top1:  0.3242 top5:  0.5742 batch_cost: 0.18924 s, reader_cost: 0.00075 s, ips: 1352.76852 samples/sec.
[2022/11/30 20:29:12] ppcls INFO: epoch:2   train step:390  lr: 0.100000, loss:  3.0324 top1:  0.3594 top5:  0.5977 batch_cost: 0.18923 s, reader_cost: 0.00075 s, ips: 1352.87838 samples/sec.
[2022/11/30 20:29:12] ppcls INFO: END epoch:2   train  loss:  3.7050 top1:  0.2448 top5:  0.4752 batch_cost: 0.18923 s, reader_cost: 0.00075 s, batch_cost_sum: 73.04130 s,
[2022/11/30 20:29:13] ppcls INFO: Already save model in ./output/ResNet50/2
[2022/11/30 20:29:15] ppcls INFO: epoch:3   train step:10   lr: 0.100000, loss:  3.2392 top1:  0.3008 top5:  0.5586 batch_cost: 0.18849 s, reader_cost: 0.00065 s, ips: 1358.16028 samples/sec.
[2022/11/30 20:29:16] ppcls INFO: epoch:3   train step:20   lr: 0.100000, loss:  3.0074 top1:  0.3359 top5:  0.6055 batch_cost: 0.18922 s, reader_cost: 0.00075 s, ips: 1352.94269 samples/sec.
[2022/11/30 20:29:18] ppcls INFO: epoch:3   train step:30   lr: 0.100000, loss:  3.2948 top1:  0.3008 top5:  0.5391 batch_cost: 0.18943 s, reader_cost: 0.00073 s, ips: 1351.43807 samples/sec.
[2022/11/30 20:29:20] ppcls INFO: epoch:3   train step:40   lr: 0.100000, loss:  3.1029 top1:  0.3359 top5:  0.5977 batch_cost: 0.18901 s, reader_cost: 0.00074 s, ips: 1354.43705 samples/sec.
[2022/11/30 20:29:22] ppcls INFO: epoch:3   train step:50   lr: 0.100000, loss:  3.1360 top1:  0.3125 top5:  0.5742 batch_cost: 0.18898 s, reader_cost: 0.00072 s, ips: 1354.61738 samples/sec.
[2022/11/30 20:29:24] ppcls INFO: epoch:3   train step:60   lr: 0.100000, loss:  3.3513 top1:  0.3047 top5:  0.5234 batch_cost: 0.18899 s, reader_cost: 0.00073 s, ips: 1354.55184 samples/sec.
[2022/11/30 20:29:26] ppcls INFO: epoch:3   train step:70   lr: 0.100000, loss:  3.1808 top1:  0.3672 top5:  0.5703 batch_cost: 0.18907 s, reader_cost: 0.00074 s, ips: 1353.98782 samples/sec.
[2022/11/30 20:29:28] ppcls INFO: epoch:3   train step:80   lr: 0.100000, loss:  3.2899 top1:  0.2891 top5:  0.5508 batch_cost: 0.18926 s, reader_cost: 0.00081 s, ips: 1352.66502 samples/sec.
[2022/11/30 20:29:30] ppcls INFO: epoch:3   train step:90   lr: 0.100000, loss:  2.7473 top1:  0.3867 top5:  0.6758 batch_cost: 0.18929 s, reader_cost: 0.00080 s, ips: 1352.42061 samples/sec.
[2022/11/30 20:29:32] ppcls INFO: epoch:3   train step:100  lr: 0.100000, loss:  2.9333 top1:  0.3398 top5:  0.5977 batch_cost: 0.18943 s, reader_cost: 0.00080 s, ips: 1351.39745 samples/sec.
[2022/11/30 20:29:34] ppcls INFO: epoch:3   train step:110  lr: 0.100000, loss:  2.9043 top1:  0.3828 top5:  0.6523 batch_cost: 0.18964 s, reader_cost: 0.00099 s, ips: 1349.91130 samples/sec.
[2022/11/30 20:29:35] ppcls INFO: epoch:3   train step:120  lr: 0.100000, loss:  2.8048 top1:  0.3672 top5:  0.6797 batch_cost: 0.18962 s, reader_cost: 0.00096 s, ips: 1350.03540 samples/sec.
[2022/11/30 20:29:37] ppcls INFO: epoch:3   train step:130  lr: 0.100000, loss:  3.1065 top1:  0.3086 top5:  0.6250 batch_cost: 0.18958 s, reader_cost: 0.00094 s, ips: 1350.36067 samples/sec.
[2022/11/30 20:29:39] ppcls INFO: epoch:3   train step:140  lr: 0.100000, loss:  2.9400 top1:  0.4062 top5:  0.6406 batch_cost: 0.18959 s, reader_cost: 0.00092 s, ips: 1350.30128 samples/sec.
[2022/11/30 20:29:41] ppcls INFO: epoch:3   train step:150  lr: 0.100000, loss:  2.9289 top1:  0.3398 top5:  0.6367 batch_cost: 0.18955 s, reader_cost: 0.00091 s, ips: 1350.59621 samples/sec.
[2022/11/30 20:29:43] ppcls INFO: epoch:3   train step:160  lr: 0.100000, loss:  3.0674 top1:  0.3359 top5:  0.5742 batch_cost: 0.18957 s, reader_cost: 0.00090 s, ips: 1350.42337 samples/sec.
[2022/11/30 20:29:45] ppcls INFO: epoch:3   train step:170  lr: 0.100000, loss:  2.8737 top1:  0.3672 top5:  0.6484 batch_cost: 0.18957 s, reader_cost: 0.00088 s, ips: 1350.42580 samples/sec.
[2022/11/30 20:29:47] ppcls INFO: epoch:3   train step:180  lr: 0.100000, loss:  2.6947 top1:  0.3984 top5:  0.6484 batch_cost: 0.18954 s, reader_cost: 0.00088 s, ips: 1350.65594 samples/sec.
[2022/11/30 20:29:49] ppcls INFO: epoch:3   train step:190  lr: 0.100000, loss:  2.8972 top1:  0.3711 top5:  0.6328 batch_cost: 0.18960 s, reader_cost: 0.00087 s, ips: 1350.18916 samples/sec.
[2022/11/30 20:29:51] ppcls INFO: epoch:3   train step:200  lr: 0.100000, loss:  2.7944 top1:  0.3359 top5:  0.5859 batch_cost: 0.18959 s, reader_cost: 0.00086 s, ips: 1350.29489 samples/sec.
[2022/11/30 20:29:53] ppcls INFO: epoch:3   train step:210  lr: 0.100000, loss:  2.6961 top1:  0.4023 top5:  0.6406 batch_cost: 0.18954 s, reader_cost: 0.00086 s, ips: 1350.66215 samples/sec.
[2022/11/30 20:29:54] ppcls INFO: epoch:3   train step:220  lr: 0.100000, loss:  2.7707 top1:  0.3906 top5:  0.6680 batch_cost: 0.18952 s, reader_cost: 0.00085 s, ips: 1350.76087 samples/sec.
[2022/11/30 20:29:56] ppcls INFO: epoch:3   train step:230  lr: 0.100000, loss:  2.6783 top1:  0.4297 top5:  0.6602 batch_cost: 0.18950 s, reader_cost: 0.00085 s, ips: 1350.93715 samples/sec.
[2022/11/30 20:29:58] ppcls INFO: epoch:3   train step:240  lr: 0.100000, loss:  2.6773 top1:  0.4297 top5:  0.6758 batch_cost: 0.18950 s, reader_cost: 0.00084 s, ips: 1350.93195 samples/sec.
[2022/11/30 20:30:00] ppcls INFO: epoch:3   train step:250  lr: 0.100000, loss:  2.9139 top1:  0.3906 top5:  0.5938 batch_cost: 0.18946 s, reader_cost: 0.00084 s, ips: 1351.20354 samples/sec.
[2022/11/30 20:30:02] ppcls INFO: epoch:3   train step:260  lr: 0.100000, loss:  2.7529 top1:  0.4141 top5:  0.6680 batch_cost: 0.18946 s, reader_cost: 0.00083 s, ips: 1351.17967 samples/sec.
[2022/11/30 20:30:04] ppcls INFO: epoch:3   train step:270  lr: 0.100000, loss:  2.4495 top1:  0.5039 top5:  0.7148 batch_cost: 0.18949 s, reader_cost: 0.00083 s, ips: 1351.00106 samples/sec.
[2022/11/30 20:30:06] ppcls INFO: epoch:3   train step:280  lr: 0.100000, loss:  2.7517 top1:  0.4219 top5:  0.6484 batch_cost: 0.18944 s, reader_cost: 0.00083 s, ips: 1351.31974 samples/sec.
[2022/11/30 20:30:08] ppcls INFO: epoch:3   train step:290  lr: 0.100000, loss:  2.3489 top1:  0.4727 top5:  0.7578 batch_cost: 0.18949 s, reader_cost: 0.00082 s, ips: 1350.95966 samples/sec.
[2022/11/30 20:30:10] ppcls INFO: epoch:3   train step:300  lr: 0.100000, loss:  2.7428 top1:  0.4336 top5:  0.6523 batch_cost: 0.18949 s, reader_cost: 0.00082 s, ips: 1350.96063 samples/sec.
[2022/11/30 20:30:11] ppcls INFO: epoch:3   train step:310  lr: 0.100000, loss:  2.5841 top1:  0.4570 top5:  0.6602 batch_cost: 0.18948 s, reader_cost: 0.00082 s, ips: 1351.03893 samples/sec.
[2022/11/30 20:30:13] ppcls INFO: epoch:3   train step:320  lr: 0.100000, loss:  2.6523 top1:  0.4453 top5:  0.6719 batch_cost: 0.18946 s, reader_cost: 0.00082 s, ips: 1351.20922 samples/sec.
[2022/11/30 20:30:15] ppcls INFO: epoch:3   train step:330  lr: 0.100000, loss:  2.5864 top1:  0.4688 top5:  0.6914 batch_cost: 0.18948 s, reader_cost: 0.00081 s, ips: 1351.06361 samples/sec.
[2022/11/30 20:30:17] ppcls INFO: epoch:3   train step:340  lr: 0.100000, loss:  2.5751 top1:  0.4414 top5:  0.7148 batch_cost: 0.18947 s, reader_cost: 0.00081 s, ips: 1351.17154 samples/sec.
[2022/11/30 20:30:19] ppcls INFO: epoch:3   train step:350  lr: 0.100000, loss:  2.4157 top1:  0.4414 top5:  0.6953 batch_cost: 0.18947 s, reader_cost: 0.00081 s, ips: 1351.13595 samples/sec.
[2022/11/30 20:30:21] ppcls INFO: epoch:3   train step:360  lr: 0.100000, loss:  2.6035 top1:  0.4570 top5:  0.6758 batch_cost: 0.18947 s, reader_cost: 0.00080 s, ips: 1351.17154 samples/sec.
[2022/11/30 20:30:23] ppcls INFO: epoch:3   train step:370  lr: 0.100000, loss:  2.6824 top1:  0.4102 top5:  0.6523 batch_cost: 0.18946 s, reader_cost: 0.00081 s, ips: 1351.19030 samples/sec.
[2022/11/30 20:30:25] ppcls INFO: epoch:3   train step:380  lr: 0.100000, loss:  2.3284 top1:  0.4609 top5:  0.7266 batch_cost: 0.18946 s, reader_cost: 0.00080 s, ips: 1351.24427 samples/sec.
[2022/11/30 20:30:27] ppcls INFO: epoch:3   train step:390  lr: 0.100000, loss:  2.3578 top1:  0.4531 top5:  0.7227 batch_cost: 0.18944 s, reader_cost: 0.00080 s, ips: 1351.37197 samples/sec.
[2022/11/30 20:30:27] ppcls INFO: END epoch:3   train  loss:  2.8139 top1:  0.3876 top5:  0.6433 batch_cost: 0.18944 s, reader_cost: 0.00080 s, batch_cost_sum: 73.31217 s,
[2022/11/30 20:30:28] ppcls INFO: Already save model in ./output/ResNet50/3
[2022/11/30 20:30:29] ppcls INFO: epoch:4   train step:10   lr: 0.100000, loss:  2.2527 top1:  0.4844 top5:  0.7227 batch_cost: 0.18846 s, reader_cost: 0.00072 s, ips: 1358.37018 samples/sec.
[2022/11/30 20:30:31] ppcls INFO: epoch:4   train step:20   lr: 0.100000, loss:  2.4161 top1:  0.4531 top5:  0.7109 batch_cost: 0.18843 s, reader_cost: 0.00073 s, ips: 1358.56325 samples/sec.
[2022/11/30 20:30:33] ppcls INFO: epoch:4   train step:30   lr: 0.100000, loss:  2.1306 top1:  0.5469 top5:  0.7305 batch_cost: 0.18878 s, reader_cost: 0.00072 s, ips: 1356.10619 samples/sec.
[2022/11/30 20:30:35] ppcls INFO: epoch:4   train step:40   lr: 0.100000, loss:  2.3952 top1:  0.4219 top5:  0.7266 batch_cost: 0.18878 s, reader_cost: 0.00071 s, ips: 1356.10842 samples/sec.
[2022/11/30 20:30:37] ppcls INFO: epoch:4   train step:50   lr: 0.100000, loss:  2.2619 top1:  0.4766 top5:  0.7266 batch_cost: 0.18891 s, reader_cost: 0.00071 s, ips: 1355.15123 samples/sec.
[2022/11/30 20:30:39] ppcls INFO: epoch:4   train step:60   lr: 0.100000, loss:  2.5166 top1:  0.4414 top5:  0.6914 batch_cost: 0.18923 s, reader_cost: 0.00106 s, ips: 1352.87149 samples/sec.
[2022/11/30 20:30:41] ppcls INFO: epoch:4   train step:70   lr: 0.100000, loss:  2.1816 top1:  0.5117 top5:  0.7500 batch_cost: 0.18925 s, reader_cost: 0.00101 s, ips: 1352.73980 samples/sec.
[2022/11/30 20:30:43] ppcls INFO: epoch:4   train step:80   lr: 0.100000, loss:  2.5174 top1:  0.4414 top5:  0.6445 batch_cost: 0.18929 s, reader_cost: 0.00096 s, ips: 1352.45357 samples/sec.
[2022/11/30 20:30:45] ppcls INFO: epoch:4   train step:90   lr: 0.100000, loss:  1.9005 top1:  0.5469 top5:  0.8203 batch_cost: 0.18932 s, reader_cost: 0.00094 s, ips: 1352.19213 samples/sec.
[2022/11/30 20:30:47] ppcls INFO: epoch:4   train step:100  lr: 0.100000, loss:  2.0219 top1:  0.5273 top5:  0.8047 batch_cost: 0.18926 s, reader_cost: 0.00091 s, ips: 1352.60092 samples/sec.
[2022/11/30 20:30:48] ppcls INFO: epoch:4   train step:110  lr: 0.100000, loss:  2.2871 top1:  0.4766 top5:  0.6992 batch_cost: 0.18931 s, reader_cost: 0.00091 s, ips: 1352.24842 samples/sec.
[2022/11/30 20:30:50] ppcls INFO: epoch:4   train step:120  lr: 0.100000, loss:  2.1314 top1:  0.5195 top5:  0.7539 batch_cost: 0.18939 s, reader_cost: 0.00090 s, ips: 1351.68089 samples/sec.
[2022/11/30 20:30:52] ppcls INFO: epoch:4   train step:130  lr: 0.100000, loss:  2.0567 top1:  0.5000 top5:  0.7930 batch_cost: 0.18939 s, reader_cost: 0.00089 s, ips: 1351.74172 samples/sec.
[2022/11/30 20:30:54] ppcls INFO: epoch:4   train step:140  lr: 0.100000, loss:  2.0626 top1:  0.5273 top5:  0.7812 batch_cost: 0.18931 s, reader_cost: 0.00088 s, ips: 1352.27874 samples/sec.
[2022/11/30 20:30:56] ppcls INFO: epoch:4   train step:150  lr: 0.100000, loss:  2.1008 top1:  0.5195 top5:  0.7734 batch_cost: 0.18933 s, reader_cost: 0.00087 s, ips: 1352.11408 samples/sec.
[2022/11/30 20:30:58] ppcls INFO: epoch:4   train step:160  lr: 0.100000, loss:  2.3373 top1:  0.4922 top5:  0.7227 batch_cost: 0.18932 s, reader_cost: 0.00087 s, ips: 1352.21906 samples/sec.
[2022/11/30 20:31:00] ppcls INFO: epoch:4   train step:170  lr: 0.100000, loss:  2.1971 top1:  0.4883 top5:  0.7422 batch_cost: 0.18931 s, reader_cost: 0.00086 s, ips: 1352.27064 samples/sec.
[2022/11/30 20:31:02] ppcls INFO: epoch:4   train step:180  lr: 0.100000, loss:  1.8533 top1:  0.5820 top5:  0.8086 batch_cost: 0.18927 s, reader_cost: 0.00085 s, ips: 1352.53987 samples/sec.
[2022/11/30 20:31:04] ppcls INFO: epoch:4   train step:190  lr: 0.100000, loss:  2.1934 top1:  0.5469 top5:  0.7500 batch_cost: 0.18921 s, reader_cost: 0.00084 s, ips: 1352.96058 samples/sec.
[2022/11/30 20:31:06] ppcls INFO: epoch:4   train step:200  lr: 0.100000, loss:  1.8732 top1:  0.5742 top5:  0.8320 batch_cost: 0.18924 s, reader_cost: 0.00084 s, ips: 1352.80381 samples/sec.
[2022/11/30 20:31:07] ppcls INFO: epoch:4   train step:210  lr: 0.100000, loss:  1.9994 top1:  0.5547 top5:  0.7969 batch_cost: 0.18920 s, reader_cost: 0.00083 s, ips: 1353.07403 samples/sec.
[2022/11/30 20:31:09] ppcls INFO: epoch:4   train step:220  lr: 0.100000, loss:  2.0454 top1:  0.5195 top5:  0.7812 batch_cost: 0.18921 s, reader_cost: 0.00083 s, ips: 1353.01111 samples/sec.
[2022/11/30 20:31:11] ppcls INFO: epoch:4   train step:230  lr: 0.100000, loss:  1.8912 top1:  0.5898 top5:  0.8164 batch_cost: 0.18925 s, reader_cost: 0.00082 s, ips: 1352.72889 samples/sec.
[2022/11/30 20:31:13] ppcls INFO: epoch:4   train step:240  lr: 0.100000, loss:  2.0654 top1:  0.5781 top5:  0.7734 batch_cost: 0.18927 s, reader_cost: 0.00082 s, ips: 1352.59524 samples/sec.
[2022/11/30 20:31:15] ppcls INFO: epoch:4   train step:250  lr: 0.100000, loss:  1.6679 top1:  0.5820 top5:  0.8320 batch_cost: 0.18929 s, reader_cost: 0.00081 s, ips: 1352.41643 samples/sec.
[2022/11/30 20:31:17] ppcls INFO: epoch:4   train step:260  lr: 0.100000, loss:  1.9749 top1:  0.5352 top5:  0.7734 batch_cost: 0.18931 s, reader_cost: 0.00081 s, ips: 1352.27201 samples/sec.
[2022/11/30 20:31:19] ppcls INFO: epoch:4   train step:270  lr: 0.100000, loss:  1.7249 top1:  0.6406 top5:  0.8477 batch_cost: 0.18933 s, reader_cost: 0.00081 s, ips: 1352.14438 samples/sec.
[2022/11/30 20:31:21] ppcls INFO: epoch:4   train step:280  lr: 0.100000, loss:  1.7926 top1:  0.5430 top5:  0.8359 batch_cost: 0.18935 s, reader_cost: 0.00080 s, ips: 1352.00723 samples/sec.
[2022/11/30 20:31:23] ppcls INFO: epoch:4   train step:290  lr: 0.100000, loss:  1.6988 top1:  0.6211 top5:  0.8359 batch_cost: 0.18931 s, reader_cost: 0.00080 s, ips: 1352.27969 samples/sec.
[2022/11/30 20:31:24] ppcls INFO: epoch:4   train step:300  lr: 0.100000, loss:  1.8390 top1:  0.5977 top5:  0.8203 batch_cost: 0.18930 s, reader_cost: 0.00080 s, ips: 1352.34791 samples/sec.
[2022/11/30 20:31:26] ppcls INFO: epoch:4   train step:310  lr: 0.100000, loss:  1.8078 top1:  0.5156 top5:  0.8438 batch_cost: 0.18932 s, reader_cost: 0.00080 s, ips: 1352.17458 samples/sec.
[2022/11/30 20:31:28] ppcls INFO: epoch:4   train step:320  lr: 0.100000, loss:  1.7597 top1:  0.6250 top5:  0.8320 batch_cost: 0.18932 s, reader_cost: 0.00079 s, ips: 1352.21621 samples/sec.
[2022/11/30 20:31:30] ppcls INFO: epoch:4   train step:330  lr: 0.100000, loss:  1.7131 top1:  0.6016 top5:  0.8438 batch_cost: 0.18931 s, reader_cost: 0.00079 s, ips: 1352.28300 samples/sec.
[2022/11/30 20:31:32] ppcls INFO: epoch:4   train step:340  lr: 0.100000, loss:  1.8055 top1:  0.5820 top5:  0.8242 batch_cost: 0.18931 s, reader_cost: 0.00079 s, ips: 1352.26432 samples/sec.
[2022/11/30 20:31:34] ppcls INFO: epoch:4   train step:350  lr: 0.100000, loss:  1.7164 top1:  0.6094 top5:  0.8203 batch_cost: 0.18929 s, reader_cost: 0.00079 s, ips: 1352.42571 samples/sec.
[2022/11/30 20:31:36] ppcls INFO: epoch:4   train step:360  lr: 0.100000, loss:  1.7513 top1:  0.6172 top5:  0.8203 batch_cost: 0.18928 s, reader_cost: 0.00079 s, ips: 1352.52238 samples/sec.
[2022/11/30 20:31:38] ppcls INFO: epoch:4   train step:370  lr: 0.100000, loss:  1.5150 top1:  0.6758 top5:  0.8594 batch_cost: 0.18928 s, reader_cost: 0.00079 s, ips: 1352.52137 samples/sec.
[2022/11/30 20:31:40] ppcls INFO: epoch:4   train step:380  lr: 0.100000, loss:  1.8038 top1:  0.6172 top5:  0.8086 batch_cost: 0.18925 s, reader_cost: 0.00079 s, ips: 1352.67757 samples/sec.
[2022/11/30 20:31:41] ppcls INFO: epoch:4   train step:390  lr: 0.100000, loss:  1.6692 top1:  0.6094 top5:  0.8242 batch_cost: 0.18926 s, reader_cost: 0.00079 s, ips: 1352.63214 samples/sec.
[2022/11/30 20:31:42] ppcls INFO: END epoch:4   train  loss:  2.0287 top1:  0.5417 top5:  0.7789 batch_cost: 0.18928 s, reader_cost: 0.00079 s, batch_cost_sum: 73.25318 s,
[2022/11/30 20:31:42] ppcls INFO: Already save model in ./output/ResNet50/4
[2022/11/30 20:31:44] ppcls INFO: epoch:5   train step:10   lr: 0.100000, loss:  1.7651 top1:  0.6094 top5:  0.8320 batch_cost: 0.18980 s, reader_cost: 0.00064 s, ips: 1348.77645 samples/sec.
[2022/11/30 20:31:46] ppcls INFO: epoch:5   train step:20   lr: 0.100000, loss:  1.6278 top1:  0.6602 top5:  0.8398 batch_cost: 0.19028 s, reader_cost: 0.00087 s, ips: 1345.37001 samples/sec.
[2022/11/30 20:31:48] ppcls INFO: epoch:5   train step:30   lr: 0.100000, loss:  1.6956 top1:  0.5977 top5:  0.8242 batch_cost: 0.18984 s, reader_cost: 0.00081 s, ips: 1348.50911 samples/sec.
[2022/11/30 20:31:50] ppcls INFO: epoch:5   train step:40   lr: 0.100000, loss:  1.6076 top1:  0.6562 top5:  0.8477 batch_cost: 0.18974 s, reader_cost: 0.00080 s, ips: 1349.20354 samples/sec.
[2022/11/30 20:31:52] ppcls INFO: epoch:5   train step:50   lr: 0.100000, loss:  1.6146 top1:  0.6172 top5:  0.8594 batch_cost: 0.18950 s, reader_cost: 0.00081 s, ips: 1350.95441 samples/sec.
[2022/11/30 20:31:54] ppcls INFO: epoch:5   train step:60   lr: 0.100000, loss:  1.4694 top1:  0.7031 top5:  0.8516 batch_cost: 0.18964 s, reader_cost: 0.00080 s, ips: 1349.94755 samples/sec.
[2022/11/30 20:31:56] ppcls INFO: epoch:5   train step:70   lr: 0.100000, loss:  1.6892 top1:  0.6289 top5:  0.8164 batch_cost: 0.18962 s, reader_cost: 0.00081 s, ips: 1350.05827 samples/sec.
[2022/11/30 20:31:58] ppcls INFO: epoch:5   train step:80   lr: 0.100000, loss:  1.6363 top1:  0.6172 top5:  0.8242 batch_cost: 0.18958 s, reader_cost: 0.00080 s, ips: 1350.37535 samples/sec.
[2022/11/30 20:32:00] ppcls INFO: epoch:5   train step:90   lr: 0.100000, loss:  1.4813 top1:  0.6992 top5:  0.8672 batch_cost: 0.18974 s, reader_cost: 0.00080 s, ips: 1349.24318 samples/sec.
[2022/11/30 20:32:01] ppcls INFO: epoch:5   train step:100  lr: 0.100000, loss:  1.4143 top1:  0.6445 top5:  0.8789 batch_cost: 0.18970 s, reader_cost: 0.00080 s, ips: 1349.49693 samples/sec.
[2022/11/30 20:32:03] ppcls INFO: epoch:5   train step:110  lr: 0.100000, loss:  1.3779 top1:  0.6680 top5:  0.8672 batch_cost: 0.18978 s, reader_cost: 0.00080 s, ips: 1348.90155 samples/sec.
[2022/11/30 20:32:05] ppcls INFO: epoch:5   train step:120  lr: 0.100000, loss:  1.3816 top1:  0.6914 top5:  0.8672 batch_cost: 0.18978 s, reader_cost: 0.00080 s, ips: 1348.95907 samples/sec.
[2022/11/30 20:32:07] ppcls INFO: epoch:5   train step:130  lr: 0.100000, loss:  1.5086 top1:  0.6602 top5:  0.8477 batch_cost: 0.18980 s, reader_cost: 0.00080 s, ips: 1348.80186 samples/sec.
[2022/11/30 20:32:09] ppcls INFO: epoch:5   train step:140  lr: 0.100000, loss:  1.5482 top1:  0.6289 top5:  0.8320 batch_cost: 0.18979 s, reader_cost: 0.00082 s, ips: 1348.86996 samples/sec.
[2022/11/30 20:32:11] ppcls INFO: epoch:5   train step:150  lr: 0.100000, loss:  1.3244 top1:  0.6719 top5:  0.9023 batch_cost: 0.18981 s, reader_cost: 0.00081 s, ips: 1348.73288 samples/sec.
[2022/11/30 20:32:13] ppcls INFO: epoch:5   train step:160  lr: 0.100000, loss:  1.4386 top1:  0.6758 top5:  0.8945 batch_cost: 0.18978 s, reader_cost: 0.00081 s, ips: 1348.90175 samples/sec.
[2022/11/30 20:32:15] ppcls INFO: epoch:5   train step:170  lr: 0.100000, loss:  1.4803 top1:  0.6641 top5:  0.8555 batch_cost: 0.18975 s, reader_cost: 0.00080 s, ips: 1349.15162 samples/sec.
[2022/11/30 20:32:17] ppcls INFO: epoch:5   train step:180  lr: 0.100000, loss:  1.1921 top1:  0.7109 top5:  0.9297 batch_cost: 0.18969 s, reader_cost: 0.00080 s, ips: 1349.56385 samples/sec.
[2022/11/30 20:32:19] ppcls INFO: epoch:5   train step:190  lr: 0.100000, loss:  1.5406 top1:  0.6719 top5:  0.8594 batch_cost: 0.18969 s, reader_cost: 0.00079 s, ips: 1349.54403 samples/sec.
[2022/11/30 20:32:20] ppcls INFO: epoch:5   train step:200  lr: 0.100000, loss:  1.4156 top1:  0.6719 top5:  0.8711 batch_cost: 0.18976 s, reader_cost: 0.00079 s, ips: 1349.04132 samples/sec.
[2022/11/30 20:32:22] ppcls INFO: epoch:5   train step:210  lr: 0.100000, loss:  1.5265 top1:  0.6484 top5:  0.8555 batch_cost: 0.18974 s, reader_cost: 0.00080 s, ips: 1349.19543 samples/sec.
[2022/11/30 20:32:24] ppcls INFO: epoch:5   train step:220  lr: 0.100000, loss:  1.4040 top1:  0.6953 top5:  0.8945 batch_cost: 0.18971 s, reader_cost: 0.00079 s, ips: 1349.45526 samples/sec.
[2022/11/30 20:32:26] ppcls INFO: epoch:5   train step:230  lr: 0.100000, loss:  1.2069 top1:  0.7422 top5:  0.9023 batch_cost: 0.18973 s, reader_cost: 0.00079 s, ips: 1349.25056 samples/sec.
[2022/11/30 20:32:28] ppcls INFO: epoch:5   train step:240  lr: 0.100000, loss:  1.3280 top1:  0.6797 top5:  0.8672 batch_cost: 0.18969 s, reader_cost: 0.00079 s, ips: 1349.53981 samples/sec.
[2022/11/30 20:32:30] ppcls INFO: epoch:5   train step:250  lr: 0.100000, loss:  1.2800 top1:  0.6953 top5:  0.9062 batch_cost: 0.18967 s, reader_cost: 0.00078 s, ips: 1349.70387 samples/sec.
[2022/11/30 20:32:32] ppcls INFO: epoch:5   train step:260  lr: 0.100000, loss:  1.3359 top1:  0.6602 top5:  0.8906 batch_cost: 0.18965 s, reader_cost: 0.00078 s, ips: 1349.82753 samples/sec.
[2022/11/30 20:32:34] ppcls INFO: epoch:5   train step:270  lr: 0.100000, loss:  1.2649 top1:  0.7305 top5:  0.8828 batch_cost: 0.18964 s, reader_cost: 0.00078 s, ips: 1349.92271 samples/sec.
[2022/11/30 20:32:36] ppcls INFO: epoch:5   train step:280  lr: 0.100000, loss:  1.3948 top1:  0.6836 top5:  0.8711 batch_cost: 0.18964 s, reader_cost: 0.00078 s, ips: 1349.94702 samples/sec.
[2022/11/30 20:32:37] ppcls INFO: epoch:5   train step:290  lr: 0.100000, loss:  1.1831 top1:  0.7266 top5:  0.8945 batch_cost: 0.18959 s, reader_cost: 0.00077 s, ips: 1350.28588 samples/sec.
[2022/11/30 20:32:39] ppcls INFO: epoch:5   train step:300  lr: 0.100000, loss:  1.3278 top1:  0.7070 top5:  0.8906 batch_cost: 0.18958 s, reader_cost: 0.00077 s, ips: 1350.32346 samples/sec.
[2022/11/30 20:32:41] ppcls INFO: epoch:5   train step:310  lr: 0.100000, loss:  1.1223 top1:  0.7344 top5:  0.9023 batch_cost: 0.18958 s, reader_cost: 0.00077 s, ips: 1350.34035 samples/sec.
[2022/11/30 20:32:43] ppcls INFO: epoch:5   train step:320  lr: 0.100000, loss:  1.5241 top1:  0.6758 top5:  0.8555 batch_cost: 0.18955 s, reader_cost: 0.00077 s, ips: 1350.54222 samples/sec.
[2022/11/30 20:32:45] ppcls INFO: epoch:5   train step:330  lr: 0.100000, loss:  1.3025 top1:  0.7109 top5:  0.8633 batch_cost: 0.18955 s, reader_cost: 0.00077 s, ips: 1350.58210 samples/sec.
[2022/11/30 20:32:47] ppcls INFO: epoch:5   train step:340  lr: 0.100000, loss:  1.2222 top1:  0.7266 top5:  0.8984 batch_cost: 0.18953 s, reader_cost: 0.00077 s, ips: 1350.70786 samples/sec.
[2022/11/30 20:32:49] ppcls INFO: epoch:5   train step:350  lr: 0.100000, loss:  1.1840 top1:  0.7070 top5:  0.8984 batch_cost: 0.18949 s, reader_cost: 0.00077 s, ips: 1351.00185 samples/sec.
[2022/11/30 20:32:51] ppcls INFO: epoch:5   train step:360  lr: 0.100000, loss:  1.1357 top1:  0.7617 top5:  0.9023 batch_cost: 0.18947 s, reader_cost: 0.00077 s, ips: 1351.11963 samples/sec.
[2022/11/30 20:32:53] ppcls INFO: epoch:5   train step:370  lr: 0.100000, loss:  1.1576 top1:  0.7422 top5:  0.9023 batch_cost: 0.18947 s, reader_cost: 0.00077 s, ips: 1351.14216 samples/sec.
[2022/11/30 20:32:55] ppcls INFO: epoch:5   train step:380  lr: 0.100000, loss:  1.0876 top1:  0.7344 top5:  0.9141 batch_cost: 0.18950 s, reader_cost: 0.00077 s, ips: 1350.94406 samples/sec.
[2022/11/30 20:32:56] ppcls INFO: epoch:5   train step:390  lr: 0.100000, loss:  1.1361 top1:  0.7617 top5:  0.9219 batch_cost: 0.18949 s, reader_cost: 0.00077 s, ips: 1351.00917 samples/sec.
[2022/11/30 20:32:56] ppcls INFO: END epoch:5   train  loss:  1.4181 top1:  0.6793 top5:  0.8676 batch_cost: 0.18949 s, reader_cost: 0.00077 s, batch_cost_sum: 73.14236 s,
[2022/11/30 20:32:57] ppcls INFO: Already save model in ./output/ResNet50/5
[2022/11/30 20:32:59] ppcls INFO: epoch:6   train step:10   lr: 0.100000, loss:  1.0434 top1:  0.7812 top5:  0.9297 batch_cost: 0.18995 s, reader_cost: 0.00070 s, ips: 1347.75361 samples/sec.
[2022/11/30 20:33:01] ppcls INFO: epoch:6   train step:20   lr: 0.100000, loss:  1.0898 top1:  0.7656 top5:  0.9023 batch_cost: 0.19047 s, reader_cost: 0.00080 s, ips: 1344.04424 samples/sec.
[2022/11/30 20:33:03] ppcls INFO: epoch:6   train step:30   lr: 0.100000, loss:  1.1318 top1:  0.7500 top5:  0.8984 batch_cost: 0.19064 s, reader_cost: 0.00083 s, ips: 1342.82997 samples/sec.
[2022/11/30 20:33:05] ppcls INFO: epoch:6   train step:40   lr: 0.100000, loss:  1.2075 top1:  0.7070 top5:  0.8984 batch_cost: 0.19011 s, reader_cost: 0.00080 s, ips: 1346.59036 samples/sec.
[2022/11/30 20:33:07] ppcls INFO: epoch:6   train step:50   lr: 0.100000, loss:  1.3495 top1:  0.6953 top5:  0.8750 batch_cost: 0.18992 s, reader_cost: 0.00082 s, ips: 1347.90860 samples/sec.
[2022/11/30 20:33:09] ppcls INFO: epoch:6   train step:60   lr: 0.100000, loss:  1.0685 top1:  0.7656 top5:  0.9141 batch_cost: 0.18967 s, reader_cost: 0.00082 s, ips: 1349.67854 samples/sec.
[2022/11/30 20:33:10] ppcls INFO: epoch:6   train step:70   lr: 0.100000, loss:  0.9502 top1:  0.7852 top5:  0.9258 batch_cost: 0.18955 s, reader_cost: 0.00081 s, ips: 1350.55013 samples/sec.
[2022/11/30 20:33:12] ppcls INFO: epoch:6   train step:80   lr: 0.100000, loss:  1.1443 top1:  0.7344 top5:  0.9102 batch_cost: 0.18955 s, reader_cost: 0.00079 s, ips: 1350.58728 samples/sec.
[2022/11/30 20:33:14] ppcls INFO: epoch:6   train step:90   lr: 0.100000, loss:  1.2700 top1:  0.7461 top5:  0.8750 batch_cost: 0.18938 s, reader_cost: 0.00078 s, ips: 1351.77460 samples/sec.
[2022/11/30 20:33:16] ppcls INFO: epoch:6   train step:100  lr: 0.100000, loss:  1.2091 top1:  0.7500 top5:  0.8828 batch_cost: 0.18936 s, reader_cost: 0.00078 s, ips: 1351.91139 samples/sec.
[2022/11/30 20:33:18] ppcls INFO: epoch:6   train step:110  lr: 0.100000, loss:  1.2283 top1:  0.7305 top5:  0.8711 batch_cost: 0.18938 s, reader_cost: 0.00078 s, ips: 1351.78729 samples/sec.
[2022/11/30 20:33:20] ppcls INFO: epoch:6   train step:120  lr: 0.100000, loss:  1.1466 top1:  0.7109 top5:  0.8984 batch_cost: 0.18943 s, reader_cost: 0.00077 s, ips: 1351.45424 samples/sec.
[2022/11/30 20:33:22] ppcls INFO: epoch:6   train step:130  lr: 0.100000, loss:  1.0431 top1:  0.7500 top5:  0.9258 batch_cost: 0.18940 s, reader_cost: 0.00076 s, ips: 1351.63786 samples/sec.
[2022/11/30 20:33:24] ppcls INFO: epoch:6   train step:140  lr: 0.100000, loss:  1.1361 top1:  0.7344 top5:  0.9102 batch_cost: 0.18939 s, reader_cost: 0.00076 s, ips: 1351.73972 samples/sec.
[2022/11/30 20:33:26] ppcls INFO: epoch:6   train step:150  lr: 0.100000, loss:  1.0402 top1:  0.7773 top5:  0.9023 batch_cost: 0.18936 s, reader_cost: 0.00075 s, ips: 1351.89466 samples/sec.
[2022/11/30 20:33:28] ppcls INFO: epoch:6   train step:160  lr: 0.100000, loss:  0.8847 top1:  0.7930 top5:  0.9258 batch_cost: 0.18938 s, reader_cost: 0.00076 s, ips: 1351.75870 samples/sec.
[2022/11/30 20:33:29] ppcls INFO: epoch:6   train step:170  lr: 0.100000, loss:  0.9048 top1:  0.7969 top5:  0.9219 batch_cost: 0.18937 s, reader_cost: 0.00076 s, ips: 1351.83986 samples/sec.
[2022/11/30 20:33:32] ppcls INFO: epoch:6   train step:180  lr: 0.100000, loss:  0.9337 top1:  0.8047 top5:  0.9219 batch_cost: 0.19046 s, reader_cost: 0.00191 s, ips: 1344.10330 samples/sec.
[2022/11/30 20:33:33] ppcls INFO: epoch:6   train step:190  lr: 0.100000, loss:  0.9596 top1:  0.7695 top5:  0.9102 batch_cost: 0.19041 s, reader_cost: 0.00185 s, ips: 1344.44353 samples/sec.
[2022/11/30 20:33:35] ppcls INFO: epoch:6   train step:200  lr: 0.100000, loss:  1.0600 top1:  0.7539 top5:  0.9023 batch_cost: 0.19036 s, reader_cost: 0.00179 s, ips: 1344.84822 samples/sec.
[2022/11/30 20:33:37] ppcls INFO: epoch:6   train step:210  lr: 0.100000, loss:  0.8946 top1:  0.7969 top5:  0.9492 batch_cost: 0.19033 s, reader_cost: 0.00174 s, ips: 1345.06229 samples/sec.
[2022/11/30 20:33:39] ppcls INFO: epoch:6   train step:220  lr: 0.100000, loss:  0.9609 top1:  0.7891 top5:  0.9180 batch_cost: 0.19030 s, reader_cost: 0.00169 s, ips: 1345.21981 samples/sec.
[2022/11/30 20:33:41] ppcls INFO: epoch:6   train step:230  lr: 0.100000, loss:  1.0036 top1:  0.7695 top5:  0.9141 batch_cost: 0.19028 s, reader_cost: 0.00166 s, ips: 1345.36730 samples/sec.
[2022/11/30 20:33:43] ppcls INFO: epoch:6   train step:240  lr: 0.100000, loss:  0.9536 top1:  0.8125 top5:  0.9141 batch_cost: 0.19030 s, reader_cost: 0.00162 s, ips: 1345.26715 samples/sec.
[2022/11/30 20:33:45] ppcls INFO: epoch:6   train step:250  lr: 0.100000, loss:  1.1727 top1:  0.7227 top5:  0.9062 batch_cost: 0.19022 s, reader_cost: 0.00159 s, ips: 1345.83469 samples/sec.
[2022/11/30 20:33:47] ppcls INFO: epoch:6   train step:260  lr: 0.100000, loss:  1.0221 top1:  0.7773 top5:  0.9219 batch_cost: 0.19020 s, reader_cost: 0.00156 s, ips: 1345.96890 samples/sec.
[2022/11/30 20:33:49] ppcls INFO: epoch:6   train step:270  lr: 0.100000, loss:  0.9253 top1:  0.7695 top5:  0.9375 batch_cost: 0.19018 s, reader_cost: 0.00153 s, ips: 1346.09636 samples/sec.
[2022/11/30 20:33:50] ppcls INFO: epoch:6   train step:280  lr: 0.100000, loss:  0.9850 top1:  0.7812 top5:  0.9062 batch_cost: 0.19018 s, reader_cost: 0.00150 s, ips: 1346.09577 samples/sec.
[2022/11/30 20:33:52] ppcls INFO: epoch:6   train step:290  lr: 0.100000, loss:  1.0175 top1:  0.7734 top5:  0.9102 batch_cost: 0.19016 s, reader_cost: 0.00148 s, ips: 1346.24292 samples/sec.
[2022/11/30 20:33:54] ppcls INFO: epoch:6   train step:300  lr: 0.100000, loss:  0.9692 top1:  0.7773 top5:  0.9023 batch_cost: 0.19013 s, reader_cost: 0.00145 s, ips: 1346.44301 samples/sec.
[2022/11/30 20:33:56] ppcls INFO: epoch:6   train step:310  lr: 0.100000, loss:  0.8741 top1:  0.7969 top5:  0.9219 batch_cost: 0.19016 s, reader_cost: 0.00146 s, ips: 1346.24278 samples/sec.
[2022/11/30 20:33:58] ppcls INFO: epoch:6   train step:320  lr: 0.100000, loss:  0.8894 top1:  0.7930 top5:  0.9414 batch_cost: 0.19012 s, reader_cost: 0.00144 s, ips: 1346.50791 samples/sec.
[2022/11/30 20:34:00] ppcls INFO: epoch:6   train step:330  lr: 0.100000, loss:  0.9337 top1:  0.7969 top5:  0.9375 batch_cost: 0.19008 s, reader_cost: 0.00142 s, ips: 1346.80709 samples/sec.
[2022/11/30 20:34:02] ppcls INFO: epoch:6   train step:340  lr: 0.100000, loss:  0.8133 top1:  0.8320 top5:  0.9375 batch_cost: 0.19003 s, reader_cost: 0.00140 s, ips: 1347.13781 samples/sec.
[2022/11/30 20:34:04] ppcls INFO: epoch:6   train step:350  lr: 0.100000, loss:  0.9620 top1:  0.7891 top5:  0.9062 batch_cost: 0.19006 s, reader_cost: 0.00138 s, ips: 1346.97353 samples/sec.
[2022/11/30 20:34:06] ppcls INFO: epoch:6   train step:360  lr: 0.100000, loss:  0.9152 top1:  0.7852 top5:  0.9297 batch_cost: 0.19008 s, reader_cost: 0.00136 s, ips: 1346.82107 samples/sec.
[2022/11/30 20:34:08] ppcls INFO: epoch:6   train step:370  lr: 0.100000, loss:  0.9313 top1:  0.7930 top5:  0.9375 batch_cost: 0.19006 s, reader_cost: 0.00135 s, ips: 1346.97709 samples/sec.
[2022/11/30 20:34:09] ppcls INFO: epoch:6   train step:380  lr: 0.100000, loss:  0.9190 top1:  0.8164 top5:  0.9336 batch_cost: 0.19005 s, reader_cost: 0.00133 s, ips: 1347.02864 samples/sec.
[2022/11/30 20:34:11] ppcls INFO: epoch:6   train step:390  lr: 0.100000, loss:  0.9235 top1:  0.8008 top5:  0.9258 batch_cost: 0.19003 s, reader_cost: 0.00132 s, ips: 1347.16586 samples/sec.
[2022/11/30 20:34:12] ppcls INFO: END epoch:6   train  loss:  1.0278 top1:  0.7707 top5:  0.9122 batch_cost: 0.19002 s, reader_cost: 0.00132 s, batch_cost_sum: 73.53966 s,
[2022/11/30 20:34:12] ppcls INFO: Already save model in ./output/ResNet50/6
[2022/11/30 20:34:14] ppcls INFO: epoch:7   train step:10   lr: 0.100000, loss:  1.0079 top1:  0.8008 top5:  0.9062 batch_cost: 0.18920 s, reader_cost: 0.00065 s, ips: 1353.03454 samples/sec.
LAUNCH INFO 2022-11-30 20:34:56,836 Terminating with signal 15
LAUNCH INFO 2022-11-30 20:35:02,573 Exit with signal 15
[2022/11/30 20:34:16] ppcls INFO: epoch:7   train step:20   lr: 0.100000, loss:  1.0010 top1:  0.7734 top5:  0.9297 batch_cost: 0.18936 s, reader_cost: 0.00077 s, ips: 1351.94540 samples/sec.
[2022/11/30 20:34:18] ppcls INFO: epoch:7   train step:30   lr: 0.100000, loss:  0.7679 top1:  0.8516 top5:  0.9180 batch_cost: 0.18903 s, reader_cost: 0.00072 s, ips: 1354.31264 samples/sec.
[2022/11/30 20:34:20] ppcls INFO: epoch:7   train step:40   lr: 0.100000, loss:  0.8121 top1:  0.7969 top5:  0.9492 batch_cost: 0.18905 s, reader_cost: 0.00074 s, ips: 1354.15929 samples/sec.
[2022/11/30 20:34:22] ppcls INFO: epoch:7   train step:50   lr: 0.100000, loss:  0.7790 top1:  0.8086 top5:  0.9336 batch_cost: 0.18895 s, reader_cost: 0.00072 s, ips: 1354.84757 samples/sec.
[2022/11/30 20:34:24] ppcls INFO: epoch:7   train step:60   lr: 0.100000, loss:  0.8779 top1:  0.8086 top5:  0.9258 batch_cost: 0.18892 s, reader_cost: 0.00073 s, ips: 1355.06479 samples/sec.
[2022/11/30 20:34:26] ppcls INFO: epoch:7   train step:70   lr: 0.100000, loss:  0.6783 top1:  0.8594 top5:  0.9531 batch_cost: 0.18898 s, reader_cost: 0.00074 s, ips: 1354.65281 samples/sec.
[2022/11/30 20:34:28] ppcls INFO: epoch:7   train step:80   lr: 0.100000, loss:  0.8599 top1:  0.8125 top5:  0.9141 batch_cost: 0.18904 s, reader_cost: 0.00073 s, ips: 1354.19150 samples/sec.
[2022/11/30 20:34:29] ppcls INFO: epoch:7   train step:90   lr: 0.100000, loss:  0.8006 top1:  0.8398 top5:  0.9375 batch_cost: 0.18921 s, reader_cost: 0.00096 s, ips: 1352.96248 samples/sec.
[2022/11/30 20:34:31] ppcls INFO: epoch:7   train step:100  lr: 0.100000, loss:  0.7254 top1:  0.8320 top5:  0.9453 batch_cost: 0.18926 s, reader_cost: 0.00093 s, ips: 1352.63780 samples/sec.
[2022/11/30 20:34:33] ppcls INFO: epoch:7   train step:110  lr: 0.100000, loss:  0.8555 top1:  0.7930 top5:  0.9219 batch_cost: 0.18919 s, reader_cost: 0.00091 s, ips: 1353.14061 samples/sec.
[2022/11/30 20:34:35] ppcls INFO: epoch:7   train step:120  lr: 0.100000, loss:  0.9001 top1:  0.8281 top5:  0.9141 batch_cost: 0.18924 s, reader_cost: 0.00089 s, ips: 1352.80817 samples/sec.
[2022/11/30 20:34:37] ppcls INFO: epoch:7   train step:130  lr: 0.100000, loss:  0.8204 top1:  0.8438 top5:  0.9297 batch_cost: 0.18921 s, reader_cost: 0.00088 s, ips: 1353.00588 samples/sec.
[2022/11/30 20:34:39] ppcls INFO: epoch:7   train step:140  lr: 0.100000, loss:  0.7245 top1:  0.8320 top5:  0.9570 batch_cost: 0.18918 s, reader_cost: 0.00087 s, ips: 1353.21879 samples/sec.
[2022/11/30 20:34:41] ppcls INFO: epoch:7   train step:150  lr: 0.100000, loss:  0.7518 top1:  0.8398 top5:  0.9453 batch_cost: 0.18915 s, reader_cost: 0.00087 s, ips: 1353.43337 samples/sec.
[2022/11/30 20:34:43] ppcls INFO: epoch:7   train step:160  lr: 0.100000, loss:  0.7795 top1:  0.8086 top5:  0.9336 batch_cost: 0.18918 s, reader_cost: 0.00086 s, ips: 1353.23549 samples/sec.
[2022/11/30 20:34:45] ppcls INFO: epoch:7   train step:170  lr: 0.100000, loss:  0.8290 top1:  0.7930 top5:  0.9297 batch_cost: 0.18916 s, reader_cost: 0.00086 s, ips: 1353.32055 samples/sec.
[2022/11/30 20:34:46] ppcls INFO: epoch:7   train step:180  lr: 0.100000, loss:  0.7944 top1:  0.8086 top5:  0.9336 batch_cost: 0.18914 s, reader_cost: 0.00085 s, ips: 1353.52534 samples/sec.
[2022/11/30 20:34:48] ppcls INFO: epoch:7   train step:190  lr: 0.100000, loss:  0.8170 top1:  0.8203 top5:  0.9336 batch_cost: 0.18922 s, reader_cost: 0.00085 s, ips: 1352.93535 samples/sec.
[2022/11/30 20:34:50] ppcls INFO: epoch:7   train step:200  lr: 0.100000, loss:  0.8055 top1:  0.8438 top5:  0.9297 batch_cost: 0.18929 s, reader_cost: 0.00085 s, ips: 1352.45238 samples/sec.
[2022/11/30 20:34:52] ppcls INFO: epoch:7   train step:210  lr: 0.100000, loss:  0.7633 top1:  0.8438 top5:  0.9492 batch_cost: 0.18926 s, reader_cost: 0.00084 s, ips: 1352.63915 samples/sec.
[2022/11/30 20:34:54] ppcls INFO: epoch:7   train step:220  lr: 0.100000, loss:  0.7623 top1:  0.8359 top5:  0.9414 batch_cost: 0.18926 s, reader_cost: 0.00084 s, ips: 1352.66157 samples/sec.
