grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
LAUNCH INFO 2023-07-28 07:14:19,033 -----------  Configuration  ----------------------
LAUNCH INFO 2023-07-28 07:14:19,033 auto_parallel_config: None
LAUNCH INFO 2023-07-28 07:14:19,033 devices: 0,1,2,3,4,5,6,7
LAUNCH INFO 2023-07-28 07:14:19,033 elastic_level: -1
LAUNCH INFO 2023-07-28 07:14:19,033 elastic_timeout: 30
LAUNCH INFO 2023-07-28 07:14:19,033 gloo_port: 6767
LAUNCH INFO 2023-07-28 07:14:19,033 host: None
LAUNCH INFO 2023-07-28 07:14:19,033 ips: None
LAUNCH INFO 2023-07-28 07:14:19,033 job_id: default
LAUNCH INFO 2023-07-28 07:14:19,033 legacy: False
LAUNCH INFO 2023-07-28 07:14:19,033 log_dir: log
LAUNCH INFO 2023-07-28 07:14:19,033 log_level: INFO
LAUNCH INFO 2023-07-28 07:14:19,033 log_overwrite: False
LAUNCH INFO 2023-07-28 07:14:19,033 master: None
LAUNCH INFO 2023-07-28 07:14:19,033 max_restart: 3
LAUNCH INFO 2023-07-28 07:14:19,033 nnodes: 1
LAUNCH INFO 2023-07-28 07:14:19,033 nproc_per_node: None
LAUNCH INFO 2023-07-28 07:14:19,034 rank: -1
LAUNCH INFO 2023-07-28 07:14:19,034 run_mode: collective
LAUNCH INFO 2023-07-28 07:14:19,034 server_num: None
LAUNCH INFO 2023-07-28 07:14:19,034 servers: 
LAUNCH INFO 2023-07-28 07:14:19,034 start_port: 6070
LAUNCH INFO 2023-07-28 07:14:19,034 trainer_num: None
LAUNCH INFO 2023-07-28 07:14:19,034 trainers: 
LAUNCH INFO 2023-07-28 07:14:19,034 training_script: ppcls/static/train.py
LAUNCH INFO 2023-07-28 07:14:19,034 training_script_args: ['-c', 'ppcls/configs/ImageNet/ResNet/ResNet50_amp_O2_ultra.yaml', '-o', 'DataLoader.Train.sampler.batch_size=256', '-o', 'Global.seed=1234', '-o', 'Global.epochs=8', '-o', 'DataLoader.Train.loader.num_workers=4', '-o', 'Global.eval_during_train=False', '-o', 'fuse_elewise_add_act_ops=True', '-o', 'enable_addto=True']
LAUNCH INFO 2023-07-28 07:14:19,034 with_gloo: 1
LAUNCH INFO 2023-07-28 07:14:19,034 --------------------------------------------------
LAUNCH INFO 2023-07-28 07:14:19,034 Job: default, mode collective, replicas 1[1:1], elastic False
LAUNCH INFO 2023-07-28 07:14:19,043 Run Pod: xwapwt, replicas 8, status ready
LAUNCH INFO 2023-07-28 07:14:19,153 Watching Pod: xwapwt, replicas 8, status running
grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
A new field (seed) detected!
A new field (fuse_elewise_add_act_ops) detected!
A new field (enable_addto) detected!
[2023/07/28 07:14:21] ppcls INFO: 
===========================================================
==        PaddleClas is powered by PaddlePaddle !        ==
===========================================================
==                                                       ==
==   For more info please go to the following website.   ==
==                                                       ==
==       https://github.com/PaddlePaddle/PaddleClas      ==
===========================================================

[2023/07/28 07:14:21] ppcls INFO: Global : 
[2023/07/28 07:14:21] ppcls INFO:     checkpoints : None
[2023/07/28 07:14:21] ppcls INFO:     pretrained_model : None
[2023/07/28 07:14:21] ppcls INFO:     output_dir : ./output/
[2023/07/28 07:14:21] ppcls INFO:     device : gpu
[2023/07/28 07:14:21] ppcls INFO:     save_interval : 1
[2023/07/28 07:14:21] ppcls INFO:     eval_during_train : False
[2023/07/28 07:14:21] ppcls INFO:     eval_interval : 1
[2023/07/28 07:14:21] ppcls INFO:     epochs : 8
[2023/07/28 07:14:21] ppcls INFO:     print_batch_step : 10
[2023/07/28 07:14:21] ppcls INFO:     use_visualdl : False
[2023/07/28 07:14:21] ppcls INFO:     image_channel : 4
[2023/07/28 07:14:21] ppcls INFO:     image_shape : [4, 224, 224]
[2023/07/28 07:14:21] ppcls INFO:     save_inference_dir : ./inference
[2023/07/28 07:14:21] ppcls INFO:     to_static : False
[2023/07/28 07:14:21] ppcls INFO:     use_dali : True
[2023/07/28 07:14:21] ppcls INFO:     seed : 1234
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: AMP : 
[2023/07/28 07:14:21] ppcls INFO:     scale_loss : 128.0
[2023/07/28 07:14:21] ppcls INFO:     use_dynamic_loss_scaling : True
[2023/07/28 07:14:21] ppcls INFO:     level : O2
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: Arch : 
[2023/07/28 07:14:21] ppcls INFO:     name : ResNet50
[2023/07/28 07:14:21] ppcls INFO:     class_num : 1000
[2023/07/28 07:14:21] ppcls INFO:     input_image_channel : 4
[2023/07/28 07:14:21] ppcls INFO:     data_format : NHWC
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: Loss : 
[2023/07/28 07:14:21] ppcls INFO:     Train : 
[2023/07/28 07:14:21] ppcls INFO:         CELoss : 
[2023/07/28 07:14:21] ppcls INFO:             weight : 1.0
[2023/07/28 07:14:21] ppcls INFO:     Eval : 
[2023/07/28 07:14:21] ppcls INFO:         CELoss : 
[2023/07/28 07:14:21] ppcls INFO:             weight : 1.0
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: Optimizer : 
[2023/07/28 07:14:21] ppcls INFO:     name : Momentum
[2023/07/28 07:14:21] ppcls INFO:     momentum : 0.9
[2023/07/28 07:14:21] ppcls INFO:     multi_precision : True
[2023/07/28 07:14:21] ppcls INFO:     lr : 
[2023/07/28 07:14:21] ppcls INFO:         name : Piecewise
[2023/07/28 07:14:21] ppcls INFO:         learning_rate : 0.1
[2023/07/28 07:14:21] ppcls INFO:         decay_epochs : [30, 60, 90]
[2023/07/28 07:14:21] ppcls INFO:         values : [0.1, 0.01, 0.001, 0.0001]
[2023/07/28 07:14:21] ppcls INFO:     regularizer : 
[2023/07/28 07:14:21] ppcls INFO:         name : L2
[2023/07/28 07:14:21] ppcls INFO:         coeff : 0.0001
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: DataLoader : 
[2023/07/28 07:14:21] ppcls INFO:     Train : 
[2023/07/28 07:14:21] ppcls INFO:         dataset : 
[2023/07/28 07:14:21] ppcls INFO:             name : ImageNetDataset
[2023/07/28 07:14:21] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2023/07/28 07:14:21] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/train_list.txt
[2023/07/28 07:14:21] ppcls INFO:             transform_ops : 
[2023/07/28 07:14:21] ppcls INFO:                 DecodeImage : 
[2023/07/28 07:14:21] ppcls INFO:                     to_rgb : True
[2023/07/28 07:14:21] ppcls INFO:                     channel_first : False
[2023/07/28 07:14:21] ppcls INFO:                 RandCropImage : 
[2023/07/28 07:14:21] ppcls INFO:                     size : 224
[2023/07/28 07:14:21] ppcls INFO:                 RandFlipImage : 
[2023/07/28 07:14:21] ppcls INFO:                     flip_code : 1
[2023/07/28 07:14:21] ppcls INFO:                 NormalizeImage : 
[2023/07/28 07:14:21] ppcls INFO:                     scale : 1.0/255.0
[2023/07/28 07:14:21] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2023/07/28 07:14:21] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2023/07/28 07:14:21] ppcls INFO:                     order : 
[2023/07/28 07:14:21] ppcls INFO:                     output_fp16 : True
[2023/07/28 07:14:21] ppcls INFO:                     channel_num : 4
[2023/07/28 07:14:21] ppcls INFO:         sampler : 
[2023/07/28 07:14:21] ppcls INFO:             name : DistributedBatchSampler
[2023/07/28 07:14:21] ppcls INFO:             batch_size : 256
[2023/07/28 07:14:21] ppcls INFO:             drop_last : False
[2023/07/28 07:14:21] ppcls INFO:             shuffle : True
[2023/07/28 07:14:21] ppcls INFO:         loader : 
[2023/07/28 07:14:21] ppcls INFO:             num_workers : 4
[2023/07/28 07:14:21] ppcls INFO:             use_shared_memory : True
[2023/07/28 07:14:21] ppcls INFO:     Eval : 
[2023/07/28 07:14:21] ppcls INFO:         dataset : 
[2023/07/28 07:14:21] ppcls INFO:             name : ImageNetDataset
[2023/07/28 07:14:21] ppcls INFO:             image_root : ./dataset/ILSVRC2012/
[2023/07/28 07:14:21] ppcls INFO:             cls_label_path : ./dataset/ILSVRC2012/val_list.txt
[2023/07/28 07:14:21] ppcls INFO:             transform_ops : 
[2023/07/28 07:14:21] ppcls INFO:                 DecodeImage : 
[2023/07/28 07:14:21] ppcls INFO:                     to_rgb : True
[2023/07/28 07:14:21] ppcls INFO:                     channel_first : False
[2023/07/28 07:14:21] ppcls INFO:                 ResizeImage : 
[2023/07/28 07:14:21] ppcls INFO:                     resize_short : 256
[2023/07/28 07:14:21] ppcls INFO:                 CropImage : 
[2023/07/28 07:14:21] ppcls INFO:                     size : 224
[2023/07/28 07:14:21] ppcls INFO:                 NormalizeImage : 
[2023/07/28 07:14:21] ppcls INFO:                     scale : 1.0/255.0
[2023/07/28 07:14:21] ppcls INFO:                     mean : [0.485, 0.456, 0.406]
[2023/07/28 07:14:21] ppcls INFO:                     std : [0.229, 0.224, 0.225]
[2023/07/28 07:14:21] ppcls INFO:                     order : 
[2023/07/28 07:14:21] ppcls INFO:                     channel_num : 4
[2023/07/28 07:14:21] ppcls INFO:         sampler : 
[2023/07/28 07:14:21] ppcls INFO:             name : DistributedBatchSampler
[2023/07/28 07:14:21] ppcls INFO:             batch_size : 64
[2023/07/28 07:14:21] ppcls INFO:             drop_last : False
[2023/07/28 07:14:21] ppcls INFO:             shuffle : False
[2023/07/28 07:14:21] ppcls INFO:         loader : 
[2023/07/28 07:14:21] ppcls INFO:             num_workers : 4
[2023/07/28 07:14:21] ppcls INFO:             use_shared_memory : True
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: Infer : 
[2023/07/28 07:14:21] ppcls INFO:     infer_imgs : docs/images/inference_deployment/whl_demo.jpg
[2023/07/28 07:14:21] ppcls INFO:     batch_size : 10
[2023/07/28 07:14:21] ppcls INFO:     transforms : 
[2023/07/28 07:14:21] ppcls INFO:         DecodeImage : 
[2023/07/28 07:14:21] ppcls INFO:             to_rgb : True
[2023/07/28 07:14:21] ppcls INFO:             channel_first : False
[2023/07/28 07:14:21] ppcls INFO:         ResizeImage : 
[2023/07/28 07:14:21] ppcls INFO:             resize_short : 256
[2023/07/28 07:14:21] ppcls INFO:         CropImage : 
[2023/07/28 07:14:21] ppcls INFO:             size : 224
[2023/07/28 07:14:21] ppcls INFO:         NormalizeImage : 
[2023/07/28 07:14:21] ppcls INFO:             scale : 1.0/255.0
[2023/07/28 07:14:21] ppcls INFO:             mean : [0.485, 0.456, 0.406]
[2023/07/28 07:14:21] ppcls INFO:             std : [0.229, 0.224, 0.225]
[2023/07/28 07:14:21] ppcls INFO:             order : 
[2023/07/28 07:14:21] ppcls INFO:             channel_num : 4
[2023/07/28 07:14:21] ppcls INFO:         ToCHWImage : None
[2023/07/28 07:14:21] ppcls INFO:     PostProcess : 
[2023/07/28 07:14:21] ppcls INFO:         name : Topk
[2023/07/28 07:14:21] ppcls INFO:         topk : 5
[2023/07/28 07:14:21] ppcls INFO:         class_id_map_file : ppcls/utils/imagenet1k_label_list.txt
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: Metric : 
[2023/07/28 07:14:21] ppcls INFO:     Train : 
[2023/07/28 07:14:21] ppcls INFO:         TopkAcc : 
[2023/07/28 07:14:21] ppcls INFO:             topk : [1, 5]
[2023/07/28 07:14:21] ppcls INFO:     Eval : 
[2023/07/28 07:14:21] ppcls INFO:         TopkAcc : 
[2023/07/28 07:14:21] ppcls INFO:             topk : [1, 5]
[2023/07/28 07:14:21] ppcls INFO: ------------------------------------------------------------
[2023/07/28 07:14:21] ppcls INFO: fuse_elewise_add_act_ops : True
[2023/07/28 07:14:21] ppcls INFO: enable_addto : True
[2023-07-28 07:14:21,498] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[2023/07/28 07:14:21] ppcls INFO: DALI fused Operator conversion(Train): [DecodeImage, RandCropImage] -> DecodeRandomResizedCrop: {'device': 'mixed', 'output_type': <DALIImageType.RGB: 0>, 'device_memory_padding': 211025920, 'host_memory_padding': 140544512, 'random_area': [0.08, 1.0], 'random_aspect_ratio': [0.75, 1.3333333333333333], 'num_attempts': 100, 'resize_x': 224, 'resize_y': 224}
[2023/07/28 07:14:21] ppcls INFO: DALI fused Operator conversion(Train): [RandCropImage, RandFlipImage, NormalizeImage] -> CropMirrorNormalize: {'dtype': <DALIDataType.FLOAT16: 8>, 'output_layout': 'CHW', 'crop': (224, 224), 'mean': [123.675, 116.28, 103.53000000000002], 'std': [58.395, 57.120000000000005, 57.375], 'pad_output': True, 'prob': 0.5, 'device': 'gpu'}
[2023/07/28 07:14:21] ppcls INFO: Building DALI Train pipeline with num_shards: 8, num_gpus: 8
[/opt/dali/dali/operators/image/resize/resampling_attr.cc:100] The default behavior for LINEAR interpolation type has been changed to apply an antialiasing filter. If you didn't mean to apply an antialiasing filter, please use `antialias=False`
W0728 07:14:26.868127 19500 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 11.2
W0728 07:14:26.873968 19500 gpu_resources.cc:149] device: 0, cuDNN Version: 8.2.
[2023/07/28 07:14:27] ppcls WARNING: "init_res" will be deprecated, please use "init_net" instead.
[2023-07-28 07:14:27,304] [    INFO] distributed_strategy.py:160 - distributed strategy initialized
[2023-07-28 07:14:27,305] [ WARNING] fleet.py:1092 - It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
I0728 07:14:27.770112 19500 fuse_pass_base.cc:59] ---  detected 33 subgraphs
I0728 07:14:27.863085 19500 fuse_pass_base.cc:59] ---  detected 33 subgraphs
I0728 07:14:28.300966 19500 fuse_pass_base.cc:59] ---  detected 16 subgraphs
I0728 07:14:28.330175 19500 fuse_pass_base.cc:59] ---  detected 16 subgraphs
server not ready, wait 3 sec to retry...
I0728 07:14:33.523943 19500 interpretercore.cc:237] New Executor is Running.
[2023/07/28 07:14:34] ppcls WARNING: Only support FP16 evaluation when AMP O2 is enabled.
I0728 07:14:37.131501 19500 interpreter_util.cc:518] Standalone Executor is Used.
[2023/07/28 07:14:48] ppcls INFO: epoch:0   train step:10   lr: 0.100000, loss:  7.2072 top1:  0.0000 top5:  0.0039 batch_cost: 0.18718 s, reader_cost: 0.00063 s, ips: 1367.69679 samples/sec.
[2023/07/28 07:14:50] ppcls INFO: epoch:0   train step:20   lr: 0.100000, loss:  7.4200 top1:  0.0000 top5:  0.0078 batch_cost: 0.18716 s, reader_cost: 0.00085 s, ips: 1367.79283 samples/sec.
[2023/07/28 07:14:52] ppcls INFO: epoch:0   train step:30   lr: 0.100000, loss:  7.4940 top1:  0.0078 top5: -0.0234 batch_cost: 0.18676 s, reader_cost: 0.00085 s, ips: 1370.74681 samples/sec.
[2023/07/28 07:14:54] ppcls INFO: epoch:0   train step:40   lr: 0.100000, loss:  7.1825 top1:  0.0000 top5:  0.0000 batch_cost: 0.18678 s, reader_cost: 0.00081 s, ips: 1370.62605 samples/sec.
[2023/07/28 07:14:56] ppcls INFO: epoch:0   train step:50   lr: 0.100000, loss:  7.1041 top1:  0.0000 top5:  0.0039 batch_cost: 0.18668 s, reader_cost: 0.00080 s, ips: 1371.35567 samples/sec.
[2023/07/28 07:14:58] ppcls INFO: epoch:0   train step:60   lr: 0.100000, loss:  7.0339 top1:  0.0000 top5:  0.0000 batch_cost: 0.18663 s, reader_cost: 0.00083 s, ips: 1371.67878 samples/sec.
[2023/07/28 07:15:00] ppcls INFO: epoch:0   train step:70   lr: 0.100000, loss:  6.9338 top1:  0.0039 top5:  0.0117 batch_cost: 0.18656 s, reader_cost: 0.00082 s, ips: 1372.24307 samples/sec.
[2023/07/28 07:15:01] ppcls INFO: epoch:0   train step:80   lr: 0.100000, loss:  6.9573 top1:  0.0039 top5:  0.0078 batch_cost: 0.18646 s, reader_cost: 0.00082 s, ips: 1372.98448 samples/sec.
[2023/07/28 07:15:03] ppcls INFO: epoch:0   train step:90   lr: 0.100000, loss:  6.9069 top1:  0.0000 top5:  0.0117 batch_cost: 0.18652 s, reader_cost: 0.00081 s, ips: 1372.51466 samples/sec.
[2023/07/28 07:15:05] ppcls INFO: epoch:0   train step:100  lr: 0.100000, loss:  6.9043 top1:  0.0039 top5:  0.0039 batch_cost: 0.18649 s, reader_cost: 0.00081 s, ips: 1372.74118 samples/sec.
[2023/07/28 07:15:07] ppcls INFO: epoch:0   train step:110  lr: 0.100000, loss:  6.8805 top1:  0.0039 top5:  0.0156 batch_cost: 0.18649 s, reader_cost: 0.00081 s, ips: 1372.75585 samples/sec.
[2023/07/28 07:15:09] ppcls INFO: epoch:0   train step:120  lr: 0.100000, loss:  6.9405 top1:  0.0000 top5:  0.0000 batch_cost: 0.18649 s, reader_cost: 0.00079 s, ips: 1372.72507 samples/sec.
[2023/07/28 07:15:11] ppcls INFO: epoch:0   train step:130  lr: 0.100000, loss:  6.8723 top1:  0.0000 top5:  0.0000 batch_cost: 0.18648 s, reader_cost: 0.00079 s, ips: 1372.83410 samples/sec.
[2023/07/28 07:15:13] ppcls INFO: epoch:0   train step:140  lr: 0.100000, loss:  6.8645 top1:  0.0000 top5:  0.0117 batch_cost: 0.18647 s, reader_cost: 0.00079 s, ips: 1372.87494 samples/sec.
[2023/07/28 07:15:15] ppcls INFO: epoch:0   train step:150  lr: 0.100000, loss:  6.8461 top1:  0.0000 top5:  0.0156 batch_cost: 0.18647 s, reader_cost: 0.00079 s, ips: 1372.86089 samples/sec.
[2023/07/28 07:15:16] ppcls INFO: epoch:0   train step:160  lr: 0.100000, loss:  6.8409 top1:  0.0039 top5:  0.0156 batch_cost: 0.18650 s, reader_cost: 0.00078 s, ips: 1372.66777 samples/sec.
[2023/07/28 07:15:18] ppcls INFO: epoch:0   train step:170  lr: 0.100000, loss:  6.7991 top1:  0.0078 top5:  0.0195 batch_cost: 0.18648 s, reader_cost: 0.00078 s, ips: 1372.78710 samples/sec.
[2023/07/28 07:15:20] ppcls INFO: epoch:0   train step:180  lr: 0.100000, loss:  6.7638 top1:  0.0000 top5:  0.0078 batch_cost: 0.18646 s, reader_cost: 0.00077 s, ips: 1372.92571 samples/sec.
[2023/07/28 07:15:22] ppcls INFO: epoch:0   train step:190  lr: 0.100000, loss:  6.7644 top1:  0.0039 top5: -0.0156 batch_cost: 0.18646 s, reader_cost: 0.00077 s, ips: 1372.92668 samples/sec.
[2023/07/28 07:15:24] ppcls INFO: epoch:0   train step:200  lr: 0.100000, loss:  6.6923 top1:  0.0039 top5: -0.0195 batch_cost: 0.18644 s, reader_cost: 0.00076 s, ips: 1373.06770 samples/sec.
[2023/07/28 07:15:26] ppcls INFO: epoch:0   train step:210  lr: 0.100000, loss:  6.6676 top1:  0.0039 top5:  0.0312 batch_cost: 0.18645 s, reader_cost: 0.00076 s, ips: 1373.01217 samples/sec.
[2023/07/28 07:15:28] ppcls INFO: epoch:0   train step:220  lr: 0.100000, loss:  6.6148 top1:  0.0039 top5:  0.0195 batch_cost: 0.18645 s, reader_cost: 0.00076 s, ips: 1373.05041 samples/sec.
[2023/07/28 07:15:29] ppcls INFO: epoch:0   train step:230  lr: 0.100000, loss:  6.6186 top1:  0.0039 top5:  0.0234 batch_cost: 0.18646 s, reader_cost: 0.00076 s, ips: 1372.96169 samples/sec.
[2023/07/28 07:15:31] ppcls INFO: epoch:0   train step:240  lr: 0.100000, loss:  6.4907 top1:  0.0039 top5:  0.0312 batch_cost: 0.18647 s, reader_cost: 0.00075 s, ips: 1372.88074 samples/sec.
[2023/07/28 07:15:33] ppcls INFO: epoch:0   train step:250  lr: 0.100000, loss:  6.4795 top1:  0.0078 top5: -0.0312 batch_cost: 0.18647 s, reader_cost: 0.00076 s, ips: 1372.87727 samples/sec.
[2023/07/28 07:15:35] ppcls INFO: epoch:0   train step:260  lr: 0.100000, loss:  6.4616 top1:  0.0117 top5:  0.0547 batch_cost: 0.18649 s, reader_cost: 0.00078 s, ips: 1372.74084 samples/sec.
[2023/07/28 07:15:37] ppcls INFO: epoch:0   train step:270  lr: 0.100000, loss:  6.4353 top1:  0.0117 top5:  0.0547 batch_cost: 0.18651 s, reader_cost: 0.00078 s, ips: 1372.59704 samples/sec.
[2023/07/28 07:15:39] ppcls INFO: epoch:0   train step:280  lr: 0.100000, loss:  6.4671 top1:  0.0156 top5:  0.0391 batch_cost: 0.18653 s, reader_cost: 0.00078 s, ips: 1372.42689 samples/sec.
[2023/07/28 07:15:41] ppcls INFO: epoch:0   train step:290  lr: 0.100000, loss:  6.3690 top1:  0.0156 top5:  0.0508 batch_cost: 0.18654 s, reader_cost: 0.00078 s, ips: 1372.36699 samples/sec.
[2023/07/28 07:15:43] ppcls INFO: epoch:0   train step:300  lr: 0.100000, loss:  6.3380 top1:  0.0156 top5:  0.0664 batch_cost: 0.18655 s, reader_cost: 0.00079 s, ips: 1372.27560 samples/sec.
[2023/07/28 07:15:44] ppcls INFO: epoch:0   train step:310  lr: 0.100000, loss:  6.2341 top1:  0.0195 top5:  0.0820 batch_cost: 0.18656 s, reader_cost: 0.00078 s, ips: 1372.19818 samples/sec.
[2023/07/28 07:15:46] ppcls INFO: epoch:0   train step:320  lr: 0.100000, loss:  6.2619 top1:  0.0391 top5:  0.0625 batch_cost: 0.18655 s, reader_cost: 0.00078 s, ips: 1372.25974 samples/sec.
[2023/07/28 07:15:48] ppcls INFO: epoch:0   train step:330  lr: 0.100000, loss:  6.1217 top1:  0.0312 top5:  0.0742 batch_cost: 0.18655 s, reader_cost: 0.00078 s, ips: 1372.26215 samples/sec.
[2023/07/28 07:15:50] ppcls INFO: epoch:0   train step:340  lr: 0.100000, loss:  5.8467 top1:  0.0312 top5:  0.1016 batch_cost: 0.18655 s, reader_cost: 0.00078 s, ips: 1372.25133 samples/sec.
[2023/07/28 07:15:52] ppcls INFO: epoch:0   train step:350  lr: 0.100000, loss:  6.0125 top1:  0.0195 top5: -0.0859 batch_cost: 0.18656 s, reader_cost: 0.00078 s, ips: 1372.18409 samples/sec.
[2023/07/28 07:15:54] ppcls INFO: epoch:0   train step:360  lr: 0.100000, loss:  5.8801 top1:  0.0312 top5:  0.1133 batch_cost: 0.18657 s, reader_cost: 0.00078 s, ips: 1372.15435 samples/sec.
[2023/07/28 07:15:56] ppcls INFO: epoch:0   train step:370  lr: 0.100000, loss:  5.9861 top1:  0.0391 top5:  0.1289 batch_cost: 0.18657 s, reader_cost: 0.00078 s, ips: 1372.12585 samples/sec.
[2023/07/28 07:15:57] ppcls INFO: epoch:0   train step:380  lr: 0.100000, loss:  5.9061 top1:  0.0352 top5:  0.1016 batch_cost: 0.18656 s, reader_cost: 0.00078 s, ips: 1372.18352 samples/sec.
[2023/07/28 07:15:59] ppcls INFO: epoch:0   train step:390  lr: 0.100000, loss:  5.8113 top1:  0.0352 top5:  0.1055 batch_cost: 0.18658 s, reader_cost: 0.00078 s, ips: 1372.09483 samples/sec.
[2023/07/28 07:16:01] ppcls INFO: epoch:0   train step:400  lr: 0.100000, loss:  5.7901 top1:  0.0312 top5:  0.1016 batch_cost: 0.18658 s, reader_cost: 0.00077 s, ips: 1372.07692 samples/sec.
[2023/07/28 07:16:03] ppcls INFO: epoch:0   train step:410  lr: 0.100000, loss:  5.8714 top1:  0.0352 top5:  0.1016 batch_cost: 0.18659 s, reader_cost: 0.00077 s, ips: 1372.01586 samples/sec.
[2023/07/28 07:16:05] ppcls INFO: epoch:0   train step:420  lr: 0.100000, loss:  5.6412 top1:  0.0273 top5: -0.1016 batch_cost: 0.18659 s, reader_cost: 0.00078 s, ips: 1371.97187 samples/sec.
[2023/07/28 07:16:07] ppcls INFO: epoch:0   train step:430  lr: 0.100000, loss:  5.6083 top1:  0.0508 top5:  0.1445 batch_cost: 0.18660 s, reader_cost: 0.00078 s, ips: 1371.93752 samples/sec.
[2023/07/28 07:16:09] ppcls INFO: epoch:0   train step:440  lr: 0.100000, loss:  5.5070 top1:  0.0391 top5:  0.1602 batch_cost: 0.18660 s, reader_cost: 0.00078 s, ips: 1371.88250 samples/sec.
[2023/07/28 07:16:11] ppcls INFO: epoch:0   train step:450  lr: 0.100000, loss:  5.5519 top1:  0.0625 top5: -0.1641 batch_cost: 0.18661 s, reader_cost: 0.00078 s, ips: 1371.84844 samples/sec.
[2023/07/28 07:16:12] ppcls INFO: epoch:0   train step:460  lr: 0.100000, loss:  5.4770 top1:  0.0508 top5:  0.1406 batch_cost: 0.18663 s, reader_cost: 0.00078 s, ips: 1371.70193 samples/sec.
[2023/07/28 07:16:14] ppcls INFO: epoch:0   train step:470  lr: 0.100000, loss:  5.4214 top1:  0.0547 top5:  0.1797 batch_cost: 0.18663 s, reader_cost: 0.00078 s, ips: 1371.69853 samples/sec.
[2023/07/28 07:16:16] ppcls INFO: epoch:0   train step:480  lr: 0.100000, loss:  5.5413 top1:  0.0547 top5:  0.1445 batch_cost: 0.18663 s, reader_cost: 0.00077 s, ips: 1371.71818 samples/sec.
[2023/07/28 07:16:18] ppcls INFO: epoch:0   train step:490  lr: 0.100000, loss:  5.3142 top1:  0.0703 top5:  0.1719 batch_cost: 0.18663 s, reader_cost: 0.00077 s, ips: 1371.73060 samples/sec.
[2023/07/28 07:16:20] ppcls INFO: epoch:0   train step:500  lr: 0.100000, loss:  5.3429 top1: -0.0391 top5:  0.1719 batch_cost: 0.18663 s, reader_cost: 0.00077 s, ips: 1371.70493 samples/sec.
[2023/07/28 07:16:22] ppcls INFO: epoch:0   train step:510  lr: 0.100000, loss:  5.3725 top1:  0.0742 top5:  0.1719 batch_cost: 0.18665 s, reader_cost: 0.00076 s, ips: 1371.58196 samples/sec.
[2023/07/28 07:16:24] ppcls INFO: epoch:0   train step:520  lr: 0.100000, loss:  5.1455 top1:  0.0664 top5:  0.1953 batch_cost: 0.18667 s, reader_cost: 0.00076 s, ips: 1371.38360 samples/sec.
[2023/07/28 07:16:26] ppcls INFO: epoch:0   train step:530  lr: 0.100000, loss:  5.3911 top1:  0.0469 top5: -0.1328 batch_cost: 0.18668 s, reader_cost: 0.00076 s, ips: 1371.36343 samples/sec.
[2023/07/28 07:16:27] ppcls INFO: epoch:0   train step:540  lr: 0.100000, loss:  5.1366 top1:  0.1055 top5: -0.2148 batch_cost: 0.18668 s, reader_cost: 0.00075 s, ips: 1371.36605 samples/sec.
[2023/07/28 07:16:29] ppcls INFO: epoch:0   train step:550  lr: 0.100000, loss:  5.3107 top1:  0.0664 top5:  0.1758 batch_cost: 0.18668 s, reader_cost: 0.00075 s, ips: 1371.35283 samples/sec.
[2023/07/28 07:16:31] ppcls INFO: epoch:0   train step:560  lr: 0.100000, loss:  4.9053 top1:  0.1055 top5:  0.2578 batch_cost: 0.18668 s, reader_cost: 0.00075 s, ips: 1371.31441 samples/sec.
[2023/07/28 07:16:33] ppcls INFO: epoch:0   train step:570  lr: 0.100000, loss:  5.1781 top1:  0.0703 top5: -0.2344 batch_cost: 0.18669 s, reader_cost: 0.00075 s, ips: 1371.28628 samples/sec.
[2023/07/28 07:16:35] ppcls INFO: epoch:0   train step:580  lr: 0.100000, loss:  5.1749 top1:  0.0859 top5:  0.2148 batch_cost: 0.18669 s, reader_cost: 0.00075 s, ips: 1371.27751 samples/sec.
[2023/07/28 07:16:37] ppcls INFO: epoch:0   train step:590  lr: 0.100000, loss:  5.1458 top1:  0.0820 top5: -0.2031 batch_cost: 0.18670 s, reader_cost: 0.00075 s, ips: 1371.20908 samples/sec.
[2023/07/28 07:16:39] ppcls INFO: epoch:0   train step:600  lr: 0.100000, loss:  5.1148 top1:  0.0781 top5:  0.2109 batch_cost: 0.18670 s, reader_cost: 0.00075 s, ips: 1371.19608 samples/sec.
[2023/07/28 07:16:41] ppcls INFO: epoch:0   train step:610  lr: 0.100000, loss:  5.0999 top1:  0.0625 top5: -0.1719 batch_cost: 0.18669 s, reader_cost: 0.00074 s, ips: 1371.22678 samples/sec.
[2023/07/28 07:16:42] ppcls INFO: epoch:0   train step:620  lr: 0.100000, loss:  4.8361 top1:  0.1328 top5:  0.2969 batch_cost: 0.18670 s, reader_cost: 0.00074 s, ips: 1371.18875 samples/sec.
[2023/07/28 07:16:44] ppcls INFO: epoch:0   train step:630  lr: 0.100000, loss:  5.1651 top1:  0.0781 top5:  0.2188 batch_cost: 0.18670 s, reader_cost: 0.00074 s, ips: 1371.16351 samples/sec.
[2023/07/28 07:16:46] ppcls INFO: epoch:0   train step:640  lr: 0.100000, loss:  4.7431 top1: -0.1406 top5:  0.3164 batch_cost: 0.18671 s, reader_cost: 0.00074 s, ips: 1371.08892 samples/sec.
[2023/07/28 07:16:48] ppcls INFO: epoch:0   train step:650  lr: 0.100000, loss:  4.8074 top1:  0.1094 top5:  0.2695 batch_cost: 0.18672 s, reader_cost: 0.00074 s, ips: 1371.05074 samples/sec.
[2023/07/28 07:16:50] ppcls INFO: epoch:0   train step:660  lr: 0.100000, loss:  4.8544 top1:  0.0938 top5:  0.2812 batch_cost: 0.18672 s, reader_cost: 0.00073 s, ips: 1371.04939 samples/sec.
[2023/07/28 07:16:52] ppcls INFO: epoch:0   train step:670  lr: 0.100000, loss:  4.7174 top1:  0.1406 top5:  0.2969 batch_cost: 0.18672 s, reader_cost: 0.00073 s, ips: 1371.00039 samples/sec.
[2023/07/28 07:16:54] ppcls INFO: epoch:0   train step:680  lr: 0.100000, loss:  4.6729 top1:  0.1445 top5: -0.2891 batch_cost: 0.18672 s, reader_cost: 0.00073 s, ips: 1371.00401 samples/sec.
[2023/07/28 07:16:55] ppcls INFO: epoch:0   train step:690  lr: 0.100000, loss:  4.6538 top1:  0.1133 top5: -0.3242 batch_cost: 0.18673 s, reader_cost: 0.00073 s, ips: 1370.97224 samples/sec.
[2023/07/28 07:16:57] ppcls INFO: epoch:0   train step:700  lr: 0.100000, loss:  4.7209 top1:  0.1328 top5:  0.2773 batch_cost: 0.18673 s, reader_cost: 0.00073 s, ips: 1370.93365 samples/sec.
[2023/07/28 07:16:59] ppcls INFO: epoch:0   train step:710  lr: 0.100000, loss:  4.4779 top1:  0.1289 top5: -0.3008 batch_cost: 0.18673 s, reader_cost: 0.00073 s, ips: 1370.95182 samples/sec.
[2023/07/28 07:17:01] ppcls INFO: epoch:0   train step:720  lr: 0.100000, loss:  4.6751 top1:  0.1328 top5: -0.3242 batch_cost: 0.18673 s, reader_cost: 0.00072 s, ips: 1370.95643 samples/sec.
[2023/07/28 07:17:03] ppcls INFO: epoch:0   train step:730  lr: 0.100000, loss:  4.5189 top1:  0.1562 top5: -0.3359 batch_cost: 0.18673 s, reader_cost: 0.00072 s, ips: 1370.96785 samples/sec.
[2023/07/28 07:17:05] ppcls INFO: epoch:0   train step:740  lr: 0.100000, loss:  4.7536 top1:  0.1016 top5: -0.2617 batch_cost: 0.18674 s, reader_cost: 0.00072 s, ips: 1370.91801 samples/sec.
[2023/07/28 07:17:07] ppcls INFO: epoch:0   train step:750  lr: 0.100000, loss:  4.6066 top1:  0.1328 top5:  0.3281 batch_cost: 0.18674 s, reader_cost: 0.00072 s, ips: 1370.89604 samples/sec.
[2023/07/28 07:17:09] ppcls INFO: epoch:0   train step:760  lr: 0.100000, loss:  4.3396 top1:  0.1367 top5:  0.3320 batch_cost: 0.18674 s, reader_cost: 0.00072 s, ips: 1370.91197 samples/sec.
[2023/07/28 07:17:10] ppcls INFO: epoch:0   train step:770  lr: 0.100000, loss:  4.4759 top1:  0.1367 top5:  0.3242 batch_cost: 0.18674 s, reader_cost: 0.00071 s, ips: 1370.89013 samples/sec.
[2023/07/28 07:17:12] ppcls INFO: epoch:0   train step:780  lr: 0.100000, loss:  4.6265 top1:  0.1797 top5:  0.3398 batch_cost: 0.18674 s, reader_cost: 0.00071 s, ips: 1370.86294 samples/sec.
[2023/07/28 07:17:14] ppcls INFO: epoch:0   train step:790  lr: 0.100000, loss:  4.3201 top1:  0.1836 top5:  0.3477 batch_cost: 0.18675 s, reader_cost: 0.00071 s, ips: 1370.83666 samples/sec.
[2023/07/28 07:17:16] ppcls INFO: epoch:0   train step:800  lr: 0.100000, loss:  4.3217 top1:  0.1406 top5: -0.3555 batch_cost: 0.18675 s, reader_cost: 0.00071 s, ips: 1370.80691 samples/sec.
[2023/07/28 07:17:18] ppcls INFO: epoch:0   train step:810  lr: 0.100000, loss:  4.4182 top1:  0.1445 top5: -0.3359 batch_cost: 0.18676 s, reader_cost: 0.00071 s, ips: 1370.76737 samples/sec.
[2023/07/28 07:17:20] ppcls INFO: epoch:0   train step:820  lr: 0.100000, loss:  4.3225 top1:  0.1211 top5: -0.3398 batch_cost: 0.18676 s, reader_cost: 0.00071 s, ips: 1370.75275 samples/sec.
[2023/07/28 07:17:22] ppcls INFO: epoch:0   train step:830  lr: 0.100000, loss:  4.4653 top1:  0.1406 top5:  0.3320 batch_cost: 0.18676 s, reader_cost: 0.00071 s, ips: 1370.72602 samples/sec.
[2023/07/28 07:17:24] ppcls INFO: epoch:0   train step:840  lr: 0.100000, loss:  4.3814 top1:  0.1484 top5:  0.3359 batch_cost: 0.18677 s, reader_cost: 0.00070 s, ips: 1370.67791 samples/sec.
[2023/07/28 07:17:25] ppcls INFO: epoch:0   train step:850  lr: 0.100000, loss:  4.2261 top1: -0.1367 top5: -0.3672 batch_cost: 0.18677 s, reader_cost: 0.00070 s, ips: 1370.69947 samples/sec.
[2023/07/28 07:17:27] ppcls INFO: epoch:0   train step:860  lr: 0.100000, loss:  4.2179 top1:  0.1250 top5:  0.3320 batch_cost: 0.18677 s, reader_cost: 0.00070 s, ips: 1370.69162 samples/sec.
[2023/07/28 07:17:29] ppcls INFO: epoch:0   train step:870  lr: 0.100000, loss:  4.2506 top1:  0.1406 top5:  0.3398 batch_cost: 0.18677 s, reader_cost: 0.00070 s, ips: 1370.67098 samples/sec.
[2023/07/28 07:17:31] ppcls INFO: END epoch:0   train  loss:  5.6825 top1:  0.0565 top5:  0.0243 batch_cost: 0.18677 s, reader_cost: 0.00070 s, batch_cost_sum: 163.42611 s,
[2023/07/28 07:17:32] ppcls INFO: Already save model in ./output/ResNet50/0
[2023/07/28 07:17:34] ppcls INFO: epoch:1   train step:10   lr: 0.100000, loss:  4.2150 top1:  0.1680 top5:  0.3750 batch_cost: 0.18642 s, reader_cost: 0.00059 s, ips: 1373.24348 samples/sec.
[2023/07/28 07:17:36] ppcls INFO: epoch:1   train step:20   lr: 0.100000, loss:  3.9901 top1:  0.2109 top5:  0.4141 batch_cost: 0.18695 s, reader_cost: 0.00062 s, ips: 1369.38449 samples/sec.
[2023/07/28 07:17:37] ppcls INFO: epoch:1   train step:30   lr: 0.100000, loss:  3.9956 top1:  0.1953 top5: -0.4062 batch_cost: 0.18681 s, reader_cost: 0.00059 s, ips: 1370.37458 samples/sec.
[2023/07/28 07:17:39] ppcls INFO: epoch:1   train step:40   lr: 0.100000, loss:  4.0963 top1:  0.1719 top5:  0.3828 batch_cost: 0.18711 s, reader_cost: 0.00060 s, ips: 1368.20065 samples/sec.
[2023/07/28 07:17:41] ppcls INFO: epoch:1   train step:50   lr: 0.100000, loss:  4.2213 top1:  0.1523 top5:  0.3984 batch_cost: 0.18707 s, reader_cost: 0.00060 s, ips: 1368.50421 samples/sec.
[2023/07/28 07:17:43] ppcls INFO: epoch:1   train step:60   lr: 0.100000, loss:  3.9532 top1:  0.2109 top5:  0.4297 batch_cost: 0.18705 s, reader_cost: 0.00061 s, ips: 1368.63657 samples/sec.
[2023/07/28 07:17:45] ppcls INFO: epoch:1   train step:70   lr: 0.100000, loss:  4.0840 top1:  0.1680 top5:  0.4141 batch_cost: 0.18702 s, reader_cost: 0.00062 s, ips: 1368.83181 samples/sec.
[2023/07/28 07:17:47] ppcls INFO: epoch:1   train step:80   lr: 0.100000, loss:  3.8679 top1: -0.1992 top5: -0.4688 batch_cost: 0.18703 s, reader_cost: 0.00062 s, ips: 1368.79138 samples/sec.
[2023/07/28 07:17:49] ppcls INFO: epoch:1   train step:90   lr: 0.100000, loss:  4.0263 top1:  0.1953 top5: -0.4258 batch_cost: 0.18700 s, reader_cost: 0.00081 s, ips: 1368.99078 samples/sec.
[2023/07/28 07:17:51] ppcls INFO: epoch:1   train step:100  lr: 0.100000, loss:  3.8497 top1:  0.2148 top5:  0.4141 batch_cost: 0.18699 s, reader_cost: 0.00079 s, ips: 1369.05309 samples/sec.
[2023/07/28 07:17:52] ppcls INFO: epoch:1   train step:110  lr: 0.100000, loss:  3.6882 top1:  0.2383 top5: -0.4688 batch_cost: 0.18694 s, reader_cost: 0.00077 s, ips: 1369.43168 samples/sec.
[2023/07/28 07:17:54] ppcls INFO: epoch:1   train step:120  lr: 0.100000, loss:  3.9693 top1:  0.1797 top5: -0.3906 batch_cost: 0.18695 s, reader_cost: 0.00076 s, ips: 1369.31641 samples/sec.
[2023/07/28 07:17:56] ppcls INFO: epoch:1   train step:130  lr: 0.100000, loss:  3.9320 top1:  0.1953 top5: -0.4297 batch_cost: 0.18694 s, reader_cost: 0.00074 s, ips: 1369.42664 samples/sec.
[2023/07/28 07:17:58] ppcls INFO: epoch:1   train step:140  lr: 0.100000, loss:  3.6810 top1:  0.2422 top5:  0.4805 batch_cost: 0.18693 s, reader_cost: 0.00074 s, ips: 1369.46345 samples/sec.
[2023/07/28 07:18:00] ppcls INFO: epoch:1   train step:150  lr: 0.100000, loss:  3.9478 top1:  0.1953 top5: -0.4102 batch_cost: 0.18696 s, reader_cost: 0.00073 s, ips: 1369.26846 samples/sec.
[2023/07/28 07:18:02] ppcls INFO: epoch:1   train step:160  lr: 0.100000, loss:  3.8586 top1:  0.2227 top5:  0.4297 batch_cost: 0.18695 s, reader_cost: 0.00072 s, ips: 1369.35401 samples/sec.
[2023/07/28 07:18:04] ppcls INFO: epoch:1   train step:170  lr: 0.100000, loss:  3.9371 top1:  0.2305 top5: -0.4453 batch_cost: 0.18693 s, reader_cost: 0.00072 s, ips: 1369.51029 samples/sec.
[2023/07/28 07:18:05] ppcls INFO: epoch:1   train step:180  lr: 0.100000, loss:  3.5882 top1:  0.2695 top5: -0.5039 batch_cost: 0.18693 s, reader_cost: 0.00072 s, ips: 1369.48059 samples/sec.
[2023/07/28 07:18:07] ppcls INFO: epoch:1   train step:190  lr: 0.100000, loss:  3.7368 top1:  0.2305 top5: -0.4727 batch_cost: 0.18693 s, reader_cost: 0.00071 s, ips: 1369.46790 samples/sec.
[2023/07/28 07:18:09] ppcls INFO: epoch:1   train step:200  lr: 0.100000, loss:  3.6059 top1:  0.2500 top5:  0.4961 batch_cost: 0.18695 s, reader_cost: 0.00070 s, ips: 1369.32252 samples/sec.
[2023/07/28 07:18:11] ppcls INFO: epoch:1   train step:210  lr: 0.100000, loss:  3.4402 top1:  0.2695 top5: -0.5273 batch_cost: 0.18697 s, reader_cost: 0.00070 s, ips: 1369.22135 samples/sec.
[2023/07/28 07:18:13] ppcls INFO: epoch:1   train step:220  lr: 0.100000, loss:  3.6220 top1:  0.2539 top5:  0.5039 batch_cost: 0.18696 s, reader_cost: 0.00070 s, ips: 1369.30966 samples/sec.
[2023/07/28 07:18:15] ppcls INFO: epoch:1   train step:230  lr: 0.100000, loss:  3.5396 top1:  0.2578 top5:  0.5000 batch_cost: 0.18697 s, reader_cost: 0.00070 s, ips: 1369.22139 samples/sec.
[2023/07/28 07:18:17] ppcls INFO: epoch:1   train step:240  lr: 0.100000, loss:  3.6019 top1:  0.2773 top5:  0.4961 batch_cost: 0.18700 s, reader_cost: 0.00069 s, ips: 1368.97460 samples/sec.
[2023/07/28 07:18:19] ppcls INFO: epoch:1   train step:250  lr: 0.100000, loss:  3.5686 top1: -0.2930 top5: -0.4727 batch_cost: 0.18699 s, reader_cost: 0.00069 s, ips: 1369.04008 samples/sec.
[2023/07/28 07:18:20] ppcls INFO: epoch:1   train step:260  lr: 0.100000, loss:  3.6289 top1:  0.2656 top5:  0.4766 batch_cost: 0.18700 s, reader_cost: 0.00069 s, ips: 1368.98871 samples/sec.
[2023/07/28 07:18:22] ppcls INFO: epoch:1   train step:270  lr: 0.100000, loss:  3.5783 top1:  0.2656 top5:  0.5117 batch_cost: 0.18700 s, reader_cost: 0.00068 s, ips: 1369.00065 samples/sec.
[2023/07/28 07:18:24] ppcls INFO: epoch:1   train step:280  lr: 0.100000, loss:  3.4273 top1:  0.2930 top5:  0.5234 batch_cost: 0.18701 s, reader_cost: 0.00069 s, ips: 1368.93965 samples/sec.
[2023/07/28 07:18:26] ppcls INFO: epoch:1   train step:290  lr: 0.100000, loss:  3.6575 top1:  0.2305 top5:  0.5039 batch_cost: 0.18700 s, reader_cost: 0.00068 s, ips: 1368.98676 samples/sec.
[2023/07/28 07:18:28] ppcls INFO: epoch:1   train step:300  lr: 0.100000, loss:  3.2093 top1: -0.2930 top5:  0.5820 batch_cost: 0.18701 s, reader_cost: 0.00068 s, ips: 1368.91172 samples/sec.
[2023/07/28 07:18:30] ppcls INFO: epoch:1   train step:310  lr: 0.100000, loss:  3.3649 top1:  0.2617 top5:  0.5430 batch_cost: 0.18702 s, reader_cost: 0.00068 s, ips: 1368.85413 samples/sec.
[2023/07/28 07:18:32] ppcls INFO: epoch:1   train step:320  lr: 0.100000, loss:  3.1941 top1:  0.3438 top5:  0.6055 batch_cost: 0.18701 s, reader_cost: 0.00068 s, ips: 1368.88291 samples/sec.
[2023/07/28 07:18:34] ppcls INFO: epoch:1   train step:330  lr: 0.100000, loss:  3.2344 top1:  0.3008 top5: -0.5898 batch_cost: 0.18703 s, reader_cost: 0.00068 s, ips: 1368.78784 samples/sec.
[2023/07/28 07:18:35] ppcls INFO: epoch:1   train step:340  lr: 0.100000, loss:  3.2162 top1:  0.3594 top5: -0.5781 batch_cost: 0.18702 s, reader_cost: 0.00068 s, ips: 1368.85384 samples/sec.
[2023/07/28 07:18:37] ppcls INFO: epoch:1   train step:350  lr: 0.100000, loss:  3.3684 top1:  0.3125 top5: -0.5273 batch_cost: 0.18703 s, reader_cost: 0.00068 s, ips: 1368.75392 samples/sec.
[2023/07/28 07:18:39] ppcls INFO: epoch:1   train step:360  lr: 0.100000, loss:  3.1873 top1:  0.2969 top5:  0.5625 batch_cost: 0.18703 s, reader_cost: 0.00068 s, ips: 1368.78950 samples/sec.
[2023/07/28 07:18:41] ppcls INFO: epoch:1   train step:370  lr: 0.100000, loss:  3.2355 top1:  0.2969 top5:  0.5859 batch_cost: 0.18706 s, reader_cost: 0.00067 s, ips: 1368.56336 samples/sec.
[2023/07/28 07:18:43] ppcls INFO: epoch:1   train step:380  lr: 0.100000, loss:  2.9369 top1:  0.3633 top5:  0.6406 batch_cost: 0.18707 s, reader_cost: 0.00067 s, ips: 1368.45110 samples/sec.
[2023/07/28 07:18:45] ppcls INFO: epoch:1   train step:390  lr: 0.100000, loss:  3.0672 top1:  0.3320 top5:  0.5742 batch_cost: 0.18707 s, reader_cost: 0.00067 s, ips: 1368.49667 samples/sec.
[2023/07/28 07:18:47] ppcls INFO: epoch:1   train step:400  lr: 0.100000, loss:  2.9441 top1:  0.3477 top5: -0.6250 batch_cost: 0.18707 s, reader_cost: 0.00067 s, ips: 1368.44984 samples/sec.
[2023/07/28 07:18:49] ppcls INFO: epoch:1   train step:410  lr: 0.100000, loss:  3.0204 top1:  0.3398 top5: -0.5703 batch_cost: 0.18708 s, reader_cost: 0.00066 s, ips: 1368.39189 samples/sec.
[2023/07/28 07:18:50] ppcls INFO: epoch:1   train step:420  lr: 0.100000, loss:  3.0922 top1:  0.3320 top5:  0.5898 batch_cost: 0.18709 s, reader_cost: 0.00066 s, ips: 1368.32273 samples/sec.
[2023/07/28 07:18:52] ppcls INFO: epoch:1   train step:430  lr: 0.100000, loss:  2.9743 top1:  0.3633 top5:  0.6328 batch_cost: 0.18711 s, reader_cost: 0.00066 s, ips: 1368.20488 samples/sec.
[2023/07/28 07:18:54] ppcls INFO: epoch:1   train step:440  lr: 0.100000, loss:  2.8718 top1:  0.3945 top5:  0.6328 batch_cost: 0.18711 s, reader_cost: 0.00067 s, ips: 1368.19798 samples/sec.
[2023/07/28 07:18:56] ppcls INFO: epoch:1   train step:450  lr: 0.100000, loss:  3.1401 top1:  0.3477 top5:  0.5742 batch_cost: 0.18711 s, reader_cost: 0.00066 s, ips: 1368.18747 samples/sec.
[2023/07/28 07:18:58] ppcls INFO: epoch:1   train step:460  lr: 0.100000, loss:  2.8973 top1:  0.3906 top5:  0.6680 batch_cost: 0.18712 s, reader_cost: 0.00066 s, ips: 1368.12202 samples/sec.
[2023/07/28 07:19:00] ppcls INFO: epoch:1   train step:470  lr: 0.100000, loss:  2.8635 top1:  0.3711 top5:  0.6094 batch_cost: 0.18711 s, reader_cost: 0.00066 s, ips: 1368.15352 samples/sec.
[2023/07/28 07:19:02] ppcls INFO: epoch:1   train step:480  lr: 0.100000, loss:  2.7501 top1:  0.4375 top5:  0.6289 batch_cost: 0.18711 s, reader_cost: 0.00066 s, ips: 1368.16971 samples/sec.
[2023/07/28 07:19:04] ppcls INFO: epoch:1   train step:490  lr: 0.100000, loss:  2.8523 top1:  0.3711 top5:  0.6289 batch_cost: 0.18712 s, reader_cost: 0.00066 s, ips: 1368.12564 samples/sec.
[2023/07/28 07:19:05] ppcls INFO: epoch:1   train step:500  lr: 0.100000, loss:  2.7816 top1:  0.3984 top5:  0.6328 batch_cost: 0.18711 s, reader_cost: 0.00066 s, ips: 1368.15592 samples/sec.
[2023/07/28 07:19:07] ppcls INFO: epoch:1   train step:510  lr: 0.100000, loss:  3.0062 top1:  0.3594 top5:  0.5859 batch_cost: 0.18713 s, reader_cost: 0.00066 s, ips: 1368.03777 samples/sec.
[2023/07/28 07:19:09] ppcls INFO: epoch:1   train step:520  lr: 0.100000, loss:  2.7722 top1:  0.3906 top5:  0.6445 batch_cost: 0.18714 s, reader_cost: 0.00066 s, ips: 1367.96977 samples/sec.
[2023/07/28 07:19:11] ppcls INFO: epoch:1   train step:530  lr: 0.100000, loss:  2.8305 top1:  0.3555 top5: -0.6172 batch_cost: 0.18714 s, reader_cost: 0.00066 s, ips: 1367.93668 samples/sec.
[2023/07/28 07:19:13] ppcls INFO: epoch:1   train step:540  lr: 0.100000, loss:  2.9260 top1:  0.4102 top5:  0.6172 batch_cost: 0.18714 s, reader_cost: 0.00066 s, ips: 1367.95798 samples/sec.
[2023/07/28 07:19:15] ppcls INFO: epoch:1   train step:550  lr: 0.100000, loss:  2.8102 top1:  0.3828 top5: -0.6367 batch_cost: 0.18715 s, reader_cost: 0.00065 s, ips: 1367.91519 samples/sec.
[2023/07/28 07:19:17] ppcls INFO: epoch:1   train step:560  lr: 0.100000, loss:  2.7987 top1:  0.4023 top5:  0.6875 batch_cost: 0.18714 s, reader_cost: 0.00065 s, ips: 1367.92499 samples/sec.
[2023/07/28 07:19:19] ppcls INFO: epoch:1   train step:570  lr: 0.100000, loss:  2.9193 top1:  0.3398 top5: -0.6445 batch_cost: 0.18716 s, reader_cost: 0.00065 s, ips: 1367.81147 samples/sec.
[2023/07/28 07:19:20] ppcls INFO: epoch:1   train step:580  lr: 0.100000, loss:  2.6835 top1:  0.4219 top5:  0.6719 batch_cost: 0.18716 s, reader_cost: 0.00065 s, ips: 1367.82166 samples/sec.
[2023/07/28 07:19:22] ppcls INFO: epoch:1   train step:590  lr: 0.100000, loss:  2.8757 top1:  0.3711 top5:  0.6367 batch_cost: 0.18717 s, reader_cost: 0.00065 s, ips: 1367.70869 samples/sec.
[2023/07/28 07:19:24] ppcls INFO: epoch:1   train step:600  lr: 0.100000, loss:  2.2506 top1:  0.5469 top5: -0.7344 batch_cost: 0.18718 s, reader_cost: 0.00065 s, ips: 1367.64777 samples/sec.
[2023/07/28 07:19:26] ppcls INFO: epoch:1   train step:610  lr: 0.100000, loss:  2.6072 top1:  0.3711 top5:  0.6602 batch_cost: 0.18719 s, reader_cost: 0.00065 s, ips: 1367.63076 samples/sec.
[2023/07/28 07:19:28] ppcls INFO: epoch:1   train step:620  lr: 0.100000, loss:  2.6730 top1:  0.4062 top5:  0.6836 batch_cost: 0.18719 s, reader_cost: 0.00065 s, ips: 1367.56546 samples/sec.
[2023/07/28 07:19:30] ppcls INFO: epoch:1   train step:630  lr: 0.100000, loss:  2.4630 top1:  0.4375 top5:  0.6797 batch_cost: 0.18720 s, reader_cost: 0.00064 s, ips: 1367.53812 samples/sec.
[2023/07/28 07:19:32] ppcls INFO: epoch:1   train step:640  lr: 0.100000, loss:  2.5420 top1:  0.4180 top5:  0.6836 batch_cost: 0.18720 s, reader_cost: 0.00064 s, ips: 1367.50999 samples/sec.
[2023/07/28 07:19:34] ppcls INFO: epoch:1   train step:650  lr: 0.100000, loss:  2.5062 top1:  0.4414 top5:  0.6797 batch_cost: 0.18722 s, reader_cost: 0.00064 s, ips: 1367.38944 samples/sec.
[2023/07/28 07:19:35] ppcls INFO: epoch:1   train step:660  lr: 0.100000, loss:  2.4226 top1:  0.4336 top5:  0.7070 batch_cost: 0.18722 s, reader_cost: 0.00064 s, ips: 1367.34171 samples/sec.
[2023/07/28 07:19:37] ppcls INFO: epoch:1   train step:670  lr: 0.100000, loss:  2.4109 top1:  0.4727 top5: -0.6953 batch_cost: 0.18723 s, reader_cost: 0.00064 s, ips: 1367.27418 samples/sec.
[2023/07/28 07:19:39] ppcls INFO: epoch:1   train step:680  lr: 0.100000, loss:  2.2866 top1:  0.4570 top5:  0.7305 batch_cost: 0.18724 s, reader_cost: 0.00064 s, ips: 1367.25950 samples/sec.
[2023/07/28 07:19:41] ppcls INFO: epoch:1   train step:690  lr: 0.100000, loss:  2.1970 top1:  0.4844 top5:  0.7656 batch_cost: 0.18724 s, reader_cost: 0.00064 s, ips: 1367.21522 samples/sec.
[2023/07/28 07:19:43] ppcls INFO: epoch:1   train step:700  lr: 0.100000, loss:  2.4692 top1:  0.4844 top5:  0.6875 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.11581 samples/sec.
[2023/07/28 07:19:45] ppcls INFO: epoch:1   train step:710  lr: 0.100000, loss:  2.4757 top1:  0.4609 top5: -0.6992 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.08737 samples/sec.
[2023/07/28 07:19:47] ppcls INFO: epoch:1   train step:720  lr: 0.100000, loss:  2.3725 top1:  0.4570 top5:  0.7031 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.06585 samples/sec.
[2023/07/28 07:19:49] ppcls INFO: epoch:1   train step:730  lr: 0.100000, loss:  2.3295 top1:  0.4609 top5:  0.7422 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.07409 samples/sec.
[2023/07/28 07:19:50] ppcls INFO: epoch:1   train step:740  lr: 0.100000, loss:  2.3390 top1:  0.5039 top5:  0.7266 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.05082 samples/sec.
[2023/07/28 07:19:52] ppcls INFO: epoch:1   train step:750  lr: 0.100000, loss:  2.4741 top1:  0.4141 top5: -0.7109 batch_cost: 0.18727 s, reader_cost: 0.00064 s, ips: 1367.01633 samples/sec.
[2023/07/28 07:19:54] ppcls INFO: epoch:1   train step:760  lr: 0.100000, loss:  2.5742 top1:  0.4570 top5: -0.6914 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.05977 samples/sec.
[2023/07/28 07:19:56] ppcls INFO: epoch:1   train step:770  lr: 0.100000, loss:  2.1770 top1:  0.5078 top5: -0.7383 batch_cost: 0.18726 s, reader_cost: 0.00064 s, ips: 1367.04717 samples/sec.
[2023/07/28 07:19:58] ppcls INFO: epoch:1   train step:780  lr: 0.100000, loss:  2.2273 top1:  0.5000 top5:  0.7227 batch_cost: 0.18728 s, reader_cost: 0.00064 s, ips: 1366.97345 samples/sec.
[2023/07/28 07:20:00] ppcls INFO: epoch:1   train step:790  lr: 0.100000, loss:  2.3179 top1:  0.5195 top5: -0.7148 batch_cost: 0.18728 s, reader_cost: 0.00064 s, ips: 1366.93408 samples/sec.
[2023/07/28 07:20:02] ppcls INFO: epoch:1   train step:800  lr: 0.100000, loss:  2.1299 top1:  0.5469 top5:  0.7773 batch_cost: 0.18729 s, reader_cost: 0.00064 s, ips: 1366.83882 samples/sec.
[2023/07/28 07:20:04] ppcls INFO: epoch:1   train step:810  lr: 0.100000, loss:  2.1984 top1:  0.4961 top5: -0.7578 batch_cost: 0.18729 s, reader_cost: 0.00065 s, ips: 1366.87467 samples/sec.
[2023/07/28 07:20:05] ppcls INFO: epoch:1   train step:820  lr: 0.100000, loss:  2.1443 top1:  0.4961 top5:  0.7344 batch_cost: 0.18729 s, reader_cost: 0.00065 s, ips: 1366.83395 samples/sec.
[2023/07/28 07:20:07] ppcls INFO: epoch:1   train step:830  lr: 0.100000, loss:  2.2233 top1:  0.5078 top5: -0.7656 batch_cost: 0.18730 s, reader_cost: 0.00065 s, ips: 1366.79478 samples/sec.
[2023/07/28 07:20:09] ppcls INFO: epoch:1   train step:840  lr: 0.100000, loss:  2.2332 top1:  0.4844 top5:  0.7461 batch_cost: 0.18731 s, reader_cost: 0.00065 s, ips: 1366.74008 samples/sec.
[2023/07/28 07:20:11] ppcls INFO: epoch:1   train step:850  lr: 0.100000, loss:  1.9228 top1:  0.5859 top5:  0.8125 batch_cost: 0.18731 s, reader_cost: 0.00065 s, ips: 1366.74709 samples/sec.
[2023/07/28 07:20:13] ppcls INFO: epoch:1   train step:860  lr: 0.100000, loss:  1.9975 top1:  0.5586 top5:  0.7773 batch_cost: 0.18731 s, reader_cost: 0.00065 s, ips: 1366.68781 samples/sec.
[2023/07/28 07:20:15] ppcls INFO: epoch:1   train step:870  lr: 0.100000, loss:  2.3922 top1:  0.4219 top5:  0.7227 batch_cost: 0.18731 s, reader_cost: 0.00065 s, ips: 1366.69057 samples/sec.
[2023/07/28 07:20:17] ppcls INFO: END epoch:1   train  loss:  3.0592 top1:  0.3302 top5:  0.1167 batch_cost: 0.18731 s, reader_cost: 0.00065 s, batch_cost_sum: 163.90025 s,
[2023/07/28 07:20:18] ppcls INFO: Already save model in ./output/ResNet50/1
[2023/07/28 07:20:19] ppcls INFO: epoch:2   train step:10   lr: 0.100000, loss:  2.2413 top1:  0.5000 top5:  0.7383 batch_cost: 0.18757 s, reader_cost: 0.00055 s, ips: 1364.84074 samples/sec.
[2023/07/28 07:20:21] ppcls INFO: epoch:2   train step:20   lr: 0.100000, loss:  1.9226 top1:  0.5859 top5:  0.7969 batch_cost: 0.18735 s, reader_cost: 0.00063 s, ips: 1366.44089 samples/sec.
[2023/07/28 07:20:23] ppcls INFO: epoch:2   train step:30   lr: 0.100000, loss:  1.7402 top1:  0.5742 top5:  0.8398 batch_cost: 0.18731 s, reader_cost: 0.00066 s, ips: 1366.69654 samples/sec.
[2023/07/28 07:20:25] ppcls INFO: epoch:2   train step:40   lr: 0.100000, loss:  1.9348 top1:  0.5742 top5:  0.8047 batch_cost: 0.18739 s, reader_cost: 0.00063 s, ips: 1366.11131 samples/sec.
[2023/07/28 07:20:27] ppcls INFO: epoch:2   train step:50   lr: 0.100000, loss:  1.8970 top1:  0.5977 top5: -0.7773 batch_cost: 0.18743 s, reader_cost: 0.00062 s, ips: 1365.86891 samples/sec.
[2023/07/28 07:20:29] ppcls INFO: epoch:2   train step:60   lr: 0.100000, loss:  1.7382 top1:  0.5781 top5: -0.8438 batch_cost: 0.18757 s, reader_cost: 0.00060 s, ips: 1364.79854 samples/sec.
[2023/07/28 07:20:31] ppcls INFO: epoch:2   train step:70   lr: 0.100000, loss:  1.9026 top1:  0.5703 top5:  0.7891 batch_cost: 0.18759 s, reader_cost: 0.00060 s, ips: 1364.68271 samples/sec.
[2023/07/28 07:20:33] ppcls INFO: epoch:2   train step:80   lr: 0.100000, loss:  1.9786 top1:  0.5430 top5:  0.8008 batch_cost: 0.18753 s, reader_cost: 0.00060 s, ips: 1365.13956 samples/sec.
[2023/07/28 07:20:34] ppcls INFO: epoch:2   train step:90   lr: 0.100000, loss:  1.9763 top1:  0.5664 top5: -0.7812 batch_cost: 0.18758 s, reader_cost: 0.00060 s, ips: 1364.74063 samples/sec.
[2023/07/28 07:20:36] ppcls INFO: epoch:2   train step:100  lr: 0.100000, loss:  1.8657 top1:  0.5625 top5: -0.8008 batch_cost: 0.18759 s, reader_cost: 0.00059 s, ips: 1364.66641 samples/sec.
[2023/07/28 07:20:38] ppcls INFO: epoch:2   train step:110  lr: 0.100000, loss:  1.8548 top1:  0.5938 top5: -0.8320 batch_cost: 0.18756 s, reader_cost: 0.00059 s, ips: 1364.88689 samples/sec.
[2023/07/28 07:20:40] ppcls INFO: epoch:2   train step:120  lr: 0.100000, loss:  1.8575 top1:  0.5586 top5:  0.7969 batch_cost: 0.18760 s, reader_cost: 0.00059 s, ips: 1364.61200 samples/sec.
[2023/07/28 07:20:42] ppcls INFO: epoch:2   train step:130  lr: 0.100000, loss:  1.8569 top1:  0.6055 top5: -0.8008 batch_cost: 0.18762 s, reader_cost: 0.00059 s, ips: 1364.48982 samples/sec.
[2023/07/28 07:20:44] ppcls INFO: epoch:2   train step:140  lr: 0.100000, loss:  1.8002 top1:  0.5625 top5: -0.7969 batch_cost: 0.18759 s, reader_cost: 0.00059 s, ips: 1364.65592 samples/sec.
[2023/07/28 07:20:46] ppcls INFO: epoch:2   train step:150  lr: 0.100000, loss:  1.8072 top1:  0.5781 top5:  0.8047 batch_cost: 0.18759 s, reader_cost: 0.00059 s, ips: 1364.64940 samples/sec.
[2023/07/28 07:20:48] ppcls INFO: epoch:2   train step:160  lr: 0.100000, loss:  1.7017 top1:  0.5820 top5:  0.8164 batch_cost: 0.18762 s, reader_cost: 0.00059 s, ips: 1364.42822 samples/sec.
[2023/07/28 07:20:49] ppcls INFO: epoch:2   train step:170  lr: 0.100000, loss:  1.6926 top1:  0.6289 top5:  0.8320 batch_cost: 0.18760 s, reader_cost: 0.00060 s, ips: 1364.64089 samples/sec.
[2023/07/28 07:20:51] ppcls INFO: epoch:2   train step:180  lr: 0.100000, loss:  1.8509 top1:  0.5703 top5:  0.7812 batch_cost: 0.18758 s, reader_cost: 0.00060 s, ips: 1364.77533 samples/sec.
[2023/07/28 07:20:53] ppcls INFO: epoch:2   train step:190  lr: 0.100000, loss:  1.4977 top1:  0.6641 top5: -0.8516 batch_cost: 0.18754 s, reader_cost: 0.00060 s, ips: 1365.01031 samples/sec.
[2023/07/28 07:20:55] ppcls INFO: epoch:2   train step:200  lr: 0.100000, loss:  1.8209 top1:  0.6016 top5:  0.7891 batch_cost: 0.18752 s, reader_cost: 0.00060 s, ips: 1365.18277 samples/sec.
[2023/07/28 07:20:57] ppcls INFO: epoch:2   train step:210  lr: 0.100000, loss:  1.5491 top1:  0.6523 top5:  0.8477 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.28236 samples/sec.
[2023/07/28 07:20:59] ppcls INFO: epoch:2   train step:220  lr: 0.100000, loss:  1.5670 top1:  0.6055 top5:  0.8516 batch_cost: 0.18750 s, reader_cost: 0.00060 s, ips: 1365.34750 samples/sec.
[2023/07/28 07:21:01] ppcls INFO: epoch:2   train step:230  lr: 0.100000, loss:  1.4365 top1:  0.6680 top5:  0.8594 batch_cost: 0.18749 s, reader_cost: 0.00060 s, ips: 1365.38623 samples/sec.
[2023/07/28 07:21:03] ppcls INFO: epoch:2   train step:240  lr: 0.100000, loss:  1.6164 top1:  0.6367 top5:  0.8477 batch_cost: 0.18748 s, reader_cost: 0.00060 s, ips: 1365.50408 samples/sec.
[2023/07/28 07:21:04] ppcls INFO: epoch:2   train step:250  lr: 0.100000, loss:  1.5779 top1:  0.6211 top5:  0.8633 batch_cost: 0.18749 s, reader_cost: 0.00060 s, ips: 1365.41571 samples/sec.
[2023/07/28 07:21:06] ppcls INFO: epoch:2   train step:260  lr: 0.100000, loss:  1.5086 top1:  0.6562 top5: -0.8555 batch_cost: 0.18749 s, reader_cost: 0.00060 s, ips: 1365.42225 samples/sec.
[2023/07/28 07:21:08] ppcls INFO: epoch:2   train step:270  lr: 0.100000, loss:  1.6029 top1:  0.6328 top5:  0.8164 batch_cost: 0.18749 s, reader_cost: 0.00060 s, ips: 1365.40971 samples/sec.
[2023/07/28 07:21:10] ppcls INFO: epoch:2   train step:280  lr: 0.100000, loss:  1.6139 top1:  0.6641 top5:  0.8477 batch_cost: 0.18750 s, reader_cost: 0.00060 s, ips: 1365.31761 samples/sec.
[2023/07/28 07:21:12] ppcls INFO: epoch:2   train step:290  lr: 0.100000, loss:  1.2946 top1:  0.7148 top5:  0.8945 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.26505 samples/sec.
[2023/07/28 07:21:14] ppcls INFO: epoch:2   train step:300  lr: 0.100000, loss:  1.5722 top1:  0.6484 top5:  0.8555 batch_cost: 0.18752 s, reader_cost: 0.00060 s, ips: 1365.19454 samples/sec.
[2023/07/28 07:21:16] ppcls INFO: epoch:2   train step:310  lr: 0.100000, loss:  1.5872 top1:  0.6562 top5:  0.8477 batch_cost: 0.18753 s, reader_cost: 0.00060 s, ips: 1365.14539 samples/sec.
[2023/07/28 07:21:18] ppcls INFO: epoch:2   train step:320  lr: 0.100000, loss:  1.3734 top1:  0.6719 top5:  0.8750 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.24073 samples/sec.
[2023/07/28 07:21:19] ppcls INFO: epoch:2   train step:330  lr: 0.100000, loss:  1.5386 top1:  0.6562 top5:  0.8594 batch_cost: 0.18752 s, reader_cost: 0.00060 s, ips: 1365.19823 samples/sec.
[2023/07/28 07:21:21] ppcls INFO: epoch:2   train step:340  lr: 0.100000, loss:  1.2704 top1:  0.7070 top5: -0.8984 batch_cost: 0.18750 s, reader_cost: 0.00060 s, ips: 1365.30079 samples/sec.
[2023/07/28 07:21:23] ppcls INFO: epoch:2   train step:350  lr: 0.100000, loss:  1.5354 top1:  0.6602 top5:  0.8594 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.27406 samples/sec.
[2023/07/28 07:21:25] ppcls INFO: epoch:2   train step:360  lr: 0.100000, loss:  1.5996 top1:  0.6172 top5:  0.8398 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.28045 samples/sec.
[2023/07/28 07:21:27] ppcls INFO: epoch:2   train step:370  lr: 0.100000, loss:  1.2922 top1:  0.7305 top5:  0.8945 batch_cost: 0.18752 s, reader_cost: 0.00060 s, ips: 1365.20947 samples/sec.
[2023/07/28 07:21:29] ppcls INFO: epoch:2   train step:380  lr: 0.100000, loss:  1.2924 top1:  0.6914 top5: -0.8984 batch_cost: 0.18751 s, reader_cost: 0.00060 s, ips: 1365.22828 samples/sec.
[2023/07/28 07:21:31] ppcls INFO: epoch:2   train step:390  lr: 0.100000, loss:  1.3943 top1:  0.6602 top5: -0.8828 batch_cost: 0.18754 s, reader_cost: 0.00060 s, ips: 1365.04262 samples/sec.
[2023/07/28 07:21:33] ppcls INFO: epoch:2   train step:400  lr: 0.100000, loss:  1.2648 top1:  0.7031 top5: -0.8984 batch_cost: 0.18753 s, reader_cost: 0.00060 s, ips: 1365.15066 samples/sec.
[2023/07/28 07:21:34] ppcls INFO: epoch:2   train step:410  lr: 0.100000, loss:  1.2483 top1:  0.7344 top5:  0.8867 batch_cost: 0.18754 s, reader_cost: 0.00060 s, ips: 1365.07083 samples/sec.
[2023/07/28 07:21:36] ppcls INFO: epoch:2   train step:420  lr: 0.100000, loss:  1.4326 top1:  0.6836 top5: -0.8867 batch_cost: 0.18753 s, reader_cost: 0.00060 s, ips: 1365.09649 samples/sec.
[2023/07/28 07:21:38] ppcls INFO: epoch:2   train step:430  lr: 0.100000, loss:  1.2985 top1:  0.7188 top5: -0.8906 batch_cost: 0.18754 s, reader_cost: 0.00060 s, ips: 1365.02787 samples/sec.
[2023/07/28 07:21:40] ppcls INFO: epoch:2   train step:440  lr: 0.100000, loss:  1.3578 top1:  0.7109 top5:  0.8789 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.95388 samples/sec.
[2023/07/28 07:21:42] ppcls INFO: epoch:2   train step:450  lr: 0.100000, loss:  1.4789 top1:  0.6328 top5: -0.8633 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94265 samples/sec.
[2023/07/28 07:21:44] ppcls INFO: epoch:2   train step:460  lr: 0.100000, loss:  1.2997 top1:  0.6914 top5: -0.8789 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.86512 samples/sec.
[2023/07/28 07:21:46] ppcls INFO: epoch:2   train step:470  lr: 0.100000, loss:  1.3928 top1:  0.6914 top5:  0.8555 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.91221 samples/sec.
[2023/07/28 07:21:48] ppcls INFO: epoch:2   train step:480  lr: 0.100000, loss:  1.3200 top1:  0.7070 top5: -0.8672 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94848 samples/sec.
[2023/07/28 07:21:49] ppcls INFO: epoch:2   train step:490  lr: 0.100000, loss:  1.2181 top1:  0.7070 top5:  0.9102 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.89311 samples/sec.
[2023/07/28 07:21:51] ppcls INFO: epoch:2   train step:500  lr: 0.100000, loss:  1.1427 top1:  0.7734 top5:  0.8906 batch_cost: 0.18757 s, reader_cost: 0.00060 s, ips: 1364.81913 samples/sec.
[2023/07/28 07:21:53] ppcls INFO: epoch:2   train step:510  lr: 0.100000, loss:  1.2827 top1:  0.6953 top5:  0.8828 batch_cost: 0.18757 s, reader_cost: 0.00060 s, ips: 1364.84411 samples/sec.
[2023/07/28 07:21:55] ppcls INFO: epoch:2   train step:520  lr: 0.100000, loss:  1.4839 top1:  0.6875 top5: -0.8516 batch_cost: 0.18757 s, reader_cost: 0.00060 s, ips: 1364.84993 samples/sec.
[2023/07/28 07:21:57] ppcls INFO: epoch:2   train step:530  lr: 0.100000, loss:  1.2703 top1:  0.7422 top5: -0.8945 batch_cost: 0.18757 s, reader_cost: 0.00060 s, ips: 1364.84226 samples/sec.
[2023/07/28 07:21:59] ppcls INFO: epoch:2   train step:540  lr: 0.100000, loss:  1.1324 top1:  0.7383 top5: -0.9141 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.91169 samples/sec.
[2023/07/28 07:22:01] ppcls INFO: epoch:2   train step:550  lr: 0.100000, loss:  1.2389 top1:  0.7148 top5: -0.8867 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92743 samples/sec.
[2023/07/28 07:22:03] ppcls INFO: epoch:2   train step:560  lr: 0.100000, loss:  1.0345 top1:  0.7852 top5: -0.9141 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94279 samples/sec.
[2023/07/28 07:22:04] ppcls INFO: epoch:2   train step:570  lr: 0.100000, loss:  1.1506 top1:  0.7188 top5: -0.8789 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.90259 samples/sec.
[2023/07/28 07:22:06] ppcls INFO: epoch:2   train step:580  lr: 0.100000, loss:  1.1546 top1:  0.7383 top5: -0.8789 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.88132 samples/sec.
[2023/07/28 07:22:08] ppcls INFO: epoch:2   train step:590  lr: 0.100000, loss:  1.3842 top1:  0.6797 top5:  0.8633 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.91157 samples/sec.
[2023/07/28 07:22:10] ppcls INFO: epoch:2   train step:600  lr: 0.100000, loss:  0.9147 top1:  0.7930 top5:  0.9492 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.89364 samples/sec.
[2023/07/28 07:22:12] ppcls INFO: epoch:2   train step:610  lr: 0.100000, loss:  0.9982 top1:  0.7891 top5:  0.9141 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.91419 samples/sec.
[2023/07/28 07:22:14] ppcls INFO: epoch:2   train step:620  lr: 0.100000, loss:  1.2203 top1:  0.7344 top5: -0.8867 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.95647 samples/sec.
[2023/07/28 07:22:16] ppcls INFO: epoch:2   train step:630  lr: 0.100000, loss:  1.1183 top1:  0.7344 top5:  0.9141 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92338 samples/sec.
[2023/07/28 07:22:18] ppcls INFO: epoch:2   train step:640  lr: 0.100000, loss:  1.0830 top1:  0.7617 top5:  0.9023 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94427 samples/sec.
[2023/07/28 07:22:19] ppcls INFO: epoch:2   train step:650  lr: 0.100000, loss:  1.1332 top1:  0.7383 top5:  0.9062 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.89264 samples/sec.
[2023/07/28 07:22:21] ppcls INFO: epoch:2   train step:660  lr: 0.100000, loss:  0.9817 top1:  0.7734 top5: -0.9102 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.93781 samples/sec.
[2023/07/28 07:22:23] ppcls INFO: epoch:2   train step:670  lr: 0.100000, loss:  1.0892 top1:  0.7773 top5: -0.9102 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.93750 samples/sec.
[2023/07/28 07:22:25] ppcls INFO: epoch:2   train step:680  lr: 0.100000, loss:  1.2211 top1:  0.7266 top5:  0.8867 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.89228 samples/sec.
[2023/07/28 07:22:27] ppcls INFO: epoch:2   train step:690  lr: 0.100000, loss:  0.8756 top1:  0.8164 top5:  0.9180 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.91738 samples/sec.
[2023/07/28 07:22:29] ppcls INFO: epoch:2   train step:700  lr: 0.100000, loss:  1.0837 top1:  0.7617 top5: -0.9102 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92177 samples/sec.
[2023/07/28 07:22:31] ppcls INFO: epoch:2   train step:710  lr: 0.100000, loss:  1.0218 top1:  0.7930 top5: -0.9023 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94592 samples/sec.
[2023/07/28 07:22:33] ppcls INFO: epoch:2   train step:720  lr: 0.100000, loss:  1.1470 top1:  0.7383 top5:  0.9102 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94407 samples/sec.
[2023/07/28 07:22:35] ppcls INFO: epoch:2   train step:730  lr: 0.100000, loss:  1.1750 top1:  0.7383 top5:  0.9023 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.93420 samples/sec.
[2023/07/28 07:22:36] ppcls INFO: epoch:2   train step:740  lr: 0.100000, loss:  1.2318 top1:  0.7266 top5:  0.8750 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92407 samples/sec.
[2023/07/28 07:22:38] ppcls INFO: epoch:2   train step:750  lr: 0.100000, loss:  1.0760 top1:  0.7578 top5:  0.8984 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92915 samples/sec.
[2023/07/28 07:22:40] ppcls INFO: epoch:2   train step:760  lr: 0.100000, loss:  0.9275 top1:  0.8086 top5: -0.9297 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.99894 samples/sec.
[2023/07/28 07:22:42] ppcls INFO: epoch:2   train step:770  lr: 0.100000, loss:  0.9880 top1:  0.8047 top5: -0.9023 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.98606 samples/sec.
[2023/07/28 07:22:44] ppcls INFO: epoch:2   train step:780  lr: 0.100000, loss:  0.9871 top1:  0.7695 top5:  0.9219 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94381 samples/sec.
[2023/07/28 07:22:46] ppcls INFO: epoch:2   train step:790  lr: 0.100000, loss:  1.0478 top1:  0.7852 top5:  0.9023 batch_cost: 0.18755 s, reader_cost: 0.00060 s, ips: 1364.94481 samples/sec.
[2023/07/28 07:22:48] ppcls INFO: epoch:2   train step:800  lr: 0.100000, loss:  0.9945 top1:  0.8047 top5:  0.9102 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92328 samples/sec.
[2023/07/28 07:22:50] ppcls INFO: epoch:2   train step:810  lr: 0.100000, loss:  1.2291 top1:  0.7305 top5:  0.9062 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.86729 samples/sec.
[2023/07/28 07:22:51] ppcls INFO: epoch:2   train step:820  lr: 0.100000, loss:  1.1325 top1:  0.7617 top5:  0.8906 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.86278 samples/sec.
[2023/07/28 07:22:53] ppcls INFO: epoch:2   train step:830  lr: 0.100000, loss:  1.0547 top1:  0.7539 top5:  0.9102 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.88906 samples/sec.
[2023/07/28 07:22:55] ppcls INFO: epoch:2   train step:840  lr: 0.100000, loss:  1.0511 top1:  0.7656 top5:  0.9141 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.88118 samples/sec.
[2023/07/28 07:22:57] ppcls INFO: epoch:2   train step:850  lr: 0.100000, loss:  1.0476 top1:  0.7773 top5: -0.9258 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.89063 samples/sec.
[2023/07/28 07:22:59] ppcls INFO: epoch:2   train step:860  lr: 0.100000, loss:  0.9642 top1:  0.7812 top5:  0.9375 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.92401 samples/sec.
[2023/07/28 07:23:01] ppcls INFO: epoch:2   train step:870  lr: 0.100000, loss:  0.8947 top1:  0.7695 top5:  0.9297 batch_cost: 0.18756 s, reader_cost: 0.00060 s, ips: 1364.93178 samples/sec.
[2023/07/28 07:23:02] ppcls INFO: END epoch:2   train  loss:  1.3996 top1:  0.6733 top5:  0.1939 batch_cost: 0.18755 s, reader_cost: 0.00060 s, batch_cost_sum: 164.10976 s,
[2023/07/28 07:23:04] ppcls INFO: Already save model in ./output/ResNet50/2
[2023/07/28 07:23:05] ppcls INFO: epoch:3   train step:10   lr: 0.100000, loss:  1.0630 top1:  0.7656 top5:  0.9141 batch_cost: 0.18726 s, reader_cost: 0.00069 s, ips: 1367.08500 samples/sec.
[2023/07/28 07:23:07] ppcls INFO: epoch:3   train step:20   lr: 0.100000, loss:  0.8148 top1:  0.8008 top5:  0.9453 batch_cost: 0.18706 s, reader_cost: 0.00061 s, ips: 1368.53418 samples/sec.
[2023/07/28 07:23:09] ppcls INFO: epoch:3   train step:30   lr: 0.100000, loss:  0.8742 top1:  0.7500 top5:  0.9453 batch_cost: 0.18710 s, reader_cost: 0.00060 s, ips: 1368.24779 samples/sec.
[2023/07/28 07:23:11] ppcls INFO: epoch:3   train step:40   lr: 0.100000, loss:  0.9561 top1:  0.7852 top5:  0.9297 batch_cost: 0.18703 s, reader_cost: 0.00063 s, ips: 1368.72969 samples/sec.
[2023/07/28 07:23:13] ppcls INFO: epoch:3   train step:50   lr: 0.100000, loss:  0.9155 top1:  0.8242 top5: -0.9219 batch_cost: 0.18724 s, reader_cost: 0.00087 s, ips: 1367.23619 samples/sec.
[2023/07/28 07:23:15] ppcls INFO: epoch:3   train step:60   lr: 0.100000, loss:  0.8011 top1:  0.8398 top5:  0.9531 batch_cost: 0.18730 s, reader_cost: 0.00081 s, ips: 1366.81364 samples/sec.
[2023/07/28 07:23:17] ppcls INFO: epoch:3   train step:70   lr: 0.100000, loss:  0.8407 top1:  0.8320 top5: -0.9492 batch_cost: 0.18732 s, reader_cost: 0.00078 s, ips: 1366.63186 samples/sec.
[2023/07/28 07:23:18] ppcls INFO: epoch:3   train step:80   lr: 0.100000, loss:  0.8222 top1:  0.8164 top5: -0.9414 batch_cost: 0.18730 s, reader_cost: 0.00076 s, ips: 1366.77956 samples/sec.
[2023/07/28 07:23:20] ppcls INFO: epoch:3   train step:90   lr: 0.100000, loss:  0.8960 top1:  0.8086 top5:  0.9141 batch_cost: 0.18730 s, reader_cost: 0.00074 s, ips: 1366.81607 samples/sec.
[2023/07/28 07:23:22] ppcls INFO: epoch:3   train step:100  lr: 0.100000, loss:  0.9767 top1:  0.7969 top5:  0.9180 batch_cost: 0.18742 s, reader_cost: 0.00073 s, ips: 1365.95058 samples/sec.
[2023/07/28 07:23:24] ppcls INFO: epoch:3   train step:110  lr: 0.100000, loss:  0.9446 top1:  0.8008 top5:  0.9102 batch_cost: 0.18736 s, reader_cost: 0.00072 s, ips: 1366.37128 samples/sec.
[2023/07/28 07:23:26] ppcls INFO: epoch:3   train step:120  lr: 0.100000, loss:  0.9300 top1:  0.8086 top5: -0.9180 batch_cost: 0.18736 s, reader_cost: 0.00071 s, ips: 1366.36895 samples/sec.
[2023/07/28 07:23:28] ppcls INFO: epoch:3   train step:130  lr: 0.100000, loss:  0.8890 top1:  0.8047 top5:  0.9258 batch_cost: 0.18740 s, reader_cost: 0.00070 s, ips: 1366.08376 samples/sec.
[2023/07/28 07:23:30] ppcls INFO: epoch:3   train step:140  lr: 0.100000, loss:  0.9095 top1:  0.7891 top5: -0.9375 batch_cost: 0.18740 s, reader_cost: 0.00069 s, ips: 1366.09307 samples/sec.
[2023/07/28 07:23:32] ppcls INFO: epoch:3   train step:150  lr: 0.100000, loss:  0.8517 top1:  0.8125 top5:  0.9297 batch_cost: 0.18741 s, reader_cost: 0.00069 s, ips: 1366.02327 samples/sec.
LAUNCH INFO 2023-07-28 07:24:17,571 Terminating with signal 15
LAUNCH INFO 2023-07-28 07:24:23,308 Exit with signal 15
[2023/07/28 07:23:33] ppcls INFO: epoch:3   train step:160  lr: 0.100000, loss:  0.8502 top1:  0.8086 top5:  0.9375 batch_cost: 0.18747 s, reader_cost: 0.00068 s, ips: 1365.57126 samples/sec.
[2023/07/28 07:23:35] ppcls INFO: epoch:3   train step:170  lr: 0.100000, loss:  0.7036 top1:  0.8203 top5:  0.9453 batch_cost: 0.18741 s, reader_cost: 0.00068 s, ips: 1365.99452 samples/sec.
[2023/07/28 07:23:37] ppcls INFO: epoch:3   train step:180  lr: 0.100000, loss:  0.9833 top1:  0.7734 top5:  0.9023 batch_cost: 0.18740 s, reader_cost: 0.00067 s, ips: 1366.06474 samples/sec.
[2023/07/28 07:23:39] ppcls INFO: epoch:3   train step:190  lr: 0.100000, loss:  0.9806 top1: -0.7930 top5:  0.9180 batch_cost: 0.18741 s, reader_cost: 0.00067 s, ips: 1365.98428 samples/sec.
[2023/07/28 07:23:41] ppcls INFO: epoch:3   train step:200  lr: 0.100000, loss:  0.6875 top1:  0.8516 top5: -0.9531 batch_cost: 0.18744 s, reader_cost: 0.00067 s, ips: 1365.76184 samples/sec.
[2023/07/28 07:23:43] ppcls INFO: epoch:3   train step:210  lr: 0.100000, loss:  0.8599 top1:  0.7969 top5:  0.9297 batch_cost: 0.18744 s, reader_cost: 0.00068 s, ips: 1365.74227 samples/sec.
[2023/07/28 07:23:45] ppcls INFO: epoch:3   train step:220  lr: 0.100000, loss:  0.8266 top1:  0.8203 top5:  0.9297 batch_cost: 0.18745 s, reader_cost: 0.00068 s, ips: 1365.67569 samples/sec.
[2023/07/28 07:23:47] ppcls INFO: epoch:3   train step:230  lr: 0.100000, loss:  0.8364 top1:  0.8164 top5: -0.9297 batch_cost: 0.18746 s, reader_cost: 0.00067 s, ips: 1365.65248 samples/sec.
[2023/07/28 07:23:48] ppcls INFO: epoch:3   train step:240  lr: 0.100000, loss:  0.7240 top1:  0.8359 top5:  0.9570 batch_cost: 0.18744 s, reader_cost: 0.00067 s, ips: 1365.73558 samples/sec.
[2023/07/28 07:23:50] ppcls INFO: epoch:3   train step:250  lr: 0.100000, loss:  0.9607 top1:  0.7891 top5:  0.9219 batch_cost: 0.18747 s, reader_cost: 0.00067 s, ips: 1365.55936 samples/sec.
[2023/07/28 07:23:52] ppcls INFO: epoch:3   train step:260  lr: 0.100000, loss:  0.8505 top1:  0.8086 top5: -0.9258 batch_cost: 0.18745 s, reader_cost: 0.00067 s, ips: 1365.71342 samples/sec.
[2023/07/28 07:23:54] ppcls INFO: epoch:3   train step:270  lr: 0.100000, loss:  0.7073 top1:  0.8594 top5:  0.9609 batch_cost: 0.18746 s, reader_cost: 0.00066 s, ips: 1365.63073 samples/sec.
[2023/07/28 07:23:56] ppcls INFO: epoch:3   train step:280  lr: 0.100000, loss:  0.6391 top1:  0.8750 top5:  0.9414 batch_cost: 0.18746 s, reader_cost: 0.00066 s, ips: 1365.63203 samples/sec.
[2023/07/28 07:23:58] ppcls INFO: epoch:3   train step:290  lr: 0.100000, loss:  0.8146 top1:  0.8203 top5:  0.9375 batch_cost: 0.18746 s, reader_cost: 0.00066 s, ips: 1365.64256 samples/sec.
[2023/07/28 07:24:00] ppcls INFO: epoch:3   train step:300  lr: 0.100000, loss:  0.7333 top1:  0.8438 top5:  0.9375 batch_cost: 0.18745 s, reader_cost: 0.00066 s, ips: 1365.68285 samples/sec.
[2023/07/28 07:24:02] ppcls INFO: epoch:3   train step:310  lr: 0.100000, loss:  0.8116 top1:  0.8047 top5:  0.9297 batch_cost: 0.18747 s, reader_cost: 0.00066 s, ips: 1365.53599 samples/sec.
[2023/07/28 07:24:03] ppcls INFO: epoch:3   train step:320  lr: 0.100000, loss:  0.7206 top1:  0.8477 top5:  0.9219 batch_cost: 0.18746 s, reader_cost: 0.00066 s, ips: 1365.62909 samples/sec.
[2023/07/28 07:24:05] ppcls INFO: epoch:3   train step:330  lr: 0.100000, loss:  0.7974 top1:  0.8164 top5: -0.9414 batch_cost: 0.18747 s, reader_cost: 0.00066 s, ips: 1365.57414 samples/sec.
[2023/07/28 07:24:07] ppcls INFO: epoch:3   train step:340  lr: 0.100000, loss:  0.8480 top1:  0.8125 top5:  0.9180 batch_cost: 0.18745 s, reader_cost: 0.00065 s, ips: 1365.67464 samples/sec.
[2023/07/28 07:24:09] ppcls INFO: epoch:3   train step:350  lr: 0.100000, loss:  0.7402 top1:  0.8359 top5:  0.9336 batch_cost: 0.18746 s, reader_cost: 0.00065 s, ips: 1365.65739 samples/sec.
[2023/07/28 07:24:11] ppcls INFO: epoch:3   train step:360  lr: 0.100000, loss:  0.8071 top1:  0.8008 top5: -0.9375 batch_cost: 0.18744 s, reader_cost: 0.00065 s, ips: 1365.75645 samples/sec.
[2023/07/28 07:24:13] ppcls INFO: epoch:3   train step:370  lr: 0.100000, loss:  0.5329 top1:  0.8906 top5: -0.9531 batch_cost: 0.18744 s, reader_cost: 0.00065 s, ips: 1365.80568 samples/sec.
[2023/07/28 07:24:15] ppcls INFO: epoch:3   train step:380  lr: 0.100000, loss:  0.7559 top1:  0.8281 top5:  0.9414 batch_cost: 0.18745 s, reader_cost: 0.00065 s, ips: 1365.71041 samples/sec.
