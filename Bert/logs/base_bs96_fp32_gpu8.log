/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _nlv = LooseVersion(_np_version)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p16 = _nlv < LooseVersion("1.16")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p17 = _nlv < LooseVersion("1.17")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p18 = _nlv < LooseVersion("1.18")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p19 = _nlv < LooseVersion("1.19")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p20 = _nlv < LooseVersion("1.20")
/usr/local/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/function.py:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(_np_version) >= LooseVersion("1.17.0"):
Namespace(adam_epsilon=1e-08, batch_size=96, device='gpu', enable_addto=False, gradient_merge_steps=88, input_dir='./data/wikicorpus_en_seqlen128', learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_predictions_per_seq=80, max_steps=500, model_name_or_path='bert-base-uncased', model_type='bert', output_dir='./tmp2/', profiler_options=None, save_steps=20000, scale_loss=32768, seed=42, use_amp=0, use_pure_fp16=False, warmup_steps=0, weight_decay=0.0)
[32m[2022-06-08 16:00:18,900] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
W0608 16:00:20.832890  2181 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 11.2
W0608 16:00:20.837560  2181 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.
W0608 16:00:32.273465  2181 build_strategy.cc:123] Currently, fuse_broadcast_ops only works under Reduce mode.
tobal step: 10, epoch: 0, batch: 9, loss: 11.246871, avg_reader_cost: 0.06827 sec, avg_batch_cost: 0.66179 sec, avg_samples: 96.00000, ips: 131.49696 sequences/sec
tobal step: 20, epoch: 0, batch: 19, loss: 11.161523, avg_reader_cost: 0.00022 sec, avg_batch_cost: 0.58407 sec, avg_samples: 96.00000, ips: 164.30026 sequences/sec
tobal step: 30, epoch: 0, batch: 29, loss: 11.215019, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.58654 sec, avg_samples: 96.00000, ips: 163.63722 sequences/sec
tobal step: 40, epoch: 0, batch: 39, loss: 11.188222, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58856 sec, avg_samples: 96.00000, ips: 163.07749 sequences/sec
tobal step: 50, epoch: 0, batch: 49, loss: 11.259479, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.58843 sec, avg_samples: 96.00000, ips: 163.11639 sequences/sec
tobal step: 60, epoch: 0, batch: 59, loss: 11.194427, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.58916 sec, avg_samples: 96.00000, ips: 162.91327 sequences/sec
tobal step: 70, epoch: 0, batch: 69, loss: 11.239295, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.58921 sec, avg_samples: 96.00000, ips: 162.89759 sequences/sec
tobal step: 80, epoch: 0, batch: 79, loss: 11.227909, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58899 sec, avg_samples: 96.00000, ips: 162.95975 sequences/sec
tobal step: 90, epoch: 0, batch: 89, loss: 11.294785, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59108 sec, avg_samples: 96.00000, ips: 162.38240 sequences/sec
tobal step: 100, epoch: 0, batch: 99, loss: 11.143709, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59017 sec, avg_samples: 96.00000, ips: 162.63027 sequences/sec
tobal step: 110, epoch: 0, batch: 109, loss: 11.147650, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58937 sec, avg_samples: 96.00000, ips: 162.85236 sequences/sec
tobal step: 120, epoch: 0, batch: 119, loss: 11.211628, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59127 sec, avg_samples: 96.00000, ips: 162.32911 sequences/sec
tobal step: 130, epoch: 0, batch: 129, loss: 11.167460, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59087 sec, avg_samples: 96.00000, ips: 162.44240 sequences/sec
tobal step: 140, epoch: 0, batch: 139, loss: 11.271014, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59030 sec, avg_samples: 96.00000, ips: 162.59717 sequences/sec
tobal step: 150, epoch: 0, batch: 149, loss: 11.100046, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59277 sec, avg_samples: 96.00000, ips: 161.92043 sequences/sec
tobal step: 160, epoch: 0, batch: 159, loss: 11.170861, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59248 sec, avg_samples: 96.00000, ips: 162.00104 sequences/sec
tobal step: 170, epoch: 0, batch: 169, loss: 11.119526, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59181 sec, avg_samples: 96.00000, ips: 162.18294 sequences/sec
tobal step: 180, epoch: 0, batch: 179, loss: 10.507096, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59288 sec, avg_samples: 96.00000, ips: 161.89246 sequences/sec
tobal step: 190, epoch: 0, batch: 189, loss: 10.604044, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59219 sec, avg_samples: 96.00000, ips: 162.07711 sequences/sec
tobal step: 200, epoch: 0, batch: 199, loss: 10.597165, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59340 sec, avg_samples: 96.00000, ips: 161.74581 sequences/sec
tobal step: 210, epoch: 0, batch: 209, loss: 10.530011, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59404 sec, avg_samples: 96.00000, ips: 161.57644 sequences/sec
tobal step: 220, epoch: 0, batch: 219, loss: 10.481635, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59288 sec, avg_samples: 96.00000, ips: 161.88864 sequences/sec
tobal step: 230, epoch: 0, batch: 229, loss: 10.461276, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59378 sec, avg_samples: 96.00000, ips: 161.64451 sequences/sec
tobal step: 240, epoch: 0, batch: 239, loss: 10.512783, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59473 sec, avg_samples: 96.00000, ips: 161.38826 sequences/sec
tobal step: 250, epoch: 0, batch: 249, loss: 10.553946, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.59394 sec, avg_samples: 96.00000, ips: 161.60487 sequences/sec
tobal step: 260, epoch: 0, batch: 259, loss: 10.658183, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.59546 sec, avg_samples: 96.00000, ips: 161.18175 sequences/sec
tobal step: 270, epoch: 0, batch: 269, loss: 10.611153, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59620 sec, avg_samples: 96.00000, ips: 160.98533 sequences/sec
tobal step: 280, epoch: 0, batch: 279, loss: 10.583233, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59553 sec, avg_samples: 96.00000, ips: 161.16566 sequences/sec
tobal step: 290, epoch: 0, batch: 289, loss: 10.614481, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59641 sec, avg_samples: 96.00000, ips: 160.93318 sequences/sec
tobal step: 300, epoch: 0, batch: 299, loss: 10.543908, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59637 sec, avg_samples: 96.00000, ips: 160.94149 sequences/sec
tobal step: 310, epoch: 0, batch: 309, loss: 10.437071, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59635 sec, avg_samples: 96.00000, ips: 160.94889 sequences/sec
tobal step: 320, epoch: 0, batch: 319, loss: 10.397195, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59716 sec, avg_samples: 96.00000, ips: 160.72636 sequences/sec
tobal step: 330, epoch: 0, batch: 329, loss: 10.535649, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59656 sec, avg_samples: 96.00000, ips: 160.88786 sequences/sec
tobal step: 340, epoch: 0, batch: 339, loss: 10.449653, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59700 sec, avg_samples: 96.00000, ips: 160.77167 sequences/sec
tobal step: 350, epoch: 0, batch: 349, loss: 10.479215, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59842 sec, avg_samples: 96.00000, ips: 160.39232 sequences/sec
tobal step: 360, epoch: 0, batch: 359, loss: 10.416052, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59843 sec, avg_samples: 96.00000, ips: 160.38927 sequences/sec
tobal step: 370, epoch: 0, batch: 369, loss: 10.502677, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59852 sec, avg_samples: 96.00000, ips: 160.36471 sequences/sec
tobal step: 380, epoch: 0, batch: 379, loss: 10.348300, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.59886 sec, avg_samples: 96.00000, ips: 160.26681 sequences/sec
tobal step: 390, epoch: 0, batch: 389, loss: 10.341205, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.59890 sec, avg_samples: 96.00000, ips: 160.25807 sequences/sec
tobal step: 400, epoch: 0, batch: 399, loss: 10.425369, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59873 sec, avg_samples: 96.00000, ips: 160.30508 sequences/sec
tobal step: 410, epoch: 0, batch: 409, loss: 10.353920, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59909 sec, avg_samples: 96.00000, ips: 160.21093 sequences/sec
tobal step: 420, epoch: 0, batch: 419, loss: 10.421957, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59957 sec, avg_samples: 96.00000, ips: 160.08113 sequences/sec
tobal step: 430, epoch: 0, batch: 429, loss: 10.432819, avg_reader_cost: 0.00015 sec, avg_batch_cost: 0.59820 sec, avg_samples: 96.00000, ips: 160.44279 sequences/sec
tobal step: 440, epoch: 0, batch: 439, loss: 10.392723, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.62019 sec, avg_samples: 96.00000, ips: 154.75918 sequences/sec
tobal step: 450, epoch: 0, batch: 449, loss: 10.359747, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.57795 sec, avg_samples: 96.00000, ips: 166.06438 sequences/sec
tobal step: 460, epoch: 0, batch: 459, loss: 10.312761, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59964 sec, avg_samples: 96.00000, ips: 160.06082 sequences/sec
tobal step: 470, epoch: 0, batch: 469, loss: 10.403232, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59942 sec, avg_samples: 96.00000, ips: 160.12183 sequences/sec
tobal step: 480, epoch: 0, batch: 479, loss: 10.447329, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59956 sec, avg_samples: 96.00000, ips: 160.08171 sequences/sec
tobal step: 490, epoch: 0, batch: 489, loss: 10.343081, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.60009 sec, avg_samples: 96.00000, ips: 159.93991 sequences/sec
tobal step: 500, epoch: 0, batch: 499, loss: 10.311861, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.59956 sec, avg_samples: 96.00000, ips: 160.07965 sequences/sec
