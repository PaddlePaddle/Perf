Namespace(adam_epsilon=1e-06, batch_size=96, device='gpu', enable_addto=False, gradient_merge_steps=1, input_dir='./wikicorpus_en_seqlen128', learning_rate=0.0001, logging_steps=10, max_grad_norm=1.0, max_predictions_per_seq=20, max_steps=400, model_name_or_path='bert-base-uncased', model_type='bert', output_dir='./tmp2/', save_steps=20000, scale_loss=32768, seed=42, use_amp=1, use_pure_fp16=False, warmup_steps=10000, weight_decay=0.01)
[32m[2021-12-27 15:01:11,785] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:38890', '127.0.0.1:53519', '127.0.0.1:40530', '127.0.0.1:40882', '127.0.0.1:55156', '127.0.0.1:55865', '127.0.0.1:60286']
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:38890', '127.0.0.1:53519', '127.0.0.1:40530', '127.0.0.1:40882', '127.0.0.1:55156', '127.0.0.1:55865', '127.0.0.1:60286']
W1227 15:01:20.209935 35149 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2
W1227 15:01:20.214776 35149 device_context.cc:465] device: 0, cuDNN Version: 8.1.
W1227 15:01:32.340416 35149 build_strategy.cc:110] Currently, fuse_broadcast_ops only works under Reduce mode.
W1227 15:01:32.463214 35149 fuse_all_reduce_op_pass.cc:76] Find all_reduce operators: 206. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 20.
tobal step: 10, epoch: 0, batch: 9, loss: 11.231386, avg_reader_cost: 0.07468 sec, avg_batch_cost: 0.51506 sec, avg_samples: 96.00000, ips: 186.38744 sequences/sec
tobal step: 20, epoch: 0, batch: 19, loss: 11.188507, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16603 sec, avg_samples: 96.00000, ips: 578.19173 sequences/sec
tobal step: 30, epoch: 0, batch: 29, loss: 11.128752, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16457 sec, avg_samples: 96.00000, ips: 583.32226 sequences/sec
tobal step: 40, epoch: 0, batch: 39, loss: 11.051072, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16377 sec, avg_samples: 96.00000, ips: 586.17810 sequences/sec
tobal step: 50, epoch: 0, batch: 49, loss: 10.997976, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16301 sec, avg_samples: 96.00000, ips: 588.92652 sequences/sec
tobal step: 60, epoch: 0, batch: 59, loss: 10.912537, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.16397 sec, avg_samples: 96.00000, ips: 585.48540 sequences/sec
tobal step: 70, epoch: 0, batch: 69, loss: 10.826759, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16424 sec, avg_samples: 96.00000, ips: 584.49731 sequences/sec
tobal step: 80, epoch: 0, batch: 79, loss: 10.646481, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16395 sec, avg_samples: 96.00000, ips: 585.55275 sequences/sec
tobal step: 90, epoch: 0, batch: 89, loss: 10.558637, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.16386 sec, avg_samples: 96.00000, ips: 585.85623 sequences/sec
tobal step: 100, epoch: 0, batch: 99, loss: 10.416534, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16365 sec, avg_samples: 96.00000, ips: 586.62372 sequences/sec
tobal step: 110, epoch: 0, batch: 109, loss: 10.276159, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.16443 sec, avg_samples: 96.00000, ips: 583.81780 sequences/sec
tobal step: 120, epoch: 0, batch: 119, loss: 10.306070, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.16392 sec, avg_samples: 96.00000, ips: 585.66126 sequences/sec
tobal step: 130, epoch: 0, batch: 129, loss: 10.176027, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16404 sec, avg_samples: 96.00000, ips: 585.20545 sequences/sec
tobal step: 140, epoch: 0, batch: 139, loss: 10.143075, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16341 sec, avg_samples: 96.00000, ips: 587.47465 sequences/sec
tobal step: 150, epoch: 0, batch: 4, loss: 10.073109, avg_reader_cost: 0.00716 sec, avg_batch_cost: 0.17945 sec, avg_samples: 48.00000, ips: 267.47661 sequences/sec
tobal step: 160, epoch: 0, batch: 14, loss: 10.047268, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16453 sec, avg_samples: 96.00000, ips: 583.48050 sequences/sec
tobal step: 170, epoch: 0, batch: 24, loss: 10.021811, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16380 sec, avg_samples: 96.00000, ips: 586.06915 sequences/sec
tobal step: 180, epoch: 0, batch: 34, loss: 9.943341, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16355 sec, avg_samples: 96.00000, ips: 586.96869 sequences/sec
tobal step: 190, epoch: 0, batch: 44, loss: 9.861634, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.16467 sec, avg_samples: 96.00000, ips: 582.98351 sequences/sec
tobal step: 200, epoch: 0, batch: 54, loss: 9.906373, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16439 sec, avg_samples: 96.00000, ips: 583.96310 sequences/sec
tobal step: 210, epoch: 0, batch: 64, loss: 9.844532, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16370 sec, avg_samples: 96.00000, ips: 586.42850 sequences/sec
tobal step: 220, epoch: 0, batch: 74, loss: 9.765805, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16395 sec, avg_samples: 96.00000, ips: 585.54867 sequences/sec
tobal step: 230, epoch: 0, batch: 84, loss: 9.805462, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.16391 sec, avg_samples: 96.00000, ips: 585.68664 sequences/sec
tobal step: 240, epoch: 0, batch: 94, loss: 9.717840, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16404 sec, avg_samples: 96.00000, ips: 585.20843 sequences/sec
tobal step: 250, epoch: 0, batch: 104, loss: 9.717579, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16477 sec, avg_samples: 96.00000, ips: 582.64641 sequences/sec
tobal step: 260, epoch: 0, batch: 114, loss: 9.640308, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16546 sec, avg_samples: 96.00000, ips: 580.18589 sequences/sec
tobal step: 270, epoch: 0, batch: 124, loss: 9.590757, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16394 sec, avg_samples: 96.00000, ips: 585.57225 sequences/sec
tobal step: 280, epoch: 0, batch: 134, loss: 9.637993, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16400 sec, avg_samples: 96.00000, ips: 585.36428 sequences/sec
tobal step: 290, epoch: 0, batch: 0, loss: 9.653378, avg_reader_cost: 0.03473 sec, avg_batch_cost: 0.20583 sec, avg_samples: 9.60000, ips: 46.64138 sequences/sec
tobal step: 300, epoch: 0, batch: 10, loss: 9.602652, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16853 sec, avg_samples: 96.00000, ips: 569.64139 sequences/sec
tobal step: 310, epoch: 0, batch: 20, loss: 9.506096, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16431 sec, avg_samples: 96.00000, ips: 584.25814 sequences/sec
tobal step: 320, epoch: 0, batch: 30, loss: 9.496438, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16355 sec, avg_samples: 96.00000, ips: 586.98923 sequences/sec
tobal step: 330, epoch: 0, batch: 40, loss: 9.537356, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16360 sec, avg_samples: 96.00000, ips: 586.81454 sequences/sec
tobal step: 340, epoch: 0, batch: 50, loss: 9.365299, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16343 sec, avg_samples: 96.00000, ips: 587.39434 sequences/sec
tobal step: 350, epoch: 0, batch: 60, loss: 9.448449, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16433 sec, avg_samples: 96.00000, ips: 584.20101 sequences/sec
tobal step: 360, epoch: 0, batch: 70, loss: 9.433884, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16404 sec, avg_samples: 96.00000, ips: 585.23139 sequences/sec
tobal step: 370, epoch: 0, batch: 80, loss: 9.263991, avg_reader_cost: 0.00007 sec, avg_batch_cost: 0.16407 sec, avg_samples: 96.00000, ips: 585.09915 sequences/sec
tobal step: 380, epoch: 0, batch: 90, loss: 9.312882, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.16398 sec, avg_samples: 96.00000, ips: 585.43841 sequences/sec
tobal step: 390, epoch: 0, batch: 100, loss: 9.264273, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16411 sec, avg_samples: 96.00000, ips: 584.98916 sequences/sec
tobal step: 400, epoch: 0, batch: 110, loss: 9.183577, avg_reader_cost: 0.00006 sec, avg_batch_cost: 0.16370 sec, avg_samples: 96.00000, ips: 586.43644 sequences/sec
