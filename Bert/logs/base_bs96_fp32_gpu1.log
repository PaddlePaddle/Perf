/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _nlv = LooseVersion(_np_version)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p16 = _nlv < LooseVersion("1.16")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p17 = _nlv < LooseVersion("1.17")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p18 = _nlv < LooseVersion("1.18")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p19 = _nlv < LooseVersion("1.19")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p20 = _nlv < LooseVersion("1.20")
/usr/local/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/function.py:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(_np_version) >= LooseVersion("1.17.0"):
Namespace(adam_epsilon=1e-08, batch_size=96, device='gpu', enable_addto=False, gradient_merge_steps=88, input_dir='./data/wikicorpus_en_seqlen128', learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_predictions_per_seq=80, max_steps=500, model_name_or_path='bert-base-uncased', model_type='bert', output_dir='./tmp2/', profiler_options=None, save_steps=20000, scale_loss=32768, seed=42, use_amp=0, use_pure_fp16=False, warmup_steps=0, weight_decay=0.0)
[32m[2022-06-08 15:45:21,366] [    INFO][0m - Downloading https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-uncased-vocab.txt and saved to /root/.paddlenlp/models/bert-base-uncased[0m
[32m[2022-06-08 15:45:21,366] [    INFO][0m - Downloading bert-base-uncased-vocab.txt from https://bj.bcebos.com/paddle-hapi/models/bert/bert-base-uncased-vocab.txt[0m
  0%|          | 0.00/226k [00:00<?, ?B/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226k/226k [00:00<00:00, 26.6MB/s]
W0608 15:45:23.384567  1502 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 11.2
W0608 15:45:23.443271  1502 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.
W0608 15:45:37.145443  1502 build_strategy.cc:123] Currently, fuse_broadcast_ops only works under Reduce mode.
tobal step: 10, epoch: 0, batch: 9, loss: 11.246871, avg_reader_cost: 0.06475 sec, avg_batch_cost: 0.65360 sec, avg_samples: 96.00000, ips: 133.64005 sequences/sec
tobal step: 20, epoch: 0, batch: 19, loss: 11.161523, avg_reader_cost: 0.00021 sec, avg_batch_cost: 0.58345 sec, avg_samples: 96.00000, ips: 164.47863 sequences/sec
tobal step: 30, epoch: 0, batch: 29, loss: 11.215019, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58465 sec, avg_samples: 96.00000, ips: 164.16605 sequences/sec
tobal step: 40, epoch: 0, batch: 39, loss: 11.188222, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58626 sec, avg_samples: 96.00000, ips: 163.71470 sequences/sec
tobal step: 50, epoch: 0, batch: 49, loss: 11.259479, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58717 sec, avg_samples: 96.00000, ips: 163.46304 sequences/sec
tobal step: 60, epoch: 0, batch: 59, loss: 11.194427, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.58851 sec, avg_samples: 96.00000, ips: 163.08832 sequences/sec
tobal step: 70, epoch: 0, batch: 69, loss: 11.239295, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.58771 sec, avg_samples: 96.00000, ips: 163.31637 sequences/sec
tobal step: 80, epoch: 0, batch: 79, loss: 11.227909, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.58902 sec, avg_samples: 96.00000, ips: 162.95088 sequences/sec
tobal step: 90, epoch: 0, batch: 89, loss: 11.294785, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59011 sec, avg_samples: 96.00000, ips: 162.64834 sequences/sec
tobal step: 100, epoch: 0, batch: 99, loss: 11.143709, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.58892 sec, avg_samples: 96.00000, ips: 162.97933 sequences/sec
tobal step: 110, epoch: 0, batch: 109, loss: 11.147650, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.58896 sec, avg_samples: 96.00000, ips: 162.96364 sequences/sec
tobal step: 120, epoch: 0, batch: 119, loss: 11.211628, avg_reader_cost: 0.00045 sec, avg_batch_cost: 0.59131 sec, avg_samples: 96.00000, ips: 162.22576 sequences/sec
tobal step: 130, epoch: 0, batch: 129, loss: 11.167460, avg_reader_cost: 0.00018 sec, avg_batch_cost: 0.59068 sec, avg_samples: 96.00000, ips: 162.47717 sequences/sec
tobal step: 140, epoch: 0, batch: 139, loss: 11.271013, avg_reader_cost: 0.00017 sec, avg_batch_cost: 0.59070 sec, avg_samples: 96.00000, ips: 162.47117 sequences/sec
tobal step: 150, epoch: 0, batch: 149, loss: 11.100046, avg_reader_cost: 0.00014 sec, avg_batch_cost: 0.59093 sec, avg_samples: 96.00000, ips: 162.41765 sequences/sec
tobal step: 160, epoch: 0, batch: 159, loss: 11.170861, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59112 sec, avg_samples: 96.00000, ips: 162.37215 sequences/sec
tobal step: 170, epoch: 0, batch: 169, loss: 11.119526, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.59173 sec, avg_samples: 96.00000, ips: 162.21015 sequences/sec
tobal step: 180, epoch: 0, batch: 179, loss: 10.507095, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59210 sec, avg_samples: 96.00000, ips: 162.10842 sequences/sec
tobal step: 190, epoch: 0, batch: 189, loss: 10.604044, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59199 sec, avg_samples: 96.00000, ips: 162.14096 sequences/sec
tobal step: 200, epoch: 0, batch: 199, loss: 10.597165, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59305 sec, avg_samples: 96.00000, ips: 161.84353 sequences/sec
tobal step: 210, epoch: 0, batch: 209, loss: 10.530011, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59271 sec, avg_samples: 96.00000, ips: 161.94432 sequences/sec
tobal step: 220, epoch: 0, batch: 219, loss: 10.481635, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.59272 sec, avg_samples: 96.00000, ips: 161.93590 sequences/sec
tobal step: 230, epoch: 0, batch: 229, loss: 10.461276, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59347 sec, avg_samples: 96.00000, ips: 161.73734 sequences/sec
tobal step: 240, epoch: 0, batch: 239, loss: 10.512781, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59527 sec, avg_samples: 96.00000, ips: 161.24041 sequences/sec
tobal step: 250, epoch: 0, batch: 249, loss: 10.553946, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59422 sec, avg_samples: 96.00000, ips: 161.53318 sequences/sec
tobal step: 260, epoch: 0, batch: 259, loss: 10.658183, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59398 sec, avg_samples: 96.00000, ips: 161.59216 sequences/sec
tobal step: 270, epoch: 0, batch: 269, loss: 10.611153, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59577 sec, avg_samples: 96.00000, ips: 161.11100 sequences/sec
tobal step: 280, epoch: 0, batch: 279, loss: 10.583233, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59514 sec, avg_samples: 96.00000, ips: 161.28237 sequences/sec
tobal step: 290, epoch: 0, batch: 289, loss: 10.614481, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.59611 sec, avg_samples: 96.00000, ips: 161.02356 sequences/sec
tobal step: 300, epoch: 0, batch: 299, loss: 10.543908, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59690 sec, avg_samples: 96.00000, ips: 160.80571 sequences/sec
tobal step: 310, epoch: 0, batch: 309, loss: 10.437070, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59651 sec, avg_samples: 96.00000, ips: 160.90698 sequences/sec
tobal step: 320, epoch: 0, batch: 319, loss: 10.397195, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59608 sec, avg_samples: 96.00000, ips: 161.02788 sequences/sec
tobal step: 330, epoch: 0, batch: 329, loss: 10.535649, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.59631 sec, avg_samples: 96.00000, ips: 160.96330 sequences/sec
tobal step: 340, epoch: 0, batch: 339, loss: 10.449654, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59709 sec, avg_samples: 96.00000, ips: 160.75102 sequences/sec
tobal step: 350, epoch: 0, batch: 349, loss: 10.479215, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59673 sec, avg_samples: 96.00000, ips: 160.84536 sequences/sec
tobal step: 360, epoch: 0, batch: 359, loss: 10.416052, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59843 sec, avg_samples: 96.00000, ips: 160.38400 sequences/sec
tobal step: 370, epoch: 0, batch: 369, loss: 10.502677, avg_reader_cost: 0.00019 sec, avg_batch_cost: 0.59780 sec, avg_samples: 96.00000, ips: 160.53779 sequences/sec
tobal step: 380, epoch: 0, batch: 379, loss: 10.348300, avg_reader_cost: 0.00016 sec, avg_batch_cost: 0.59796 sec, avg_samples: 96.00000, ips: 160.50212 sequences/sec
tobal step: 390, epoch: 0, batch: 389, loss: 10.341205, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59780 sec, avg_samples: 96.00000, ips: 160.55490 sequences/sec
tobal step: 400, epoch: 0, batch: 399, loss: 10.425369, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59864 sec, avg_samples: 96.00000, ips: 160.33967 sequences/sec
tobal step: 410, epoch: 0, batch: 409, loss: 10.353921, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.59837 sec, avg_samples: 96.00000, ips: 160.40765 sequences/sec
tobal step: 420, epoch: 0, batch: 419, loss: 10.421957, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59972 sec, avg_samples: 96.00000, ips: 160.05098 sequences/sec
tobal step: 430, epoch: 0, batch: 429, loss: 10.432819, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.59811 sec, avg_samples: 96.00000, ips: 160.48248 sequences/sec
tobal step: 440, epoch: 0, batch: 439, loss: 10.392723, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.62035 sec, avg_samples: 96.00000, ips: 154.72735 sequences/sec
tobal step: 450, epoch: 0, batch: 449, loss: 10.359748, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.57895 sec, avg_samples: 96.00000, ips: 165.79075 sequences/sec
tobal step: 460, epoch: 0, batch: 459, loss: 10.312761, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.59930 sec, avg_samples: 96.00000, ips: 160.15592 sequences/sec
tobal step: 470, epoch: 0, batch: 469, loss: 10.403232, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59988 sec, avg_samples: 96.00000, ips: 159.99800 sequences/sec
tobal step: 480, epoch: 0, batch: 479, loss: 10.447329, avg_reader_cost: 0.00042 sec, avg_batch_cost: 0.60019 sec, avg_samples: 96.00000, ips: 159.83793 sequences/sec
tobal step: 490, epoch: 0, batch: 489, loss: 10.343081, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.59991 sec, avg_samples: 96.00000, ips: 159.99953 sequences/sec
tobal step: 500, epoch: 0, batch: 499, loss: 10.311861, avg_reader_cost: 0.00013 sec, avg_batch_cost: 0.59949 sec, avg_samples: 96.00000, ips: 160.10260 sequences/sec
