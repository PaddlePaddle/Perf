/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _nlv = LooseVersion(_np_version)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p16 = _nlv < LooseVersion("1.16")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p17 = _nlv < LooseVersion("1.17")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p18 = _nlv < LooseVersion("1.18")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p19 = _nlv < LooseVersion("1.19")
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  _np_version_under1p20 = _nlv < LooseVersion("1.20")
/usr/local/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  other = LooseVersion(other)
/usr/local/lib/python3.7/site-packages/pandas/compat/numpy/function.py:125: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if LooseVersion(_np_version) >= LooseVersion("1.17.0"):
Namespace(adam_epsilon=1e-08, batch_size=96, device='gpu', enable_addto=False, gradient_merge_steps=88, input_dir='./data/wikicorpus_en_seqlen128', learning_rate=5e-05, logging_steps=10, max_grad_norm=1.0, max_predictions_per_seq=80, max_steps=500, model_name_or_path='bert-base-uncased', model_type='bert', output_dir='./tmp2/', profiler_options=None, save_steps=20000, scale_loss=32768, seed=42, use_amp=1, use_pure_fp16=False, warmup_steps=0, weight_decay=0.0)
[32m[2022-06-08 15:52:53,064] [    INFO][0m - Already cached /root/.paddlenlp/models/bert-base-uncased/bert-base-uncased-vocab.txt[0m
W0608 15:52:55.280305  1696 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 11.2
W0608 15:52:55.284751  1696 gpu_context.cc:306] device: 0, cuDNN Version: 8.1.
W0608 15:53:06.616894  1696 build_strategy.cc:123] Currently, fuse_broadcast_ops only works under Reduce mode.
tobal step: 10, epoch: 0, batch: 9, loss: 11.246908, avg_reader_cost: 0.06786 sec, avg_batch_cost: 0.25797 sec, avg_samples: 96.00000, ips: 294.63143 sequences/sec
tobal step: 20, epoch: 0, batch: 19, loss: 11.161420, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.14564 sec, avg_samples: 96.00000, ips: 658.61190 sequences/sec
tobal step: 30, epoch: 0, batch: 29, loss: 11.215065, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14607 sec, avg_samples: 96.00000, ips: 656.78303 sequences/sec
tobal step: 40, epoch: 0, batch: 39, loss: 11.188272, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14581 sec, avg_samples: 96.00000, ips: 657.92552 sequences/sec
tobal step: 50, epoch: 0, batch: 49, loss: 11.259440, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.14626 sec, avg_samples: 96.00000, ips: 655.81397 sequences/sec
tobal step: 60, epoch: 0, batch: 59, loss: 11.194447, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14633 sec, avg_samples: 96.00000, ips: 655.68400 sequences/sec
tobal step: 70, epoch: 0, batch: 69, loss: 11.239302, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.14621 sec, avg_samples: 96.00000, ips: 656.10367 sequences/sec
tobal step: 80, epoch: 0, batch: 79, loss: 11.227917, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14637 sec, avg_samples: 96.00000, ips: 655.50649 sequences/sec
tobal step: 90, epoch: 0, batch: 89, loss: 11.294772, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.14767 sec, avg_samples: 96.00000, ips: 649.60994 sequences/sec
tobal step: 100, epoch: 0, batch: 99, loss: 11.143671, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14637 sec, avg_samples: 96.00000, ips: 655.38816 sequences/sec
tobal step: 110, epoch: 0, batch: 109, loss: 11.147483, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14630 sec, avg_samples: 96.00000, ips: 655.84548 sequences/sec
tobal step: 120, epoch: 0, batch: 119, loss: 11.211671, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14655 sec, avg_samples: 96.00000, ips: 654.68145 sequences/sec
tobal step: 130, epoch: 0, batch: 129, loss: 11.167442, avg_reader_cost: 0.00012 sec, avg_batch_cost: 0.14633 sec, avg_samples: 96.00000, ips: 655.54405 sequences/sec
tobal step: 140, epoch: 0, batch: 139, loss: 11.271073, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14624 sec, avg_samples: 96.00000, ips: 656.04659 sequences/sec
tobal step: 150, epoch: 0, batch: 149, loss: 11.099893, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14643 sec, avg_samples: 96.00000, ips: 655.18629 sequences/sec
tobal step: 160, epoch: 0, batch: 159, loss: 11.170856, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14637 sec, avg_samples: 96.00000, ips: 655.45868 sequences/sec
tobal step: 170, epoch: 0, batch: 169, loss: 11.119464, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14649 sec, avg_samples: 96.00000, ips: 654.86927 sequences/sec
tobal step: 180, epoch: 0, batch: 179, loss: 10.507214, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14766 sec, avg_samples: 96.00000, ips: 649.71916 sequences/sec
tobal step: 190, epoch: 0, batch: 189, loss: 10.604165, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14658 sec, avg_samples: 96.00000, ips: 654.50288 sequences/sec
tobal step: 200, epoch: 0, batch: 199, loss: 10.597339, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14643 sec, avg_samples: 96.00000, ips: 655.14258 sequences/sec
tobal step: 210, epoch: 0, batch: 209, loss: 10.530060, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14641 sec, avg_samples: 96.00000, ips: 655.24439 sequences/sec
tobal step: 220, epoch: 0, batch: 219, loss: 10.481645, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14637 sec, avg_samples: 96.00000, ips: 655.44022 sequences/sec
tobal step: 230, epoch: 0, batch: 229, loss: 10.461338, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14633 sec, avg_samples: 96.00000, ips: 655.65186 sequences/sec
tobal step: 240, epoch: 0, batch: 239, loss: 10.512857, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14672 sec, avg_samples: 96.00000, ips: 653.84532 sequences/sec
tobal step: 250, epoch: 0, batch: 249, loss: 10.554065, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14651 sec, avg_samples: 96.00000, ips: 654.78174 sequences/sec
tobal step: 260, epoch: 0, batch: 259, loss: 10.658222, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14646 sec, avg_samples: 96.00000, ips: 655.03035 sequences/sec
tobal step: 270, epoch: 0, batch: 269, loss: 10.611183, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14783 sec, avg_samples: 96.00000, ips: 648.98089 sequences/sec
tobal step: 280, epoch: 0, batch: 279, loss: 10.583233, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.14659 sec, avg_samples: 96.00000, ips: 654.41449 sequences/sec
tobal step: 290, epoch: 0, batch: 289, loss: 10.614505, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14662 sec, avg_samples: 96.00000, ips: 654.28199 sequences/sec
tobal step: 300, epoch: 0, batch: 299, loss: 10.543955, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14656 sec, avg_samples: 96.00000, ips: 654.58087 sequences/sec
tobal step: 310, epoch: 0, batch: 309, loss: 10.437088, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14667 sec, avg_samples: 96.00000, ips: 654.19355 sequences/sec
tobal step: 320, epoch: 0, batch: 319, loss: 10.397196, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14674 sec, avg_samples: 96.00000, ips: 653.89863 sequences/sec
tobal step: 330, epoch: 0, batch: 329, loss: 10.535621, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14671 sec, avg_samples: 96.00000, ips: 653.98741 sequences/sec
tobal step: 340, epoch: 0, batch: 339, loss: 10.449677, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14655 sec, avg_samples: 96.00000, ips: 654.66793 sequences/sec
tobal step: 350, epoch: 0, batch: 349, loss: 10.479181, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14668 sec, avg_samples: 96.00000, ips: 654.11543 sequences/sec
tobal step: 360, epoch: 0, batch: 359, loss: 10.416098, avg_reader_cost: 0.00011 sec, avg_batch_cost: 0.14809 sec, avg_samples: 96.00000, ips: 647.77542 sequences/sec
tobal step: 370, epoch: 0, batch: 369, loss: 10.502658, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14652 sec, avg_samples: 96.00000, ips: 654.75544 sequences/sec
tobal step: 380, epoch: 0, batch: 379, loss: 10.348340, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14692 sec, avg_samples: 96.00000, ips: 652.94271 sequences/sec
tobal step: 390, epoch: 0, batch: 389, loss: 10.341221, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14667 sec, avg_samples: 96.00000, ips: 654.13764 sequences/sec
tobal step: 400, epoch: 0, batch: 399, loss: 10.425386, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14702 sec, avg_samples: 96.00000, ips: 652.58291 sequences/sec
tobal step: 410, epoch: 0, batch: 409, loss: 10.353910, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14665 sec, avg_samples: 96.00000, ips: 654.22437 sequences/sec
tobal step: 420, epoch: 0, batch: 419, loss: 10.422018, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14680 sec, avg_samples: 96.00000, ips: 653.57480 sequences/sec
tobal step: 430, epoch: 0, batch: 429, loss: 10.432800, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14649 sec, avg_samples: 96.00000, ips: 654.91295 sequences/sec
tobal step: 440, epoch: 0, batch: 439, loss: 10.392756, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.15665 sec, avg_samples: 96.00000, ips: 612.45887 sequences/sec
tobal step: 450, epoch: 0, batch: 449, loss: 10.359748, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.13805 sec, avg_samples: 96.00000, ips: 694.92083 sequences/sec
tobal step: 460, epoch: 0, batch: 459, loss: 10.312779, avg_reader_cost: 0.00010 sec, avg_batch_cost: 0.14686 sec, avg_samples: 96.00000, ips: 653.24376 sequences/sec
tobal step: 470, epoch: 0, batch: 469, loss: 10.403310, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14692 sec, avg_samples: 96.00000, ips: 653.09108 sequences/sec
tobal step: 480, epoch: 0, batch: 479, loss: 10.447341, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14691 sec, avg_samples: 96.00000, ips: 653.09977 sequences/sec
tobal step: 490, epoch: 0, batch: 489, loss: 10.343206, avg_reader_cost: 0.00008 sec, avg_batch_cost: 0.14693 sec, avg_samples: 96.00000, ips: 652.98845 sequences/sec
tobal step: 500, epoch: 0, batch: 499, loss: 10.311908, avg_reader_cost: 0.00009 sec, avg_batch_cost: 0.14670 sec, avg_samples: 96.00000, ips: 653.99729 sequences/sec
