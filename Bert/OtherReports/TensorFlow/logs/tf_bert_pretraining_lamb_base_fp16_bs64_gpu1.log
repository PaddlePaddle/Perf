+ batch_size=64
+ num_gpus=1
+ precision=fp16
++ expr 67584 / 64 / 1
+ num_accumulation_steps_phase1=1056
+ train_steps=100
+ bert_model=base
+ bash scripts/run_pretraining_lamb.sh 64 64 8 7.5e-4 5e-4 fp16 true 1 2000 200 100 200 1056 512 base
Container nvidia build =  13409399
Saving checkpoints to /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449
Logs written to /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768.211218022449.log
Container nvidia build =  13409399
XLA activated
2021-12-18 02:24:49.564621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1218 02:24:51.031748 140063694366528 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /workspace/bert/run_pretraining.py:589: The name tf.enable_resource_variables is deprecated. Please use tf.compat.v1.enable_resource_variables instead.

W1218 02:24:51.700274 140063694366528 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:589: The name tf.enable_resource_variables is deprecated. Please use tf.compat.v1.enable_resource_variables instead.

INFO:tensorflow:Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f60b68d3400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I1218 02:24:51.700962 140063694366528 estimator.py:212] Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f60b68d3400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f60b69341e0>) includes params argument, but params are not passed to Estimator.
W1218 02:24:51.701639 140063694366528 model_fn.py:630] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f60b69341e0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:***** Running training *****
I1218 02:24:51.702028 140063694366528 run_pretraining.py:625] ***** Running training *****
INFO:tensorflow:  Batch size = 64
I1218 02:24:51.702097 140063694366528 run_pretraining.py:626]   Batch size = 64
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

W1218 02:24:51.826936 140063694366528 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

INFO:tensorflow:Calling model_fn.
I1218 02:24:51.930293 140063694366528 estimator.py:1148] Calling model_fn.
INFO:tensorflow:*** Features ***
I1218 02:24:51.930444 140063694366528 run_pretraining.py:258] *** Features ***
INFO:tensorflow:  name = input_ids, shape = (64, 128)
I1218 02:24:51.930555 140063694366528 run_pretraining.py:260]   name = input_ids, shape = (64, 128)
INFO:tensorflow:  name = input_mask, shape = (64, 128)
I1218 02:24:51.930637 140063694366528 run_pretraining.py:260]   name = input_mask, shape = (64, 128)
INFO:tensorflow:  name = masked_lm_ids, shape = (64, 20)
I1218 02:24:51.930708 140063694366528 run_pretraining.py:260]   name = masked_lm_ids, shape = (64, 20)
INFO:tensorflow:  name = masked_lm_positions, shape = (64, 20)
I1218 02:24:51.930774 140063694366528 run_pretraining.py:260]   name = masked_lm_positions, shape = (64, 20)
INFO:tensorflow:  name = masked_lm_weights, shape = (64, 20)
I1218 02:24:51.930840 140063694366528 run_pretraining.py:260]   name = masked_lm_weights, shape = (64, 20)
INFO:tensorflow:  name = next_sentence_labels, shape = (64, 1)
I1218 02:24:51.930903 140063694366528 run_pretraining.py:260]   name = next_sentence_labels, shape = (64, 1)
INFO:tensorflow:  name = segment_ids, shape = (64, 128)
I1218 02:24:51.930968 140063694366528 run_pretraining.py:260]   name = segment_ids, shape = (64, 128)
WARNING:tensorflow:From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W1218 02:24:51.931147 140063694366528 module_wrapper.py:139] From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W1218 02:24:51.932217 140063694366528 module_wrapper.py:139] From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W1218 02:24:53.618215 140063694366528 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /workspace/bert/optimization.py:130: The name tf.is_finite is deprecated. Please use tf.math.is_finite instead.

W1218 02:24:56.805132 140063694366528 module_wrapper.py:139] From /workspace/bert/optimization.py:130: The name tf.is_finite is deprecated. Please use tf.math.is_finite instead.

WARNING:tensorflow:From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

W1218 02:24:57.047064 140063694366528 module_wrapper.py:139] From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

INFO:tensorflow:Done calling model_fn.
I1218 02:25:06.150929 140063694366528 estimator.py:1150] Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
I1218 02:25:06.152145 140063694366528 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
I1218 02:25:10.230557 140063694366528 monitored_session.py:240] Graph was finalized.
2021-12-18 02:25:10.245037: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599995000 Hz
2021-12-18 02:25:10.249792: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x125b7300 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-12-18 02:25:10.249825: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-12-18 02:25:10.253045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-12-18 02:25:11.388509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.480302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.487613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.514604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.527362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.543076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.561576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.581778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.584300: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13bbc050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-12-18 02:25:11.584324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584330: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584335: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584340: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584352: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584356: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.584361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 02:25:11.594899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.596881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:03:00.0
2021-12-18 02:25:11.596966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.598910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 1 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:04:00.0
2021-12-18 02:25:11.598980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.600920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 2 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:05:00.0
2021-12-18 02:25:11.600981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.602918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 3 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:06:00.0
2021-12-18 02:25:11.602982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.604907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 4 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:07:00.0
2021-12-18 02:25:11.604965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.606912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 5 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:08:00.0
2021-12-18 02:25:11.606971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.608904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 6 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:09:00.0
2021-12-18 02:25:11.608960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.610888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 7 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:0a:00.0
2021-12-18 02:25:11.610929: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-18 02:25:11.642927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-18 02:25:11.656297: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-12-18 02:25:11.671898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-12-18 02:25:11.694579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-12-18 02:25:11.700994: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2021-12-18 02:25:11.702211: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-18 02:25:11.702329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.704421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.706425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.708467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.710462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.712452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.714462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.716447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.718425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.720403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.722377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.724383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.726372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.728376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.730347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.732314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:11.734390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-12-18 02:25:11.734428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-18 02:25:14.258470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-12-18 02:25:14.258528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 1 2 3 4 5 6 7 
2021-12-18 02:25:14.258540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N Y Y Y N N N Y 
2021-12-18 02:25:14.258546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 1:   Y N Y Y N N Y N 
2021-12-18 02:25:14.258551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 2:   Y Y N Y N Y N N 
2021-12-18 02:25:14.258557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 3:   Y Y Y N Y N N N 
2021-12-18 02:25:14.258562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 4:   N N N Y N Y Y Y 
2021-12-18 02:25:14.258567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 5:   N N Y N Y N Y Y 
2021-12-18 02:25:14.258572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 6:   N Y N N Y Y N Y 
2021-12-18 02:25:14.258578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 7:   Y N N N Y Y Y N 
2021-12-18 02:25:14.259081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.261170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.263653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.266534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.269450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.272344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.275275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.278162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.281104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.283996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30166 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)
2021-12-18 02:25:14.284406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.287302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30166 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-12-18 02:25:14.287603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.290469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30166 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:05:00.0, compute capability: 7.0)
2021-12-18 02:25:14.290792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.293635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 30166 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0)
2021-12-18 02:25:14.293915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.296845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 30166 MB memory) -> physical GPU (device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0)
2021-12-18 02:25:14.297115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.299982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 30166 MB memory) -> physical GPU (device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)
2021-12-18 02:25:14.300247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.303165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 30166 MB memory) -> physical GPU (device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0)
2021-12-18 02:25:14.303453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 02:25:14.306327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 30166 MB memory) -> physical GPU (device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0)
2021-12-18 02:25:17.415277: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:17.437653: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:20.040200: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1648] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2021-12-18 02:25:23.946031: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:23.952268: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Running local_init_op.
I1218 02:25:24.695609 140063694366528 session_manager.py:500] Running local_init_op.
2021-12-18 02:25:25.246762: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:25.246997: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Done running local_init_op.
I1218 02:25:25.361450 140063694366528 session_manager.py:502] Done running local_init_op.
2021-12-18 02:25:25.920504: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:25.926835: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:27.018180: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:27.018484: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1/model.ckpt.
I1218 02:25:35.270441 140063694366528 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1/model.ckpt.
2021-12-18 02:25:36.040515: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:36.048639: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:42.268094: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:42.268461: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:42.272809: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:42.274775: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:42.277920: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
WARNING:tensorflow:From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

W1218 02:25:42.469729 140063694366528 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

2021-12-18 02:25:42.966718: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:42.966977: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 02:25:55.442748: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:25:55.577412: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


2021-12-18 02:26:05.631968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-18 02:26:06.235197: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-18 02:26:35.350067: I tensorflow/compiler/jit/xla_compilation_cache.cc:243] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:loss = 11.176496, step = 0
I1218 02:26:37.228839 140063694366528 basic_session_run_hooks.py:262] loss = 11.176496, step = 0
2021-12-18 02:26:49.300854: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:26:49.440052: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 02:27:27.662592 140063694366528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 02:27:27.799697 140063694366528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 02:27:27.926372 140063694366528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 02:27:28.056372 140063694366528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 02:27:28.184557 140063694366528 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
2021-12-18 02:49:45.838737: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 02:49:45.972834: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


INFO:tensorflow:loss = 11.139301, step = 4 (1430.105 sec)
I1218 02:50:27.333867 140063694366528 basic_session_run_hooks.py:260] loss = 11.139301, step = 4 (1430.105 sec)
INFO:tensorflow:loss = 11.143675, step = 13 (1293.888 sec)
I1218 03:12:01.222420 140063694366528 basic_session_run_hooks.py:260] loss = 11.143675, step = 13 (1293.888 sec)
INFO:tensorflow:loss = 11.125276, step = 23 (1292.955 sec)
I1218 03:33:34.177744 140063694366528 basic_session_run_hooks.py:260] loss = 11.125276, step = 23 (1292.955 sec)
decayed_learning_rate_at_crossover_point = 7.500000e-04, adjusted_init_lr = 7.500000e-04
Initializing LAMB Optimizer
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:29:47.120817 - Iteration: 1  throughput_train : 277.099 seq/s mlm_loss : 10.4465  nsp_loss : 0.7031  total_loss : 11.1497  avg_loss_step : 11.1525  learning_rate : 0.0  loss_scaler : 4294967296 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:32:03.144242 - Iteration: 1  throughput_train : 497.172 seq/s mlm_loss : 10.4501  nsp_loss : 0.7054  total_loss : 11.1555  avg_loss_step : 11.1520  learning_rate : 0.0  loss_scaler : 2147483648 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:34:19.169286 - Iteration: 1  throughput_train : 497.177 seq/s mlm_loss : 10.4380  nsp_loss : 0.7209  total_loss : 11.1589  avg_loss_step : 11.1532  learning_rate : 0.0  loss_scaler : 1073741824 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:36:38.073116 - Iteration: 1  throughput_train : 486.821 seq/s mlm_loss : 10.4316  nsp_loss : 0.7119  total_loss : 11.1435  avg_loss_step : 11.1519  learning_rate : 0.0  loss_scaler : 536870912 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:38:55.963705 - Iteration: 1  throughput_train : 490.421 seq/s mlm_loss : 10.4518  nsp_loss : 0.7197  total_loss : 11.1715  avg_loss_step : 11.1520  learning_rate : 0.0  loss_scaler : 268435456 
Skipping time record for  1  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:41:38.673627 - Iteration: 2  throughput_train : 415.592 seq/s mlm_loss : 10.4411  nsp_loss : 0.6960  total_loss : 11.1371  avg_loss_step : 11.1527  learning_rate : 0.0  loss_scaler : 134217728 
Skipping time record for  2  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:43:55.922697 - Iteration: 3  throughput_train : 492.700 seq/s mlm_loss : 10.4357  nsp_loss : 0.6957  total_loss : 11.1314  avg_loss_step : 11.1514  learning_rate : 3.75e-07  loss_scaler : 134217728 
Skipping time record for  3  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:46:12.527807 - Iteration: 4  throughput_train : 495.027 seq/s mlm_loss : 10.4510  nsp_loss : 0.6974  total_loss : 11.1484  avg_loss_step : 11.1521  learning_rate : 7.5e-07  loss_scaler : 134217728 
Skipping time record for  4  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:48:29.536710 - Iteration: 5  throughput_train : 493.570 seq/s mlm_loss : 10.4619  nsp_loss : 0.7135  total_loss : 11.1754  avg_loss_step : 11.1515  learning_rate : 1.125e-06  loss_scaler : 134217728 
Skipping time record for  5  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 02:51:39.655345 - Iteration: 6  throughput_train : 355.662 seq/s mlm_loss : 10.4540  nsp_loss : 0.6948  total_loss : 11.1488  avg_loss_step : 11.1516  learning_rate : 1.5e-06  loss_scaler : 134217728 
DLL 2021-12-18 02:53:56.191154 - Iteration: 7  throughput_train : 495.313 seq/s mlm_loss : 10.4335  nsp_loss : 0.7062  total_loss : 11.1397  avg_loss_step : 11.1513  learning_rate : 1.8750001e-06  loss_scaler : 134217728 
DLL 2021-12-18 02:56:12.982085 - Iteration: 8  throughput_train : 494.368 seq/s mlm_loss : 10.4648  nsp_loss : 0.6940  total_loss : 11.1588  avg_loss_step : 11.1507  learning_rate : 2.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 02:58:29.512373 - Iteration: 9  throughput_train : 495.297 seq/s mlm_loss : 10.4453  nsp_loss : 0.6966  total_loss : 11.1419  avg_loss_step : 11.1493  learning_rate : 2.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:00:45.745386 - Iteration: 10  throughput_train : 496.394 seq/s mlm_loss : 10.4325  nsp_loss : 0.6895  total_loss : 11.1220  avg_loss_step : 11.1476  learning_rate : 3e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:03:02.603304 - Iteration: 11  throughput_train : 494.148 seq/s mlm_loss : 10.4393  nsp_loss : 0.7038  total_loss : 11.1431  avg_loss_step : 11.1454  learning_rate : 3.3750002e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:05:19.403242 - Iteration: 12  throughput_train : 494.308 seq/s mlm_loss : 10.4407  nsp_loss : 0.7021  total_loss : 11.1428  avg_loss_step : 11.1449  learning_rate : 3.7500001e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:07:36.266560 - Iteration: 13  throughput_train : 494.098 seq/s mlm_loss : 10.4442  nsp_loss : 0.6958  total_loss : 11.1401  avg_loss_step : 11.1439  learning_rate : 4.125e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:09:52.717097 - Iteration: 14  throughput_train : 495.601 seq/s mlm_loss : 10.4314  nsp_loss : 0.6971  total_loss : 11.1284  avg_loss_step : 11.1417  learning_rate : 4.5e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:12:09.395028 - Iteration: 15  throughput_train : 494.910 seq/s mlm_loss : 10.4361  nsp_loss : 0.6973  total_loss : 11.1334  avg_loss_step : 11.1404  learning_rate : 4.8750003e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:14:25.895495 - Iteration: 16  throughput_train : 495.551 seq/s mlm_loss : 10.4451  nsp_loss : 0.6900  total_loss : 11.1351  avg_loss_step : 11.1388  learning_rate : 5.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:16:42.193051 - Iteration: 17  throughput_train : 496.271 seq/s mlm_loss : 10.4640  nsp_loss : 0.6899  total_loss : 11.1539  avg_loss_step : 11.1379  learning_rate : 5.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:18:58.883876 - Iteration: 18  throughput_train : 494.859 seq/s mlm_loss : 10.4388  nsp_loss : 0.7079  total_loss : 11.1467  avg_loss_step : 11.1357  learning_rate : 6e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:21:15.928883 - Iteration: 19  throughput_train : 493.510 seq/s mlm_loss : 10.4248  nsp_loss : 0.6965  total_loss : 11.1212  avg_loss_step : 11.1342  learning_rate : 6.3750003e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:23:32.522782 - Iteration: 20  throughput_train : 495.210 seq/s mlm_loss : 10.4176  nsp_loss : 0.6992  total_loss : 11.1168  avg_loss_step : 11.1304  learning_rate : 6.7500005e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:25:48.763618 - Iteration: 21  throughput_train : 496.477 seq/s mlm_loss : 10.4189  nsp_loss : 0.6884  total_loss : 11.1073  avg_loss_step : 11.1290  learning_rate : 7.125e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:28:05.239428 - Iteration: 22  throughput_train : 495.629 seq/s mlm_loss : 10.4285  nsp_loss : 0.6748  total_loss : 11.1033  avg_loss_step : 11.1270  learning_rate : 7.5000003e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:30:21.798895 - Iteration: 23  throughput_train : 495.325 seq/s mlm_loss : 10.4246  nsp_loss : 0.6898  total_loss : 11.1144  avg_loss_step : 11.1240  learning_rate : 7.875e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:32:38.334585 - Iteration: 24  throughput_train : 495.402 seq/s mlm_loss : 10.4284  nsp_loss : 0.6854  total_loss : 11.1138  avg_loss_step : 11.1204  learning_rate : 8.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:34:54.712701 - Iteration: 25  throughput_train : 495.985 seq/s mlm_loss : 10.4220  nsp_loss : 0.7037  total_loss : 11.1257  avg_loss_step : 11.1177  learning_rate : 8.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:37:11.552684 - Iteration: 26  throughput_train : 494.309 seq/s mlm_loss : 10.4320  nsp_loss : 0.6927  total_loss : 11.1247  avg_loss_step : 11.1153  learning_rate : 9e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:39:28.234462 - Iteration: 27  throughput_train : 494.879 seq/s mlm_loss : 10.4091  nsp_loss : 0.6861  total_loss : 11.0952  avg_loss_step : 11.1119  learning_rate : 9.375e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:41:44.809093 - Iteration: 28  throughput_train : 495.259 seq/s mlm_loss : 10.4144  nsp_loss : 0.6906  total_loss : 11.1049  avg_loss_step : 11.1086  learning_rate : 9.750001e-06  loss_scaler : 134217728 
DLL 2021-12-18 03:44:01.443790 - Iteration: 29  throughput_train : 495.051 seq/s mlm_loss : 10.4349  nsp_loss : 0.6963  total_loss : 11.1312  avg_loss_step : 11.1058  learning_rate : 1.0125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:46:18.271681 - Iteration: 30  throughput_train : 494.353 seq/s mlm_loss : 10.4094  nsp_loss : 0.6748  total_loss : 11.0842  avg_loss_step : 11.1036  learning_rate : 1.05e-05  loss_scaler : 134217728 INFO:tensorflow:loss = 11.119477, step = 32 (1293.978 sec)
I1218 03:55:08.155463 140063694366528 basic_session_run_hooks.py:260] loss = 11.119477, step = 32 (1293.978 sec)
INFO:tensorflow:loss = 11.031046, step = 42 (1291.804 sec)
I1218 04:16:39.959914 140063694366528 basic_session_run_hooks.py:260] loss = 11.031046, step = 42 (1291.804 sec)
INFO:tensorflow:loss = 11.010572, step = 51 (1293.008 sec)
I1218 04:38:12.968077 140063694366528 basic_session_run_hooks.py:260] loss = 11.010572, step = 51 (1293.008 sec)
INFO:tensorflow:loss = 10.929038, step = 61 (1293.912 sec)
I1218 04:59:46.880349 140063694366528 basic_session_run_hooks.py:260] loss = 10.929038, step = 61 (1293.912 sec)

DLL 2021-12-18 03:48:34.839119 - Iteration: 31  throughput_train : 495.303 seq/s mlm_loss : 10.4146  nsp_loss : 0.6724  total_loss : 11.0869  avg_loss_step : 11.0992  learning_rate : 1.0875e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:50:51.371589 - Iteration: 32  throughput_train : 495.425 seq/s mlm_loss : 10.3795  nsp_loss : 0.6988  total_loss : 11.0783  avg_loss_step : 11.0961  learning_rate : 1.125e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:53:08.070101 - Iteration: 33  throughput_train : 494.834 seq/s mlm_loss : 10.3924  nsp_loss : 0.6742  total_loss : 11.0666  avg_loss_step : 11.0926  learning_rate : 1.1625e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:55:24.580536 - Iteration: 34  throughput_train : 495.510 seq/s mlm_loss : 10.4058  nsp_loss : 0.6764  total_loss : 11.0822  avg_loss_step : 11.0900  learning_rate : 1.2e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:57:41.303308 - Iteration: 35  throughput_train : 494.727 seq/s mlm_loss : 10.3852  nsp_loss : 0.7199  total_loss : 11.1051  avg_loss_step : 11.0857  learning_rate : 1.2375001e-05  loss_scaler : 134217728 
DLL 2021-12-18 03:59:58.031432 - Iteration: 36  throughput_train : 494.716 seq/s mlm_loss : 10.4086  nsp_loss : 0.6722  total_loss : 11.0809  avg_loss_step : 11.0824  learning_rate : 1.2750001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:02:14.565622 - Iteration: 37  throughput_train : 495.425 seq/s mlm_loss : 10.3926  nsp_loss : 0.6747  total_loss : 11.0673  avg_loss_step : 11.0780  learning_rate : 1.3125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:04:30.666294 - Iteration: 38  throughput_train : 496.984 seq/s mlm_loss : 10.3981  nsp_loss : 0.6722  total_loss : 11.0703  avg_loss_step : 11.0718  learning_rate : 1.3500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:06:47.169886 - Iteration: 39  throughput_train : 495.529 seq/s mlm_loss : 10.4098  nsp_loss : 0.6903  total_loss : 11.1001  avg_loss_step : 11.0707  learning_rate : 1.3875e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:09:03.476869 - Iteration: 40  throughput_train : 496.237 seq/s mlm_loss : 10.3795  nsp_loss : 0.6645  total_loss : 11.0441  avg_loss_step : 11.0657  learning_rate : 1.425e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:11:19.923369 - Iteration: 41  throughput_train : 495.734 seq/s mlm_loss : 10.3845  nsp_loss : 0.6957  total_loss : 11.0802  avg_loss_step : 11.0615  learning_rate : 1.4625e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:13:36.186112 - Iteration: 42  throughput_train : 496.394 seq/s mlm_loss : 10.3646  nsp_loss : 0.6920  total_loss : 11.0565  avg_loss_step : 11.0571  learning_rate : 1.50000005e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:15:52.273570 - Iteration: 43  throughput_train : 497.032 seq/s mlm_loss : 10.3573  nsp_loss : 0.6827  total_loss : 11.0400  avg_loss_step : 11.0534  learning_rate : 1.5375e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:18:08.926331 - Iteration: 44  throughput_train : 495.001 seq/s mlm_loss : 10.3514  nsp_loss : 0.6783  total_loss : 11.0297  avg_loss_step : 11.0484  learning_rate : 1.575e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:20:25.393563 - Iteration: 45  throughput_train : 495.657 seq/s mlm_loss : 10.3550  nsp_loss : 0.6940  total_loss : 11.0490  avg_loss_step : 11.0435  learning_rate : 1.6125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:22:41.934919 - Iteration: 46  throughput_train : 495.398 seq/s mlm_loss : 10.3669  nsp_loss : 0.6649  total_loss : 11.0318  avg_loss_step : 11.0389  learning_rate : 1.65e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:24:58.571784 - Iteration: 47  throughput_train : 495.043 seq/s mlm_loss : 10.3344  nsp_loss : 0.7133  total_loss : 11.0477  avg_loss_step : 11.0347  learning_rate : 1.6875001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:27:15.063631 - Iteration: 48  throughput_train : 495.570 seq/s mlm_loss : 10.3246  nsp_loss : 0.6898  total_loss : 11.0144  avg_loss_step : 11.0288  learning_rate : 1.725e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:29:31.621962 - Iteration: 49  throughput_train : 495.326 seq/s mlm_loss : 10.3185  nsp_loss : 0.7136  total_loss : 11.0321  avg_loss_step : 11.0247  learning_rate : 1.7625001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:31:48.022306 - Iteration: 50  throughput_train : 495.902 seq/s mlm_loss : 10.3099  nsp_loss : 0.6981  total_loss : 11.0079  avg_loss_step : 11.0191  learning_rate : 1.8e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:34:04.478276 - Iteration: 51  throughput_train : 495.706 seq/s mlm_loss : 10.3203  nsp_loss : 0.6820  total_loss : 11.0023  avg_loss_step : 11.0145  learning_rate : 1.8375e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:36:21.043601 - Iteration: 52  throughput_train : 495.315 seq/s mlm_loss : 10.3347  nsp_loss : 0.7035  total_loss : 11.0382  avg_loss_step : 11.0093  learning_rate : 1.875e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:38:37.637513 - Iteration: 53  throughput_train : 495.196 seq/s mlm_loss : 10.3111  nsp_loss : 0.6734  total_loss : 10.9845  avg_loss_step : 11.0024  learning_rate : 1.9125e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:40:54.078232 - Iteration: 54  throughput_train : 495.749 seq/s mlm_loss : 10.3301  nsp_loss : 0.6710  total_loss : 11.0011  avg_loss_step : 10.9983  learning_rate : 1.9500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:43:10.924538 - Iteration: 55  throughput_train : 494.284 seq/s mlm_loss : 10.2889  nsp_loss : 0.6589  total_loss : 10.9479  avg_loss_step : 10.9929  learning_rate : 1.9875e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:45:27.896482 - Iteration: 56  throughput_train : 493.825 seq/s mlm_loss : 10.3118  nsp_loss : 0.6459  total_loss : 10.9577  avg_loss_step : 10.9871  learning_rate : 2.0250001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:47:44.690857 - Iteration: 57  throughput_train : 494.464 seq/s mlm_loss : 10.3083  nsp_loss : 0.6704  total_loss : 10.9787  avg_loss_step : 10.9826  learning_rate : 2.0625e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:50:01.375139 - Iteration: 58  throughput_train : 494.874 seq/s mlm_loss : 10.2687  nsp_loss : 0.6651  total_loss : 10.9338  avg_loss_step : 10.9745  learning_rate : 2.1e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:52:18.099125 - Iteration: 59  throughput_train : 494.733 seq/s mlm_loss : 10.2808  nsp_loss : 0.6873  total_loss : 10.9680  avg_loss_step : 10.9691  learning_rate : 2.1375e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:54:34.594256 - Iteration: 60  throughput_train : 495.550 seq/s mlm_loss : 10.3271  nsp_loss : 0.7012  total_loss : 11.0284  avg_loss_step : 10.9635  learning_rate : 2.175e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:56:50.938646 - Iteration: 61  throughput_train : 496.095 seq/s mlm_loss : 10.2770  nsp_loss : 0.6818  total_loss : 10.9589  avg_loss_step : 10.9568  learning_rate : 2.2125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 04:59:07.450077 - Iteration: 62  throughput_train : 495.485 seq/s mlm_loss : 10.2779  nsp_loss : 0.6851  total_loss : 10.9630  avg_loss_step : 10.9504  learning_rate : 2.25e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:01:23.878559 - Iteration: 63  throughput_train : 495.789 seq/s mlm_loss : 10.2511  nsp_loss : 0.6989  total_loss : 10.9500  avg_loss_step : 10.9450  learning_rate : 2.2875001e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:03:40.685122 - Iteration: 64  throughput_train : 494.421 seq/s mlm_loss : 10.2571  nsp_loss : 0.6989  total_loss : 10.9560  avg_loss_step : 10.9376  learning_rate : 2.325e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:05:57.495572 - Iteration: 65  throughput_train : 494.410 seq/s mlm_loss : 10.2526  nsp_loss : 0.6228  total_loss : 10.8754  avg_loss_step : 10.9308  learning_rate : 2.3625002e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:08:14.097286 - Iteration: 66  throughput_train : 495.160 seq/s mlm_loss : 10.2189  nsp_loss : 0.7106  total_loss : 10.9296  avg_loss_step : 10.9247  learning_rate : 2.4e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:10:30.622838 - Iteration: 67  throughput_train : 495.429 seq/s mlm_loss : 10.2183  nsp_loss : 0.6867  total_loss : 10.9051  avg_loss_step : 10.9182  learning_rate : 2.4375e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:12:47.252558 - Iteration: 68  throughput_train : 495.056 seq/s mlm_loss : 10.2254  nsp_loss : 0.6941  total_loss : 10.9196  avg_loss_step : 10.9114  learning_rate : 2.4750001e-05  loss_scaler : 134217728 INFO:tensorflow:loss = 10.908645, step = 70 (1293.406 sec)
I1218 05:21:20.286800 140063694366528 basic_session_run_hooks.py:260] loss = 10.908645, step = 70 (1293.406 sec)
INFO:tensorflow:loss = 10.80699, step = 80 (1292.621 sec)
I1218 05:42:52.907902 140063694366528 basic_session_run_hooks.py:260] loss = 10.80699, step = 80 (1292.621 sec)
INFO:tensorflow:loss = 10.685196, step = 89 (1294.347 sec)
I1218 06:04:27.254507 140063694366528 basic_session_run_hooks.py:260] loss = 10.685196, step = 89 (1294.347 sec)
INFO:tensorflow:Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1/model.ckpt.
I1218 06:05:08.549738 140063694366528 basic_session_run_hooks.py:606] Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218022449/phase_1/model.ckpt.
INFO:tensorflow:Loss for final step: 10.701468.
I1218 06:05:13.726865 140063694366528 estimator.py:371] Loss for final step: 10.701468.
INFO:tensorflow:-----------------------------
I1218 06:05:13.728926 140063694366528 run_pretraining.py:644] -----------------------------
INFO:tensorflow:Total Training Time = 13222.03 for Sentences = 6082560
I1218 06:05:13.729007 140063694366528 run_pretraining.py:646] Total Training Time = 13222.03 for Sentences = 6082560
INFO:tensorflow:Total Training Time W/O Overhead = 11599.07 for Sentences = 5406720
I1218 06:05:13.729077 140063694366528 run_pretraining.py:648] Total Training Time W/O Overhead = 11599.07 for Sentences = 5406720
INFO:tensorflow:Throughput Average (sentences/sec) with overhead = 460.03
I1218 06:05:13.729129 140063694366528 run_pretraining.py:649] Throughput Average (sentences/sec) with overhead = 460.03
INFO:tensorflow:Throughput Average (sentences/sec) = 466.13
I1218 06:05:13.729202 140063694366528 run_pretraining.py:650] Throughput Average (sentences/sec) = 466.13
INFO:tensorflow:-----------------------------
I1218 06:05:13.729372 140063694366528 run_pretraining.py:652] -----------------------------

DLL 2021-12-18 05:15:03.883333 - Iteration: 69  throughput_train : 495.060 seq/s mlm_loss : 10.1919  nsp_loss : 0.7114  total_loss : 10.9033  avg_loss_step : 10.9038  learning_rate : 2.5125e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:17:20.477241 - Iteration: 70  throughput_train : 495.187 seq/s mlm_loss : 10.1968  nsp_loss : 0.6859  total_loss : 10.8827  avg_loss_step : 10.8973  learning_rate : 2.5500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:19:36.762562 - Iteration: 71  throughput_train : 496.306 seq/s mlm_loss : 10.2065  nsp_loss : 0.6992  total_loss : 10.9057  avg_loss_step : 10.8904  learning_rate : 2.5875e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:21:53.287331 - Iteration: 72  throughput_train : 495.450 seq/s mlm_loss : 10.1789  nsp_loss : 0.6957  total_loss : 10.8746  avg_loss_step : 10.8847  learning_rate : 2.6250002e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:24:09.683324 - Iteration: 73  throughput_train : 495.902 seq/s mlm_loss : 10.1978  nsp_loss : 0.6768  total_loss : 10.8746  avg_loss_step : 10.8752  learning_rate : 2.6625e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:26:25.944333 - Iteration: 74  throughput_train : 496.399 seq/s mlm_loss : 10.1647  nsp_loss : 0.7034  total_loss : 10.8681  avg_loss_step : 10.8682  learning_rate : 2.7000002e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:28:42.404491 - Iteration: 75  throughput_train : 495.674 seq/s mlm_loss : 10.1571  nsp_loss : 0.6894  total_loss : 10.8465  avg_loss_step : 10.8613  learning_rate : 2.7375001e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:30:58.841907 - Iteration: 76  throughput_train : 495.761 seq/s mlm_loss : 10.1955  nsp_loss : 0.6765  total_loss : 10.8720  avg_loss_step : 10.8537  learning_rate : 2.775e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:33:15.279514 - Iteration: 77  throughput_train : 495.763 seq/s mlm_loss : 10.1535  nsp_loss : 0.6840  total_loss : 10.8375  avg_loss_step : 10.8458  learning_rate : 2.8125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:35:31.779979 - Iteration: 78  throughput_train : 495.533 seq/s mlm_loss : 10.1792  nsp_loss : 0.7145  total_loss : 10.8937  avg_loss_step : 10.8370  learning_rate : 2.85e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:37:48.359694 - Iteration: 79  throughput_train : 495.243 seq/s mlm_loss : 10.1532  nsp_loss : 0.6957  total_loss : 10.8489  avg_loss_step : 10.8297  learning_rate : 2.8875002e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:40:05.138506 - Iteration: 80  throughput_train : 494.516 seq/s mlm_loss : 10.1543  nsp_loss : 0.6798  total_loss : 10.8341  avg_loss_step : 10.8212  learning_rate : 2.925e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:42:21.655357 - Iteration: 81  throughput_train : 495.460 seq/s mlm_loss : 10.1597  nsp_loss : 0.7013  total_loss : 10.8609  avg_loss_step : 10.8144  learning_rate : 2.9625002e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:44:38.493024 - Iteration: 82  throughput_train : 494.311 seq/s mlm_loss : 10.1025  nsp_loss : 0.6645  total_loss : 10.7670  avg_loss_step : 10.8085  learning_rate : 3.0000001e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:46:55.017219 - Iteration: 83  throughput_train : 495.433 seq/s mlm_loss : 10.1234  nsp_loss : 0.6611  total_loss : 10.7845  avg_loss_step : 10.7992  learning_rate : 3.0375e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:49:11.797602 - Iteration: 84  throughput_train : 494.513 seq/s mlm_loss : 10.0939  nsp_loss : 0.6890  total_loss : 10.7829  avg_loss_step : 10.7912  learning_rate : 3.075e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:51:28.060654 - Iteration: 85  throughput_train : 496.391 seq/s mlm_loss : 10.1399  nsp_loss : 0.6671  total_loss : 10.8070  avg_loss_step : 10.7833  learning_rate : 3.1125e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:53:44.844125 - Iteration: 86  throughput_train : 494.496 seq/s mlm_loss : 10.0437  nsp_loss : 0.6683  total_loss : 10.7120  avg_loss_step : 10.7762  learning_rate : 3.15e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:56:01.363083 - Iteration: 87  throughput_train : 495.459 seq/s mlm_loss : 10.0793  nsp_loss : 0.6987  total_loss : 10.7780  avg_loss_step : 10.7673  learning_rate : 3.1875003e-05  loss_scaler : 134217728 
DLL 2021-12-18 05:58:18.137221 - Iteration: 88  throughput_train : 494.538 seq/s mlm_loss : 10.0822  nsp_loss : 0.6662  total_loss : 10.7484  avg_loss_step : 10.7606  learning_rate : 3.2250002e-05  loss_scaler : 134217728 
DLL 2021-12-18 06:00:34.591084 - Iteration: 89  throughput_train : 495.696 seq/s mlm_loss : 10.0334  nsp_loss : 0.6771  total_loss : 10.7105  avg_loss_step : 10.7509  learning_rate : 3.2625e-05  loss_scaler : 134217728 
DLL 2021-12-18 06:02:51.586235 - Iteration: 90  throughput_train : 493.743 seq/s mlm_loss : 10.0599  nsp_loss : 0.6690  total_loss : 10.7288  avg_loss_step : 10.7419  learning_rate : 3.3e-05  loss_scaler : 134217728 
DLL 2021-12-18 06:05:08.548386 - Iteration: 91  throughput_train : 495.163 seq/s mlm_loss : 10.0289  nsp_loss : 0.6725  total_loss : 10.7015  avg_loss_step : 10.7340  learning_rate : 3.3375e-05  loss_scaler : 134217728 
DLL 2021-12-18 06:05:13.729255 -  throughput_train : 466.134 seq/s
