+ batch_size=48
+ num_gpus=1
+ precision=fp32
++ expr 67584 / 48 / 1
+ num_accumulation_steps_phase1=1408
+ train_steps=100
+ bert_model=base
+ bash scripts/run_pretraining_lamb.sh 48 64 8 7.5e-4 5e-4 fp32 true 1 2000 200 100 200 1408 512 base
Container nvidia build =  13409399
Saving checkpoints to /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301
Logs written to /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768.211217135301.log
Container nvidia build =  13409399
XLA activated
2021-12-17 13:53:01.638528: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1217 13:53:03.223354 139970955851584 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

INFO:tensorflow:Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4b1ee7b470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I1217 13:53:04.019237 139970955851584 estimator.py:212] Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4b1ee7b470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f4b1eedc1e0>) includes params argument, but params are not passed to Estimator.
W1217 13:53:04.019910 139970955851584 model_fn.py:630] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f4b1eedc1e0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:***** Running training *****
I1217 13:53:04.020318 139970955851584 run_pretraining.py:625] ***** Running training *****
INFO:tensorflow:  Batch size = 48
I1217 13:53:04.020382 139970955851584 run_pretraining.py:626]   Batch size = 48
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

W1217 13:53:04.112752 139970955851584 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

INFO:tensorflow:Calling model_fn.
I1217 13:53:04.213318 139970955851584 estimator.py:1148] Calling model_fn.
INFO:tensorflow:*** Features ***
I1217 13:53:04.213461 139970955851584 run_pretraining.py:258] *** Features ***
INFO:tensorflow:  name = input_ids, shape = (48, 128)
I1217 13:53:04.213565 139970955851584 run_pretraining.py:260]   name = input_ids, shape = (48, 128)
INFO:tensorflow:  name = input_mask, shape = (48, 128)
I1217 13:53:04.213639 139970955851584 run_pretraining.py:260]   name = input_mask, shape = (48, 128)
INFO:tensorflow:  name = masked_lm_ids, shape = (48, 20)
I1217 13:53:04.213704 139970955851584 run_pretraining.py:260]   name = masked_lm_ids, shape = (48, 20)
INFO:tensorflow:  name = masked_lm_positions, shape = (48, 20)
I1217 13:53:04.213768 139970955851584 run_pretraining.py:260]   name = masked_lm_positions, shape = (48, 20)
INFO:tensorflow:  name = masked_lm_weights, shape = (48, 20)
I1217 13:53:04.213829 139970955851584 run_pretraining.py:260]   name = masked_lm_weights, shape = (48, 20)
INFO:tensorflow:  name = next_sentence_labels, shape = (48, 1)
I1217 13:53:04.213889 139970955851584 run_pretraining.py:260]   name = next_sentence_labels, shape = (48, 1)
INFO:tensorflow:  name = segment_ids, shape = (48, 128)
I1217 13:53:04.213947 139970955851584 run_pretraining.py:260]   name = segment_ids, shape = (48, 128)
WARNING:tensorflow:From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W1217 13:53:04.214122 139970955851584 module_wrapper.py:139] From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W1217 13:53:04.215110 139970955851584 module_wrapper.py:139] From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W1217 13:53:05.656220 139970955851584 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

W1217 13:53:08.532627 139970955851584 module_wrapper.py:139] From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

INFO:tensorflow:Done calling model_fn.
I1217 13:53:15.716890 139970955851584 estimator.py:1150] Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
I1217 13:53:15.718017 139970955851584 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
I1217 13:53:19.596048 139970955851584 monitored_session.py:240] Graph was finalized.
2021-12-17 13:53:19.610887: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599995000 Hz
2021-12-17 13:53:19.616079: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x582a7f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-12-17 13:53:19.616120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-12-17 13:53:19.624022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-12-17 13:53:21.088380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.093451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.128530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.138956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.152010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.167837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.185840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.206572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.209182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57afc70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-12-17 13:53:21.209207: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209213: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209219: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209224: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209229: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209233: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209239: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.209244: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-17 13:53:21.219900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.221896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:03:00.0
2021-12-17 13:53:21.221984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.223920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 1 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:04:00.0
2021-12-17 13:53:21.223994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.225942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 2 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:05:00.0
2021-12-17 13:53:21.226009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.227949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 3 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:06:00.0
2021-12-17 13:53:21.228009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.229941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 4 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:07:00.0
2021-12-17 13:53:21.230001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.232080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 5 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:08:00.0
2021-12-17 13:53:21.232160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.234093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 6 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:09:00.0
2021-12-17 13:53:21.234163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.236101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 7 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:0a:00.0
2021-12-17 13:53:21.236146: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-17 13:53:21.239246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-17 13:53:21.240585: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-12-17 13:53:21.240909: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-12-17 13:53:21.243647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-12-17 13:53:21.244223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2021-12-17 13:53:21.244421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-17 13:53:21.244504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.246535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.248513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.250530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.252503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.254488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.256499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.258484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.260473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.262463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.264440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.266411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.268380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.270360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.272325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.274286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:21.276216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-12-17 13:53:21.276251: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-17 13:53:23.730182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-12-17 13:53:23.730234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 1 2 3 4 5 6 7 
2021-12-17 13:53:23.730246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N Y Y Y N N N Y 
2021-12-17 13:53:23.730252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 1:   Y N Y Y N N Y N 
2021-12-17 13:53:23.730257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 2:   Y Y N Y N Y N N 
2021-12-17 13:53:23.730261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 3:   Y Y Y N Y N N N 
2021-12-17 13:53:23.730266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 4:   N N N Y N Y Y Y 
2021-12-17 13:53:23.730271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 5:   N N Y N Y N Y Y 
2021-12-17 13:53:23.730275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 6:   N Y N N Y Y N Y 
2021-12-17 13:53:23.730279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 7:   Y N N N Y Y Y N 
2021-12-17 13:53:23.730784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.732889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.734932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.736958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.738986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.740988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.742992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.744988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.746999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.749002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30166 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)
2021-12-17 13:53:23.749449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.751445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30166 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-12-17 13:53:23.751735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.753716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30166 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:05:00.0, compute capability: 7.0)
2021-12-17 13:53:23.753984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.755973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 30166 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0)
2021-12-17 13:53:23.756238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.758212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 30166 MB memory) -> physical GPU (device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0)
2021-12-17 13:53:23.758465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.760441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 30166 MB memory) -> physical GPU (device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)
2021-12-17 13:53:23.760717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.762695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 30166 MB memory) -> physical GPU (device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0)
2021-12-17 13:53:23.762963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-17 13:53:23.764943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 30166 MB memory) -> physical GPU (device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0)
2021-12-17 13:53:29.183061: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1648] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
INFO:tensorflow:Running local_init_op.
I1217 13:53:33.922851 139970955851584 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1217 13:53:34.470591 139970955851584 session_manager.py:502] Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1/model.ckpt.
I1217 13:53:44.540743 139970955851584 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1/model.ckpt.
WARNING:tensorflow:From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

W1217 13:53:51.687354 139970955851584 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

2021-12-17 13:54:06.184637: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-17 13:54:06.754215: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-17 13:54:27.392633: I tensorflow/compiler/jit/xla_compilation_cache.cc:243] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:loss = 11.181321, step = 0
I1217 13:54:32.973915 139970955851584 basic_session_run_hooks.py:262] loss = 11.181321, step = 0
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1217 13:55:04.188328 139970955851584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1217 13:55:04.503034 139970955851584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1217 13:55:04.808198 139970955851584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1217 13:55:05.115971 139970955851584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1217 13:55:05.424314 139970955851584 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
INFO:tensorflow:loss = 11.210768, step = 7 (3202.024 sec)
I1217 14:47:54.997977 139970955851584 basic_session_run_hooks.py:260] loss = 11.210768, step = 7 (3202.024 sec)
INFO:tensorflow:loss = 11.218951, step = 14 (3118.547 sec)
I1217 15:39:53.545338 139970955851584 basic_session_run_hooks.py:260] loss = 11.218951, step = 14 (3118.547 sec)
INFO:tensorflow:loss = 11.153883, step = 21 (3118.247 sec)
I1217 16:31:51.792120 139970955851584 basic_session_run_hooks.py:260] loss = 11.153883, step = 21 (3118.247 sec)
INFO:tensorflow:loss = 11.0950775, step = 28 (3119.122 sec)
I1217 17:23:50.914310 139970955851584 basic_session_run_hooks.py:260] loss = 11.0950775, step = 28 (3119.122 sec)
INFO:tensorflow:loss = 11.082014, step = 35 (3118.631 sec)
I1217 18:15:49.544996 139970955851584 basic_session_run_hooks.py:260] loss = 11.082014, step = 35 (3118.631 sec)
decayed_learning_rate_at_crossover_point = 7.500000e-04, adjusted_init_lr = 7.500000e-04
Initializing LAMB Optimizer
Skipping time record for  1  due to checkpoint-saving/warmup overhead
DLL 2021-12-17 14:02:46.201349 - Iteration: 2  throughput_train : 126.645 seq/s mlm_loss : 10.4625  nsp_loss : 0.6852  total_loss : 11.1477  avg_loss_step : 11.1758  learning_rate : 0.0 
Skipping time record for  2  due to checkpoint-saving/warmup overhead
DLL 2021-12-17 14:10:04.725116 - Iteration: 3  throughput_train : 154.182 seq/s mlm_loss : 10.4679  nsp_loss : 0.7076  total_loss : 11.1755  avg_loss_step : 11.1756  learning_rate : 3.75e-07 
Skipping time record for  3  due to checkpoint-saving/warmup overhead
DLL 2021-12-17 14:17:23.472059 - Iteration: 4  throughput_train : 154.103 seq/s mlm_loss : 10.4498  nsp_loss : 0.7438  total_loss : 11.1936  avg_loss_step : 11.1757  learning_rate : 7.5e-07 
Skipping time record for  4  due to checkpoint-saving/warmup overhead
DLL 2021-12-17 14:24:42.374617 - Iteration: 5  throughput_train : 154.050 seq/s mlm_loss : 10.4317  nsp_loss : 0.7624  total_loss : 11.1941  avg_loss_step : 11.1774  learning_rate : 1.125e-06 
Skipping time record for  5  due to checkpoint-saving/warmup overhead
DLL 2021-12-17 14:32:01.149562 - Iteration: 6  throughput_train : 154.094 seq/s mlm_loss : 10.4620  nsp_loss : 0.7504  total_loss : 11.2123  avg_loss_step : 11.1751  learning_rate : 1.5e-06 
DLL 2021-12-17 14:39:20.000911 - Iteration: 7  throughput_train : 154.065 seq/s mlm_loss : 10.4741  nsp_loss : 0.7173  total_loss : 11.1914  avg_loss_step : 11.1739  learning_rate : 1.8750001e-06 
DLL 2021-12-17 14:46:38.680906 - Iteration: 8  throughput_train : 154.126 seq/s mlm_loss : 10.4984  nsp_loss : 0.6994  total_loss : 11.1978  avg_loss_step : 11.1738  learning_rate : 2.25e-06 
DLL 2021-12-17 14:54:28.794667 - Iteration: 9  throughput_train : 143.816 seq/s mlm_loss : 10.4671  nsp_loss : 0.6921  total_loss : 11.1592  avg_loss_step : 11.1723  learning_rate : 2.625e-06 
DLL 2021-12-17 15:01:48.036559 - Iteration: 10  throughput_train : 153.928 seq/s mlm_loss : 10.4549  nsp_loss : 0.7267  total_loss : 11.1815  avg_loss_step : 11.1712  learning_rate : 3e-06 
DLL 2021-12-17 15:09:07.272280 - Iteration: 11  throughput_train : 153.931 seq/s mlm_loss : 10.4613  nsp_loss : 0.7774  total_loss : 11.2387  avg_loss_step : 11.1705  learning_rate : 3.3750002e-06 
DLL 2021-12-17 15:16:26.223044 - Iteration: 12  throughput_train : 154.030 seq/s mlm_loss : 10.4791  nsp_loss : 0.7125  total_loss : 11.1916  avg_loss_step : 11.1684  learning_rate : 3.7500001e-06 
DLL 2021-12-17 15:23:45.338378 - Iteration: 13  throughput_train : 153.973 seq/s mlm_loss : 10.4271  nsp_loss : 0.7408  total_loss : 11.1680  avg_loss_step : 11.1672  learning_rate : 4.125e-06 
DLL 2021-12-17 15:31:04.272064 - Iteration: 14  throughput_train : 154.036 seq/s mlm_loss : 10.4344  nsp_loss : 0.7157  total_loss : 11.1501  avg_loss_step : 11.1663  learning_rate : 4.5e-06 
DLL 2021-12-17 15:38:23.458941 - Iteration: 15  throughput_train : 153.948 seq/s mlm_loss : 10.4641  nsp_loss : 0.7539  total_loss : 11.2180  avg_loss_step : 11.1635  learning_rate : 4.8750003e-06 
DLL 2021-12-17 15:45:42.475702 - Iteration: 16  throughput_train : 154.008 seq/s mlm_loss : 10.4460  nsp_loss : 0.7048  total_loss : 11.1508  avg_loss_step : 11.1607  learning_rate : 5.25e-06 
DLL 2021-12-17 15:53:01.507971 - Iteration: 17  throughput_train : 154.002 seq/s mlm_loss : 10.4275  nsp_loss : 0.7036  total_loss : 11.1311  avg_loss_step : 11.1592  learning_rate : 5.625e-06 
DLL 2021-12-17 16:00:20.624028 - Iteration: 18  throughput_train : 153.973 seq/s mlm_loss : 10.4128  nsp_loss : 0.6998  total_loss : 11.1126  avg_loss_step : 11.1561  learning_rate : 6e-06 
DLL 2021-12-17 16:07:39.657550 - Iteration: 19  throughput_train : 154.001 seq/s mlm_loss : 10.4591  nsp_loss : 0.7371  total_loss : 11.1961  avg_loss_step : 11.1537  learning_rate : 6.3750003e-06 
DLL 2021-12-17 16:14:58.635349 - Iteration: 20  throughput_train : 154.021 seq/s mlm_loss : 10.4239  nsp_loss : 0.7064  total_loss : 11.1303  avg_loss_step : 11.1509  learning_rate : 6.7500005e-06 
DLL 2021-12-17 16:22:17.647629 - Iteration: 21  throughput_train : 154.009 seq/s mlm_loss : 10.4777  nsp_loss : 0.7036  total_loss : 11.1814  avg_loss_step : 11.1484  learning_rate : 7.125e-06 
DLL 2021-12-17 16:29:36.583568 - Iteration: 22  throughput_train : 154.035 seq/s mlm_loss : 10.4263  nsp_loss : 0.6931  total_loss : 11.1194  avg_loss_step : 11.1450  learning_rate : 7.5000003e-06 
DLL 2021-12-17 16:36:55.873593 - Iteration: 23  throughput_train : 153.913 seq/s mlm_loss : 10.4284  nsp_loss : 0.6920  total_loss : 11.1204  avg_loss_step : 11.1432  learning_rate : 7.875e-06 
DLL 2021-12-17 16:44:14.792881 - Iteration: 24  throughput_train : 154.041 seq/s mlm_loss : 10.4267  nsp_loss : 0.6868  total_loss : 11.1135  avg_loss_step : 11.1400  learning_rate : 8.25e-06 
DLL 2021-12-17 16:51:34.500944 - Iteration: 25  throughput_train : 153.767 seq/s mlm_loss : 10.4313  nsp_loss : 0.7040  total_loss : 11.1353  avg_loss_step : 11.1377  learning_rate : 8.625e-06 
DLL 2021-12-17 16:58:53.585380 - Iteration: 26  throughput_train : 153.984 seq/s mlm_loss : 10.4477  nsp_loss : 0.6985  total_loss : 11.1461  avg_loss_step : 11.1331  learning_rate : 9e-06 
DLL 2021-12-17 17:06:12.764560 - Iteration: 27  throughput_train : 153.952 seq/s mlm_loss : 10.4608  nsp_loss : 0.7159  total_loss : 11.1767  avg_loss_step : 11.1303  learning_rate : 9.375e-06 
DLL 2021-12-17 17:13:31.823485 - Iteration: 28  throughput_train : 153.993 seq/s mlm_loss : 10.4367  nsp_loss : 0.6992  total_loss : 11.1359  avg_loss_step : 11.1269  learning_rate : 9.750001e-06 
DLL 2021-12-17 17:20:50.983404 - Iteration: 29  throughput_train : 153.958 seq/s mlm_loss : 10.4170  nsp_loss : 0.6911  total_loss : 11.1081  avg_loss_step : 11.1225  learning_rate : 1.0125001e-05 
DLL 2021-12-17 17:28:10.044361 - Iteration: 30  throughput_train : 153.992 seq/s mlm_loss : 10.4387  nsp_loss : 0.6986  total_loss : 11.1373  avg_loss_step : 11.1166  learning_rate : 1.05e-05 
DLL 2021-12-17 17:35:29.177830 - Iteration: 31  throughput_train : 153.967 seq/s mlm_loss : 10.4284  nsp_loss : 0.6934  total_loss : 11.1218  avg_loss_step : 11.1153  learning_rate : 1.0875e-05 
DLL 2021-12-17 17:42:48.106500 - Iteration: 32  throughput_train : 154.038 seq/s mlm_loss : 10.4427  nsp_loss : 0.6827  total_loss : 11.1254  avg_loss_step : 11.1111  learning_rate : 1.125e-05 
DLL 2021-12-17 17:50:07.381319 - Iteration: 33  throughput_train : 153.918 seq/s mlm_loss : 10.4029  nsp_loss : 0.6925  total_loss : 11.0954  avg_loss_step : 11.1073  learning_rate : 1.1625e-05 
DLL 2021-12-17 17:57:26.339400 - Iteration: 34  throughput_train : 154.029 seq/s mlm_loss : 10.4283  nsp_loss : 0.6943  total_loss : 11.1226  avg_loss_step : 11.1034  learning_rate : 1.2e-05 
DLL 2021-12-17 18:04:45.498446 - Iteration: 35  throughput_train : 153.959 seq/s mlm_loss : 10.4117  nsp_loss : 0.6898  total_loss : 11.1015  avg_loss_step : 11.0988  learning_rate : 1.2375001e-05 
DLL 2021-12-17 18:12:04.659962 - Iteration: 36  throughput_train : 153.957 seq/s mlm_loss : 10.3914  nsp_loss : 0.6920  total_loss : 11.0834  avg_loss_step : 11.0955  learning_rate : 1.2750001e-05 
DLL 2021-12-17 18:19:23.863635 - Iteration: 37  throughput_train : 153.943 seq/s mlm_loss : 10.4064  nsp_loss : 0.6872  total_loss : 11.0937  avg_loss_step : 11.0919  learning_rate : 1.3125001e-05 
DLL 2021-12-17 18:26:42.861554 - Iteration: 38  throughput_train : 154.014 seq/s mlm_loss : 10.3759  nsp_loss : 0.6859  total_loss : 11.0618  avg_loss_step : 11.0877  learning_rate : 1.3500001e-05 
DLL 2021-12-17 18:34:01.974786 - Iteration: 39  throughput_train : 153.976 seq/s mlm_loss : 10.3950  nsp_loss : 0.6936  total_loss : 11.0886  avg_loss_step : 11.0827  learning_rate : 1.3875e-05 
DLL 2021-12-17 18:41:21.092769 - Iteration: 40  throughput_train : 153.973 seq/s mlm_loss : 10.3552  nsp_loss : 0.6825  total_loss : 11.0376  avg_loss_step : 11.0777  learning_rate : 1.425e-05 
DLL 2021-12-17 18:48:40.113989 - Iteration: 41  throughput_train : 154.006 seq/s mlm_loss : 10.3751  nsp_loss : 0.7072  total_loss : 11.0823  avg_loss_step : 11.0729  learning_rate : 1.4625e-05 INFO:tensorflow:loss = 11.058255, step = 42 (3118.439 sec)
I1217 19:07:47.984011 139970955851584 basic_session_run_hooks.py:260] loss = 11.058255, step = 42 (3118.439 sec)
INFO:tensorflow:loss = 11.019201, step = 49 (3118.228 sec)
I1217 19:59:46.211947 139970955851584 basic_session_run_hooks.py:260] loss = 11.019201, step = 49 (3118.228 sec)
INFO:tensorflow:loss = 10.953591, step = 56 (3118.242 sec)
I1217 20:51:44.453871 139970955851584 basic_session_run_hooks.py:260] loss = 10.953591, step = 56 (3118.242 sec)
INFO:tensorflow:loss = 10.905184, step = 63 (3117.384 sec)
I1217 21:43:41.837925 139970955851584 basic_session_run_hooks.py:260] loss = 10.905184, step = 63 (3117.384 sec)
INFO:tensorflow:loss = 10.877478, step = 71 (3117.990 sec)
I1217 22:35:39.827543 139970955851584 basic_session_run_hooks.py:260] loss = 10.877478, step = 71 (3117.990 sec)
INFO:tensorflow:loss = 10.802317, step = 78 (3118.718 sec)
I1217 23:27:38.545832 139970955851584 basic_session_run_hooks.py:260] loss = 10.802317, step = 78 (3118.718 sec)

DLL 2021-12-17 18:55:59.077996 - Iteration: 42  throughput_train : 154.026 seq/s mlm_loss : 10.3886  nsp_loss : 0.6705  total_loss : 11.0590  avg_loss_step : 11.0689  learning_rate : 1.50000005e-05 
DLL 2021-12-17 19:03:18.144303 - Iteration: 43  throughput_train : 153.991 seq/s mlm_loss : 10.3660  nsp_loss : 0.6954  total_loss : 11.0614  avg_loss_step : 11.0638  learning_rate : 1.5375e-05 
DLL 2021-12-17 19:10:37.307248 - Iteration: 44  throughput_train : 153.957 seq/s mlm_loss : 10.3741  nsp_loss : 0.6769  total_loss : 11.0510  avg_loss_step : 11.0586  learning_rate : 1.575e-05 
DLL 2021-12-17 19:17:56.246209 - Iteration: 45  throughput_train : 154.034 seq/s mlm_loss : 10.3714  nsp_loss : 0.6723  total_loss : 11.0437  avg_loss_step : 11.0538  learning_rate : 1.6125001e-05 
DLL 2021-12-17 19:25:15.226844 - Iteration: 46  throughput_train : 154.021 seq/s mlm_loss : 10.3681  nsp_loss : 0.6837  total_loss : 11.0518  avg_loss_step : 11.0477  learning_rate : 1.65e-05 
DLL 2021-12-17 19:32:34.514534 - Iteration: 47  throughput_train : 153.914 seq/s mlm_loss : 10.3675  nsp_loss : 0.6624  total_loss : 11.0299  avg_loss_step : 11.0441  learning_rate : 1.6875001e-05 
DLL 2021-12-17 19:39:53.532660 - Iteration: 48  throughput_train : 154.008 seq/s mlm_loss : 10.3378  nsp_loss : 0.6592  total_loss : 10.9970  avg_loss_step : 11.0385  learning_rate : 1.725e-05 
DLL 2021-12-17 19:47:12.655623 - Iteration: 49  throughput_train : 153.970 seq/s mlm_loss : 10.3470  nsp_loss : 0.6771  total_loss : 11.0241  avg_loss_step : 11.0320  learning_rate : 1.7625001e-05 
DLL 2021-12-17 19:54:31.554852 - Iteration: 50  throughput_train : 154.049 seq/s mlm_loss : 10.3176  nsp_loss : 0.6867  total_loss : 11.0043  avg_loss_step : 11.0268  learning_rate : 1.8e-05 
DLL 2021-12-17 20:01:50.618510 - Iteration: 51  throughput_train : 153.991 seq/s mlm_loss : 10.3791  nsp_loss : 0.6925  total_loss : 11.0716  avg_loss_step : 11.0217  learning_rate : 1.8375e-05 
DLL 2021-12-17 20:09:09.529050 - Iteration: 52  throughput_train : 154.045 seq/s mlm_loss : 10.3059  nsp_loss : 0.6822  total_loss : 10.9882  avg_loss_step : 11.0163  learning_rate : 1.875e-05 
DLL 2021-12-17 20:16:28.550743 - Iteration: 53  throughput_train : 154.006 seq/s mlm_loss : 10.3219  nsp_loss : 0.6907  total_loss : 11.0126  avg_loss_step : 11.0098  learning_rate : 1.9125e-05 
DLL 2021-12-17 20:23:47.421843 - Iteration: 54  throughput_train : 154.058 seq/s mlm_loss : 10.3030  nsp_loss : 0.6858  total_loss : 10.9889  avg_loss_step : 11.0052  learning_rate : 1.9500001e-05 
DLL 2021-12-17 20:31:06.532466 - Iteration: 55  throughput_train : 153.975 seq/s mlm_loss : 10.3059  nsp_loss : 0.6923  total_loss : 10.9982  avg_loss_step : 10.9990  learning_rate : 1.9875e-05 
DLL 2021-12-17 20:38:25.864303 - Iteration: 56  throughput_train : 153.898 seq/s mlm_loss : 10.3330  nsp_loss : 0.6883  total_loss : 11.0214  avg_loss_step : 10.9936  learning_rate : 2.0250001e-05 
DLL 2021-12-17 20:45:44.921077 - Iteration: 57  throughput_train : 153.994 seq/s mlm_loss : 10.3193  nsp_loss : 0.6625  total_loss : 10.9818  avg_loss_step : 10.9861  learning_rate : 2.0625e-05 
DLL 2021-12-17 20:53:03.979447 - Iteration: 58  throughput_train : 153.994 seq/s mlm_loss : 10.2980  nsp_loss : 0.6731  total_loss : 10.9711  avg_loss_step : 10.9817  learning_rate : 2.1e-05 
DLL 2021-12-17 21:00:23.132218 - Iteration: 59  throughput_train : 153.960 seq/s mlm_loss : 10.3077  nsp_loss : 0.6436  total_loss : 10.9514  avg_loss_step : 10.9741  learning_rate : 2.1375e-05 
DLL 2021-12-17 21:07:41.915118 - Iteration: 60  throughput_train : 154.089 seq/s mlm_loss : 10.2814  nsp_loss : 0.6823  total_loss : 10.9636  avg_loss_step : 10.9680  learning_rate : 2.175e-05 
DLL 2021-12-17 21:15:00.943494 - Iteration: 61  throughput_train : 154.003 seq/s mlm_loss : 10.2668  nsp_loss : 0.6841  total_loss : 10.9509  avg_loss_step : 10.9614  learning_rate : 2.2125001e-05 
DLL 2021-12-17 21:22:19.744711 - Iteration: 62  throughput_train : 154.083 seq/s mlm_loss : 10.2021  nsp_loss : 0.6687  total_loss : 10.8708  avg_loss_step : 10.9544  learning_rate : 2.25e-05 
DLL 2021-12-17 21:29:38.650772 - Iteration: 63  throughput_train : 154.046 seq/s mlm_loss : 10.2605  nsp_loss : 0.6650  total_loss : 10.9255  avg_loss_step : 10.9455  learning_rate : 2.2875001e-05 
DLL 2021-12-17 21:36:57.597080 - Iteration: 64  throughput_train : 154.032 seq/s mlm_loss : 10.3022  nsp_loss : 0.6662  total_loss : 10.9684  avg_loss_step : 10.9409  learning_rate : 2.325e-05 
DLL 2021-12-17 21:44:16.479037 - Iteration: 65  throughput_train : 154.055 seq/s mlm_loss : 10.2492  nsp_loss : 0.7024  total_loss : 10.9517  avg_loss_step : 10.9321  learning_rate : 2.3625002e-05 
DLL 2021-12-17 21:51:35.298618 - Iteration: 66  throughput_train : 154.077 seq/s mlm_loss : 10.2585  nsp_loss : 0.7121  total_loss : 10.9705  avg_loss_step : 10.9267  learning_rate : 2.4e-05 
DLL 2021-12-17 21:58:54.235752 - Iteration: 67  throughput_train : 154.035 seq/s mlm_loss : 10.2276  nsp_loss : 0.6525  total_loss : 10.8801  avg_loss_step : 10.9194  learning_rate : 2.4375e-05 
DLL 2021-12-17 22:06:13.470471 - Iteration: 68  throughput_train : 153.932 seq/s mlm_loss : 10.2511  nsp_loss : 0.6376  total_loss : 10.8888  avg_loss_step : 10.9110  learning_rate : 2.4750001e-05 
DLL 2021-12-17 22:13:32.449425 - Iteration: 69  throughput_train : 154.020 seq/s mlm_loss : 10.2045  nsp_loss : 0.6902  total_loss : 10.8947  avg_loss_step : 10.9065  learning_rate : 2.5125e-05 
DLL 2021-12-17 22:20:51.482543 - Iteration: 70  throughput_train : 154.002 seq/s mlm_loss : 10.2276  nsp_loss : 0.6318  total_loss : 10.8594  avg_loss_step : 10.8975  learning_rate : 2.5500001e-05 
DLL 2021-12-17 22:28:10.547374 - Iteration: 71  throughput_train : 153.991 seq/s mlm_loss : 10.1539  nsp_loss : 0.6376  total_loss : 10.7915  avg_loss_step : 10.8894  learning_rate : 2.5875e-05 
DLL 2021-12-17 22:35:29.547725 - Iteration: 72  throughput_train : 154.013 seq/s mlm_loss : 10.1907  nsp_loss : 0.7269  total_loss : 10.9176  avg_loss_step : 10.8832  learning_rate : 2.6250002e-05 
DLL 2021-12-17 22:42:48.633886 - Iteration: 73  throughput_train : 153.983 seq/s mlm_loss : 10.1997  nsp_loss : 0.6738  total_loss : 10.8735  avg_loss_step : 10.8771  learning_rate : 2.6625e-05 
DLL 2021-12-17 22:50:07.607726 - Iteration: 74  throughput_train : 154.023 seq/s mlm_loss : 10.2179  nsp_loss : 0.6629  total_loss : 10.8807  avg_loss_step : 10.8680  learning_rate : 2.7000002e-05 
DLL 2021-12-17 22:57:26.638288 - Iteration: 75  throughput_train : 154.002 seq/s mlm_loss : 10.1830  nsp_loss : 0.6826  total_loss : 10.8656  avg_loss_step : 10.8601  learning_rate : 2.7375001e-05 
DLL 2021-12-17 23:04:45.760323 - Iteration: 76  throughput_train : 153.971 seq/s mlm_loss : 10.1984  nsp_loss : 0.6668  total_loss : 10.8651  avg_loss_step : 10.8507  learning_rate : 2.775e-05 
DLL 2021-12-17 23:12:04.984083 - Iteration: 77  throughput_train : 153.936 seq/s mlm_loss : 10.1553  nsp_loss : 0.6938  total_loss : 10.8491  avg_loss_step : 10.8452  learning_rate : 2.8125001e-05 
DLL 2021-12-17 23:19:24.012650 - Iteration: 78  throughput_train : 154.003 seq/s mlm_loss : 10.1607  nsp_loss : 0.6886  total_loss : 10.8493  avg_loss_step : 10.8341  learning_rate : 2.85e-05 
DLL 2021-12-17 23:26:43.348547 - Iteration: 79  throughput_train : 153.896 seq/s mlm_loss : 10.1298  nsp_loss : 0.7181  total_loss : 10.8478  avg_loss_step : 10.8269  learning_rate : 2.8875002e-05 
DLL 2021-12-17 23:34:02.591136 - Iteration: 80  throughput_train : 153.929 seq/s mlm_loss : 10.1287  nsp_loss : 0.7050  total_loss : 10.8337  avg_loss_step : 10.8180  learning_rate : 2.925e-05 
DLL 2021-12-17 23:41:21.761782 - Iteration: 81  throughput_train : 153.954 seq/s mlm_loss : 10.0981  nsp_loss : 0.7089  total_loss : 10.8071  avg_loss_step : 10.8112  learning_rate : 2.9625002e-05 
DLL 2021-12-17 23:48:40.678754 - Iteration: 82  throughput_train : 154.043 seq/s mlm_loss : 10.1091  nsp_loss : 0.6708  total_loss : 10.7799  avg_loss_step : 10.8034  learning_rate : 3.0000001e-05 
DLL 2021-12-17 23:55:59.738955 - Iteration: 83  throughput_train : 153.992 seq/s mlm_loss : 10.1296  nsp_loss : 0.6606  total_loss : 10.7903  avg_loss_step : 10.7939  learning_rate : 3.0375e-05 INFO:tensorflow:loss = 10.68548, step = 85 (3118.837 sec)
I1218 00:19:37.382536 139970955851584 basic_session_run_hooks.py:260] loss = 10.68548, step = 85 (3118.837 sec)
INFO:tensorflow:Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1/model.ckpt.
I1218 00:54:33.236971 139970955851584 basic_session_run_hooks.py:606] Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp32_gbs167584_gbs232768_211217135301/phase_1/model.ckpt.
INFO:tensorflow:Loss for final step: 10.702559.
I1218 00:54:37.778572 139970955851584 estimator.py:371] Loss for final step: 10.702559.
INFO:tensorflow:-----------------------------
I1218 00:54:37.780992 139970955851584 run_pretraining.py:644] -----------------------------
INFO:tensorflow:Total Training Time = 39693.76 for Sentences = 6082560
I1218 00:54:37.781069 139970955851584 run_pretraining.py:646] Total Training Time = 39693.76 for Sentences = 6082560
INFO:tensorflow:Total Training Time W/O Overhead = 37336.08 for Sentences = 5744640
I1218 00:54:37.781139 139970955851584 run_pretraining.py:648] Total Training Time W/O Overhead = 37336.08 for Sentences = 5744640
INFO:tensorflow:Throughput Average (sentences/sec) with overhead = 153.24
I1218 00:54:37.781190 139970955851584 run_pretraining.py:649] Throughput Average (sentences/sec) with overhead = 153.24
INFO:tensorflow:Throughput Average (sentences/sec) = 153.86
I1218 00:54:37.781249 139970955851584 run_pretraining.py:650] Throughput Average (sentences/sec) = 153.86
INFO:tensorflow:-----------------------------
I1218 00:54:37.781411 139970955851584 run_pretraining.py:652] -----------------------------

DLL 2021-12-18 00:03:18.980649 - Iteration: 84  throughput_train : 153.930 seq/s mlm_loss : 10.1148  nsp_loss : 0.6799  total_loss : 10.7947  avg_loss_step : 10.7855  learning_rate : 3.075e-05 
DLL 2021-12-18 00:10:38.159928 - Iteration: 85  throughput_train : 153.951 seq/s mlm_loss : 10.1413  nsp_loss : 0.6790  total_loss : 10.8203  avg_loss_step : 10.7756  learning_rate : 3.1125e-05 
DLL 2021-12-18 00:17:57.244773 - Iteration: 86  throughput_train : 153.983 seq/s mlm_loss : 10.0964  nsp_loss : 0.6995  total_loss : 10.7960  avg_loss_step : 10.7669  learning_rate : 3.15e-05 
DLL 2021-12-18 00:25:16.341178 - Iteration: 87  throughput_train : 153.980 seq/s mlm_loss : 10.0759  nsp_loss : 0.6690  total_loss : 10.7449  avg_loss_step : 10.7613  learning_rate : 3.1875003e-05 
DLL 2021-12-18 00:32:35.450438 - Iteration: 88  throughput_train : 153.975 seq/s mlm_loss : 10.0693  nsp_loss : 0.6961  total_loss : 10.7654  avg_loss_step : 10.7504  learning_rate : 3.2250002e-05 
DLL 2021-12-18 00:39:54.469957 - Iteration: 89  throughput_train : 154.006 seq/s mlm_loss : 9.9919  nsp_loss : 0.6950  total_loss : 10.6869  avg_loss_step : 10.7417  learning_rate : 3.2625e-05 
DLL 2021-12-18 00:47:13.595388 - Iteration: 90  throughput_train : 153.969 seq/s mlm_loss : 10.0545  nsp_loss : 0.6618  total_loss : 10.7163  avg_loss_step : 10.7339  learning_rate : 3.3e-05 
DLL 2021-12-18 00:54:33.235679 - Iteration: 91  throughput_train : 153.993 seq/s mlm_loss : 10.0131  nsp_loss : 0.6895  total_loss : 10.7026  avg_loss_step : 10.7242  learning_rate : 3.3375e-05 
DLL 2021-12-18 00:54:37.781300 -  throughput_train : 153.863 seq/s
