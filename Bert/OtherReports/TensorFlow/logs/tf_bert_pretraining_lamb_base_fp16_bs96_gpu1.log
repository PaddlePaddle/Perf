+ batch_size=96
+ num_gpus=1
+ precision=fp16
++ expr 67584 / 96 / 1
+ num_accumulation_steps_phase1=704
+ train_steps=100
+ bert_model=base
+ bash scripts/run_pretraining_lamb.sh 96 64 8 7.5e-4 5e-4 fp16 true 1 2000 200 100 200 704 512 base
Container nvidia build =  13409399
Saving checkpoints to /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126
Logs written to /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768.211218064126.log
Container nvidia build =  13409399
XLA activated
2021-12-18 06:41:26.405716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:152: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod-0.19.1-py3.6-linux-x86_64.egg/horovod/tensorflow/__init__.py:178: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1218 06:41:27.800987 139680925849408 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /workspace/bert/run_pretraining.py:589: The name tf.enable_resource_variables is deprecated. Please use tf.compat.v1.enable_resource_variables instead.

W1218 06:41:28.398299 139680925849408 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:589: The name tf.enable_resource_variables is deprecated. Please use tf.compat.v1.enable_resource_variables instead.

INFO:tensorflow:Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0797c51550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I1218 06:41:28.399047 139680925849408 estimator.py:212] Using config: {'_model_dir': '/results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': graph_options {
  optimizer_options {
    global_jit_level: ON_1
  }
  rewrite_options {
    memory_optimization: NO_MEM_OPT
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10000, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0797c51550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f0797cb21e0>) includes params argument, but params are not passed to Estimator.
W1218 06:41:28.399700 139680925849408 model_fn.py:630] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f0797cb21e0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:***** Running training *****
I1218 06:41:28.400081 139680925849408 run_pretraining.py:625] ***** Running training *****
INFO:tensorflow:  Batch size = 96
I1218 06:41:28.400146 139680925849408 run_pretraining.py:626]   Batch size = 96
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

W1218 06:41:28.499162 139680925849408 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

INFO:tensorflow:Calling model_fn.
I1218 06:41:28.601995 139680925849408 estimator.py:1148] Calling model_fn.
INFO:tensorflow:*** Features ***
I1218 06:41:28.602172 139680925849408 run_pretraining.py:258] *** Features ***
INFO:tensorflow:  name = input_ids, shape = (96, 128)
I1218 06:41:28.602269 139680925849408 run_pretraining.py:260]   name = input_ids, shape = (96, 128)
INFO:tensorflow:  name = input_mask, shape = (96, 128)
I1218 06:41:28.602353 139680925849408 run_pretraining.py:260]   name = input_mask, shape = (96, 128)
INFO:tensorflow:  name = masked_lm_ids, shape = (96, 20)
I1218 06:41:28.602448 139680925849408 run_pretraining.py:260]   name = masked_lm_ids, shape = (96, 20)
INFO:tensorflow:  name = masked_lm_positions, shape = (96, 20)
I1218 06:41:28.602514 139680925849408 run_pretraining.py:260]   name = masked_lm_positions, shape = (96, 20)
INFO:tensorflow:  name = masked_lm_weights, shape = (96, 20)
I1218 06:41:28.602588 139680925849408 run_pretraining.py:260]   name = masked_lm_weights, shape = (96, 20)
INFO:tensorflow:  name = next_sentence_labels, shape = (96, 1)
I1218 06:41:28.602648 139680925849408 run_pretraining.py:260]   name = next_sentence_labels, shape = (96, 1)
INFO:tensorflow:  name = segment_ids, shape = (96, 128)
I1218 06:41:28.602710 139680925849408 run_pretraining.py:260]   name = segment_ids, shape = (96, 128)
WARNING:tensorflow:From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W1218 06:41:28.602918 139680925849408 module_wrapper.py:139] From /workspace/bert/modeling.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W1218 06:41:28.603975 139680925849408 module_wrapper.py:139] From /workspace/bert/modeling.py:427: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W1218 06:41:30.183901 139680925849408 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:296: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /workspace/bert/optimization.py:130: The name tf.is_finite is deprecated. Please use tf.math.is_finite instead.

W1218 06:41:33.117674 139680925849408 module_wrapper.py:139] From /workspace/bert/optimization.py:130: The name tf.is_finite is deprecated. Please use tf.math.is_finite instead.

WARNING:tensorflow:From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

W1218 06:41:33.338227 139680925849408 module_wrapper.py:139] From /workspace/bert/optimization.py:142: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.

INFO:tensorflow:Done calling model_fn.
I1218 06:41:41.787771 139680925849408 estimator.py:1150] Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
I1218 06:41:41.788963 139680925849408 basic_session_run_hooks.py:541] Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
I1218 06:41:45.682942 139680925849408 monitored_session.py:240] Graph was finalized.
2021-12-18 06:41:45.697630: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599995000 Hz
2021-12-18 06:41:45.703771: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x139fa580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-12-18 06:41:45.703810: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-12-18 06:41:45.708303: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-12-18 06:41:46.918547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:46.956163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:46.979694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:46.995135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.020546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.023755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.041854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.062689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.065271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13f5e060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-12-18 06:41:47.065294: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065300: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065312: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065318: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065329: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.065334: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0
2021-12-18 06:41:47.076061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.078033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:03:00.0
2021-12-18 06:41:47.078122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.080051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 1 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:04:00.0
2021-12-18 06:41:47.080120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.082062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 2 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:05:00.0
2021-12-18 06:41:47.082121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.084060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 3 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:06:00.0
2021-12-18 06:41:47.084122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.086083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 4 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:07:00.0
2021-12-18 06:41:47.086143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.088100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 5 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:08:00.0
2021-12-18 06:41:47.088154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.090107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 6 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:09:00.0
2021-12-18 06:41:47.090173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.092102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 7 with properties: 
name: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:0a:00.0
2021-12-18 06:41:47.092138: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-18 06:41:47.095178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-18 06:41:47.096500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-12-18 06:41:47.096816: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-12-18 06:41:47.099507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-12-18 06:41:47.100079: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2021-12-18 06:41:47.100271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-18 06:41:47.100362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.102372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.104347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.106355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.108349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.110323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.112289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.114269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.116252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.118235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.120191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.122151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.124108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.126074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.128053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.130016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:47.131940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7
2021-12-18 06:41:47.131972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2021-12-18 06:41:49.675289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-12-18 06:41:49.675347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 1 2 3 4 5 6 7 
2021-12-18 06:41:49.675363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N Y Y Y N N N Y 
2021-12-18 06:41:49.675368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 1:   Y N Y Y N N Y N 
2021-12-18 06:41:49.675373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 2:   Y Y N Y N Y N N 
2021-12-18 06:41:49.675378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 3:   Y Y Y N Y N N N 
2021-12-18 06:41:49.675383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 4:   N N N Y N Y Y Y 
2021-12-18 06:41:49.675388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 5:   N N Y N Y N Y Y 
2021-12-18 06:41:49.675394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 6:   N Y N N Y Y N Y 
2021-12-18 06:41:49.675406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 7:   Y N N N Y Y Y N 
2021-12-18 06:41:49.675975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.678077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.680114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.682134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.684153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.686163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.688170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.690178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.692223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.694253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30166 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0)
2021-12-18 06:41:49.694741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.696769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30166 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-12-18 06:41:49.697091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.699088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 30166 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:05:00.0, compute capability: 7.0)
2021-12-18 06:41:49.699391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.701396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 30166 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0)
2021-12-18 06:41:49.701680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.703677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 30166 MB memory) -> physical GPU (device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0)
2021-12-18 06:41:49.703954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.705944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 30166 MB memory) -> physical GPU (device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:08:00.0, compute capability: 7.0)
2021-12-18 06:41:49.706252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.708241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 30166 MB memory) -> physical GPU (device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0)
2021-12-18 06:41:49.708609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-18 06:41:49.710606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 30166 MB memory) -> physical GPU (device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:0a:00.0, compute capability: 7.0)
2021-12-18 06:41:52.781748: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:41:52.793330: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:41:55.064970: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1648] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2021-12-18 06:41:58.986855: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:41:58.993462: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Running local_init_op.
I1218 06:41:59.686728 139680925849408 session_manager.py:500] Running local_init_op.
2021-12-18 06:42:00.175535: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:00.175767: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Done running local_init_op.
I1218 06:42:00.308417 139680925849408 session_manager.py:502] Done running local_init_op.
2021-12-18 06:42:00.919442: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:00.925759: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:02.022124: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:02.022416: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
INFO:tensorflow:Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1/model.ckpt.
I1218 06:42:10.389208 139680925849408 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1/model.ckpt.
2021-12-18 06:42:11.352602: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:11.361489: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:17.831733: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:17.832113: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:17.836228: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:17.838180: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:17.841237: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
WARNING:tensorflow:From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

W1218 06:42:18.083772 139680925849408 module_wrapper.py:139] From /workspace/bert/run_pretraining.py:147: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.

2021-12-18 06:42:18.729857: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:18.730118: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1343] No whitelist ops found, nothing to do
2021-12-18 06:42:35.180896: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:42:35.319609: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


2021-12-18 06:42:45.356890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2021-12-18 06:42:46.002865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2021-12-18 06:43:19.383469: I tensorflow/compiler/jit/xla_compilation_cache.cc:243] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:loss = 11.141882, step = 0
I1218 06:43:21.353903 139680925849408 basic_session_run_hooks.py:262] loss = 11.141882, step = 0
2021-12-18 06:43:37.691579: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 06:43:37.835135: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 06:44:18.331382 139680925849408 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 06:44:18.520173 139680925849408 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 06:44:18.704081 139680925849408 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 06:44:18.883600 139680925849408 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
W1218 06:44:19.060940 139680925849408 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
2021-12-18 07:14:47.470496: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1987] Running auto_mixed_precision graph optimizer
2021-12-18 07:14:47.608834: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1035] Automatic Mixed Precision Grappler Pass Summary:

Total processable nodes: 24313
Recognized nodes available for conversion: 15663
Total nodes converted: 599
Total FP16 Cast ops used (excluding Const and Variable casts): 102
Whitelisted nodes converted: 300
Blacklisted nodes blocking conversion: 1401
Nodes blocked from conversion by blacklisted nodes: 4428

For more information regarding mixed precision training, including how to make automatic mixed precision aware of a custom op type, please see the documentation available here:
https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tfamp


INFO:tensorflow:loss = 11.12152, step = 9 (1930.727 sec)
I1218 07:15:32.080760 139680925849408 basic_session_run_hooks.py:260] loss = 11.12152, step = 9 (1930.727 sec)
INFO:tensorflow:loss = 11.077648, step = 23 (1777.945 sec)
I1218 07:45:10.026045 139680925849408 basic_session_run_hooks.py:260] loss = 11.077648, step = 23 (1777.945 sec)
decayed_learning_rate_at_crossover_point = 7.500000e-04, adjusted_init_lr = 7.500000e-04
Initializing LAMB Optimizer
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:46:28.125331 - Iteration: 1  throughput_train : 271.322 seq/s mlm_loss : 10.4412  nsp_loss : 0.6961  total_loss : 11.1373  avg_loss_step : 11.1228  learning_rate : 0.0  loss_scaler : 4294967296 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:48:32.884338 - Iteration: 1  throughput_train : 542.092 seq/s mlm_loss : 10.4509  nsp_loss : 0.6990  total_loss : 11.1499  avg_loss_step : 11.1234  learning_rate : 0.0  loss_scaler : 2147483648 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:50:36.985610 - Iteration: 1  throughput_train : 544.951 seq/s mlm_loss : 10.4343  nsp_loss : 0.7006  total_loss : 11.1348  avg_loss_step : 11.1231  learning_rate : 0.0  loss_scaler : 1073741824 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:52:41.396985 - Iteration: 1  throughput_train : 543.592 seq/s mlm_loss : 10.4323  nsp_loss : 0.7006  total_loss : 11.1329  avg_loss_step : 11.1227  learning_rate : 0.0  loss_scaler : 536870912 
Skipping time record for  0  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:54:51.109663 - Iteration: 1  throughput_train : 521.365 seq/s mlm_loss : 10.4357  nsp_loss : 0.6836  total_loss : 11.1192  avg_loss_step : 11.1222  learning_rate : 0.0  loss_scaler : 268435456 
Skipping time record for  1  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:57:21.360245 - Iteration: 2  throughput_train : 450.063 seq/s mlm_loss : 10.4332  nsp_loss : 0.6648  total_loss : 11.0979  avg_loss_step : 11.1223  learning_rate : 0.0  loss_scaler : 134217728 
Skipping time record for  2  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 06:59:26.854865 - Iteration: 3  throughput_train : 538.899 seq/s mlm_loss : 10.4343  nsp_loss : 0.7171  total_loss : 11.1514  avg_loss_step : 11.1228  learning_rate : 3.75e-07  loss_scaler : 134217728 
Skipping time record for  3  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 07:01:32.519804 - Iteration: 4  throughput_train : 538.167 seq/s mlm_loss : 10.4345  nsp_loss : 0.6882  total_loss : 11.1228  avg_loss_step : 11.1202  learning_rate : 7.5e-07  loss_scaler : 134217728 
Skipping time record for  4  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 07:03:37.752124 - Iteration: 5  throughput_train : 540.031 seq/s mlm_loss : 10.4436  nsp_loss : 0.6773  total_loss : 11.1209  avg_loss_step : 11.1209  learning_rate : 1.125e-06  loss_scaler : 134217728 
Skipping time record for  5  due to checkpoint-saving/warmup overhead
DLL 2021-12-18 07:05:43.424868 - Iteration: 6  throughput_train : 538.138 seq/s mlm_loss : 10.4329  nsp_loss : 0.6832  total_loss : 11.1161  avg_loss_step : 11.1209  learning_rate : 1.5e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:07:48.769263 - Iteration: 7  throughput_train : 539.546 seq/s mlm_loss : 10.4422  nsp_loss : 0.6822  total_loss : 11.1243  avg_loss_step : 11.1209  learning_rate : 1.8750001e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:09:54.515025 - Iteration: 8  throughput_train : 537.816 seq/s mlm_loss : 10.4431  nsp_loss : 0.6929  total_loss : 11.1360  avg_loss_step : 11.1200  learning_rate : 2.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:12:00.780864 - Iteration: 9  throughput_train : 535.603 seq/s mlm_loss : 10.4399  nsp_loss : 0.7059  total_loss : 11.1458  avg_loss_step : 11.1191  learning_rate : 2.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:14:05.996168 - Iteration: 10  throughput_train : 540.095 seq/s mlm_loss : 10.4168  nsp_loss : 0.6930  total_loss : 11.1098  avg_loss_step : 11.1192  learning_rate : 3e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:17:11.397474 - Iteration: 11  throughput_train : 364.692 seq/s mlm_loss : 10.4403  nsp_loss : 0.7276  total_loss : 11.1679  avg_loss_step : 11.1176  learning_rate : 3.3750002e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:19:16.444321 - Iteration: 12  throughput_train : 540.820 seq/s mlm_loss : 10.4083  nsp_loss : 0.6700  total_loss : 11.0783  avg_loss_step : 11.1168  learning_rate : 3.7500001e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:21:21.454613 - Iteration: 13  throughput_train : 540.985 seq/s mlm_loss : 10.4287  nsp_loss : 0.6660  total_loss : 11.0946  avg_loss_step : 11.1160  learning_rate : 4.125e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:23:26.571074 - Iteration: 14  throughput_train : 540.521 seq/s mlm_loss : 10.4505  nsp_loss : 0.6763  total_loss : 11.1268  avg_loss_step : 11.1144  learning_rate : 4.5e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:25:31.766257 - Iteration: 15  throughput_train : 540.185 seq/s mlm_loss : 10.3940  nsp_loss : 0.7020  total_loss : 11.0960  avg_loss_step : 11.1131  learning_rate : 4.8750003e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:27:37.170368 - Iteration: 16  throughput_train : 539.285 seq/s mlm_loss : 10.4384  nsp_loss : 0.7045  total_loss : 11.1429  avg_loss_step : 11.1126  learning_rate : 5.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:29:42.310087 - Iteration: 17  throughput_train : 540.423 seq/s mlm_loss : 10.4404  nsp_loss : 0.6953  total_loss : 11.1358  avg_loss_step : 11.1119  learning_rate : 5.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:31:47.478304 - Iteration: 18  throughput_train : 540.298 seq/s mlm_loss : 10.4213  nsp_loss : 0.7023  total_loss : 11.1237  avg_loss_step : 11.1105  learning_rate : 6e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:33:52.888320 - Iteration: 19  throughput_train : 539.266 seq/s mlm_loss : 10.4055  nsp_loss : 0.6480  total_loss : 11.0535  avg_loss_step : 11.1087  learning_rate : 6.3750003e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:35:58.149499 - Iteration: 20  throughput_train : 539.901 seq/s mlm_loss : 10.4136  nsp_loss : 0.6904  total_loss : 11.1039  avg_loss_step : 11.1047  learning_rate : 6.7500005e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:38:03.420786 - Iteration: 21  throughput_train : 539.860 seq/s mlm_loss : 10.4146  nsp_loss : 0.6486  total_loss : 11.0632  avg_loss_step : 11.1050  learning_rate : 7.125e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:40:08.360680 - Iteration: 22  throughput_train : 541.282 seq/s mlm_loss : 10.4213  nsp_loss : 0.6988  total_loss : 11.1201  avg_loss_step : 11.1025  learning_rate : 7.5000003e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:42:13.440160 - Iteration: 23  throughput_train : 540.687 seq/s mlm_loss : 10.4346  nsp_loss : 0.6887  total_loss : 11.1233  avg_loss_step : 11.1003  learning_rate : 7.875e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:44:18.684869 - Iteration: 24  throughput_train : 539.974 seq/s mlm_loss : 10.4318  nsp_loss : 0.7157  total_loss : 11.1474  avg_loss_step : 11.0995  learning_rate : 8.25e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:46:24.441903 - Iteration: 25  throughput_train : 537.769 seq/s mlm_loss : 10.4070  nsp_loss : 0.7056  total_loss : 11.1126  avg_loss_step : 11.0949  learning_rate : 8.625e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:48:30.324807 - Iteration: 26  throughput_train : 537.232 seq/s mlm_loss : 10.4156  nsp_loss : 0.7122  total_loss : 11.1279  avg_loss_step : 11.0931  learning_rate : 9e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:50:35.608819 - Iteration: 27  throughput_train : 539.801 seq/s mlm_loss : 10.4064  nsp_loss : 0.6928  total_loss : 11.0992  avg_loss_step : 11.0898  learning_rate : 9.375e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:52:40.952501 - Iteration: 28  throughput_train : 539.536 seq/s mlm_loss : 10.3784  nsp_loss : 0.6585  total_loss : 11.0369  avg_loss_step : 11.0883  learning_rate : 9.750001e-06  loss_scaler : 134217728 
DLL 2021-12-18 07:54:46.623954 - Iteration: 29  throughput_train : 538.135 seq/s mlm_loss : 10.3998  nsp_loss : 0.6666  total_loss : 11.0664  avg_loss_step : 11.0854  learning_rate : 1.0125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 07:56:51.981673 - Iteration: 30  throughput_train : 539.484 seq/s mlm_loss : 10.4038  nsp_loss : 0.6690  total_loss : 11.0728  avg_loss_step : 11.0823  learning_rate : 1.05e-05  loss_scaler : 134217728 INFO:tensorflow:loss = 11.047099, step = 37 (1781.811 sec)
I1218 08:14:51.837207 139680925849408 basic_session_run_hooks.py:260] loss = 11.047099, step = 37 (1781.811 sec)
INFO:tensorflow:loss = 11.029539, step = 51 (1780.740 sec)
I1218 08:44:32.576995 139680925849408 basic_session_run_hooks.py:260] loss = 11.029539, step = 51 (1780.740 sec)
INFO:tensorflow:loss = 10.86974, step = 66 (1781.223 sec)
I1218 09:14:13.800010 139680925849408 basic_session_run_hooks.py:260] loss = 10.86974, step = 66 (1781.223 sec)

DLL 2021-12-18 07:58:57.289827 - Iteration: 31  throughput_train : 539.695 seq/s mlm_loss : 10.3979  nsp_loss : 0.7075  total_loss : 11.1053  avg_loss_step : 11.0800  learning_rate : 1.0875e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:01:02.638702 - Iteration: 32  throughput_train : 539.526 seq/s mlm_loss : 10.3812  nsp_loss : 0.6860  total_loss : 11.0672  avg_loss_step : 11.0773  learning_rate : 1.125e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:03:08.182600 - Iteration: 33  throughput_train : 538.695 seq/s mlm_loss : 10.4140  nsp_loss : 0.6688  total_loss : 11.0827  avg_loss_step : 11.0725  learning_rate : 1.1625e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:05:13.568199 - Iteration: 34  throughput_train : 539.368 seq/s mlm_loss : 10.3917  nsp_loss : 0.6827  total_loss : 11.0744  avg_loss_step : 11.0712  learning_rate : 1.2e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:07:18.786278 - Iteration: 35  throughput_train : 540.086 seq/s mlm_loss : 10.3567  nsp_loss : 0.6926  total_loss : 11.0493  avg_loss_step : 11.0682  learning_rate : 1.2375001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:09:23.961731 - Iteration: 36  throughput_train : 540.265 seq/s mlm_loss : 10.3926  nsp_loss : 0.6594  total_loss : 11.0520  avg_loss_step : 11.0641  learning_rate : 1.2750001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:11:29.524326 - Iteration: 37  throughput_train : 538.603 seq/s mlm_loss : 10.3933  nsp_loss : 0.6771  total_loss : 11.0704  avg_loss_step : 11.0606  learning_rate : 1.3125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:13:34.844933 - Iteration: 38  throughput_train : 539.644 seq/s mlm_loss : 10.3819  nsp_loss : 0.6800  total_loss : 11.0619  avg_loss_step : 11.0558  learning_rate : 1.3500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:15:40.014118 - Iteration: 39  throughput_train : 540.300 seq/s mlm_loss : 10.3594  nsp_loss : 0.6893  total_loss : 11.0487  avg_loss_step : 11.0537  learning_rate : 1.3875e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:17:45.450898 - Iteration: 40  throughput_train : 539.142 seq/s mlm_loss : 10.3785  nsp_loss : 0.7027  total_loss : 11.0813  avg_loss_step : 11.0491  learning_rate : 1.425e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:19:50.909084 - Iteration: 41  throughput_train : 539.061 seq/s mlm_loss : 10.3746  nsp_loss : 0.6871  total_loss : 11.0616  avg_loss_step : 11.0450  learning_rate : 1.4625e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:21:56.023222 - Iteration: 42  throughput_train : 540.533 seq/s mlm_loss : 10.3635  nsp_loss : 0.6759  total_loss : 11.0394  avg_loss_step : 11.0414  learning_rate : 1.50000005e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:24:01.255550 - Iteration: 43  throughput_train : 540.026 seq/s mlm_loss : 10.3581  nsp_loss : 0.6999  total_loss : 11.0580  avg_loss_step : 11.0373  learning_rate : 1.5375e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:26:06.448112 - Iteration: 44  throughput_train : 540.193 seq/s mlm_loss : 10.3423  nsp_loss : 0.6917  total_loss : 11.0340  avg_loss_step : 11.0348  learning_rate : 1.575e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:28:11.822393 - Iteration: 45  throughput_train : 539.415 seq/s mlm_loss : 10.3735  nsp_loss : 0.6617  total_loss : 11.0353  avg_loss_step : 11.0293  learning_rate : 1.6125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:30:17.382736 - Iteration: 46  throughput_train : 538.611 seq/s mlm_loss : 10.3427  nsp_loss : 0.7054  total_loss : 11.0480  avg_loss_step : 11.0252  learning_rate : 1.65e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:32:22.962737 - Iteration: 47  throughput_train : 538.530 seq/s mlm_loss : 10.3026  nsp_loss : 0.6575  total_loss : 10.9601  avg_loss_step : 11.0186  learning_rate : 1.6875001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:34:28.573977 - Iteration: 48  throughput_train : 538.402 seq/s mlm_loss : 10.3517  nsp_loss : 0.6585  total_loss : 11.0102  avg_loss_step : 11.0147  learning_rate : 1.725e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:36:34.027984 - Iteration: 49  throughput_train : 539.073 seq/s mlm_loss : 10.3230  nsp_loss : 0.6921  total_loss : 11.0151  avg_loss_step : 11.0114  learning_rate : 1.7625001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:38:39.345104 - Iteration: 50  throughput_train : 539.662 seq/s mlm_loss : 10.3304  nsp_loss : 0.6693  total_loss : 10.9997  avg_loss_step : 11.0064  learning_rate : 1.8e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:40:44.719864 - Iteration: 51  throughput_train : 539.411 seq/s mlm_loss : 10.3321  nsp_loss : 0.6980  total_loss : 11.0302  avg_loss_step : 11.0018  learning_rate : 1.8375e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:42:49.984211 - Iteration: 52  throughput_train : 539.887 seq/s mlm_loss : 10.3005  nsp_loss : 0.6573  total_loss : 10.9578  avg_loss_step : 10.9958  learning_rate : 1.875e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:44:55.147990 - Iteration: 53  throughput_train : 540.325 seq/s mlm_loss : 10.3082  nsp_loss : 0.6834  total_loss : 10.9916  avg_loss_step : 10.9910  learning_rate : 1.9125e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:47:00.377723 - Iteration: 54  throughput_train : 540.029 seq/s mlm_loss : 10.2958  nsp_loss : 0.6780  total_loss : 10.9738  avg_loss_step : 10.9850  learning_rate : 1.9500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:49:05.506372 - Iteration: 55  throughput_train : 540.464 seq/s mlm_loss : 10.2600  nsp_loss : 0.6922  total_loss : 10.9523  avg_loss_step : 10.9814  learning_rate : 1.9875e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:51:11.516053 - Iteration: 56  throughput_train : 536.702 seq/s mlm_loss : 10.2894  nsp_loss : 0.6974  total_loss : 10.9867  avg_loss_step : 10.9756  learning_rate : 2.0250001e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:53:16.819831 - Iteration: 57  throughput_train : 539.718 seq/s mlm_loss : 10.2966  nsp_loss : 0.7184  total_loss : 11.0150  avg_loss_step : 10.9714  learning_rate : 2.0625e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:55:22.150228 - Iteration: 58  throughput_train : 539.601 seq/s mlm_loss : 10.2684  nsp_loss : 0.7145  total_loss : 10.9829  avg_loss_step : 10.9647  learning_rate : 2.1e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:57:27.798644 - Iteration: 59  throughput_train : 538.243 seq/s mlm_loss : 10.2679  nsp_loss : 0.6991  total_loss : 10.9670  avg_loss_step : 10.9597  learning_rate : 2.1375e-05  loss_scaler : 134217728 
DLL 2021-12-18 08:59:33.216828 - Iteration: 60  throughput_train : 539.228 seq/s mlm_loss : 10.2835  nsp_loss : 0.6789  total_loss : 10.9624  avg_loss_step : 10.9534  learning_rate : 2.175e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:01:38.404750 - Iteration: 61  throughput_train : 540.215 seq/s mlm_loss : 10.2788  nsp_loss : 0.6626  total_loss : 10.9414  avg_loss_step : 10.9488  learning_rate : 2.2125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:03:43.899933 - Iteration: 62  throughput_train : 538.894 seq/s mlm_loss : 10.2731  nsp_loss : 0.6906  total_loss : 10.9636  avg_loss_step : 10.9425  learning_rate : 2.25e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:05:49.132695 - Iteration: 63  throughput_train : 540.025 seq/s mlm_loss : 10.2375  nsp_loss : 0.6882  total_loss : 10.9257  avg_loss_step : 10.9361  learning_rate : 2.2875001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:07:54.675653 - Iteration: 64  throughput_train : 538.690 seq/s mlm_loss : 10.2499  nsp_loss : 0.6765  total_loss : 10.9265  avg_loss_step : 10.9305  learning_rate : 2.325e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:09:59.988542 - Iteration: 65  throughput_train : 539.672 seq/s mlm_loss : 10.2619  nsp_loss : 0.6972  total_loss : 10.9592  avg_loss_step : 10.9247  learning_rate : 2.3625002e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:12:05.263891 - Iteration: 66  throughput_train : 539.837 seq/s mlm_loss : 10.2270  nsp_loss : 0.6892  total_loss : 10.9162  avg_loss_step : 10.9172  learning_rate : 2.4e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:14:10.767377 - Iteration: 67  throughput_train : 538.854 seq/s mlm_loss : 10.2325  nsp_loss : 0.6909  total_loss : 10.9234  avg_loss_step : 10.9098  learning_rate : 2.4375e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:16:16.049005 - Iteration: 68  throughput_train : 539.812 seq/s mlm_loss : 10.2503  nsp_loss : 0.6684  total_loss : 10.9186  avg_loss_step : 10.9051  learning_rate : 2.4750001e-05  loss_scaler : 134217728 INFO:tensorflow:loss = 10.763696, step = 80 (1781.498 sec)
I1218 09:43:55.297736 139680925849408 basic_session_run_hooks.py:260] loss = 10.763696, step = 80 (1781.498 sec)
INFO:tensorflow:Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1/model.ckpt.
I1218 10:04:20.549264 139680925849408 basic_session_run_hooks.py:606] Saving checkpoints for 90 into /results/tf_bert_pretraining_lamb_base_fp16_gbs167584_gbs232768_211218064126/phase_1/model.ckpt.
INFO:tensorflow:Loss for final step: 10.771559.
I1218 10:04:25.382799 139680925849408 estimator.py:371] Loss for final step: 10.771559.
INFO:tensorflow:-----------------------------
I1218 10:04:25.384385 139680925849408 run_pretraining.py:644] -----------------------------
INFO:tensorflow:Total Training Time = 12176.98 for Sentences = 6082560
I1218 10:04:25.384474 139680925849408 run_pretraining.py:646] Total Training Time = 12176.98 for Sentences = 6082560
INFO:tensorflow:Total Training Time W/O Overhead = 10709.85 for Sentences = 5406720
I1218 10:04:25.384559 139680925849408 run_pretraining.py:648] Total Training Time W/O Overhead = 10709.85 for Sentences = 5406720
INFO:tensorflow:Throughput Average (sentences/sec) with overhead = 499.51
I1218 10:04:25.384612 139680925849408 run_pretraining.py:649] Throughput Average (sentences/sec) with overhead = 499.51
INFO:tensorflow:Throughput Average (sentences/sec) = 504.84
I1218 10:04:25.384692 139680925849408 run_pretraining.py:650] Throughput Average (sentences/sec) = 504.84
INFO:tensorflow:-----------------------------
I1218 10:04:25.384859 139680925849408 run_pretraining.py:652] -----------------------------

DLL 2021-12-18 09:18:21.666514 - Iteration: 69  throughput_train : 538.366 seq/s mlm_loss : 10.2162  nsp_loss : 0.6776  total_loss : 10.8938  avg_loss_step : 10.8988  learning_rate : 2.5125e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:20:27.012668 - Iteration: 70  throughput_train : 539.529 seq/s mlm_loss : 10.2143  nsp_loss : 0.6882  total_loss : 10.9024  avg_loss_step : 10.8937  learning_rate : 2.5500001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:22:32.638093 - Iteration: 71  throughput_train : 538.340 seq/s mlm_loss : 10.1777  nsp_loss : 0.7192  total_loss : 10.8969  avg_loss_step : 10.8855  learning_rate : 2.5875e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:24:38.309110 - Iteration: 72  throughput_train : 538.135 seq/s mlm_loss : 10.1783  nsp_loss : 0.6841  total_loss : 10.8624  avg_loss_step : 10.8791  learning_rate : 2.6250002e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:26:43.677344 - Iteration: 73  throughput_train : 539.438 seq/s mlm_loss : 10.1995  nsp_loss : 0.6986  total_loss : 10.8981  avg_loss_step : 10.8710  learning_rate : 2.6625e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:28:49.339602 - Iteration: 74  throughput_train : 538.182 seq/s mlm_loss : 10.1832  nsp_loss : 0.7138  total_loss : 10.8970  avg_loss_step : 10.8660  learning_rate : 2.7000002e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:30:54.750634 - Iteration: 75  throughput_train : 539.260 seq/s mlm_loss : 10.1706  nsp_loss : 0.6742  total_loss : 10.8448  avg_loss_step : 10.8575  learning_rate : 2.7375001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:33:00.013844 - Iteration: 76  throughput_train : 539.889 seq/s mlm_loss : 10.1552  nsp_loss : 0.6823  total_loss : 10.8375  avg_loss_step : 10.8513  learning_rate : 2.775e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:35:05.260455 - Iteration: 77  throughput_train : 539.957 seq/s mlm_loss : 10.1679  nsp_loss : 0.6336  total_loss : 10.8015  avg_loss_step : 10.8440  learning_rate : 2.8125001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:37:10.747939 - Iteration: 78  throughput_train : 538.928 seq/s mlm_loss : 10.1570  nsp_loss : 0.7112  total_loss : 10.8682  avg_loss_step : 10.8370  learning_rate : 2.85e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:39:16.195932 - Iteration: 79  throughput_train : 539.101 seq/s mlm_loss : 10.1484  nsp_loss : 0.6855  total_loss : 10.8339  avg_loss_step : 10.8287  learning_rate : 2.8875002e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:41:21.521458 - Iteration: 80  throughput_train : 539.618 seq/s mlm_loss : 10.1404  nsp_loss : 0.6773  total_loss : 10.8177  avg_loss_step : 10.8218  learning_rate : 2.925e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:43:26.645981 - Iteration: 81  throughput_train : 540.483 seq/s mlm_loss : 10.1641  nsp_loss : 0.6739  total_loss : 10.8380  avg_loss_step : 10.8152  learning_rate : 2.9625002e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:45:31.875948 - Iteration: 82  throughput_train : 540.040 seq/s mlm_loss : 10.1492  nsp_loss : 0.6856  total_loss : 10.8349  avg_loss_step : 10.8073  learning_rate : 3.0000001e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:47:37.126221 - Iteration: 83  throughput_train : 539.948 seq/s mlm_loss : 10.0999  nsp_loss : 0.6937  total_loss : 10.7936  avg_loss_step : 10.7995  learning_rate : 3.0375e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:49:42.360818 - Iteration: 84  throughput_train : 540.003 seq/s mlm_loss : 10.1095  nsp_loss : 0.6526  total_loss : 10.7621  avg_loss_step : 10.7927  learning_rate : 3.075e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:51:47.591436 - Iteration: 85  throughput_train : 540.035 seq/s mlm_loss : 10.0968  nsp_loss : 0.6733  total_loss : 10.7701  avg_loss_step : 10.7847  learning_rate : 3.1125e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:53:53.533642 - Iteration: 86  throughput_train : 536.978 seq/s mlm_loss : 10.1218  nsp_loss : 0.7146  total_loss : 10.8365  avg_loss_step : 10.7765  learning_rate : 3.15e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:55:59.356231 - Iteration: 87  throughput_train : 537.489 seq/s mlm_loss : 10.1129  nsp_loss : 0.6993  total_loss : 10.8122  avg_loss_step : 10.7679  learning_rate : 3.1875003e-05  loss_scaler : 134217728 
DLL 2021-12-18 09:58:04.606800 - Iteration: 88  throughput_train : 539.941 seq/s mlm_loss : 10.0959  nsp_loss : 0.6864  total_loss : 10.7823  avg_loss_step : 10.7612  learning_rate : 3.2250002e-05  loss_scaler : 134217728 
DLL 2021-12-18 10:00:09.895716 - Iteration: 89  throughput_train : 539.786 seq/s mlm_loss : 10.0551  nsp_loss : 0.7080  total_loss : 10.7631  avg_loss_step : 10.7520  learning_rate : 3.2625e-05  loss_scaler : 134217728 
DLL 2021-12-18 10:02:14.988391 - Iteration: 90  throughput_train : 540.634 seq/s mlm_loss : 10.0399  nsp_loss : 0.6699  total_loss : 10.7098  avg_loss_step : 10.7440  learning_rate : 3.3e-05  loss_scaler : 134217728 
DLL 2021-12-18 10:04:20.548028 - Iteration: 91  throughput_train : 539.733 seq/s mlm_loss : 10.0516  nsp_loss : 0.7200  total_loss : 10.7716  avg_loss_step : 10.7360  learning_rate : 3.3375e-05  loss_scaler : 134217728 
DLL 2021-12-18 10:04:25.384745 -  throughput_train : 504.836 seq/s
